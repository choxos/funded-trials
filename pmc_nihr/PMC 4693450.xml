<?xml version="1.0" encoding="UTF-8"?>
<OAI-PMH xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/ http://www.openarchives.org/OAI/2.0/OAI-PMH.xsd">
  <responseDate>2024-05-03T11:11:49Z</responseDate>
  <request verb="GetRecord" identifier="oai:pubmedcentral.nih.gov:4693450" metadataPrefix="pmc">https:/www.ncbi.nlm.nih.gov/pmc/oai/oai.cgi</request>
  <GetRecord>
    <record>
      <header>
        <identifier>oai:pubmedcentral.nih.gov:4693450</identifier>
        <datestamp>2016-01-31</datestamp>
        <setSpec>elsevierwt</setSpec>
        <setSpec>pmc-open</setSpec>
      </header>
      <metadata>
        <article xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" xsi:schemaLocation="https://jats.nlm.nih.gov/ns/archiving/1.3/ https://jats.nlm.nih.gov/archiving/1.3/xsd/JATS-archivearticle1-3.xsd" article-type="research-article">
          <front>
            <journal-meta>
              <journal-id journal-id-type="nlm-ta">Psychiatry Res</journal-id>
              <journal-id journal-id-type="iso-abbrev">Psychiatry Res</journal-id>
              <journal-title-group>
                <journal-title>Psychiatry Research</journal-title>
              </journal-title-group>
              <issn pub-type="ppub">0165-1781</issn>
              <issn pub-type="epub">1872-7123</issn>
              <publisher>
                <publisher-name>Elsevier/North-Holland Biomedical Press</publisher-name>
              </publisher>
            </journal-meta>
            <article-meta>
              <article-id pub-id-type="accession">PMC4693450</article-id>
              <article-id pub-id-type="pmcid">PMC4693450</article-id>
              <article-id pub-id-type="pmc-uid">4693450</article-id>
              <article-id pub-id-type="pmid">26619915</article-id>
              <article-id pub-id-type="publisher-id">S0165-1781(15)30014-7</article-id>
              <article-id pub-id-type="doi">10.1016/j.psychres.2015.11.007</article-id>
              <article-categories>
                <subj-group subj-group-type="heading">
                  <subject>Article</subject>
                </subj-group>
              </article-categories>
              <title-group>
                <article-title>Feedback training induces a bias for detecting happiness or fear in facial expressions that generalises to a novel task</article-title>
              </title-group>
              <contrib-group>
                <contrib contrib-type="author">
                  <name>
                    <surname>Griffiths</surname>
                    <given-names>Sarah</given-names>
                  </name>
                  <email>sarah.griffiths@bristol.ac.uk</email>
                  <xref rid="aff0005" ref-type="aff">a</xref>
                  <xref rid="aff0010" ref-type="aff">b</xref>
                  <xref rid="cor1" ref-type="corresp">⁎</xref>
                </contrib>
                <contrib contrib-type="author">
                  <name>
                    <surname>Jarrold</surname>
                    <given-names>Chris</given-names>
                  </name>
                  <xref rid="aff0005" ref-type="aff">a</xref>
                </contrib>
                <contrib contrib-type="author">
                  <name>
                    <surname>Penton-Voak</surname>
                    <given-names>Ian S.</given-names>
                  </name>
                  <xref rid="aff0005" ref-type="aff">a</xref>
                </contrib>
                <contrib contrib-type="author">
                  <name>
                    <surname>Munafò</surname>
                    <given-names>Marcus R.</given-names>
                  </name>
                  <xref rid="aff0005" ref-type="aff">a</xref>
                  <xref rid="aff0010" ref-type="aff">b</xref>
                  <xref rid="aff0015" ref-type="aff">c</xref>
                </contrib>
              </contrib-group>
              <aff id="aff0005"><label>a</label>School of Experimental Psychology, University of Bristol, Bristol, United Kingdom</aff>
              <aff id="aff0010"><label>b</label>Medical Research Council Integrative Epidemiology Unit (MRC IEU) at the University of Bristol, Bristol, United Kingdom</aff>
              <aff id="aff0015"><label>c</label>UK Centre for Tobacco and Alcohol Studies, University of Bristol, Bristol, United Kingdom</aff>
              <author-notes>
                <corresp id="cor1"><label>⁎</label>Corresponding author at: School of Experimental Psychology, University of Bristol, 12a Priory Road, Bristol BS8 1TU, United Kingdom.School of Experimental Psychology, University of Bristol12a Priory RoadBristolBS8 1TUUnited Kingdom <email>sarah.griffiths@bristol.ac.uk</email></corresp>
              </author-notes>
              <pub-date pub-type="pmc-release">
                <day>30</day>
                <month>12</month>
                <year>2015</year>
              </pub-date>
              <!-- PMC Release delay is 0 months and 0 days and was based on <pub-date
						pub-type="ppub">.-->
              <pub-date pub-type="ppub">
                <day>30</day>
                <month>12</month>
                <year>2015</year>
              </pub-date>
              <volume>230</volume>
              <issue>3</issue>
              <fpage>951</fpage>
              <lpage>957</lpage>
              <history>
                <date date-type="received">
                  <day>13</day>
                  <month>7</month>
                  <year>2015</year>
                </date>
                <date date-type="rev-recd">
                  <day>4</day>
                  <month>11</month>
                  <year>2015</year>
                </date>
                <date date-type="accepted">
                  <day>6</day>
                  <month>11</month>
                  <year>2015</year>
                </date>
              </history>
              <permissions>
                <copyright-statement>© 2015 The Authors</copyright-statement>
                <copyright-year>2015</copyright-year>
                <license license-type="CC BY" xlink:href="http://creativecommons.org/licenses/by/4.0/">
                  <license-p>This is an open access article under the CC BY license (http://creativecommons.org/licenses/by/4.0/).</license-p>
                </license>
              </permissions>
              <abstract>
                <p>Many psychological disorders are characterised by insensitivities or biases in the processing of subtle facial expressions of emotion. Training using expression morph sequences which vary the intensity of expressions may be able to address such deficits. In the current study participants were shown expressions from either happy or fearful intensity morph sequences, and trained to detect the target emotion (e.g., happy in the happy sequence) as being present in low intensity expressions. Training transfer was tested using a six alternative forced choice emotion labelling task with varying intensity expressions, which participants completed before and after training. Training increased false alarms for the target emotion in the transfer task. Hit rate for the target emotion did not increase once adjustment was made for the increase in false alarms. This suggests that training causes a bias for detecting the target emotion which generalises outside of the training task. However it does not increase accuracy for detecting the target emotion. The results are discussed in terms of the training’s utility in addressing different types of emotion processing deficits in psychological disorders.</p>
              </abstract>
              <abstract abstract-type="author-highlights">
                <title>Highlights</title>
                <p>
                  <list list-type="simple">
                    <list-item id="u0005">
                      <label>•</label>
                      <p>Participants were trained to detect happiness or fear in low intensity expressions.</p>
                    </list-item>
                    <list-item id="u0010">
                      <label>•</label>
                      <p>Training lead to a bias for recognizing happiness or fear on faces in a novel task.</p>
                    </list-item>
                    <list-item id="u0015">
                      <label>•</label>
                      <p>However, participants did not become more accurate at recognizing happiness or fear.</p>
                    </list-item>
                    <list-item id="u0020">
                      <label>•</label>
                      <p>This training may be effective in altering specific biases in emotion recognition.</p>
                    </list-item>
                  </list>
                </p>
              </abstract>
              <kwd-group>
                <title>Keywords</title>
                <kwd>Emotion</kwd>
                <kwd>Intervention</kwd>
                <kwd>Perception</kwd>
                <kwd>Sensitivity</kwd>
                <kwd>Bias</kwd>
              </kwd-group>
            </article-meta>
          </front>
          <body>
            <sec id="s0005">
              <label>1</label>
              <title>Introduction</title>
              <p>Many psychological disorders have been associated with differences in the interpretation of emotional facial expressions. These may be the result of differences in response criterion for certain emotions (biases) and/or the result of differences in perceptual sensitivity (<xref rid="bib24" ref-type="bibr">Yoon et al., 2014</xref>). Reduced perceptual sensitivity for certain emotional expressions has been found in autism spectrum disorder (<xref rid="bib8" ref-type="bibr">Law Smith et al., 2010</xref>, <xref rid="bib22" ref-type="bibr">Wallace et al., 2011</xref>), anxiety disorders (<xref rid="bib6" ref-type="bibr">Frenkel et al., 2009</xref>) and eating disorders (<xref rid="bib13" ref-type="bibr">Ridout et al., 2012</xref>). Whereas bias for perceiving sadness has been found in depression (<xref rid="bib3" ref-type="bibr">Bourke et al., 2010</xref>) and anxiety disorder (<xref rid="bib1" ref-type="bibr">Bell et al., 2011</xref>), while bias for perceiving anger has been found in conduct disorder (<xref rid="bib16" ref-type="bibr">Schönenberg and Jusyte, 2014</xref>) and social anxiety (<xref rid="bib24" ref-type="bibr">Yoon et al., 2014</xref>). Differences in bias and sensitivity to certain emotions are commonly measured using low intensity facial expressions, as insensitivity and bias may only be evident when cue intensity is reduced (<xref rid="bib6" ref-type="bibr">Frenkel et al., 2009</xref>, <xref rid="bib24" ref-type="bibr">Yoon et al., 2014</xref>).</p>
              <p>Although general emotion recognition training programmes have been developed for individuals with various psychological disorders (<xref rid="bib23" ref-type="bibr">Wölwer et al., 2005</xref>, <xref rid="bib14" ref-type="bibr">Ryan and Ni Charragain, 2010</xref>, <xref rid="bib5" ref-type="bibr">Dadds et al., 2012</xref>), there have been few attempts to deliver training which targets emotion specific deficits. Given findings of selective impairments in sensitivity to particular emotional expressions (<xref rid="bib6" ref-type="bibr">Frenkel et al., 2009</xref>, <xref rid="bib8" ref-type="bibr">Law Smith et al., 2010</xref>, <xref rid="bib13" ref-type="bibr">Ridout et al., 2012</xref>), targeted training to improve sensitivity to one specific expression may be an effective treatment. One way of doing this may be to focus training on encouraging recognition of low intensity expressions of a particular emotion.</p>
              <p>A number of procedures have been developed to address cognitive biases in psychiatric populations. These commonly aim either to modify <italic>attentional</italic> biases by training attention away from particular emotional stimuli (<xref rid="bib7" ref-type="bibr">Heeren et al., 2015</xref>), or to modify <italic>interpretation</italic> biases by training participants to interpret ambiguous scenarios as emotionally positive. More recently a paradigm has been developed which aims specifically to modify biases in the perception of ambiguous facial expressions (<xref rid="bib11" ref-type="bibr">Penton-Voak et al., 2012</xref>, <xref rid="bib12" ref-type="bibr">Penton-Voak et al., 2013</xref>). In this training, participants are presented, in a random order, with facial expressions from a 15-step morph sequence between a happy and angry (or sad) expression. They are asked to categorise each expression as being one of the two emotions. The point in the morph sequence at which each participant’ switches from one emotion category to another is determined at baseline. In training participants categorise the expressions again, this time receiving feedback about their accuracy after each response. Participants in the active training group receive feedback telling them that 2 more expressions just on the angry (or sad) side of their own category boundary (i.e. relatively ambiguous expressions that they characterized, on average as angry at baseline) are actually happy. Participants in the control group receive feedback that is consistent with their baseline category boundary. <xref rid="bib12" ref-type="bibr">Penton-Voak et al. (2013)</xref> trained youths at risk of criminal offending to categorise more expressions in a happy–angry morph sequence as happy. Immediately after training and two weeks later, those who received training to modify their category boundary showed reduced self-reported and observer-reported aggressive behaviour, compared to those who received control training. This suggested that training which targets the interpretation of low intensity expressions can affect interpretation of emotions outside the lab and have a positive effect on mood and behaviour.</p>
              <p>It is assumed that the effects on mood and behaviour observed in these facial expression recognition training studies are the result of altered perception of facial expressions that are encountered in social interactions. However, this was not explicitly tested by <xref rid="bib12" ref-type="bibr">Penton-Voak et al. (2013)</xref>, and it is not clear exactly how this form of training is influencing perception. <xref rid="bib12" ref-type="bibr">Penton-Voak et al. (2013)</xref> showed that a larger number of expressions were being identified as happy in the training task, but did not test perception of other emotional expressions or perception of expression on other faces. It is unclear whether training increased sensitivity to happiness or induced a bias to perceive all ambiguous expressions positively. As some psychological disorders are associated with biases towards negative emotions, inducing a bias towards positive emotions has potential therapeutic value. However, if morph sequence feedback training is to address deficits in sensitivity, it must increase discrimination accuracy. It is therefore important to know whether or not morph sequence feedback can increase accuracy in order to decide how this form of training may be best applied.</p>
              <p>In this study we tested whether morph sequence feedback training can increase recognition accuracy for a particular emotion. The training method was based on the procedure described by <xref rid="bib11" ref-type="bibr">Penton-Voak et al., 2012</xref>, <xref rid="bib12" ref-type="bibr">Penton-Voak et al., 2013</xref>). However, instead of using stimuli from morph sequences that morphed between two emotional expressions, we morphed from one expression, through an ambiguous expression (created by averaging 7 expressions) and towards the corresponding anti-expression (an expression in which the features have moved in the opposite direction from the emotional expression e.g., eyebrows are raised rather than lowered). We chose to morph the emotional expressions with an ambiguous expression, rather than another emotional expression, because we wanted to increase the chances of improving sensitivity to one particular emotion without reducing sensitivity to another. As the anti-expressions at the end of the sequence do not correspond strongly to a particular emotional state, reducing sensitivity to these anti-expressions should not impair emotion recognition ability.</p>
              <p>Participants were either trained to recognise happiness using a happy to anti-happy sequence or a fear using a fear to anti-fear sequence. Happiness was chosen because it has been the focus of previous training studies (<xref rid="bib11" ref-type="bibr">Penton-Voak et al., 2012</xref>, <xref rid="bib12" ref-type="bibr">Penton-Voak et al., 2013</xref>) and poor recognition of happiness has been associated with depression (<xref rid="bib18" ref-type="bibr">Surguladze et al., 2004</xref>). Fear was chosen because particularly poor recognition of fear has been found in conditions such as conduct disorder (<xref rid="bib10" ref-type="bibr">Marsh and Blair, 2008</xref>). Participants categorised faces from the morph sequences by whether or not they showed the target emotion. A 6 alternative forced choice emotion labelling task, which included 15 levels of expression intensity, was completed before and after training. In order to determine how training altered sensitivity and bias for recognition of the target expression, change in hit rate, number of false alarms, and unbiased hit rate (<xref rid="bib21" ref-type="bibr">Wagner, 1993</xref>) for happy and fearful expressions were compared between training groups. It was predicted that modification feedback training would alter recognition performance for the target emotion (either happiness or fear depending on training group) in the labelling task, while control feedback training would not.</p>
            </sec>
            <sec id="s0010">
              <label>2</label>
              <title>Method</title>
              <sec id="s0015">
                <label>2.1</label>
                <title>Participants</title>
                <p>One hundred and twenty participants were recruited from a database of volunteers at the University of Bristol. One participant withdrew during the training session, leaving data from 119 participants. All had normal or corrected to normal vision. Participants received 5 GBP or course credit for their participation. Each participant was randomly assigned to one of four training conditions; happy modification (<italic>N</italic>=29, 19 female, mean age=21, <italic>SD</italic>=3.09), happy control (<italic>N</italic>=30, 24 female, mean age=20, <italic>SD</italic>=1.65), fear modification (<italic>N</italic>=31, 26 female, mean age=21, <italic>SD</italic>=2.14), or fear control (<italic>N</italic>=29, 20 female, mean age=21, <italic>SD</italic>=3.67).</p>
              </sec>
              <sec id="s0020">
                <label>2.2</label>
                <title>Design</title>
                <p>The experiment used a 2×2 between-subject design. The factors were target emotion in training (fear or happy) and feedback type in training (modification or control). Manipulating feedback type allowed us to determine whether any effects were the result of feedback, or simply exposure to the target emotion during the training procedure. Manipulating target emotion allowed us to determine whether the training had consistent effects when applied to different emotions.</p>
                <p>In the training task the dependent variable was change in “threshold”; the proportion of morph sequence steps participants categorised as the target emotion. In the forced choice labelling task, the dependent variables were change in hit rate, change in number of false alarms and change in unbiased hit rate for the target emotions from before to after training. All participants labelled happy and fear among 4 other emotions in the labelling task. Increases in false alarms for the target emotion (detecting the target emotion in non-target expression) in a particular training group would indicate that that group had developed a bias for detecting that emotion. Participant groups were compared on their responses for the two target emotions in order to determine whether any training effects were emotion-specific or -general (e.g., does training on fear make only fear recognition more accurate, or does it also make recognition of happy more accurate). Therefore, there was an additional within-subject factor of “tested emotion” (happy, fear) in the forced choice task.</p>
                <p>Sample size was determined from a power calculation based on previous morph sequence training studies. The effect size for the change in threshold in training previous studies was <italic>d</italic>~1.0 (<xref rid="bib11" ref-type="bibr">Penton-Voak et al., 2012</xref>, <xref rid="bib12" ref-type="bibr">Penton-Voak et al., 2013</xref>). The power calculation suggested that 23 participants would be needed in each group to achieve 95% power to detect this change with an <italic>α</italic> level of 0.05. We chose to recruit 30 participants to each group, as effects on the forced choice transfer task were likely to be smaller than effects in the training task.</p>
              </sec>
              <sec id="s0025">
                <label>2.3</label>
                <title>Materials</title>
                <p>Training task stimuli were pictures of 15 expressions from a happy to anti-happy morph sequence and 15 expressions from a fear to anti-fear morph sequence displayed on a single composite male face. The composite face was created by averaging photographs of 20 adult males taken from the Karolinska Directed Emotional Faces (<xref rid="bib9" ref-type="bibr">Lundqvist et al., 1998</xref>). The computer programme Psychomorph was used to average shape, colour and texture information across pictures of the 20 faces posing each of the 6 basic facial expressions (happy, sad, angry, surprised, disgusted and fearful) (<xref rid="bib19" ref-type="bibr">Tiddeman et al., 2001</xref>). The morph sequence was created by morphing the happy and fearful composite faces with an emotionally ambiguous composite face. The emotionally ambiguous face was created by averaging all pictures of the 20 males showing the 6 expressions plus a neutral expression. The morph sequences between each expression and the ambiguous expression were then extended past the ambiguous expression to create anti-expressions (<xref rid="bib17" ref-type="bibr">Skinner and Benton, 2010</xref>). See <xref rid="s0090" ref-type="sec">Supplementary Information</xref> for further details of this process. 15 equally spaced steps between the 100% intensity expression and the 30% anti-expression were then selected (see <xref rid="f0005" ref-type="fig">Fig. 1</xref>). Therefore, moving along the sequences, the first 3 pictures contained components of the anti-expressions, while the next 12 pictures contained components of the veridical expression at increasing intensity levels. Including the 30% anti-expression at the end of the sequence, rather than stopping at the ambiguous expression ensured that roughly half the morph sequence pictures were categorised as the target emotion at baseline. This was to keep the training consistent with that employed in previous studies (<xref rid="bib11" ref-type="bibr">Penton-Voak et al., 2012</xref>, <xref rid="bib12" ref-type="bibr">Penton-Voak et al., 2013</xref>) to stop a bias developing simply because of the predominance of one response at the outset.</p>
                <p>To ensure any training effects were transferable across facial identities, a different composite face was created for use in the forced choice task. This was created from photos of 12 adult males posing each expression, taken in our laboratory at the University of Bristol. An emotionally ambiguous expression was created for this face in the same way as described above. Each of the 6 emotional expressions were morphed with the ambiguous expression and 15 equally spaced steps were selected from each sequence (see <xref rid="f0010" ref-type="fig">Fig. 2</xref>). These sequences were not extended to include the anti-expression so each step in the sequences showed increasing intensity levels of the expression. This created 6 sequences with 15 steps, giving a total of 90 stimuli.</p>
              </sec>
              <sec id="s0030">
                <label>2.4</label>
                <title>Procedure</title>
                <p>In the forced choice task, participants were asked to decide whether faces presented were happy, sad, angry, fearful, surprised or disgusted. In each trial participants saw a fixation cross appear in the centre of the computer screen (screen dimensions 30×48 cm<sup>2</sup>, viewing distance 50 cm) for between 1500 and 2500 ms (randomly jittered), followed by an picture of a face for 150 ms, followed by a mask of visual noise for 250 ms. The 6 emotion labels then appeared the screen in a circular formation. The positions of the labels were randomly selected for each participant and stayed the same throughout the testing session. The labels stayed on the screen until the participant had responded by selecting a label with the mouse, at which point the next trial started. There were two blocks of 90 trials (180 trials in total) in which each stimulus was presented once in each block (twice in total), with a break in the between blocks. Presentation order was random within blocks. Participants completed this task twice, once before and once after the training procedure.</p>
                <p>The training procedure consisted of three phases: baseline, feedback and test. In all phases participants judged whether or not faces from the happy (or fearful) morph sequence showed a happy (or fearful) expression. In each phase, participants saw the 15 pictures from the morph sequence presented in a random order. In the baseline and test phases each picture was shown 3 times, giving a total of 45 trials. In the feedback phase there were 6 blocks of 31 trials in which pictures 1–2 and 14–15 were presented once, pictures 3–5 and 11–13 were presented twice and pictures 6–10 were presented three times. This was to focus training on the critical steps in the morph sequence around the likely threshold. The proportion of faces each participant judged to be happy or fearful at baseline was used to select the baseline threshold for recognising that emotion, corresponding to a particular step in the morph sequence. In the feedback phase, participants in the control condition received feedback consistent with their baseline threshold, whereas participants in the modification condition received feedback that attempted to shift their baseline threshold by two steps, so an extra two steps on the non-emotional side of the boundary were categorised as happy/fearful. This 2-step change is the same as in previous training (<xref rid="bib11" ref-type="bibr">Penton-Voak et al., 2012</xref>, <xref rid="bib12" ref-type="bibr">Penton-Voak et al., 2013</xref>). In each trial, participants were presented with a central fixation cross for 1500–2500 ms (randomly jittered), followed by a face (562 by 762 pixels) for 150 ms. A mask of visual noise was then presented for 150 ms, after which a central question mark was displayed until participants responded by pressing one of two keys on the keyboard. In the training blocks, responses were followed by a message, displayed for 1000 ms, saying ‘Correct/Incorrect! That face was happy/fearful/not happy/not fearful’</p>
              </sec>
              <sec id="s0035">
                <label>2.5</label>
                <title>Analysis</title>
                <sec id="s0040">
                  <label>2.5.1</label>
                  <title>Training task</title>
                  <p>Change in morph sequence thresholds from baseline to test phases was analysed to check that modification feedback was increasing the tendency to detect the target emotion. Change in threshold was entered in to a 2×2 between subjects ANOVA with target emotion (fear, happy) and feedback type (modification, control) as factors. One sample <italic>t</italic>-tests were then carried out to determine whether any group showed a change in threshold that was different from zero.</p>
                </sec>
                <sec id="s0045">
                  <label>2.5.2</label>
                  <title>Forced choice emotion labelling task</title>
                  <p>Hit rate, number of false alarms and unbiased hit rate (<xref rid="bib21" ref-type="bibr">Wagner, 1993</xref>) were analysed. These measures were chosen because we were interested in whether modification training increased target emotion detection rate, and whether this was due to an increase in bias or an increase in sensitivity. Hit rate is the proportion of trials in which a particular emotion is shown that is correctly labelled. Number of false alarms is the number of times in which a particular emotion label is incorrectly used. Unbiased hit rate (<italic>Hu</italic>) gives a measure of perceptual sensitivity by taking in to account both hit rate and false alarm rate. The measure was devised for use in category judgement experiments where other methods of accounting for bias (e.g., signal detection) are inappropriate due to the study design (<xref rid="bib21" ref-type="bibr">Wagner, 1993</xref>) <italic>Hu</italic> is calculated as: <italic>Hu</italic>=(<italic>Ai</italic>/<italic>Bi</italic>)×(<italic>Ai</italic>/<italic>Ci</italic>), where <italic>Ai</italic>=frequency of hits, <italic>Bi</italic>=number of trials where <italic>i</italic> is target and <italic>Ci</italic>=frequency of <italic>i</italic> responses (hits and false alarms). <italic>Hu</italic> values were arcsine transformed before analysis as suggested by <xref rid="bib21" ref-type="bibr">Wagner (1993)</xref>. The calculation gives hit rate adjusted for the tendency to give that particular response in any trial.</p>
                  <p>For each of the three variables, change scores were calculated by taking the group mean before training from the mean after training (positive scores therefore indicate improvement). Initially, all outcome measures were analysed in mixed 2×2×2 ANOVAs with tested emotion (happy, fear) as a within-subject factor, and feedback type (modification, control) and target emotion in training (happy, fear) as between-subjects factors. Trials in which anger, sadness, surprise and disgust were shown were only included in the design in order to make the discrimination task sufficiently challenging. We did not include these emotions in our analysis as we did not hypothesise that training would affect performance on these emotions. However, data on performance for the non-target emotions can be found in the Supplementary Information.</p>
                  <p>Evidence of 3-way interactions in these analyses would suggest that modification training may be having the expected, specific effect on performance on the target emotion. For example, if there was a unique effect of fear modification training on fear recognition an interaction would be seen. Therefore, in cases where this critical 3-way interaction was observed, we carried out separate 2×2 ANOVAs for each target emotion condition, followed up with <italic>t</italic>-tests to make critical comparisons between modification and control groups on performance of the target emotion in training. For brevity, only critical interaction results from the ANOVAs are reported. Full results from all of these analyses can be found in <xref rid="s0090" ref-type="sec">Supplementary information</xref>.</p>
                  <p>The data that form the basis of the results presented here are available to researchers on request from the data.bris Research Data Repository <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.5523/bris.1vx65vv968imz1p8rvhte5vk9a" id="ir0005">http://dx.doi.org/10.5523/bris.1vx65vv968imz1p8rvhte5vk9a</ext-link>.</p>
                </sec>
              </sec>
            </sec>
            <sec id="s0050">
              <label>3</label>
              <title>Results</title>
              <sec id="s0055">
                <label>3.1</label>
                <title>Training task</title>
                <p>The 2×2 ANOVA revealed evidence for a main effect of feedback type (<italic>F</italic> [1, 115]=24.722, <italic>p</italic>&lt;0.001, <italic>η</italic><sup>2</sup>=0.177) but no evidence for a main effect of target emotion (<italic>F</italic> (1,115)=0.954, <italic>p</italic>=0.331, <italic>η</italic><sup>2</sup>=0.008), and no evidence for an interaction (<italic>F</italic> (1,115)=0.041, <italic>p</italic>=0.841, <italic>η</italic><sup>2</sup>&lt;.001). One sample <italic>t</italic>-tests provided evidence for decrease in threshold (the proportion of morph sequence steps participants categorised as the target emotion) from pre- to post-training in the groups who received modification feedback (fear, <italic>t</italic>(30)=−5.130, <italic>p</italic>&lt;0.001; happy, <italic>t</italic>(28)=−3.936, <italic>p</italic>&lt;0.001) but no evidence for a change threshold in groups who received control feedback (fear, <italic>t</italic>(28)=0.754 <italic>p</italic>=0.457; happy, <italic>t</italic>(29)=−0.516, <italic>p</italic>=0.610) (see <xref rid="f0015" ref-type="fig">Fig. 3</xref>.)</p>
              </sec>
              <sec id="s0060">
                <label>3.2</label>
                <title>Forced choice task emotion labelling task</title>
                <sec id="s0065">
                  <label>3.2.1</label>
                  <title>Hit rate</title>
                  <p>The 3-way ANOVA indicated evidence for the critical interaction between tested emotion, feedback type, and target emotion (<italic>F</italic> [1, 115]=4.15, <italic>p</italic>=0.044, <italic>ɳ</italic><sup>2</sup>=0.035). Separate 2-way ANOVAs on the two target emotion conditions indicated evidence for an interaction between feedback type and tested emotion in the fear target training group (<italic>F</italic> [1, 56]=4.61, <italic>p</italic>=0.036, <italic>ɳ</italic><sup>2</sup>=0.074), but not in the happy training group (<italic>F</italic> [1, 57]=0.61, <italic>p</italic>=0.44, <italic>ɳ</italic><sup>2</sup>=0.011). This reflects the fact that participants who received fear modification feedback showed a greater increase in fear hit rate than those who received fear control feedback (post-hoc <italic>t</italic>-tests provided weak statistical evidence for this difference; <italic>t</italic>(58)=1.88, <italic>p</italic>=0.065). However, participants who received happy modification feedback did not show a greater increase in happy hit rate than those who received happy control feedback (<italic>t</italic>(57)=0.53, <italic>p</italic>=0.60) (<xref rid="f0020" ref-type="fig">Fig. 4</xref>). This is likely due to a ceiling effect for happy hit rate as it is nearing 90% in all conditions.</p>
                </sec>
                <sec id="s0070">
                  <label>3.2.2</label>
                  <title>Number of false alarms</title>
                  <p>The 3-way ANOVA indicated evidence for the critical interaction between tested emotion, feedback type and target emotion (<italic>F</italic> [1, 115]=10.44, <italic>p</italic>=0.002, <italic>η</italic><sup>2</sup>=0.083). Separate 2-way ANOVAs on the two target emotion conditions indicated evidence for an interaction between feedback type and tested emotion in both fear (<italic>F</italic> [1, 58]=4.72, <italic>p</italic>=0.034, <italic>ɳ</italic><sup>2</sup>=0.075) and happy (<italic>F</italic> [1, 58]=5.77, <italic>p</italic>=0.020, <italic>ɳ</italic><sup>2</sup>=0.092) target conditions. These interactions reflect a greater increase in false alarms for the target emotion after modification feedback compared to control feedback for the happy target condition (<italic>t</italic>(57)=2.24, <italic>p</italic>=0.029) and fear target condition (<italic>t</italic>(58)=1.76, <italic>p</italic>=0.083) (see <xref rid="f0025" ref-type="fig">Fig. 5</xref>), although the statistical evidence from post-hoc <italic>t</italic>-tests was weaker for the difference in the fear target condition. The fact that there is a difference in happy false alarms despite no corresponding difference in happy hit rate further suggests a ceiling effect for happy hit rate.</p>
                </sec>
                <sec id="s0075">
                  <label>3.2.3</label>
                  <title>Unbiased hit rate (Hu)</title>
                  <p>The 3-way ANOVA did not indicate evidence for the critical 3-way interaction between tested emotion, feedback type and target emotion (<italic>F</italic> [1, 115]=0.007, <italic>p</italic>=0.934, <italic>ɳ</italic><sup>2</sup>&lt;0.001), suggesting that there is no effect of specific training condition on sensitivity to the target emotion (<xref rid="f0030" ref-type="fig">Fig. 6</xref>). There was a main effect of tested emotion, which reflected overall increase in unbiased hit rate for fear but not for happiness. As this increase is not related to training condition, it is likely a practice effect for recognising fear in the labelling task. The lack of practice effect for recognition of happiness may be due to ceiling effects. Additionally there was some evidence for an interaction between target emotion and feedback type (<italic>F</italic> [1, 115]=3.894, <italic>p</italic>=0.051, <italic>ɳ</italic><sup>2</sup>=0.033). This reflects the fact that the fear modification group showed a greater increase in overall performance than the happy modification group (fear modification <italic>M</italic>=0.181 <italic>SD</italic>=0.314; happy modification <italic>M</italic>=006 <italic>SD</italic>=0.239, <italic>t</italic>(58)=2.42, <italic>p</italic>=0.019) while the fear control and happy control groups did not differ in overall improvement (fear control, <italic>M</italic>=0.063, <italic>SD</italic>=0.283; happy control <italic>M</italic>=0.097, <italic>SD</italic>=0.307, <italic>t</italic>(57)=0.43, <italic>p</italic>=0.669). However, given there is no evidence that the fear modification group showed a greater overall improvement than the fear control group (<italic>t</italic>(58)=1.52, <italic>p</italic>=0.133) it cannot be concluded that fear modification training improves performance.</p>
                </sec>
              </sec>
            </sec>
            <sec id="s0080">
              <label>4</label>
              <title>Discussion</title>
              <p>We found evidence that morph sequence feedback training targeting happy or fearful expressions, influences the interpretation of other emotional expressions encountered in a transfer task. In the transfer task, which involved labelling 6 different emotional expressions at varying intensity levels, participants who received modification training showed an increase in false detections of the target emotion, indicating training had caused an interpretation bias. Participants who received fear modification training also showed an increase in correct detections of fear, although those who received happy modification training did not show an increase in correct detections of happiness. Critically however, there was no evidence for an increase in sensitivity for the target emotion after either happy or fear modification training.</p>
              <p>Previous studies using feedback training to increase the number of expressions in happy–sad and happy–angry morph sequences that are categorised as happy, found decreases in negative affect (<xref rid="bib11" ref-type="bibr">Penton-Voak et al., 2012</xref>) and decreases in self- and observer-reported aggression after modification feedback (<xref rid="bib12" ref-type="bibr">Penton-Voak et al., 2013</xref>). It was suggested that the mechanism by which this training is affecting mood and behaviour is through inducing a bias for perceiving happiness in facial expressions encountered outside of the training paradigm. This study adds support to this claim by showing that it is possible for morph sequence feedback training to induce a bias for a certain expression in an emotion recognition transfer task.</p>
              <p>There were slight differences in our morph sequence training method compared to the morph sequence training methods used in previous studies. First, we used morph sequences which ran from one expression to it’s corresponding anti-expression, rather than between two emotional expressions. Second, participants decided whether the emotion was present or not, (e.g., “happy or not happy”) rather than deciding which of two expressions were present (“happy or sad”). These changes were made to ensure that we did not train people <italic>not</italic> to select one particular emotion. However, there is no reason why either of these changes would make it more likely that we would find a bias that transferred to another task following training. We are therefore reasonably confident that our findings are applicable to the morph sequence training technique used in previous studies.</p>
              <p>Our results suggest that morph sequence training causes a bias towards choosing a particular emotional category. The fact that this bias transferred to another task, which used different face stimuli, required a different decision (six options instead of two) and a required a different motor response (mouse click instead of button press) suggests the training effect is not limited to a simple task-related response bias. However, it is not clear from the current results whether this bias is specific to recognition of emotions from facial expression, or a general bias for all decisions about emotional stimuli. It would be possible to distinguish between these two possibilities by testing the effects of training on the classification of emotional stimuli other than facial expressions (e.g., postures or tones of voice).</p>
              <p>Training procedures used in previous studies have aimed to increase the detection of happiness (<xref rid="bib11" ref-type="bibr">Penton-Voak et al., 2012</xref>, <xref rid="bib12" ref-type="bibr">Penton-Voak et al., 2013</xref>). In this study we looked at both happiness and fear training and found that both increased false alarms for the targeted emotion, although only fear training increased target hit rate. This is likely due to the happy hit rate being at ceiling in the control training conditions. Ceiling performance for happy expressions in the emotion labelling task is perhaps not surprising as happiness has been shown to be the easiest emotion to identify (<xref rid="bib4" ref-type="bibr">Calvo and Lundqvist, 2008</xref>, <xref rid="bib20" ref-type="bibr">Tottenham et al., 2009</xref>). The fact that training effects were evident for training on both emotions suggests this procedure could be used to induce biases for detecting any emotion of interest in future studies.</p>
              <p>It is notable that perceptual sensitivity did not improve following training. We had hypothesised that training using morph sequences which included low intensity examples of a particular expression would increase perceptual sensitivity to that expression; but we found this not to be the case. However, the participants in our study were adults with no known deficits in emotion recognition. Given that healthy adults are typically good at detecting basic facial expressions, it may be that improving sensitivity beyond baseline was not possible (<xref rid="bib2" ref-type="bibr">Blanch-Hartigan et al., 2012</xref>). Increases in sensitivity may be more likely in populations with particular deficits in recognising the target emotion. Nonetheless, the current results suggest that this type of morph sequence feedback training may not be useful in addressing reductions in sensitivity to emotional expressions found in conditions such as autism spectrum disorders (<xref rid="bib22" ref-type="bibr">Wallace et al., 2011</xref>). However, it may hold promise for addressing biases in facial expression perception, such as those found in depression (<xref rid="bib3" ref-type="bibr">Bourke et al., 2010</xref>), anxiety disorders (<xref rid="bib1" ref-type="bibr">Bell et al., 2011</xref>, <xref rid="bib24" ref-type="bibr">Yoon et al., 2014</xref>) or conduct disorders (<xref rid="bib16" ref-type="bibr">Schönenberg and Jusyte, 2014</xref>).</p>
              <p>Future studies should focus on determining whether alternative training techniques can improve sensitivity to low intensity expressions, as this seems to be a particular problem in many disorders (<xref rid="bib6" ref-type="bibr">Frenkel et al., 2009</xref>, <xref rid="bib8" ref-type="bibr">Law Smith et al., 2010</xref>, <xref rid="bib13" ref-type="bibr">Ridout et al., 2012</xref>). One promising method of training sensitivity using low intensity facial expression has been published recently (<xref rid="bib15" ref-type="bibr">Schonenberg et al., 2014</xref>). In this study, violent offenders showed increased sensitivity for low intensity expressions after they were implicitly trained to attend to low intensity fearful faces rather than neutral faces using a dot probe paradigm. One possible reason why this training was more effective than that used in the current study is that the expressions were displayed side by side which may have helped to demonstrate the difference between expressions which do and do not show the target emotion.</p>
              <p>In conclusion, the current study adds to the findings of previous studies showing that feedback training using expression morph sequences can lead to an increased tendency to detect a particular target emotion (<xref rid="bib11" ref-type="bibr">Penton-Voak et al., 2012</xref>, <xref rid="bib12" ref-type="bibr">Penton-Voak et al., 2013</xref>). Our findings show that it is possible to increase the tendency to detect fear as well as happiness. We also demonstrate that the training leads to a bias for detecting the target emotion in any expression rather than an increase in the ability to accurately identify the target emotion expression. Finally, we demonstrate for the first time that this experimentally induced bias in emotion recognition generalises to different identity faces shown in other tasks, suggesting it may have an effect on emotion recognition in faces outside of the lab.</p>
            </sec>
          </body>
          <back>
            <ref-list>
              <title>References</title>
              <ref id="bib1">
                <element-citation publication-type="journal" id="sbref1">
                  <person-group person-group-type="author">
                    <name>
                      <surname>Bell</surname>
                      <given-names>C.</given-names>
                    </name>
                    <name>
                      <surname>Bourke</surname>
                      <given-names>C.</given-names>
                    </name>
                    <name>
                      <surname>Colhoun</surname>
                      <given-names>H.</given-names>
                    </name>
                    <name>
                      <surname>Carter</surname>
                      <given-names>F.</given-names>
                    </name>
                    <name>
                      <surname>Frampton</surname>
                      <given-names>C.</given-names>
                    </name>
                    <name>
                      <surname>Porter</surname>
                      <given-names>R.</given-names>
                    </name>
                  </person-group>
                  <article-title>The misclassification of facial expressions in generalised social phobia</article-title>
                  <source>J. Anxiety Disord.</source>
                  <volume>25</volume>
                  <year>2011</year>
                  <fpage>278</fpage>
                  <lpage>283</lpage>
                  <pub-id pub-id-type="pmid">21041060</pub-id>
                </element-citation>
              </ref>
              <ref id="bib2">
                <element-citation publication-type="journal" id="sbref2">
                  <person-group person-group-type="author">
                    <name>
                      <surname>Blanch-Hartigan</surname>
                      <given-names>D.</given-names>
                    </name>
                    <name>
                      <surname>Andrzejewski</surname>
                      <given-names>S.A.</given-names>
                    </name>
                    <name>
                      <surname>Hill</surname>
                      <given-names>K.M.</given-names>
                    </name>
                  </person-group>
                  <article-title>The effectiveness of training to improve person perception accuracy: a meta-analysis</article-title>
                  <source>Basic Appl. Soc. Psychol.</source>
                  <volume>34</volume>
                  <year>2012</year>
                  <fpage>483</fpage>
                  <lpage>498</lpage>
                </element-citation>
              </ref>
              <ref id="bib3">
                <element-citation publication-type="journal" id="sbref3">
                  <person-group person-group-type="author">
                    <name>
                      <surname>Bourke</surname>
                      <given-names>C.</given-names>
                    </name>
                    <name>
                      <surname>Douglas</surname>
                      <given-names>K.</given-names>
                    </name>
                    <name>
                      <surname>Porter</surname>
                      <given-names>R.</given-names>
                    </name>
                  </person-group>
                  <article-title>Processing of facial emotion expression in major depression: a review</article-title>
                  <source>Aust. N. Z. J. Psychiatry</source>
                  <volume>44</volume>
                  <year>2010</year>
                  <fpage>681</fpage>
                  <lpage>696</lpage>
                  <pub-id pub-id-type="pmid">20636189</pub-id>
                </element-citation>
              </ref>
              <ref id="bib4">
                <element-citation publication-type="journal" id="sbref4">
                  <person-group person-group-type="author">
                    <name>
                      <surname>Calvo</surname>
                      <given-names>M.G.</given-names>
                    </name>
                    <name>
                      <surname>Lundqvist</surname>
                      <given-names>D.</given-names>
                    </name>
                  </person-group>
                  <article-title>Facial expressions of emotion (KDEF): identification under different display-duration conditions</article-title>
                  <source>Behav. Res. Methods</source>
                  <volume>40</volume>
                  <year>2008</year>
                  <fpage>109</fpage>
                  <lpage>115</lpage>
                  <pub-id pub-id-type="pmid">18411533</pub-id>
                </element-citation>
              </ref>
              <ref id="bib5">
                <element-citation publication-type="journal" id="sbref5">
                  <person-group person-group-type="author">
                    <name>
                      <surname>Dadds</surname>
                      <given-names>M.R.</given-names>
                    </name>
                    <name>
                      <surname>Cauchi</surname>
                      <given-names>A.J.</given-names>
                    </name>
                    <name>
                      <surname>Wimalaweera</surname>
                      <given-names>S.</given-names>
                    </name>
                    <name>
                      <surname>Hawes</surname>
                      <given-names>D.J.</given-names>
                    </name>
                    <name>
                      <surname>Brennan</surname>
                      <given-names>J.</given-names>
                    </name>
                  </person-group>
                  <article-title>Outcomes, moderators, and mediators of empathic-emotion recognition training for complex conduct problems in childhood</article-title>
                  <source>Psychiatry Res.</source>
                  <volume>199</volume>
                  <year>2012</year>
                  <fpage>201</fpage>
                  <lpage>207</lpage>
                  <pub-id pub-id-type="pmid">22703720</pub-id>
                </element-citation>
              </ref>
              <ref id="bib6">
                <element-citation publication-type="journal" id="sbref6">
                  <person-group person-group-type="author">
                    <name>
                      <surname>Frenkel</surname>
                      <given-names>T.I.</given-names>
                    </name>
                    <name>
                      <surname>Lamy</surname>
                      <given-names>D.</given-names>
                    </name>
                    <name>
                      <surname>Algom</surname>
                      <given-names>D.</given-names>
                    </name>
                    <name>
                      <surname>Bar-Haim</surname>
                      <given-names>Y.</given-names>
                    </name>
                  </person-group>
                  <article-title>Individual differences in perceptual sensitivity and response bias in anxiety: evidence from emotional faces</article-title>
                  <source>Cogn. Emot.</source>
                  <volume>23</volume>
                  <year>2009</year>
                  <fpage>688</fpage>
                  <lpage>700</lpage>
                </element-citation>
              </ref>
              <ref id="bib7">
                <element-citation publication-type="journal" id="sbref7">
                  <person-group person-group-type="author">
                    <name>
                      <surname>Heeren</surname>
                      <given-names>A.</given-names>
                    </name>
                    <name>
                      <surname>Mogoașe</surname>
                      <given-names>C.</given-names>
                    </name>
                    <name>
                      <surname>Philippot</surname>
                      <given-names>P.</given-names>
                    </name>
                    <name>
                      <surname>McNally</surname>
                      <given-names>R.J.</given-names>
                    </name>
                  </person-group>
                  <article-title>Attention bias modification for social anxiety: a systematic review and meta-analysis</article-title>
                  <source>Clin. Psychol. Rev.</source>
                  <volume>40</volume>
                  <year>2015</year>
                  <fpage>76</fpage>
                  <lpage>90</lpage>
                  <pub-id pub-id-type="pmid">26080314</pub-id>
                </element-citation>
              </ref>
              <ref id="bib8">
                <element-citation publication-type="journal" id="sbref8">
                  <person-group person-group-type="author">
                    <name>
                      <surname>Law Smith</surname>
                      <given-names>M.J.</given-names>
                    </name>
                    <name>
                      <surname>Montagne</surname>
                      <given-names>B.</given-names>
                    </name>
                    <name>
                      <surname>Perrett</surname>
                      <given-names>D.I.</given-names>
                    </name>
                    <name>
                      <surname>Gill</surname>
                      <given-names>M.</given-names>
                    </name>
                    <name>
                      <surname>Gallagher</surname>
                      <given-names>L.</given-names>
                    </name>
                  </person-group>
                  <article-title>Detecting subtle facial emotion recognition deficits in high-functioning autism using dynamic stimuli of varying intensities</article-title>
                  <source>Neuropsychologia</source>
                  <volume>48</volume>
                  <year>2010</year>
                  <fpage>2777</fpage>
                  <lpage>2781</lpage>
                  <pub-id pub-id-type="pmid">20227430</pub-id>
                </element-citation>
              </ref>
              <ref id="bib9">
                <element-citation publication-type="book" id="sbref9">
                  <person-group person-group-type="author">
                    <name>
                      <surname>Lundqvist</surname>
                      <given-names>D.</given-names>
                    </name>
                    <name>
                      <surname>Flykt</surname>
                      <given-names>A.</given-names>
                    </name>
                    <name>
                      <surname>Öhman</surname>
                      <given-names>A.</given-names>
                    </name>
                  </person-group>
                  <chapter-title>The Karolinska Directed Emotional Faces</chapter-title>
                  <year>1998</year>
                  <publisher-name>Karolinska Institute</publisher-name>
                  <publisher-loc>Stockholm, Sweden</publisher-loc>
                </element-citation>
              </ref>
              <ref id="bib10">
                <element-citation publication-type="journal" id="sbref10">
                  <person-group person-group-type="author">
                    <name>
                      <surname>Marsh</surname>
                      <given-names>A.A.</given-names>
                    </name>
                    <name>
                      <surname>Blair</surname>
                      <given-names>R.J.R.</given-names>
                    </name>
                  </person-group>
                  <article-title>Deficits in facial affect recognition among antisocial populations: a meta-analysis</article-title>
                  <source>Neurosci. Biobehav. Rev.</source>
                  <volume>32</volume>
                  <year>2008</year>
                  <fpage>454</fpage>
                  <lpage>465</lpage>
                  <pub-id pub-id-type="pmid">17915324</pub-id>
                </element-citation>
              </ref>
              <ref id="bib11">
                <element-citation publication-type="journal" id="sbref11">
                  <person-group person-group-type="author">
                    <name>
                      <surname>Penton-Voak</surname>
                      <given-names>I.</given-names>
                    </name>
                    <name>
                      <surname>Bate</surname>
                      <given-names>H.</given-names>
                    </name>
                    <name>
                      <surname>Lewis</surname>
                      <given-names>G.</given-names>
                    </name>
                    <name>
                      <surname>Munafò</surname>
                      <given-names>M.R.</given-names>
                    </name>
                  </person-group>
                  <article-title>Effects of emotion perception training on mood in undergraduate students: randomised controlled trial</article-title>
                  <source>Br. J. Psychiatry</source>
                  <volume>201</volume>
                  <year>2012</year>
                  <fpage>71</fpage>
                  <lpage>72</lpage>
                  <pub-id pub-id-type="pmid">22539781</pub-id>
                </element-citation>
              </ref>
              <ref id="bib12">
                <element-citation publication-type="journal" id="sbref12">
                  <person-group person-group-type="author">
                    <name>
                      <surname>Penton-Voak</surname>
                      <given-names>I.</given-names>
                    </name>
                    <name>
                      <surname>Thomas</surname>
                      <given-names>J.</given-names>
                    </name>
                    <name>
                      <surname>Gage</surname>
                      <given-names>S.H.</given-names>
                    </name>
                    <name>
                      <surname>McMurran</surname>
                      <given-names>M.</given-names>
                    </name>
                    <name>
                      <surname>McDonald</surname>
                      <given-names>S.</given-names>
                    </name>
                    <name>
                      <surname>Munafo</surname>
                      <given-names>M.R.</given-names>
                    </name>
                  </person-group>
                  <article-title>Increasing recognition of happiness in ambiguous facial expressions reduces anger and aggressive behavior</article-title>
                  <source>Psychol. Sci.</source>
                  <volume>24</volume>
                  <year>2013</year>
                  <fpage>688</fpage>
                  <lpage>697</lpage>
                  <pub-id pub-id-type="pmid">23531485</pub-id>
                </element-citation>
              </ref>
              <ref id="bib13">
                <element-citation publication-type="journal" id="sbref13">
                  <person-group person-group-type="author">
                    <name>
                      <surname>Ridout</surname>
                      <given-names>N.</given-names>
                    </name>
                    <name>
                      <surname>Wallis</surname>
                      <given-names>D.J.</given-names>
                    </name>
                    <name>
                      <surname>Autwal</surname>
                      <given-names>Y.</given-names>
                    </name>
                    <name>
                      <surname>Sellis</surname>
                      <given-names>J.</given-names>
                    </name>
                  </person-group>
                  <article-title>The influence of emotional intensity on facial emotion recognition in disordered eating</article-title>
                  <source>Appetite</source>
                  <volume>59</volume>
                  <year>2012</year>
                  <fpage>181</fpage>
                  <lpage>186</lpage>
                  <pub-id pub-id-type="pmid">22542716</pub-id>
                </element-citation>
              </ref>
              <ref id="bib14">
                <element-citation publication-type="journal" id="sbref14">
                  <person-group person-group-type="author">
                    <name>
                      <surname>Ryan</surname>
                      <given-names>C.</given-names>
                    </name>
                    <name>
                      <surname>Ni Charragain</surname>
                      <given-names>C.</given-names>
                    </name>
                  </person-group>
                  <article-title>Teaching emotion recognition skills to children with autism</article-title>
                  <source>J. Autism Dev. Disord.</source>
                  <volume>40</volume>
                  <year>2010</year>
                  <fpage>1505</fpage>
                  <lpage>1511</lpage>
                  <pub-id pub-id-type="pmid">20386975</pub-id>
                </element-citation>
              </ref>
              <ref id="bib15">
                <element-citation publication-type="journal" id="sbref15">
                  <person-group person-group-type="author">
                    <name>
                      <surname>Schonenberg</surname>
                      <given-names>M.</given-names>
                    </name>
                    <name>
                      <surname>Christian</surname>
                      <given-names>S.</given-names>
                    </name>
                    <name>
                      <surname>Gausser</surname>
                      <given-names>A.K.</given-names>
                    </name>
                    <name>
                      <surname>Mayer</surname>
                      <given-names>S.V.</given-names>
                    </name>
                    <name>
                      <surname>Hautzinger</surname>
                      <given-names>M.</given-names>
                    </name>
                    <name>
                      <surname>Jusyte</surname>
                      <given-names>A.</given-names>
                    </name>
                  </person-group>
                  <article-title>Addressing perceptual insensitivity to facial affect in violent offenders: first evidence for the efficacy of a novel implicit training approach</article-title>
                  <source>Psychol. Med.</source>
                  <volume>44</volume>
                  <year>2014</year>
                  <fpage>1043</fpage>
                  <lpage>1052</lpage>
                  <pub-id pub-id-type="pmid">23809680</pub-id>
                </element-citation>
              </ref>
              <ref id="bib16">
                <element-citation publication-type="journal" id="sbref16">
                  <person-group person-group-type="author">
                    <name>
                      <surname>Schönenberg</surname>
                      <given-names>M.</given-names>
                    </name>
                    <name>
                      <surname>Jusyte</surname>
                      <given-names>A.</given-names>
                    </name>
                  </person-group>
                  <article-title>Investigation of the hostile attribution bias toward ambiguous facial cues in antisocial violent offenders</article-title>
                  <source>Eur. Arch. Psychiatry Clin. Neurosci.</source>
                  <volume>264</volume>
                  <year>2014</year>
                  <fpage>61</fpage>
                  <lpage>69</lpage>
                  <pub-id pub-id-type="pmid">23990116</pub-id>
                </element-citation>
              </ref>
              <ref id="bib17">
                <element-citation publication-type="journal" id="sbref17">
                  <person-group person-group-type="author">
                    <name>
                      <surname>Skinner</surname>
                      <given-names>A.L.</given-names>
                    </name>
                    <name>
                      <surname>Benton</surname>
                      <given-names>C.P.</given-names>
                    </name>
                  </person-group>
                  <article-title>Anti-expression aftereffects reveal prototype-referenced coding of facial expressions</article-title>
                  <source>Psychol. Sci.</source>
                  <volume>21</volume>
                  <year>2010</year>
                  <fpage>1248</fpage>
                  <lpage>1253</lpage>
                  <pub-id pub-id-type="pmid">20713632</pub-id>
                </element-citation>
              </ref>
              <ref id="bib18">
                <element-citation publication-type="journal" id="sbref18">
                  <person-group person-group-type="author">
                    <name>
                      <surname>Surguladze</surname>
                      <given-names>S.A.</given-names>
                    </name>
                    <name>
                      <surname>Young</surname>
                      <given-names>A.W.</given-names>
                    </name>
                    <name>
                      <surname>Senior</surname>
                      <given-names>C.</given-names>
                    </name>
                    <name>
                      <surname>Brébion</surname>
                      <given-names>G.</given-names>
                    </name>
                    <name>
                      <surname>Travis</surname>
                      <given-names>M.J.</given-names>
                    </name>
                    <name>
                      <surname>Phillips</surname>
                      <given-names>M.L.</given-names>
                    </name>
                  </person-group>
                  <article-title>Recognition accuracy and response bias to happy and sad facial expressions in patients with major depression</article-title>
                  <source>Neuropsychology</source>
                  <volume>18</volume>
                  <year>2004</year>
                  <fpage>212</fpage>
                  <lpage>218</lpage>
                  <pub-id pub-id-type="pmid">15099143</pub-id>
                </element-citation>
              </ref>
              <ref id="bib19">
                <element-citation publication-type="journal" id="sbref19">
                  <person-group person-group-type="author">
                    <name>
                      <surname>Tiddeman</surname>
                      <given-names>B.</given-names>
                    </name>
                    <name>
                      <surname>Burt</surname>
                      <given-names>M.</given-names>
                    </name>
                    <name>
                      <surname>Perrett</surname>
                      <given-names>D.</given-names>
                    </name>
                  </person-group>
                  <article-title>Prototyping and transforming facial textures for perception research</article-title>
                  <source>IEEE Comput. Graph. Appl.</source>
                  <volume>21</volume>
                  <year>2001</year>
                  <fpage>42</fpage>
                  <lpage>50</lpage>
                </element-citation>
              </ref>
              <ref id="bib20">
                <element-citation publication-type="journal" id="sbref20">
                  <person-group person-group-type="author">
                    <name>
                      <surname>Tottenham</surname>
                      <given-names>N.</given-names>
                    </name>
                    <name>
                      <surname>Tanaka</surname>
                      <given-names>J.W.</given-names>
                    </name>
                    <name>
                      <surname>Leon</surname>
                      <given-names>A.C.</given-names>
                    </name>
                    <name>
                      <surname>McCarry</surname>
                      <given-names>T.</given-names>
                    </name>
                    <name>
                      <surname>Nurse</surname>
                      <given-names>M.</given-names>
                    </name>
                    <name>
                      <surname>Hare</surname>
                      <given-names>T.A.</given-names>
                    </name>
                    <name>
                      <surname>Marcus</surname>
                      <given-names>D.J.</given-names>
                    </name>
                    <name>
                      <surname>Westerlund</surname>
                      <given-names>A.</given-names>
                    </name>
                    <name>
                      <surname>Casey</surname>
                      <given-names>B.J.</given-names>
                    </name>
                    <name>
                      <surname>Nelson</surname>
                      <given-names>C.</given-names>
                    </name>
                  </person-group>
                  <article-title>The NimStim set of facial expressions: judgments from untrained research participants</article-title>
                  <source>Psychiatry Res.</source>
                  <volume>168</volume>
                  <year>2009</year>
                  <fpage>242</fpage>
                  <lpage>249</lpage>
                  <pub-id pub-id-type="pmid">19564050</pub-id>
                </element-citation>
              </ref>
              <ref id="bib21">
                <element-citation publication-type="journal" id="sbref21">
                  <person-group person-group-type="author">
                    <name>
                      <surname>Wagner</surname>
                      <given-names>H.L.</given-names>
                    </name>
                  </person-group>
                  <article-title>On measuring performance in category judgment studies of nonverbal behavior</article-title>
                  <source>J. Nonverbal Behav.</source>
                  <volume>17</volume>
                  <year>1993</year>
                  <fpage>3</fpage>
                  <lpage>28</lpage>
                </element-citation>
              </ref>
              <ref id="bib22">
                <element-citation publication-type="journal" id="sbref22">
                  <person-group person-group-type="author">
                    <name>
                      <surname>Wallace</surname>
                      <given-names>G.L.</given-names>
                    </name>
                    <name>
                      <surname>Case</surname>
                      <given-names>L.K.</given-names>
                    </name>
                    <name>
                      <surname>Harms</surname>
                      <given-names>M.B.</given-names>
                    </name>
                    <name>
                      <surname>Silvers</surname>
                      <given-names>J.A.</given-names>
                    </name>
                    <name>
                      <surname>Kenworthy</surname>
                      <given-names>L.</given-names>
                    </name>
                    <name>
                      <surname>Martin</surname>
                      <given-names>A.</given-names>
                    </name>
                  </person-group>
                  <article-title>Diminished sensitivity to sad facial expressions in high functioning autism spectrum disorders is associated with symptomatology and adaptive functioning</article-title>
                  <source>J. Autism Dev. Disord.</source>
                  <volume>41</volume>
                  <year>2011</year>
                  <fpage>1475</fpage>
                  <lpage>1486</lpage>
                  <pub-id pub-id-type="pmid">21347615</pub-id>
                </element-citation>
              </ref>
              <ref id="bib23">
                <element-citation publication-type="journal" id="sbref23">
                  <person-group person-group-type="author">
                    <name>
                      <surname>Wölwer</surname>
                      <given-names>W.</given-names>
                    </name>
                    <name>
                      <surname>Frommann</surname>
                      <given-names>N.</given-names>
                    </name>
                    <name>
                      <surname>Halfmann</surname>
                      <given-names>S.</given-names>
                    </name>
                    <name>
                      <surname>Piaszek</surname>
                      <given-names>A.</given-names>
                    </name>
                    <name>
                      <surname>Streit</surname>
                      <given-names>M.</given-names>
                    </name>
                    <name>
                      <surname>Gaebel</surname>
                      <given-names>W.</given-names>
                    </name>
                  </person-group>
                  <article-title>Remediation of impairments in facial affect recognition in schizophrenia: efficacy and specificity of a new training program</article-title>
                  <source>Schizophr. Res.</source>
                  <volume>80</volume>
                  <year>2005</year>
                  <fpage>295</fpage>
                  <lpage>303</lpage>
                  <pub-id pub-id-type="pmid">16125367</pub-id>
                </element-citation>
              </ref>
              <ref id="bib24">
                <element-citation publication-type="journal" id="sbref24">
                  <person-group person-group-type="author">
                    <name>
                      <surname>Yoon</surname>
                      <given-names>K.L.</given-names>
                    </name>
                    <name>
                      <surname>Yang</surname>
                      <given-names>J.W.</given-names>
                    </name>
                    <name>
                      <surname>Chong</surname>
                      <given-names>S.C.</given-names>
                    </name>
                    <name>
                      <surname>Oh</surname>
                      <given-names>K.J.</given-names>
                    </name>
                  </person-group>
                  <article-title>Perceptual sensitivity and response bias in social anxiety: an application of signal detection theory</article-title>
                  <source>Cogn. Ther. Res.</source>
                  <volume>38</volume>
                  <year>2014</year>
                  <fpage>551</fpage>
                  <lpage>558</lpage>
                </element-citation>
              </ref>
            </ref-list>
            <sec id="s0090" sec-type="supplementary-material">
              <label>Appendix A</label>
              <title>Supplementary material</title>
              <p>
                <supplementary-material content-type="local-data" id="ec0005">
                  <caption>
                    <p>Supplementary material</p>
                  </caption>
                  <media xlink:href="mmc1.docx"/>
                </supplementary-material>
              </p>
            </sec>
            <ack id="ack0005">
              <title>Acknowledgements</title>
              <p>Sarah Griffiths is funded by a <funding-source id="gs1">University of Bristol Postgraduate Science Scholarship</funding-source>. Marcus Munafò is a member of the UK Centre for Tobacco and Alcohol Studies, a UK Clinical Research Council Public Health Research: Centre of Excellence. Funding from <funding-source id="gs2">British Heart Foundation</funding-source>, <funding-source id="gs3">Cancer Research UK</funding-source>, <funding-source id="gs4">Economic and Social Research Council</funding-source>, <funding-source id="gs5">Medical Research Council</funding-source>, and the <funding-source id="gs6">National Institute for Health Research</funding-source>, under the auspices of the UK Clinical Research Collaboration, is gratefully acknowledged.</p>
            </ack>
            <fn-group>
              <fn id="s0085" fn-type="supplementary-material">
                <label>Appendix A</label>
                <p>Supplementary data associated with this article can be found in the online version at <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1016/j.psychres.2015.11.007" id="ir0010">doi:10.1016/j.psychres.2015.11.007</ext-link>.</p>
              </fn>
            </fn-group>
          </back>
          <floats-group>
            <fig id="f0005">
              <label>Fig. 1</label>
              <caption>
                <p>Anti-happy to happy and anti-fear to fear morph sequences pictures used in the training task.</p>
              </caption>
              <alt-text id="at0005">Fig. 1.</alt-text>
              <graphic xlink:href="gr1"/>
            </fig>
            <fig id="f0010">
              <label>Fig. 2</label>
              <caption>
                <p>Selected pictures from forced choice task.</p>
              </caption>
              <alt-text id="at0010">Fig. 2.</alt-text>
              <graphic xlink:href="gr2"/>
            </fig>
            <fig id="f0015">
              <label>Fig. 3</label>
              <caption>
                <p>Mean threshold change in training task from baseline to post training for participants in the 4 training conditions. Lower thresholds reflect greater proportions of morph sequence steps being categorised as being the target emotion. Error bars show standard error.</p>
              </caption>
              <alt-text id="at0015">Fig. 3.</alt-text>
              <graphic xlink:href="gr3"/>
            </fig>
            <fig id="f0020">
              <label>Fig. 4</label>
              <caption>
                <p>Mean change in hit rate for happy and afraid faces in the forced choice task for the participants in the 4 training conditions. Error bars show standard error.</p>
              </caption>
              <alt-text id="at0020">Fig. 4.</alt-text>
              <graphic xlink:href="gr4"/>
            </fig>
            <fig id="f0025">
              <label>Fig. 5</label>
              <caption>
                <p>Mean change in number of false alarms for happy and afraid faces in the forced choice task for the participants in the 4 training conditions. Error bars show standard error.</p>
              </caption>
              <alt-text id="at0025">Fig. 5.</alt-text>
              <graphic xlink:href="gr5"/>
            </fig>
            <fig id="f0030">
              <label>Fig. 6</label>
              <caption>
                <p>Mean change in unbiased hit rate for happy and afraid faces in the forced choice task for the participants in the 4 training conditions. Error bars show standard error.</p>
              </caption>
              <alt-text id="at0030">Fig. 6.</alt-text>
              <graphic xlink:href="gr6"/>
            </fig>
          </floats-group>
        </article>
      </metadata>
    </record>
  </GetRecord>
</OAI-PMH>
