<?xml version="1.0" encoding="UTF-8"?>
<OAI-PMH xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/ http://www.openarchives.org/OAI/2.0/OAI-PMH.xsd">
  <responseDate>2024-05-03T10:51:32Z</responseDate>
  <request verb="GetRecord" identifier="oai:pubmedcentral.nih.gov:6949287" metadataPrefix="pmc">https:/www.ncbi.nlm.nih.gov/pmc/oai/oai.cgi</request>
  <GetRecord>
    <record>
      <header>
        <identifier>oai:pubmedcentral.nih.gov:6949287</identifier>
        <datestamp>2020-01-10</datestamp>
        <setSpec>ncomms</setSpec>
        <setSpec>pmc-open</setSpec>
      </header>
      <metadata>
        <article xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:mml="http://www.w3.org/1998/Math/MathML" xsi:schemaLocation="https://jats.nlm.nih.gov/ns/archiving/1.3/ https://jats.nlm.nih.gov/archiving/1.3/xsd/JATS-archivearticle1-3.xsd" article-type="research-article">
          <front>
            <journal-meta>
              <journal-id journal-id-type="nlm-ta">Nat Commun</journal-id>
              <journal-id journal-id-type="iso-abbrev">Nat Commun</journal-id>
              <journal-title-group>
                <journal-title>Nature Communications</journal-title>
              </journal-title-group>
              <issn pub-type="epub">2041-1723</issn>
              <publisher>
                <publisher-name>Nature Publishing Group UK</publisher-name>
                <publisher-loc>London</publisher-loc>
              </publisher>
            </journal-meta>
            <article-meta>
              <article-id pub-id-type="accession">PMC6949287</article-id>
              <article-id pub-id-type="pmcid">PMC6949287</article-id>
              <article-id pub-id-type="pmc-uid">6949287</article-id>
              <article-id pub-id-type="pmid">31913272</article-id>
              <article-id pub-id-type="publisher-id">13922</article-id>
              <article-id pub-id-type="doi">10.1038/s41467-019-13922-8</article-id>
              <article-categories>
                <subj-group subj-group-type="heading">
                  <subject>Article</subject>
                </subj-group>
              </article-categories>
              <title-group>
                <article-title>Predicting optical coherence tomography-derived diabetic macular edema grades from fundus photographs using deep learning</article-title>
              </title-group>
              <contrib-group>
                <contrib contrib-type="author" equal-contrib="yes">
                  <name>
                    <surname>Varadarajan</surname>
                    <given-names>Avinash V.</given-names>
                  </name>
                  <xref ref-type="aff" rid="Aff1">1</xref>
                </contrib>
                <contrib contrib-type="author" equal-contrib="yes">
                  <name>
                    <surname>Bavishi</surname>
                    <given-names>Pinal</given-names>
                  </name>
                  <xref ref-type="aff" rid="Aff1">1</xref>
                </contrib>
                <contrib contrib-type="author" equal-contrib="yes">
                  <name>
                    <surname>Ruamviboonsuk</surname>
                    <given-names>Paisan</given-names>
                  </name>
                  <xref ref-type="aff" rid="Aff2">2</xref>
                </contrib>
                <contrib contrib-type="author">
                  <name>
                    <surname>Chotcomwongse</surname>
                    <given-names>Peranut</given-names>
                  </name>
                  <xref ref-type="aff" rid="Aff2">2</xref>
                </contrib>
                <contrib contrib-type="author">
                  <contrib-id contrib-id-type="orcid">http://orcid.org/0000-0003-3729-8456</contrib-id>
                  <name>
                    <surname>Venugopalan</surname>
                    <given-names>Subhashini</given-names>
                  </name>
                  <xref ref-type="aff" rid="Aff3">3</xref>
                </contrib>
                <contrib contrib-type="author">
                  <name>
                    <surname>Narayanaswamy</surname>
                    <given-names>Arunachalam</given-names>
                  </name>
                  <xref ref-type="aff" rid="Aff3">3</xref>
                </contrib>
                <contrib contrib-type="author">
                  <name>
                    <surname>Cuadros</surname>
                    <given-names>Jorge</given-names>
                  </name>
                  <xref ref-type="aff" rid="Aff4">4</xref>
                </contrib>
                <contrib contrib-type="author">
                  <name>
                    <surname>Kanai</surname>
                    <given-names>Kuniyoshi</given-names>
                  </name>
                  <xref ref-type="aff" rid="Aff5">5</xref>
                </contrib>
                <contrib contrib-type="author">
                  <name>
                    <surname>Bresnick</surname>
                    <given-names>George</given-names>
                  </name>
                  <xref ref-type="aff" rid="Aff4">4</xref>
                </contrib>
                <contrib contrib-type="author">
                  <contrib-id contrib-id-type="orcid">http://orcid.org/0000-0003-3337-0625</contrib-id>
                  <name>
                    <surname>Tadarati</surname>
                    <given-names>Mongkol</given-names>
                  </name>
                  <xref ref-type="aff" rid="Aff2">2</xref>
                </contrib>
                <contrib contrib-type="author">
                  <name>
                    <surname>Silpa-archa</surname>
                    <given-names>Sukhum</given-names>
                  </name>
                  <xref ref-type="aff" rid="Aff2">2</xref>
                </contrib>
                <contrib contrib-type="author">
                  <name>
                    <surname>Limwattanayingyong</surname>
                    <given-names>Jirawut</given-names>
                  </name>
                  <xref ref-type="aff" rid="Aff2">2</xref>
                </contrib>
                <contrib contrib-type="author">
                  <name>
                    <surname>Nganthavee</surname>
                    <given-names>Variya</given-names>
                  </name>
                  <xref ref-type="aff" rid="Aff2">2</xref>
                </contrib>
                <contrib contrib-type="author">
                  <name>
                    <surname>Ledsam</surname>
                    <given-names>Joseph R.</given-names>
                  </name>
                  <xref ref-type="aff" rid="Aff6">6</xref>
                </contrib>
                <contrib contrib-type="author">
                  <contrib-id contrib-id-type="orcid">http://orcid.org/0000-0002-9239-745X</contrib-id>
                  <name>
                    <surname>Keane</surname>
                    <given-names>Pearse A.</given-names>
                  </name>
                  <xref ref-type="aff" rid="Aff7">7</xref>
                </contrib>
                <contrib contrib-type="author">
                  <name>
                    <surname>Corrado</surname>
                    <given-names>Greg S.</given-names>
                  </name>
                  <xref ref-type="aff" rid="Aff1">1</xref>
                </contrib>
                <contrib contrib-type="author" corresp="yes" equal-contrib="yes">
                  <name>
                    <surname>Peng</surname>
                    <given-names>Lily</given-names>
                  </name>
                  <address>
                    <email>lhpeng@google.com</email>
                  </address>
                  <xref ref-type="aff" rid="Aff1">1</xref>
                </contrib>
                <contrib contrib-type="author" equal-contrib="yes">
                  <contrib-id contrib-id-type="orcid">http://orcid.org/0000-0002-3023-8824</contrib-id>
                  <name>
                    <surname>Webster</surname>
                    <given-names>Dale R.</given-names>
                  </name>
                  <xref ref-type="aff" rid="Aff1">1</xref>
                </contrib>
                <aff id="Aff1"><label>1</label><institution-wrap><institution-id institution-id-type="GRID">grid.420451.6</institution-id><institution>Google Health, Google, </institution></institution-wrap>Mountain View, CA USA </aff>
                <aff id="Aff2"><label>2</label><institution-wrap><institution-id institution-id-type="ISNI">0000 0000 9427 298X</institution-id><institution-id institution-id-type="GRID">grid.412665.2</institution-id><institution>Faculty of Medicine, Department of Ophthalmology, Rajavithi Hospital, College of Medicine, </institution><institution>Rangsit University, </institution></institution-wrap>2, Phayathai Road, Ratchathewi District, Bangkok, 10400 Thailand </aff>
                <aff id="Aff3"><label>3</label><institution-wrap><institution-id institution-id-type="GRID">grid.420451.6</institution-id><institution>Google Research, Google, Mountain View, </institution></institution-wrap>CA, USA </aff>
                <aff id="Aff4"><label>4</label>EyePACS LLC, Santa Cruz, CA USA </aff>
                <aff id="Aff5"><label>5</label><institution-wrap><institution-id institution-id-type="ISNI">0000 0001 2181 7878</institution-id><institution-id institution-id-type="GRID">grid.47840.3f</institution-id><institution>Meredith Morgan Eye Center, </institution><institution>University of California, </institution></institution-wrap>200 Minor Hall, Berkeley, CA 94720-2020 USA </aff>
                <aff id="Aff6"><label>6</label><institution-wrap><institution-id institution-id-type="ISNI">0000 0004 5999 1726</institution-id><institution-id institution-id-type="GRID">grid.498210.6</institution-id><institution>Deepmind, </institution></institution-wrap>London, UK </aff>
                <aff id="Aff7"><label>7</label><institution-wrap><institution-id institution-id-type="ISNI">0000 0000 9168 0080</institution-id><institution-id institution-id-type="GRID">grid.436474.6</institution-id><institution>NIHR Biomedical Research Centre for Ophthalmology, </institution><institution>Moorfields Eye Hospital NHS Foundation Trust and UCL Institute of Ophthalmology, </institution></institution-wrap>2/12 Wolfson Building, 11-43 Bath Street, London, EC1V 9EL UK </aff>
              </contrib-group>
              <pub-date pub-type="epub">
                <day>8</day>
                <month>1</month>
                <year>2020</year>
              </pub-date>
              <pub-date pub-type="pmc-release">
                <day>8</day>
                <month>1</month>
                <year>2020</year>
              </pub-date>
              <pub-date pub-type="collection">
                <year>2020</year>
              </pub-date>
              <volume>11</volume>
              <elocation-id>130</elocation-id>
              <history>
                <date date-type="received">
                  <day>7</day>
                  <month>12</month>
                  <year>2018</year>
                </date>
                <date date-type="accepted">
                  <day>4</day>
                  <month>12</month>
                  <year>2019</year>
                </date>
              </history>
              <permissions>
                <copyright-statement>© The Author(s) 2020</copyright-statement>
                <license license-type="OpenAccess">
                  <license-p><bold>Open Access</bold> This article is licensed under a Creative Commons Attribution 4.0 International License, which permits use, sharing, adaptation, distribution and reproduction in any medium or format, as long as you give appropriate credit to the original author(s) and the source, provide a link to the Creative Commons license, and indicate if changes were made. The images or other third party material in this article are included in the article’s Creative Commons license, unless indicated otherwise in a credit line to the material. If material is not included in the article’s Creative Commons license and your intended use is not permitted by statutory regulation or exceeds the permitted use, you will need to obtain permission directly from the copyright holder. To view a copy of this license, visit <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/">http://creativecommons.org/licenses/by/4.0/</ext-link>.</license-p>
                </license>
              </permissions>
              <abstract id="Abs1">
                <p id="Par1">Center-involved diabetic macular edema (ci-DME) is a major cause of vision loss. Although the gold standard for diagnosis involves 3D imaging, 2D imaging by fundus photography is usually used in screening settings, resulting in high false-positive and false-negative calls. To address this, we train a deep learning model to predict ci-DME from fundus photographs, with an ROC–AUC of 0.89 (95% CI: 0.87–0.91), corresponding to 85% sensitivity at 80% specificity. In comparison, retinal specialists have similar sensitivities (82–85%), but only half the specificity (45–50%, <italic>p</italic> &lt; 0.001). Our model can also detect the presence of intraretinal fluid (AUC: 0.81; 95% CI: 0.81–0.86) and subretinal fluid (AUC 0.88; 95% CI: 0.85–0.91). Using deep learning to make predictions via simple 2D images without sophisticated 3D-imaging equipment and with better than specialist performance, has broad relevance to many other applications in medical imaging.</p>
              </abstract>
              <abstract id="Abs2" abstract-type="web-summary">
                <p id="Par2">Diabetic eye disease is a cause of preventable blindness and accurate and timely referral of patients with diabetic macular edema is important to start treatment. Here the authors present a deep learning model that can predict the presence of diabetic macular edema from color fundus photographs with superior specificity and positive predictive value compared to retinal specialists.</p>
              </abstract>
              <kwd-group kwd-group-type="npg-subject">
                <title>Subject terms</title>
                <kwd>Diabetes</kwd>
                <kwd>Biomedical engineering</kwd>
                <kwd>Developing world</kwd>
              </kwd-group>
              <custom-meta-group>
                <custom-meta>
                  <meta-name>issue-copyright-statement</meta-name>
                  <meta-value>© The Author(s) 2020</meta-value>
                </custom-meta>
              </custom-meta-group>
            </article-meta>
          </front>
          <body>
            <sec id="Sec1" sec-type="introduction">
              <title>Introduction</title>
              <p id="Par3">Diabetic macular edema (DME) is a late stage of diabetic eye disease that is characterized by retinal thickening in the macula, often accompanied by hard exudate deposition, and resultant vision loss. It is one of the most common reasons for referrals to diabetic eye clinics and affects 3–33% of patients with diabetes<sup><xref ref-type="bibr" rid="CR1">1</xref></sup>. The wide range of prevalences reflects the varied bases for defining the condition and the varied composition of the populations studied. Currently, the first-line treatment for DME is anti-vascular endothelial growth factor (anti-VEGF) agents<sup><xref ref-type="bibr" rid="CR2">2</xref>–<xref ref-type="bibr" rid="CR4">4</xref></sup>. To determine eligibility for anti-VEGF treatment of DME, most of the major clinical trials measured macular thickening using optical coherence tomography (OCT) and initiated treatment if a patient met the criteria for a particular type of DME<sup><xref ref-type="bibr" rid="CR5">5</xref>,<xref ref-type="bibr" rid="CR6">6</xref></sup>. This type of DME is now commonly called center-involved DME (ci-DME) in clinical practice. As such, findings on OCT along with impaired visual acuity has become a widely accepted standard of care for determining DME treatment<sup><xref ref-type="bibr" rid="CR7">7</xref></sup>.</p>
              <p id="Par4">However, despite improvements in therapy, the detection of ci-DME remains a challenge, because adding OCTs to the screening process is too costly and logistically difficult to implement widely. Globally, there are 425 million patients with diabetes<sup><xref ref-type="bibr" rid="CR8">8</xref></sup> and most clinical guidelines recommend that all of them are screened annually<sup><xref ref-type="bibr" rid="CR9">9</xref></sup>. Currently, selection of patients who may meet treatment criteria is performed during these screenings, which typically utilize monoscopic fundus images. These images are then evaluated for the presence of hard exudates within one optic disc diameter of the center of the macula, a proxy for ci-DME<sup><xref ref-type="bibr" rid="CR10">10</xref></sup>. However, this proxy was developed based on an older standard of care and some studies have shown that hard exudates have both poor positive predictive value and poor sensitivity for ci-DME. MacKenzie et al.<sup><xref ref-type="bibr" rid="CR11">11</xref></sup> reported that only 42% of patients with hard exudates were found to have DME on OCT and Wang et al.<sup><xref ref-type="bibr" rid="CR12">12</xref></sup> reported that a third of patient eyes with DME detected on OCTs lacked features such as hard exudates on monoscopic fundus photographs. Wong et al.<sup><xref ref-type="bibr" rid="CR13">13</xref></sup> reported a false-positive rate of 86.6% for DME screening with existing strategies. As such, the potential of Diabetic Retinopathy (DR) screening and timely referral for DME is handicapped by an inability to reliably detect ci-DME via human evaluation of fundus photographs alone.</p>
              <p id="Par5">A potential solution lies in the use of deep-learning algorithms, which have been applied to a variety of medical image classification tasks<sup><xref ref-type="bibr" rid="CR14">14</xref>–<xref ref-type="bibr" rid="CR18">18</xref></sup>, including for retinal imaging<sup><xref ref-type="bibr" rid="CR19">19</xref>–<xref ref-type="bibr" rid="CR22">22</xref></sup>. Encouragingly, in addition to achieving expert-level performance for grading fundus images, deep-learning algorithms are able to make predictions for which the underlying association with fundus images were previously unknown, such as cardiovascular risk factors<sup><xref ref-type="bibr" rid="CR23">23</xref></sup> and refractive error<sup><xref ref-type="bibr" rid="CR24">24</xref></sup>.</p>
              <p id="Par6">We hypothesized that deep learning could be leveraged to directly predict the OCT-derived ci-DME grade using monoscopic fundus photographs. In this study we show that ci-DME can be predicted with significantly higher specificity at the same sensitivity as doctors, in two independent datasets, using deep learning. In addition, the model also predicts the presence of intraretinal and subretinal fluid, which are clinically relevant tasks. Subsampling experiments show a likely increase in accuracy when training includes additional data. Training on cropped images of increasing sizes around the fovea and optic disc demonstrate that the model is largely informed by the area around the fovea.</p>
            </sec>
            <sec id="Sec2" sec-type="results">
              <title>Results</title>
              <sec id="Sec3">
                <title>Deep learning can predict OCT features from fundus photographs</title>
                <p id="Par7">To leverage deep learning as a potential solution to reliably detect ci-DME, we propose developing a model trained on fundus photographs, but using ci-DME diagnoses derived from expert inspection of OCT as labels (Fig. <xref rid="Fig1" ref-type="fig">1</xref>). To train and validate the model, cases were gathered retrospectively from the Rajavithi Hospital in Bangkok, Thailand. As these cases were gathered from those referred into the retina clinic for further evaluation, the disease distribution is consistent with a population presenting to specialty clinics and are enriched for more severe disease as compared with a DR screening population. Details of the development and clinical validation datasets are presented in Table <xref rid="Tab1" ref-type="table">1</xref>. The development dataset consisted of 6039 images from 4035 patients and the primary clinical validation dataset consisted of 1033 images from 697 patients. For some patients, only one eye was included, because the fellow eye fell under the exclusion criteria. ci-DME was conservatively defined as center point thickness ≥250 μm measured via manual caliper measurements excluding the retinal pigment epithelium<sup><xref ref-type="bibr" rid="CR25">25</xref>,<xref ref-type="bibr" rid="CR26">26</xref></sup>. We trained a model using this development dataset to predict ci-DME using fundus photographs as input.<fig id="Fig1"><label>Fig. 1</label><caption><title>Illustration of our proposed approach for developing a ci-DME model.</title><p>Ground truth for ci-DME were derived from a human grader analyzing the OCT for each case. In addition, subretinal fluid and intraretinal fluid presence grades were also collected. These ground truth labels and corresponding color fundus photos were used for model training. For clinical validation, the trained model takes in a new fundus photo and generates a predicted ci-DME grade, predicted subretinal fluid, and intraretinal fluid presence grades.</p></caption><graphic xlink:href="41467_2019_13922_Fig1_HTML" id="d29e499"/></fig><table-wrap id="Tab1"><label>Table 1</label><caption><p>Baseline characteristics of the development and clinical validation datasets.</p></caption><table frame="hsides" rules="groups"><thead><tr><th>Characteristics</th><th>Development set</th><th>Primary clinical validation set</th><th>Secondary clinical validation set</th></tr></thead><tbody><tr><td>Dataset</td><td>Thailand dataset</td><td>Thailand dataset</td><td>EyePACS-DME dataset</td></tr><tr><td>Number of patients</td><td>4035</td><td>697</td><td>554</td></tr><tr><td>Number of fundus images</td><td>6039</td><td>1033</td><td>990</td></tr><tr><td>Camera used for fundus images</td><td>Kowa VX-10</td><td>Kowa VX-10</td><td>Canon CR-DGi</td></tr><tr><td>OCT device used for determining ci-DME</td><td>Heidelberg Spectralis</td><td>Heidelberg Spectralis</td><td>Optovue iVue</td></tr><tr><td>Age: mean, years (SD)</td><td><p>55.6 (10.8)</p><p><italic>n</italic> = 6038</p></td><td><p>55.8 (10.8)</p><p><italic>n</italic> = 1033</p></td><td><p>62.0 (9.8)</p><p><italic>n</italic> = 990</p></td></tr><tr><td>Gender (% male)</td><td><p>60.8%</p><p><italic>n</italic> = 6036</p></td><td><p>62.4%</p><p><italic>n</italic> = 1031</p></td><td><p>50.1%</p><p><italic>n</italic> = 990</p></td></tr><tr><td>Central retinal thickness: mean, μm (SD)</td><td><p>263.8 (146.5)</p><p><italic>n</italic> = 6039</p></td><td><p>258.4 (132.8)</p><p><italic>n</italic> = 1033</p></td><td><p>254.4 (56.3)</p><p><italic>n</italic> = 990</p></td></tr><tr><td>ci-DME, Center Point Thickness ≥ 250 μm in Thailand dataset. Central Subfield Thickness ≥ 300 μm in the Eyepacs-DME dataset</td><td><p>28.3%</p><p><italic>n</italic> = 6039</p></td><td><p>27.2%</p><p><italic>n</italic> = 1033</p></td><td><p>7.8%</p><p><italic>n</italic> = 990</p></td></tr><tr><td>Subretinal fluid presence</td><td><p>15.7%</p><p><italic>n</italic> = 6039</p></td><td><p>15.1%</p><p><italic>n</italic> = 1033</p></td><td>NA</td></tr><tr><td>Intraretinal fluid presence</td><td><p>45.5%</p><p><italic>n</italic> = 6039</p></td><td><p>46.3%</p><p><italic>n</italic> = 1033</p></td><td>NA</td></tr></tbody></table><table-wrap-foot><p>It is noteworthy that the difference between total <italic>n</italic> and subcategories is missing data (e.g., not all images had age or sex)</p></table-wrap-foot></table-wrap></p>
              </sec>
              <sec id="Sec4">
                <title>Model predicts OCT-based DME features better than retinal specialists</title>
                <p id="Par8">Our model showed a higher performance in detecting cases with and without ci-DME from monoscopic fundus images compared with manual grading of fundus images (Table <xref rid="Tab2" ref-type="table">2</xref> and Fig. <xref rid="Fig2" ref-type="fig">2</xref>). For ci-DME, the model had a sensitivity of 85% at a specificity of 80%. Three retinal specialists had sensitivities ranging from 82% to 85% at specificities ranging from 45% to 50% (Supplementary Table <xref rid="MOESM1" ref-type="media">1</xref>). The performance improvements held true even if other common criteria for calling DME for monoscopic images were used (Supplementary Fig. <xref rid="MOESM1" ref-type="media">1</xref>), such as changing the definition of DME based on the location of the hard exudates. Additional analyses were also performed at other thickness thresholds for ci-DME at center point thickness ≥ 280 μm and ≥ 300 μm, which showed similar or better results compared with the conservative ≥ 250 μm cutoff point without model retraining (Supplementary Fig. <xref rid="MOESM1" ref-type="media">2</xref>). When compared with manual grading, our model had a 30–35% absolute higher specificity at the same sensitivity (<italic>p</italic> &lt; 0.001 for comparison with each retinal specialist). When matched to have the same specificity, the model had a 11–14% absolute higher sensitivity (96% vs. 82–85%, <italic>p</italic> &lt; 0.001 for all comparisons).<table-wrap id="Tab2"><label>Table 2</label><caption><p>Performance metrics of the model and retinal specialists on the primary clinical validation set.</p></caption><table frame="hsides" rules="groups"><thead><tr><th>Metric</th><th>Model</th><th>Specialist 1</th><th>Specialist 2</th><th>Specialist 3</th></tr></thead><tbody><tr><td>Positive predictive value (%), 95% CI</td><td><p>61% [56–66%]</p><p><italic>n</italic> = 1033</p></td><td><p>37% [33–40%]</p><p><italic>n</italic> = 1004</p></td><td><p>36% [33–40%]</p><p><italic>n</italic> = 987</p></td><td><p>38% [34–42%]</p><p><italic>n</italic> = 1001</p></td></tr><tr><td>Negative predictive value (%), 95% CI</td><td><p>93% [91–95%]</p><p><italic>n</italic> = 1033</p></td><td><p>88% [85–91%]</p><p><italic>n</italic> = 1004</p></td><td><p>89% [85–92%]</p><p><italic>n</italic> = 987</p></td><td><p>88% [84–91%]</p><p><italic>n</italic> = 1001</p></td></tr><tr><td>Sensitivity (%), 95% CI</td><td><p>85% [80–89%]</p><p><italic>n</italic> = 1033</p></td><td><p>84% [80–89%]</p><p><italic>n</italic> = 1004</p></td><td><p>85% [80–89%]</p><p><italic>n</italic> = 987</p></td><td><p>82% [77–86%]</p><p><italic>n</italic> = 1001</p></td></tr><tr><td>Specificity (%), 95% CI</td><td><p>80% [77–82%]</p><p><italic>n</italic> = 1033</p></td><td><p>45% [41–48%]</p><p><italic>n</italic> = 1004</p></td><td><p>45% [41–48%]</p><p><italic>n</italic> = 987</p></td><td><p>50% [47–54%]</p><p><italic>n</italic> = 1001</p></td></tr><tr><td>Accuracy (%), 95% CI</td><td><p>81% [79–83%]</p><p><italic>n</italic> = 1033</p></td><td><p>56% [52–59%]</p><p><italic>n</italic> = 1004</p></td><td><p>56% [52–59%]</p><p><italic>n</italic> = 987</p></td><td><p>59% [56–62%]</p><p><italic>n</italic> = 1001</p></td></tr><tr><td>Cohen’s Kappa, 95% CI</td><td><p>0.57 [0.52–0.62]</p><p><italic>n</italic> = 1033</p></td><td><p>0.21 [0.16–0.25]</p><p><italic>n</italic> = 1004</p></td><td><p>0.21 [0.16–0.25]</p><p><italic>n</italic> = 987</p></td><td><p>0.24 [0.19–0.28]</p><p><italic>n</italic> = 1001</p></td></tr></tbody></table><table-wrap-foot><p>For the model we chose an operating point that matched the sensitivity of the retinal specialists to calculate the metrics. The performance metrics for the model were calculated on the entire primary clinical validation set; for the retinal specialists it was calculated only on the images that they marked as gradable. Brackets denote 95% confidence intervals. <italic>n</italic> = number of images</p></table-wrap-foot></table-wrap><fig id="Fig2"><label>Fig. 2</label><caption><title>Receiver operating characteristic curve of the model compared to retinal specialists.</title><p>The retinal specialists’ grades for predicting ci-DME on the primary clinical validation set are shown as red dots. All methods (i.e., the model and retinal specialists) rendered their grades using monoscopic fundus images only. The ground truth for ci-DME was derived using OCT (center point thickness ≥ 250 μm).</p></caption><graphic xlink:href="41467_2019_13922_Fig2_HTML" id="d29e1006"/></fig></p>
                <p id="Par9">In addition to predicting ci-DME, our model was able to predict presence of intraretinal and subretinal fluid. Our model had an area under the curve (AUC) of 0.81 (95% confidence interval (CI): 0.81–0.86) for detecting intraretinal fluid presence and an AUC of 0.88 (95% CI: 0.85–0.91) for subretinal fluid presence (Fig. <xref rid="Fig3" ref-type="fig">3</xref>).<fig id="Fig3"><label>Fig. 3</label><caption><title>Effect of data size on prediction performance on the primary clinical validation set.</title><p>A subsampled fraction of 1.0 indicates the entire training dataset. Model performance continues to increase with increased data, suggesting that the accuracy of predicting ci-DME, subretinal fluid, and intraretinal fluid presence will likely improve if the model is trained with more data. Error bars are 95% confidence interval using <italic>n</italic> = 2000 samples with replacement.</p></caption><graphic xlink:href="41467_2019_13922_Fig3_HTML" id="d29e1025"/></fig></p>
              </sec>
              <sec id="Sec5">
                <title>Model generalizes to a secondary validation set</title>
                <p id="Par10">In addition to the primary clinical validation dataset, the model was also applied to a secondary validation dataset, EyePACS-DME, to examine the model’s generalizability. This dataset consists of 990 images with moderate, severe non-proliferative DR or proliferative DR, a subset of data previously gathered during another DME study<sup><xref ref-type="bibr" rid="CR27">27</xref></sup>. The images were gathered using a Canon CR-DGi camera and OCTs were taken with a Optovue iVue machine from a US-based population (see Methods). There are some notable differences in this dataset in comparison with the primary validation dataset, particularly in terms of defining and measuring ci-DME based on central subfield thickness and incorporation of inclusion/exclusion criteria (Supplementary Table <xref rid="MOESM1" ref-type="media">2</xref>). Based on this different definition and inclusion criteria, the number of ci-DME cases in the secondary validation set was 7.8% compared with 27.2% in the primary clinical validation set. Thus, the model performance on the datasets cannot be compared directly in terms of absolute values (especially for metrics such as positive predictive value (PPV), which depend a lot on the priori distribution). However, relative comparisons between the model and graders (in this instance EyePACS certified graders) can be drawn (Fig. <xref rid="Fig4" ref-type="fig">4</xref> and Table <xref rid="Tab3" ref-type="table">3</xref>). Similar to the results of the primary validation, our model had a PPV roughly twice that of manual grading using hard exudates as proxy (35% [95% CI: 27–44%] vs. 18% [95% CI: 13–23%]) and similar negative predictive value (NPV) (96% [95% CI: 95–98%] vs. 95% [95% CI: 94–97%]). This translated to a similar sensitivity (57% [95% CI: 47–69%] vs. 55% [43–66%]) but higher specificity (91% [95% CI: 89–93%] vs. 79% [95% CI: 76–82%]).<fig id="Fig4"><label>Fig. 4</label><caption><title>Receiver operating characteristic curve of the model compared with eyepacs graders.</title><p>The graders’ grades for predicting ci-DME on the secondary clinical validation set are shown as a red dots. All methods (i.e., the model and eyepacs graders) rendered their grades using monoscopic fundus images only. The ground truth for ci-DME was derived using OCT (central subfield thickness ≥ 300 μm).</p></caption><graphic xlink:href="41467_2019_13922_Fig4_HTML" id="d29e1054"/></fig><table-wrap id="Tab3"><label>Table 3</label><caption><p>Performance metrics of the model and eyepacs graders on the secondary clinical validation set.</p></caption><table frame="hsides" rules="groups"><thead><tr><th>Metric</th><th>Model</th><th>EyePACS Graders</th></tr></thead><tbody><tr><td>Positive predictive value (%), 95% CI</td><td>35% [27–44%]</td><td>18% [13–23%]</td></tr><tr><td>Negative predictive value (%), 95% CI</td><td>96% [95–98%]</td><td>95% [94–97%]</td></tr><tr><td>Sensitivity (%), 95% CI</td><td>57% [47–69%]</td><td>55% [43–66%]</td></tr><tr><td>Specificity (%), 95% CI</td><td>91% [89–93%]</td><td>79% [76–82%]</td></tr><tr><td>Accuracy (%), 95% CI</td><td>88% [86–91%]</td><td>77% [74–80%]</td></tr><tr><td>Cohen’s Kappa, 95% CI</td><td>0.38 [0.29–0.47]</td><td>0.17 [0.11–0.24]</td></tr></tbody></table><table-wrap-foot><p>For the model we chose an operating point that matched the sensitivity of the eyepacs graders to calculate the metrics. Brackets denote 95% confidence intervals. <italic>n</italic> = 990 images for all calculations</p></table-wrap-foot></table-wrap></p>
              </sec>
              <sec id="Sec6">
                <title>More data leads to better model performance</title>
                <p id="Par11">Sub sampling experiments, where new models were trained using titrated fractions of the dataset, showed that model performance continued to increase with larger training sets (see Fig. <xref rid="Fig3" ref-type="fig">3</xref>—where AUC increases with sample size). These results suggest that the accuracy of this prediction will likely continue to increase with dataset sizes larger than that in this study.</p>
              </sec>
              <sec id="Sec7">
                <title>Features around the fovea are most relevant</title>
                <p id="Par12">Figure <xref rid="Fig5" ref-type="fig">5</xref> presents an analysis of the areas in the fundus image relevant for the model. When the model was trained on cropped fundus images containing only 0.25 optic disc diameter around the fovea (blue line), it achieved an AUC of 0.75. When it had access to 1.0 optic disc diameter around the fovea, the model achieved an AUC &gt; 0.85, comparable with its performance on the full fundus image. However, the model trained on the region around the optic disc needed to see a lot more context (2.5 optic disc diameter) around the optic disc center to achieve an AUC exceeding 0.8. Based on these results, we believe the model primarily utilizes the area around the fovea to make ci-DME predictions.<fig id="Fig5"><label>Fig. 5</label><caption><title>Model performance on crops around fovea and optic disk.</title><p>The plot shows model performance in predicting ci-DME on the primary clinical validation set, as measured by AUC when cropped circular images are used to train and validate the model. The blue line indicates the performance when cropped circular images of different sizes (radius of multiples of disc diameter from 0.05 to 2.5) centered at the fovea are used; the green line indicates the corresponding performance when the crops are centered at the optic disc. (Inset) Image depicting some of the regions of different radii (0.05, 0.125, 0.25, 0.5, and 1 disc diameter) around the fovea and optic disc. Bottom panel: Fundus image, followed by a sample of the crops extracted by centering at the optic disc (green) and fovea (blue), with the extraction radius in multiples of disc diameter indicated in each crop. Error bars are 95% confidence interval using <italic>n</italic> = 2000 samples with replacement.</p></caption><graphic xlink:href="41467_2019_13922_Fig5_HTML" id="d29e1173"/></fig></p>
              </sec>
            </sec>
            <sec id="Sec8" sec-type="discussion">
              <title>Discussion</title>
              <p id="Par13">Our previous work has shown that deep learning can be leveraged to make predictions from fundus photographs, such as cardiovascular risk factors and refractive error, which are not possible by human experts<sup><xref ref-type="bibr" rid="CR23">23</xref>,<xref ref-type="bibr" rid="CR24">24</xref></sup>. This study describes a model that far exceeds expert performance for such a prediction, but one that has high clinical relevance and potentially important implications for screening programs worldwide. The resultant model performed significantly better than retinal specialists for detecting ci-DME from fundus images in two datasets from very different populations. DME is the major cause of visual loss from DR. Prior to the use of anti-VEGF injections, the Early Treatment of Diabetic Retinopathy Study (ETDRS) showed that treatment of a subtype of DME with focal laser photocoagulation decreased the chance of vision loss<sup><xref ref-type="bibr" rid="CR28">28</xref></sup>. Today, with anti-VEGF injections, the treatment of ci-DME can improve vision by ~10–13 letters as measured using the ETDRS visual acuity chart<sup><xref ref-type="bibr" rid="CR5">5</xref></sup>. Anti-VEGF injections are now largely considered the gold standard of care with evidence that shows that delaying treatment of DME could lead to suboptimal visual improvement<sup><xref ref-type="bibr" rid="CR29">29</xref></sup>. However, the current grading guidelines in screening programs were developed before the advent of anti-VEGF therapy and are not specifically designed for detecting ci-DME. The development of models that can better detect ci-DME in DR screening programs using existing equipment (color fundus cameras) is both scientifically interesting and clinically impactful.</p>
              <p id="Par14">For DR screening in particular, our model may lead to fewer false negatives for DME. Decreasing missed referrals for patients with ci-DME presenting with no hard exudates is a clear advantage of such a system. Visual acuity alone is not enough to rule out ci-DME, as baseline characteristics from some well-known cohorts suggest that a substantial percentage of eyes with ci-DME still have good vision<sup><xref ref-type="bibr" rid="CR30">30</xref>,<xref ref-type="bibr" rid="CR31">31</xref></sup>. In addition, decreasing false positives is also important in resource-constrained settings. Although many screening programs recommend closer follow-up for patients with mild or worse DR, the urgency of follow-up varies widely, especially in low resource settings. Per international guidelines (International Council of Ophthalmology Guidelines, American Academy of Ophthalmology), for patients with mild DR and no macular edema, referral is not always required and patients can be rescreened in 1–2 years in low/intermediate resource settings and 6–12 months in high resource settings. However, patients with suspected ci-DME need to be referred within a month. For patients with moderate non-proliferative DR and no macular edema, follow-up changes from 6 to 12 months in low/intermediate resource or from 3 to 6 months in high resource settings to 1 month (all resource settings) when there is ci-DME<sup><xref ref-type="bibr" rid="CR32">32</xref>,<xref ref-type="bibr" rid="CR33">33</xref></sup>. In this study, roughly 88% of the moderate non-proliferative patients from the EyePACS-DME dataset and 77% of those from the Thailand dataset, who would have been referred urgently (and unnecessarily) using a hard exudate-based referral criterion did not have ci-DME. Higher urgency referral of patients with moderate non-proliferative DR without DME (but presenting hard exudates) can be a major issue where there are limited resources for evaluation and treatment.</p>
              <p id="Par15">Furthermore, the center point thickness distribution of the false-positive and false-negative instances is better for the model when compared with retina specialists (Supplementary Fig. <xref rid="MOESM1" ref-type="media">3</xref>). For the model, 28% of the false positives have thickness &gt;225 μm and 35% of the false negatives have thickness &lt;275 μm. In comparison, for retina specialists, only 20% of the false positives have thickness &gt;225 μm and 17% of the false negatives have thickness &lt;275 μm. This shows that a significantly larger fraction of the model false positives and negatives are borderline cases as compared with retina specialists. In addition, the new model seems to be able to detect the presence of intraretinal and/or subretinal fluid, both of which merit closer monitoring and possibly treatment<sup><xref ref-type="bibr" rid="CR34">34</xref></sup>. The ability to detect these pathologies is not a task that doctors can do accurately from fundus images.</p>
              <p id="Par16">Although the performance of the models on the secondary dataset is lower than that of the models on the primary dataset, the performance of the human graders on the secondary dataset is proportionally lower as well. From the primary to secondary dataset, PPV of the model decreased from 61% to 35%, whereas of the graders decreased from 37% to 18%; sensitivity of the model decreased from 85% to 57%, whereas of the graders decreased from 84% to 55%. However, NPV of the model increased from 93% to 96%, whereas of the graders increased from 88% to 95%; specificity of the model increased from 80% to 91%, whereas of the graders increased from 47% to 79%. These results reflect the inherent differences between the two datasets but still support the better performance of the model over graders on both datasets. Although the models trained in this study are more accurate than manual grading and the low PPVs are not inconsistent with what has been reported for other applications in the literature<sup><xref ref-type="bibr" rid="CR35">35</xref>,<xref ref-type="bibr" rid="CR36">36</xref></sup>, there is capacity for improvement. Given the results of the subsampling experiments, it is likely that the accuracy of the model may continue to increase with larger dataset sizes.</p>
              <p id="Par17">From a scientific point of view, this work demonstrates the potential of deep learning to enable diagnostics from inexpensive hardware, which was only previously possible from expensive equipment. It also lays the groundwork for understanding how the model makes these predictions. The explanation technique employed in this study indicated that the region around the fovea is more relevant than the region near the optic disc for DME prediction from fundus images. Future work could involve diving deeper into the features around this area that is picked up by deep learning but overlooked by retinal specialists.</p>
              <p id="Par18">In a small non-randomized study, Scott et al.<sup><xref ref-type="bibr" rid="CR37">37</xref></sup> showed a beneficial effect of focal and grid laser for eyes without central involvement that meet an older criteria for treatment known as clinically significant macular edema, similar to the initial ETDRS findings. These patients need to be referred from a DR screening program for closer follow-up. Our model does not evaluate such cases. To address this, one would include stereoscopic imaging in addition to OCT as ground truth to train model(s) to specifically identify these cases. Although there is some evidence of generalization to a secondary dataset, the confidence intervals are wide and the criteria for ci-DME for the EyePACS-DME dataset were different from those of the Thailand dataset. Some of the performance metrics reported in this study such as PPV and NPV are relevant only to populations whose severity distribution is similar to that of this study (e.g., patients referred to specialist clinics). Further studies should validate the model on additional larger datasets from other settings, including screening settings from other regions or geographies. Future studies should also include better standardization for ci-DME and inclusion/exclusion criteria, as well as sub-analysis of patients who were treated for DME. Moreover, additional data diversity such as the use of ci-DME labels derived from other OCT devices by other manufacturers should be included in future work. As the model was trained using treatment-naive fundus images, training on multiple images per eye (including with stereo pairs), and on eyes that have been treated for DME in the past could lead to better model performance. Although our cropping experiments (Fig. <xref rid="Fig5" ref-type="fig">5</xref>) show that the model looks at the region around the fovea for predicting ci-DME, future work could further explore interpretability of the model<sup><xref ref-type="bibr" rid="CR37">37</xref></sup>. Lastly, future work could also include health economic analysis to study the cost-effectiveness of such an approach.</p>
              <p id="Par19">Nevertheless, this study demonstrates that deep learning can be leveraged to identify the presence of ci-DME using the cheaper and more widely available fundus photograph, at an accuracy exceeding that of manual grading using expert-derived rules. Similar approaches could be particularly valuable for other medical images, such as using radiographs or low-dose computed tomography to detect conditions that would otherwise require more expensive imaging techniques that expose patients to higher radiation doses. Importantly, we also use crops around the fovea and optic disc to explain how the model is making these predictions, lending confidence that the predictions will generalize to new unseen datasets.</p>
            </sec>
            <sec id="Sec9">
              <title>Methods</title>
              <sec id="Sec10">
                <title>Ethics approvals</title>
                <p id="Par20">This study was approved by the Ethics Committees or Institutional Review Boards of hospitals or health centers where retinal images of patients with diabetes were used in this study, including the Rajavithi Hospital (Bangkok, Thailand), Alameda Health Service (Alameda, CA, USA), and the University of California, Berkeley (Berkeley, CA, USA) in accordance with the Declaration of Helsinki. Patients gave informed consent allowing their retinal images to be used. This study was registered in the Thai Clinical Trials Registry, Registration Number TCTR20180818002.</p>
              </sec>
              <sec id="Sec11">
                <title>Datasets</title>
                <p id="Par21">For algorithm development, 7072 images were gathered retrospectively from diabetic patients presenting to the retina clinic at Rajavithi Hospital in Bangkok, Thailand, from January 2010 to February 2018. Only cases that were naive to treatment (both intravitreal injections and lasers) were included. Cases where macular lesions may have hyporeflective spaces on OCT, such as Macular Telangiectasia Type 2, may interfere with the diagnosis of DME, such as idiopathic epimacular membrane, macular edema from other causes, or proliferative DR with neovascular membrane affecting the macula, were excluded from analysis.</p>
                <p id="Par22">Retinal fundus images were obtained using Kowa color fundus camera (VX-10 model, Kowa, Aichi, Japan). A single macula-centered color fundus photograph per eye was used in the study. If available, imaging from both eyes were included. OCTs were obtained using the Heidelberg Spectralis OCT (Heidelberg Engineering GmbH, Germany) and thickness measurements were measured manually (see below for measurement procedures).</p>
                <p id="Par23">Of the 7072 images in the dataset, 6039 were used for development, whereas 1033 were set aside for clinical validation. All images from a patient was present in either in development or validation sets, but not both. Fundus photographs in the validation set were manually graded by US board-certified retinal specialists to assess the presence and location of hard exudates (yes, no, ungradable, within 500 μm or 1 disc diameter or 2 disc diameters from the center of the macula) and focal laser scars. In addition, retinal specialists provided their best clinical judgment of the presence of DME that took into account all the pathology present in the image.</p>
                <p id="Par24">To study generalizability of the model, the algorithm was applied to another dataset, EyePACS-DME, which is a subset of data that had been previously gathered for another DME study<sup><xref ref-type="bibr" rid="CR27">27</xref></sup>. This dataset consisted of 990 macula-centered images from 554 patients with at least moderate DR based on grading by certified EyePACS graders (to roughly match the population of those who would be presenting to a retina clinic). No other exclusion criteria were applied to this dataset (e.g., exclusion of epiretinal membrane, etc). Fundus images were taken with a Canon CR-DGi camera (Ōta, Tokyo, Japan) and OCTs were taken with a Optovue iVue machine (Fremont, CA, USA).</p>
              </sec>
              <sec id="Sec12">
                <title>Measurement and assessment of OCT scans</title>
                <p id="Par25">For the Thailand dataset, central subfield thickness, the value representing the thickness of the center of the macula in clinical trials for DME<sup><xref ref-type="bibr" rid="CR26">26</xref></sup>, was not available in all eyes in the developmental dataset; therefore, center point thickness, which was found to have high correlation with the central subfield thickness<sup><xref ref-type="bibr" rid="CR25">25</xref></sup>, was measured for each eye to represent the thickness of the center of the macula.</p>
                <p id="Par26">The center point thickness of an eye of a patient was manually measured on the axis of the OCT scan where there was a slight elevation of the ellipsoid zone and the gap between the photoreceptor layer outer segment tip and the ellipsoid zone was the widest, indicating the center of the fovea where the cone cell density is the highest. Manual measurement was conducted using the straight-line measurement vector available with the Spectralis Eye Explorer software. The vector was put perpendicular to the highly reflective band of retinal pigment epithelium with one side of the vector rested on the highly reflective line of cone outer segment tip and the other side on the internal limiting membrane. Retinal pigment epithelium thickness was not included in this measurement. Intraretinal fluid was defined as present when a cystoid space of hypo-reflectivity was found within 500 μm of the foveal center of any OCT scans of a patient. Subretinal fluid was defined as present when a space of hypo-reflectivity was found between the retina and retinal pigment epithelium within 500 μm of the foveal center of any OCT scans of a patient.</p>
                <p id="Par27">The measurement of center point thickness and the assessment of presence of intraretinal fluid and subretinal fluid were conducted by two medical doctors experienced in clinical research and supervised by retinal specialists. Five percent of patients were randomly selected to confirm all three measurements by a retinal specialist with 20 years of post-certification experience.</p>
                <p id="Par28">Eyes were divided into cases of no ci-DME and ci-DME. ci-DME was conservatively defined as eyes with ≥250 μm center point thickness, excluding the retinal pigment epithelium based upon manual measurement<sup><xref ref-type="bibr" rid="CR25">25</xref>,<xref ref-type="bibr" rid="CR26">26</xref></sup>. In addition to ci-DME, we also trained the model in a multi-task fashion to predict subretinal fluid and intraretinal fluid (details below). Although cases with subretinal fluid and intraretinal fluid were not strictly included in the criteria in the clinical anti-VEGF trials for DME, referral for follow-up is warranted for these cases.</p>
                <p id="Par29">For the EyePACS-DME dataset, the manufacturer’s automated segmentation algorithm was used to measure central subfield thickness. A cutoff of 300 μm central subfield thickness was used as the cutoff point for ci-DME based on machine-specific adjustments<sup><xref ref-type="bibr" rid="CR38">38</xref></sup>. The presence of intraretinal and subretinal fluid were not available in this dataset.</p>
              </sec>
              <sec id="Sec13">
                <title>Model</title>
                <p id="Par30">Our deep learning algorithm for predicting ci-DME was built using the methods described by Gulshan et al.<sup><xref ref-type="bibr" rid="CR19">19</xref></sup>, using the Inception-v3<sup><xref ref-type="bibr" rid="CR39">39</xref></sup> neural network architecture. Briefly, we used a convolutional neural network<sup><xref ref-type="bibr" rid="CR40">40</xref></sup> to predict ci-DME (center point thickness ≥250 μm), subretinal fluid presence and intraretinal fluid presence in a multi-task manner. The input to the neural network was a color fundus photograph and the output was a real-valued number between 0 and 1 for each prediction, indicating its confidence. For other hyperparameter and training details see Supplementary Methods.</p>
                <p id="Par31">The parameters of the neural network were determined by training it on the fundus images and OCT-derived ci-DME grades in the development dataset. Repeatedly, the model was given a fundus image with a known output as determined by a grader looking at the patient’s corresponding OCT. The model predicted its confidence in the output, gradually adjusting its parameters over the course of the training process to become more accurate. Itis noteworthy that the model never sees the actual OCT image during training or validation.</p>
              </sec>
              <sec id="Sec14">
                <title>Evaluating the algorithm</title>
                <p id="Par32">To evaluate the performance of the model, we used the receiver operating characteristic (ROC) curve and calculated the AUC. The performance of the retinal specialists was marked by points on this curve, indicating their sensitivity and specificity (Fig. <xref rid="Fig2" ref-type="fig">2</xref>). The same model was also evaluated at increasing thresholds of thickness for ci-DME, without retraining (Supplementary Fig. <xref rid="MOESM1" ref-type="media">2</xref>). By choosing an operating point on the ROC curve that makes the model’s specificity match that of retinal specialists, we also evaluated the model using Sensitivity, Specificity, PPV, NPV, Accuracy, and Cohen’s Kappa score<sup><xref ref-type="bibr" rid="CR41">41</xref></sup> (Table <xref rid="Tab2" ref-type="table">2</xref>).</p>
              </sec>
              <sec id="Sec15">
                <title>Statistical analysis</title>
                <p id="Par33">To assess the statistical significance of these results, we used the non-parametric bootstrap procedure: from the validation set of <italic>N</italic> images, sample <italic>N</italic> images with replacement and evaluate the model on this sample. By repeating this sampling and evaluation 2000 times, we obtain a distribution of the performance metric (e.g., AUC) and report the 2.5 and 97.5 percentiles as 95% confidence intervals. For statistical comparisons, the two-tailed paired permutation test was used with 2000 random permutations<sup><xref ref-type="bibr" rid="CR42">42</xref></sup>.</p>
              </sec>
              <sec id="Sec16">
                <title>Model explanation</title>
                <p id="Par34">We performed two experiments to determine which regions in a fundus image are most informative of DME. We focussed on two regions, the macula and the optic disc. First, a group comprising ophthalmologists and optometrists manually marked the fovea and disc for all images in the Thailand dataset. We then trained and evaluated our model looking only at the region that is within a factor of optic disc diameters around the fovea (or equivalently the optic disc) with the rest of the fundus blacked out. We trained and evaluated different models for different radii, increasing the area that the model looks at to understand the importance of these regions in making the prediction (Fig. <xref rid="Fig5" ref-type="fig">5</xref>).</p>
              </sec>
              <sec id="Sec17">
                <title>Reporting summary</title>
                <p id="Par35">Further information on research design is available in the <xref rid="MOESM2" ref-type="media">Nature Research Reporting Summary</xref> linked to this article.</p>
              </sec>
            </sec>
            <sec sec-type="supplementary-material">
              <title>Supplementary information</title>
              <sec id="Sec18">
                <p>
                  <supplementary-material content-type="local-data" id="MOESM1">
                    <media xlink:href="41467_2019_13922_MOESM1_ESM.pdf">
                      <caption>
                        <p>Supplementary Information</p>
                      </caption>
                    </media>
                  </supplementary-material>
                  <supplementary-material content-type="local-data" id="MOESM2">
                    <media xlink:href="41467_2019_13922_MOESM2_ESM.pdf">
                      <caption>
                        <p>Reporting Summary</p>
                      </caption>
                    </media>
                  </supplementary-material>
                </p>
              </sec>
            </sec>
          </body>
          <back>
            <app-group>
              <app id="App1">
                <sec id="Sec19">
                  <title>Source data</title>
                  <p id="Par38">
                    <media position="anchor" xlink:href="41467_2019_13922_MOESM3_ESM.xlsx" id="MOESM3">
                      <caption>
                        <p>Source Data</p>
                      </caption>
                    </media>
                  </p>
                </sec>
              </app>
            </app-group>
            <fn-group>
              <fn>
                <p><bold>Peer review information</bold><italic>Nature Communications</italic> thanks Emily Y. Chew and the other anonymous reviewer for their contribution to the peer review of this work.</p>
              </fn>
              <fn>
                <p><bold>Publisher’s note</bold> Springer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations.</p>
              </fn>
              <fn>
                <p>These authors contributed equally: Avinash V. Varadarajan, Pinal Bavishi, Paisan Ruamviboonsuk.</p>
              </fn>
              <fn>
                <p>These authors jointly supervised this work: Lily Peng, Dale R. Webster.</p>
              </fn>
            </fn-group>
            <sec>
              <title>Supplementary information</title>
              <p><bold>Supplementary information</bold> is available for this paper at 10.1038/s41467-019-13922-8.</p>
            </sec>
            <ack>
              <title>Acknowledgements</title>
              <p>We thank Dr Nitee Ratprasatporn, Dr Withawat Sapthanakorn, Dr Vorarit Jinaratana, Dr Yun Liu, Dr Naama Hammel, Ashish Bora, Anita Misra, Dr Ali Zaidi, Dr Courtney Crawford, and Dr Jesse Smith for their assistance in reviewing the manuscript, image grading, and data collection. P.A.K. is supported by an NIHR Clinician Scientist Award (NIHR-CS-2014-14-023). The views expressed are those of the author and not necessarily those of the NHS, the NIHR, or the United Kingdom (UK) Department of Health.</p>
            </ack>
            <notes notes-type="author-contribution">
              <title>Author contributions</title>
              <p>Conception or design of the work: P.B., A.V.V., P.R., A.N., J.C., K.K., G.B., P.A.K., L.P., and D.R.W. Acquisition, analysis, or interpretation of data: A.V.V., P.B., P.R., P.C., P.S.V., A.N., M.T., S.S., J.L., V.N., J.C., K.K., G.B., J.R.L., G.S.C., L.P., and D.R.W. Drafting the work or revising it critically: A.V.V., P.B., P.R., P.C., P.S.V., A.N., M.T., S.S., J.L., V.N., J.C., K.K., G.B., J.R.L., G.S.C., P.A.K., L.P., D.R.W., and P.B. A.V.V., P.R. and L.P. take full responsibility for the work as a whole, including the study design, access to data, and the decision to submit and publish the manuscript.</p>
            </notes>
            <notes notes-type="data-availability">
              <title>Code availability</title>
              <p>We make use of the machine learning framework TensorFlow (<ext-link ext-link-type="uri" xlink:href="https://github.com/tensorflow/tensor-flow">https://github.com/tensorflow/tensor-flow</ext-link>) along with the TensorFlow library Slim (<ext-link ext-link-type="uri" xlink:href="https://github.com/google-research/tf-slim">https://github.com/google-research/tf-slim</ext-link>), which provides an implementation of the Inception-V3 architecture (<ext-link ext-link-type="uri" xlink:href="https://github.com/google-research/tf-slim/blob/master/tf_slim/nets/inception_v3.py">https://github.com/google-research/tf-slim/blob/master/tf_slim/nets/inception_v3.py</ext-link>). Our experimental framework makes use of proprietary libraries and we are unable to publicly release this code. We detail the experiments and implementation details, including the details of data augmentation, model architecture, hyperparameters, and weights initialization used, in the Methods and Supplementary Information, to allow for independent replication.</p>
            </notes>
            <notes notes-type="data-availability">
              <title>Data availability</title>
              <p>Data are available from the corresponding author(s) upon reasonable request. Restrictions apply to the sharing of patient data that support the findings of this study. This data may be made available to qualified researchers upon ethical approvals from Rajavithi Hospital and EyePACS. The source data underlying Figs. <xref rid="MOESM3" ref-type="media">2</xref>–<xref rid="MOESM3" ref-type="media">5</xref>, Tables <xref rid="MOESM3" ref-type="media">1</xref> and <xref rid="MOESM3" ref-type="media">2</xref>, and Supplementary Figs. <xref rid="MOESM3" ref-type="media">1</xref>–<xref rid="MOESM3" ref-type="media">3</xref> are provided as a Source Data file.</p>
            </notes>
            <notes notes-type="COI-statement">
              <title>Competing interests</title>
              <p id="Par36">A.V.V., P.B., P.S.V., A.N., J.R.L., G.S.C., L.P., and D.R.W. are employees of Alphabet. P.A.K. is a paid contractor of DeepMind. P.A.K. has received speaker fees from Heidelberg Engineering, Topcon, Haag-Streit, Allergan, Novartis, and Bayer. P.A.K. has served on advisory boards for Novartis and Bayer, and is an external consultant for DeepMind and Optos. J.C. and G.B. have received research support from Alphabet. P.R., P.C., M.T., S.S., J.L., V.N., and K.K. declare no competing interests.</p>
            </notes>
            <ref-list id="Bib1">
              <title>References</title>
              <ref id="CR1">
                <label>1.</label>
                <element-citation publication-type="journal">
                  <person-group person-group-type="author">
                    <name>
                      <surname>Lee</surname>
                      <given-names>R</given-names>
                    </name>
                    <name>
                      <surname>Wong</surname>
                      <given-names>TY</given-names>
                    </name>
                    <name>
                      <surname>Sabanayagam</surname>
                      <given-names>C</given-names>
                    </name>
                  </person-group>
                  <article-title>Epidemiology of diabetic retinopathy, diabetic macular edema and related vision loss</article-title>
                  <source>Eye Vis.</source>
                  <year>2015</year>
                  <volume>2</volume>
                  <fpage>17</fpage>
                  <pub-id pub-id-type="doi">10.1186/s40662-015-0026-2</pub-id>
                </element-citation>
              </ref>
              <ref id="CR2">
                <label>2.</label>
                <element-citation publication-type="journal">
                  <person-group person-group-type="author">
                    <name>
                      <surname>Schmidt-Erfurth</surname>
                      <given-names>U</given-names>
                    </name>
                    <etal/>
                  </person-group>
                  <article-title>Guidelines for the management of diabetic macular edema by the European Society of Retina Specialists (EURETINA)</article-title>
                  <source>OPH</source>
                  <year>2017</year>
                  <volume>237</volume>
                  <fpage>185</fpage>
                  <lpage>222</lpage>
                </element-citation>
              </ref>
              <ref id="CR3">
                <label>3.</label>
                <element-citation publication-type="journal">
                  <person-group person-group-type="author">
                    <name>
                      <surname>Ajlan</surname>
                      <given-names>RS</given-names>
                    </name>
                    <name>
                      <surname>Silva</surname>
                      <given-names>PS</given-names>
                    </name>
                    <name>
                      <surname>Sun</surname>
                      <given-names>JK</given-names>
                    </name>
                  </person-group>
                  <article-title>Vascular endothelial growth factor and diabetic retinal disease</article-title>
                  <source>Semin. Ophthalmol.</source>
                  <year>2016</year>
                  <volume>31</volume>
                  <fpage>40</fpage>
                  <lpage>48</lpage>
                  <pub-id pub-id-type="doi">10.3109/08820538.2015.1114833</pub-id>
                  <pub-id pub-id-type="pmid">26959128</pub-id>
                </element-citation>
              </ref>
              <ref id="CR4">
                <label>4.</label>
                <element-citation publication-type="journal">
                  <person-group person-group-type="author">
                    <name>
                      <surname>Silpa-archa</surname>
                      <given-names>S</given-names>
                    </name>
                    <name>
                      <surname>Ruamviboonsuk</surname>
                      <given-names>P</given-names>
                    </name>
                  </person-group>
                  <article-title>Diabetic retinopathy: current treatment and Thailand perspective</article-title>
                  <source>J. Med. Assoc. Thai.</source>
                  <year>2017</year>
                  <volume>100</volume>
                  <fpage>S136</fpage>
                  <lpage>S147</lpage>
                  <?supplied-pmid 29927326?>
                  <pub-id pub-id-type="pmid">29927326</pub-id>
                </element-citation>
              </ref>
              <ref id="CR5">
                <label>5.</label>
                <element-citation publication-type="journal">
                  <person-group person-group-type="author">
                    <name>
                      <surname>Nguyen</surname>
                      <given-names>QD</given-names>
                    </name>
                    <etal/>
                  </person-group>
                  <article-title>Ranibizumab for diabetic macular edema: results from 2 phase III randomized trials: RISE and RIDE</article-title>
                  <source>Ophthalmology</source>
                  <year>2012</year>
                  <volume>119</volume>
                  <fpage>789</fpage>
                  <lpage>801</lpage>
                  <pub-id pub-id-type="doi">10.1016/j.ophtha.2011.12.039</pub-id>
                  <pub-id pub-id-type="pmid">22330964</pub-id>
                </element-citation>
              </ref>
              <ref id="CR6">
                <label>6.</label>
                <element-citation publication-type="journal">
                  <person-group person-group-type="author">
                    <name>
                      <surname>Brown</surname>
                      <given-names>DM</given-names>
                    </name>
                    <etal/>
                  </person-group>
                  <article-title>Long-term outcomes of ranibizumab therapy for diabetic macular edema: the 36-month results from two phase III trials: RISE and RIDE</article-title>
                  <source>Ophthalmology</source>
                  <year>2013</year>
                  <volume>120</volume>
                  <fpage>2013</fpage>
                  <lpage>2022</lpage>
                  <pub-id pub-id-type="doi">10.1016/j.ophtha.2013.02.034</pub-id>
                  <pub-id pub-id-type="pmid">23706949</pub-id>
                </element-citation>
              </ref>
              <ref id="CR7">
                <label>7.</label>
                <mixed-citation publication-type="other">Virgili, G. et al. The Cochrane Library 10.1002/14651858.CD008081.pub3 (2015).</mixed-citation>
              </ref>
              <ref id="CR8">
                <label>8.</label>
                <mixed-citation publication-type="other">User, S. IDF diabetes atlas—across the globe. Available at: <ext-link ext-link-type="uri" xlink:href="http://www.diabetesatlas.org/across-the-globe.html">http://www.diabetesatlas.org/across-the-globe.html</ext-link>. (Accessed: 26 September 2018).</mixed-citation>
              </ref>
              <ref id="CR9">
                <label>9.</label>
                <element-citation publication-type="journal">
                  <person-group person-group-type="author">
                    <name>
                      <surname>Chakrabarti</surname>
                      <given-names>R</given-names>
                    </name>
                    <name>
                      <surname>Alex Harper</surname>
                      <given-names>C</given-names>
                    </name>
                    <name>
                      <surname>Keeffe</surname>
                      <given-names>JE</given-names>
                    </name>
                  </person-group>
                  <article-title>Diabetic retinopathy management guidelines</article-title>
                  <source>Expert. Rev. Ophthalmol.</source>
                  <year>2012</year>
                  <volume>7</volume>
                  <fpage>417</fpage>
                  <lpage>439</lpage>
                  <pub-id pub-id-type="doi">10.1586/eop.12.52</pub-id>
                </element-citation>
              </ref>
              <ref id="CR10">
                <label>10.</label>
                <element-citation publication-type="journal">
                  <person-group person-group-type="author">
                    <name>
                      <surname>Harding</surname>
                      <given-names>SP</given-names>
                    </name>
                    <name>
                      <surname>Broadbent</surname>
                      <given-names>DM</given-names>
                    </name>
                    <name>
                      <surname>Neoh</surname>
                      <given-names>C</given-names>
                    </name>
                    <name>
                      <surname>White</surname>
                      <given-names>MC</given-names>
                    </name>
                    <name>
                      <surname>Vora</surname>
                      <given-names>J</given-names>
                    </name>
                  </person-group>
                  <article-title>Sensitivity and specificity of photography and direct ophthalmoscopy in screening for sight threatening eye disease: the Liverpool diabetic eye study</article-title>
                  <source>BMJ</source>
                  <year>1995</year>
                  <volume>311</volume>
                  <fpage>1131</fpage>
                  <lpage>1135</lpage>
                  <pub-id pub-id-type="doi">10.1136/bmj.311.7013.1131</pub-id>
                  <pub-id pub-id-type="pmid">7580708</pub-id>
                </element-citation>
              </ref>
              <ref id="CR11">
                <label>11.</label>
                <element-citation publication-type="journal">
                  <person-group person-group-type="author">
                    <name>
                      <surname>Mackenzie</surname>
                      <given-names>S</given-names>
                    </name>
                    <etal/>
                  </person-group>
                  <article-title>SDOCT imaging to identify macular pathology in patients diagnosed with diabetic maculopathy by a digital photographic retinal screening programme</article-title>
                  <source>PLoS ONE</source>
                  <year>2011</year>
                  <volume>6</volume>
                  <fpage>e14811</fpage>
                  <pub-id pub-id-type="doi">10.1371/journal.pone.0014811</pub-id>
                  <pub-id pub-id-type="pmid">21573106</pub-id>
                </element-citation>
              </ref>
              <ref id="CR12">
                <label>12.</label>
                <element-citation publication-type="journal">
                  <person-group person-group-type="author">
                    <name>
                      <surname>Wang</surname>
                      <given-names>YT</given-names>
                    </name>
                    <name>
                      <surname>Tadarati</surname>
                      <given-names>M</given-names>
                    </name>
                    <name>
                      <surname>Wolfson</surname>
                      <given-names>Y</given-names>
                    </name>
                    <name>
                      <surname>Bressler</surname>
                      <given-names>SB</given-names>
                    </name>
                    <name>
                      <surname>Bressler</surname>
                      <given-names>NM</given-names>
                    </name>
                  </person-group>
                  <article-title>Comparison of prevalence of diabetic macular edema based on monocular fundus photography vs optical coherence tomography</article-title>
                  <source>JAMA Ophthalmol.</source>
                  <year>2016</year>
                  <volume>134</volume>
                  <fpage>222</fpage>
                  <pub-id pub-id-type="doi">10.1001/jamaophthalmol.2015.5332</pub-id>
                  <pub-id pub-id-type="pmid">26719967</pub-id>
                </element-citation>
              </ref>
              <ref id="CR13">
                <label>13.</label>
                <mixed-citation publication-type="other">Wong RL, et al. Are we making good use of our public resources? The false-positive rate of screening by fundus photography for diabetic macular oedema. <italic>Hong Kong Med. J</italic>. <bold>23</bold>, 356–364 (2017).</mixed-citation>
              </ref>
              <ref id="CR14">
                <label>14.</label>
                <element-citation publication-type="journal">
                  <person-group person-group-type="author">
                    <name>
                      <surname>Esteva</surname>
                      <given-names>A</given-names>
                    </name>
                    <etal/>
                  </person-group>
                  <article-title>Dermatologist-level classification of skin cancer with deep neural networks</article-title>
                  <source>Nature</source>
                  <year>2017</year>
                  <pub-id pub-id-type="doi">10.1038/nature21056</pub-id>
                  <?supplied-pmid 28658222?>
                  <pub-id pub-id-type="pmid">28658222</pub-id>
                </element-citation>
              </ref>
              <ref id="CR15">
                <label>15.</label>
                <mixed-citation publication-type="other">Liu, Y. et al. Detecting Cancer Metastases on Gigapixel Pathology Images. <italic>arXiv:1703.02442</italic> (2017).</mixed-citation>
              </ref>
              <ref id="CR16">
                <label>16.</label>
                <element-citation publication-type="journal">
                  <person-group person-group-type="author">
                    <name>
                      <surname>Ehteshami Bejnordi</surname>
                      <given-names>B</given-names>
                    </name>
                    <etal/>
                  </person-group>
                  <article-title>Diagnostic assessment of deep learning algorithms for detection of lymph node metastases in women with breast cancer</article-title>
                  <source>JAMA</source>
                  <year>2017</year>
                  <volume>318</volume>
                  <fpage>2199</fpage>
                  <lpage>2210</lpage>
                  <pub-id pub-id-type="doi">10.1001/jama.2017.14585</pub-id>
                  <pub-id pub-id-type="pmid">29234806</pub-id>
                </element-citation>
              </ref>
              <ref id="CR17">
                <label>17.</label>
                <element-citation publication-type="journal">
                  <person-group person-group-type="author">
                    <name>
                      <surname>Ardila</surname>
                      <given-names>D</given-names>
                    </name>
                    <etal/>
                  </person-group>
                  <article-title>End-to-end lung cancer screening with three-dimensional deep learning on low-dose chest computed tomography</article-title>
                  <source>Nat. Med.</source>
                  <year>2019</year>
                  <volume>25</volume>
                  <fpage>954</fpage>
                  <lpage>961</lpage>
                  <pub-id pub-id-type="doi">10.1038/s41591-019-0447-x</pub-id>
                  <pub-id pub-id-type="pmid">31110349</pub-id>
                </element-citation>
              </ref>
              <ref id="CR18">
                <label>18.</label>
                <element-citation publication-type="journal">
                  <person-group person-group-type="author">
                    <name>
                      <surname>Nagpal</surname>
                      <given-names>K</given-names>
                    </name>
                    <etal/>
                  </person-group>
                  <article-title>Development and validation of a deep learning algorithm for improving Gleason scoring of prostate cancer</article-title>
                  <source>NPJ Digit. Med.</source>
                  <year>2019</year>
                  <volume>2</volume>
                  <fpage>48</fpage>
                  <pub-id pub-id-type="doi">10.1038/s41746-019-0112-2</pub-id>
                  <pub-id pub-id-type="pmid">31304394</pub-id>
                </element-citation>
              </ref>
              <ref id="CR19">
                <label>19.</label>
                <element-citation publication-type="journal">
                  <person-group person-group-type="author">
                    <name>
                      <surname>Gulshan</surname>
                      <given-names>V</given-names>
                    </name>
                    <etal/>
                  </person-group>
                  <article-title>Development and validation of a deep learning algorithm for detection of diabetic retinopathy in retinal fundus photographs</article-title>
                  <source>JAMA</source>
                  <year>2016</year>
                  <volume>316</volume>
                  <fpage>2402</fpage>
                  <lpage>2410</lpage>
                  <pub-id pub-id-type="doi">10.1001/jama.2016.17216</pub-id>
                  <pub-id pub-id-type="pmid">27898976</pub-id>
                </element-citation>
              </ref>
              <ref id="CR20">
                <label>20.</label>
                <element-citation publication-type="journal">
                  <person-group person-group-type="author">
                    <name>
                      <surname>Ting</surname>
                      <given-names>DSW</given-names>
                    </name>
                    <etal/>
                  </person-group>
                  <article-title>Development and validation of a deep learning system for diabetic retinopathy and related eye diseases using retinal images from multiethnic populations with diabetes</article-title>
                  <source>JAMA</source>
                  <year>2017</year>
                  <volume>318</volume>
                  <fpage>2211</fpage>
                  <lpage>2223</lpage>
                  <pub-id pub-id-type="doi">10.1001/jama.2017.18152</pub-id>
                  <pub-id pub-id-type="pmid">29234807</pub-id>
                </element-citation>
              </ref>
              <ref id="CR21">
                <label>21.</label>
                <element-citation publication-type="journal">
                  <person-group person-group-type="author">
                    <name>
                      <surname>Krause</surname>
                      <given-names>J</given-names>
                    </name>
                    <etal/>
                  </person-group>
                  <article-title>Grader variability and the importance of reference standards for evaluating machine learning models for diabetic retinopathy</article-title>
                  <source>Ophthalmology</source>
                  <year>2018</year>
                  <pub-id pub-id-type="doi">10.1016/j.ophtha.2018.01.034</pub-id>
                  <?supplied-pmid 30553900?>
                  <pub-id pub-id-type="pmid">30553900</pub-id>
                </element-citation>
              </ref>
              <ref id="CR22">
                <label>22.</label>
                <element-citation publication-type="journal">
                  <person-group person-group-type="author">
                    <name>
                      <surname>Burlina</surname>
                      <given-names>PM</given-names>
                    </name>
                    <etal/>
                  </person-group>
                  <article-title>Automated grading of age-related macular degeneration from color fundus images using deep convolutional neural networks</article-title>
                  <source>JAMA Ophthalmol.</source>
                  <year>2017</year>
                  <volume>135</volume>
                  <fpage>1170</fpage>
                  <lpage>1176</lpage>
                  <pub-id pub-id-type="doi">10.1001/jamaophthalmol.2017.3782</pub-id>
                  <pub-id pub-id-type="pmid">28973096</pub-id>
                </element-citation>
              </ref>
              <ref id="CR23">
                <label>23.</label>
                <element-citation publication-type="journal">
                  <person-group person-group-type="author">
                    <name>
                      <surname>Poplin</surname>
                      <given-names>R</given-names>
                    </name>
                    <etal/>
                  </person-group>
                  <article-title>Prediction of cardiovascular risk factors from retinal fundus photographs via deep learning</article-title>
                  <source>Nat. Biomed. Eng.</source>
                  <year>2018</year>
                  <volume>2</volume>
                  <fpage>158</fpage>
                  <lpage>164</lpage>
                  <pub-id pub-id-type="doi">10.1038/s41551-018-0195-0</pub-id>
                  <pub-id pub-id-type="pmid">31015713</pub-id>
                </element-citation>
              </ref>
              <ref id="CR24">
                <label>24.</label>
                <element-citation publication-type="journal">
                  <person-group person-group-type="author">
                    <name>
                      <surname>Varadarajan</surname>
                      <given-names>AV</given-names>
                    </name>
                    <etal/>
                  </person-group>
                  <article-title>Deep learning for predicting refractive error from retinal fundus images</article-title>
                  <source>Invest. Ophthalmol. Vis. Sci.</source>
                  <year>2018</year>
                  <volume>59</volume>
                  <fpage>2861</fpage>
                  <lpage>2868</lpage>
                  <pub-id pub-id-type="doi">10.1167/iovs.18-23887</pub-id>
                  <pub-id pub-id-type="pmid">30025129</pub-id>
                </element-citation>
              </ref>
              <ref id="CR25">
                <label>25.</label>
                <element-citation publication-type="journal">
                  <person-group person-group-type="author">
                    <name>
                      <surname>Browning</surname>
                      <given-names>DJ</given-names>
                    </name>
                    <etal/>
                  </person-group>
                  <article-title>Optical coherence tomography measurements and analysis methods in optical coherence tomography studies of diabetic macular edema</article-title>
                  <source>Ophthalmology</source>
                  <year>2008</year>
                  <volume>115</volume>
                  <fpage>1366–71</fpage>
                  <lpage>1371.e1</lpage>
                  <pub-id pub-id-type="pmid">18675696</pub-id>
                </element-citation>
              </ref>
              <ref id="CR26">
                <label>26.</label>
                <element-citation publication-type="journal">
                  <person-group person-group-type="author">
                    <name>
                      <surname>Wells</surname>
                      <given-names>JA</given-names>
                    </name>
                    <etal/>
                  </person-group>
                  <article-title>Aflibercept, bevacizumab, or ranibizumab for diabetic macular edema: two-year results from a comparative effectiveness randomized clinical trial</article-title>
                  <source>Ophthalmology</source>
                  <year>2016</year>
                  <volume>123</volume>
                  <fpage>1351</fpage>
                  <pub-id pub-id-type="doi">10.1016/j.ophtha.2016.02.022</pub-id>
                  <pub-id pub-id-type="pmid">26935357</pub-id>
                </element-citation>
              </ref>
              <ref id="CR27">
                <label>27.</label>
                <element-citation publication-type="journal">
                  <person-group person-group-type="author">
                    <name>
                      <surname>Litvin</surname>
                      <given-names>TV</given-names>
                    </name>
                    <etal/>
                  </person-group>
                  <article-title>Improving accuracy of grading and referral of diabetic macular edema using location and extent of hard exudates in retinal photography</article-title>
                  <source>J. Diabetes Sci. Technol.</source>
                  <year>2016</year>
                  <volume>10</volume>
                  <fpage>262</fpage>
                  <pub-id pub-id-type="doi">10.1177/1932296815617281</pub-id>
                </element-citation>
              </ref>
              <ref id="CR28">
                <label>28.</label>
                <element-citation publication-type="journal">
                  <person-group person-group-type="author">
                    <collab>Photocoagulation for diabetic macular edema.</collab>
                  </person-group>
                  <article-title>Early treatment diabetic retinopathy study report number 1. Early Treatment Diabetic Retinopathy Study research group</article-title>
                  <source>Arch. Ophthalmol.</source>
                  <year>1985</year>
                  <volume>103</volume>
                  <fpage>1796</fpage>
                  <lpage>1806</lpage>
                  <pub-id pub-id-type="doi">10.1001/archopht.1985.01050120030015</pub-id>
                  <pub-id pub-id-type="pmid">2866759</pub-id>
                </element-citation>
              </ref>
              <ref id="CR29">
                <label>29.</label>
                <element-citation publication-type="journal">
                  <person-group person-group-type="author">
                    <name>
                      <surname>Boyer</surname>
                      <given-names>DS</given-names>
                    </name>
                    <etal/>
                  </person-group>
                  <article-title>Outcomes with as-needed ranibizumab after initial monthly therapy: long-term outcomes of the phase III RIDE and RISE trials</article-title>
                  <source>Ophthalmology</source>
                  <year>2015</year>
                  <volume>122</volume>
                  <fpage>2504</fpage>
                  <lpage>13.e1</lpage>
                  <pub-id pub-id-type="doi">10.1016/j.ophtha.2015.08.006</pub-id>
                  <pub-id pub-id-type="pmid">26452713</pub-id>
                </element-citation>
              </ref>
              <ref id="CR30">
                <label>30.</label>
                <element-citation publication-type="journal">
                  <person-group person-group-type="author">
                    <name>
                      <surname>Browning</surname>
                      <given-names>DJ</given-names>
                    </name>
                    <etal/>
                  </person-group>
                  <article-title>Association of the extent of diabetic macular edema as assessed by optical coherence tomography with visual acuity and retinal outcome variables</article-title>
                  <source>Retina</source>
                  <year>2009</year>
                  <volume>29</volume>
                  <fpage>300</fpage>
                  <lpage>305</lpage>
                  <pub-id pub-id-type="doi">10.1097/IAE.0b013e318194995d</pub-id>
                  <pub-id pub-id-type="pmid">19174719</pub-id>
                </element-citation>
              </ref>
              <ref id="CR31">
                <label>31.</label>
                <mixed-citation publication-type="other">Treatment for CI-DME in Eyes With Very Good VA Study—Full Text View—ClinicalTrials.gov. Available at: <ext-link ext-link-type="uri" xlink:href="https://clinicaltrials.gov/ct2/show/NCT01909791">https://clinicaltrials.gov/ct2/show/NCT01909791</ext-link>. (Accessed: 5 December 2018).</mixed-citation>
              </ref>
              <ref id="CR32">
                <label>32.</label>
                <mixed-citation publication-type="other">Diabetic Retinopathy PPP—Updated 2017. <italic>American Academy of Ophthalmology</italic> (2017). Available at: <ext-link ext-link-type="uri" xlink:href="https://www.aao.org/preferred-practice-pattern/diabetic-retinopathy-ppp-updated-2017">https://www.aao.org/preferred-practice-pattern/diabetic-retinopathy-ppp-updated-2017</ext-link>. (Accessed: 25 August 2018).</mixed-citation>
              </ref>
              <ref id="CR33">
                <label>33.</label>
                <mixed-citation publication-type="other">International Council of Ophthalmology: Enhancing Eye Care: Diabetic Eye Care. Available at: <ext-link ext-link-type="uri" xlink:href="http://www.icoph.org/enhancing_eyecare/diabetic_eyecare.html">http://www.icoph.org/enhancing_eyecare/diabetic_eyecare.html</ext-link>. (Accessed: 5 December 2018).</mixed-citation>
              </ref>
              <ref id="CR34">
                <label>34.</label>
                <element-citation publication-type="journal">
                  <person-group person-group-type="author">
                    <name>
                      <surname>Itoh</surname>
                      <given-names>Y</given-names>
                    </name>
                    <name>
                      <surname>Petkovsek</surname>
                      <given-names>D</given-names>
                    </name>
                    <name>
                      <surname>Kaiser</surname>
                      <given-names>PK</given-names>
                    </name>
                    <name>
                      <surname>Singh</surname>
                      <given-names>RP</given-names>
                    </name>
                    <name>
                      <surname>Ehlers</surname>
                      <given-names>JP</given-names>
                    </name>
                  </person-group>
                  <article-title>Optical coherence tomography features in diabetic macular edema and the impact on anti-VEGF response</article-title>
                  <source>Ophthalmic Surg. Lasers Imaging Retin.</source>
                  <year>2016</year>
                  <volume>47</volume>
                  <fpage>908</fpage>
                  <lpage>913</lpage>
                  <pub-id pub-id-type="doi">10.3928/23258160-20161004-03</pub-id>
                </element-citation>
              </ref>
              <ref id="CR35">
                <label>35.</label>
                <element-citation publication-type="journal">
                  <person-group person-group-type="author">
                    <name>
                      <surname>Grassmann</surname>
                      <given-names>F</given-names>
                    </name>
                    <etal/>
                  </person-group>
                  <article-title>A deep learning algorithm for prediction of age-related eye disease study severity scale for age-related macular degeneration from color fundus photography</article-title>
                  <source>Ophthalmology</source>
                  <year>2018</year>
                  <pub-id pub-id-type="doi">10.1016/j.ophtha.2018.02.037</pub-id>
                  <?supplied-pmid 29653860?>
                  <pub-id pub-id-type="pmid">29653860</pub-id>
                </element-citation>
              </ref>
              <ref id="CR36">
                <label>36.</label>
                <element-citation publication-type="journal">
                  <person-group person-group-type="author">
                    <name>
                      <surname>Halladay</surname>
                      <given-names>JR</given-names>
                    </name>
                    <name>
                      <surname>Yankaskas</surname>
                      <given-names>BC</given-names>
                    </name>
                    <name>
                      <surname>Bowling</surname>
                      <given-names>JM</given-names>
                    </name>
                    <name>
                      <surname>Alexander</surname>
                      <given-names>C</given-names>
                    </name>
                  </person-group>
                  <article-title>Positive predictive value of mammography: comparison of interpretations of screening and diagnostic images by the same radiologist and by different radiologists</article-title>
                  <source>Am. J. Roentgenol.</source>
                  <year>2010</year>
                  <volume>195</volume>
                  <fpage>782</fpage>
                  <lpage>785</lpage>
                  <pub-id pub-id-type="doi">10.2214/AJR.09.2955</pub-id>
                  <pub-id pub-id-type="pmid">20729460</pub-id>
                </element-citation>
              </ref>
              <ref id="CR37">
                <label>37.</label>
                <element-citation publication-type="journal">
                  <person-group person-group-type="author">
                    <name>
                      <surname>Scott</surname>
                      <given-names>IU</given-names>
                    </name>
                    <etal/>
                  </person-group>
                  <article-title>Effect of focal/grid photocoagulation on visual acuity and retinal thickening in eyes with non-center-involved diabetic macular edema</article-title>
                  <source>Retina</source>
                  <year>2009</year>
                  <volume>29</volume>
                  <fpage>613</fpage>
                  <lpage>617</lpage>
                  <pub-id pub-id-type="doi">10.1097/IAE.0b013e3181a2c07a</pub-id>
                  <pub-id pub-id-type="pmid">19373126</pub-id>
                </element-citation>
              </ref>
              <ref id="CR38">
                <label>38.</label>
                <mixed-citation publication-type="other">The Ivue(TM) normative database study–methodology and distribution of OCT parameters. Available at: <ext-link ext-link-type="uri" xlink:href="https://www.aaopt.org/detail/knowledge-base-article/ivuetm-normative-database-study-methodology-and-distribution-oct-parameters">https://www.aaopt.org/detail/knowledge-base-article/ivuetm-normative-database-study-methodology-and-distribution-oct-parameters</ext-link>. (Accessed: 2 October 2018).</mixed-citation>
              </ref>
              <ref id="CR39">
                <label>39.</label>
                <mixed-citation publication-type="other">Szegedy, C., Vanhoucke, V., Ioffe, S., Shlens, J. &amp; Wojna, Z. Rethinking the Inception Architecture for Computer Vision. <italic>arXiv preprint arXiv:1502.03167</italic> (2015).</mixed-citation>
              </ref>
              <ref id="CR40">
                <label>40.</label>
                <element-citation publication-type="journal">
                  <person-group person-group-type="author">
                    <name>
                      <surname>Lecun</surname>
                      <given-names>Y</given-names>
                    </name>
                    <name>
                      <surname>Bottou</surname>
                      <given-names>L</given-names>
                    </name>
                    <name>
                      <surname>Bengio</surname>
                      <given-names>Y</given-names>
                    </name>
                    <name>
                      <surname>Haffner</surname>
                      <given-names>P</given-names>
                    </name>
                  </person-group>
                  <article-title>Gradient-based learning applied to document recognition</article-title>
                  <source>Proc. IEEE</source>
                  <year>1998</year>
                  <volume>86</volume>
                  <fpage>2278</fpage>
                  <lpage>2324</lpage>
                  <pub-id pub-id-type="doi">10.1109/5.726791</pub-id>
                </element-citation>
              </ref>
              <ref id="CR41">
                <label>41.</label>
                <element-citation publication-type="journal">
                  <person-group person-group-type="author">
                    <name>
                      <surname>Cohen</surname>
                      <given-names>J</given-names>
                    </name>
                  </person-group>
                  <article-title>A coefficient of agreement for nominal scales</article-title>
                  <source>Educ. Psychol. Meas.</source>
                  <year>1960</year>
                  <volume>20</volume>
                  <fpage>37</fpage>
                  <lpage>46</lpage>
                  <pub-id pub-id-type="doi">10.1177/001316446002000104</pub-id>
                </element-citation>
              </ref>
              <ref id="CR42">
                <label>42.</label>
                <mixed-citation publication-type="other">Chihara, L. M. &amp; Hesterberg, T. C. <italic>Mathematical Statistics with Resampling and R</italic> (Wiley, 2011).</mixed-citation>
              </ref>
            </ref-list>
          </back>
        </article>
      </metadata>
    </record>
  </GetRecord>
</OAI-PMH>
