<?xml version="1.0" encoding="UTF-8"?>
<OAI-PMH xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/ http://www.openarchives.org/OAI/2.0/OAI-PMH.xsd">
  <responseDate>2024-05-03T11:32:59Z</responseDate>
  <request verb="GetRecord" identifier="oai:pubmedcentral.nih.gov:3552845" metadataPrefix="pmc">https:/www.ncbi.nlm.nih.gov/pmc/oai/oai.cgi</request>
  <GetRecord>
    <record>
      <header>
        <identifier>oai:pubmedcentral.nih.gov:3552845</identifier>
        <datestamp>2013-01-25</datestamp>
        <setSpec>plosone</setSpec>
        <setSpec>pmc-open</setSpec>
      </header>
      <metadata>
        <article xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" xsi:schemaLocation="https://jats.nlm.nih.gov/ns/archiving/1.3/ https://jats.nlm.nih.gov/archiving/1.3/xsd/JATS-archivearticle1-3.xsd" article-type="research-article">
          <front>
            <journal-meta>
              <journal-id journal-id-type="nlm-ta">PLoS One</journal-id>
              <journal-id journal-id-type="iso-abbrev">PLoS ONE</journal-id>
              <journal-id journal-id-type="publisher-id">plos</journal-id>
              <journal-id journal-id-type="pmc">plosone</journal-id>
              <journal-title-group>
                <journal-title>PLoS ONE</journal-title>
              </journal-title-group>
              <issn pub-type="epub">1932-6203</issn>
              <publisher>
                <publisher-name>Public Library of Science</publisher-name>
                <publisher-loc>San Francisco, USA</publisher-loc>
              </publisher>
            </journal-meta>
            <article-meta>
              <article-id pub-id-type="accession">PMC3552845</article-id>
              <article-id pub-id-type="pmcid">PMC3552845</article-id>
              <article-id pub-id-type="pmc-uid">3552845</article-id>
              <article-id pub-id-type="pmid">23355900</article-id>
              <article-id pub-id-type="pmid">23355900</article-id>
              <article-id pub-id-type="publisher-id">PONE-D-12-24836</article-id>
              <article-id pub-id-type="doi">10.1371/journal.pone.0054789</article-id>
              <article-categories>
                <subj-group subj-group-type="heading">
                  <subject>Research Article</subject>
                </subj-group>
                <subj-group subj-group-type="Discipline-v2">
                  <subject>Biology</subject>
                  <subj-group>
                    <subject>Neuroscience</subject>
                    <subj-group>
                      <subject>Sensory Perception</subject>
                      <subj-group>
                        <subject>Psychophysics</subject>
                      </subj-group>
                    </subj-group>
                    <subj-group>
                      <subject>Sensory Systems</subject>
                      <subj-group>
                        <subject>Auditory System</subject>
                        <subject>Visual System</subject>
                      </subj-group>
                    </subj-group>
                    <subj-group>
                      <subject>Behavioral Neuroscience</subject>
                    </subj-group>
                  </subj-group>
                </subj-group>
                <subj-group subj-group-type="Discipline-v2">
                  <subject>Social and Behavioral Sciences</subject>
                  <subj-group>
                    <subject>Psychology</subject>
                    <subj-group>
                      <subject>Behavior</subject>
                      <subj-group>
                        <subject>Human Performance</subject>
                      </subj-group>
                    </subj-group>
                    <subj-group>
                      <subject>Psychophysics</subject>
                      <subject>Sensory Perception</subject>
                    </subj-group>
                  </subj-group>
                </subj-group>
              </article-categories>
              <title-group>
                <article-title>The Duration of a Co-Occurring Sound Modulates Visual Detection Performance in Humans</article-title>
                <alt-title alt-title-type="running-head">Sound Duration Modulates Visual Perception</alt-title>
              </title-group>
              <contrib-group>
                <contrib contrib-type="author" equal-contrib="yes">
                  <name>
                    <surname>de Haas</surname>
                    <given-names>Benjamin</given-names>
                  </name>
                  <xref ref-type="aff" rid="aff1">
                    <sup>1</sup>
                  </xref>
                  <xref ref-type="aff" rid="aff2">
                    <sup>2</sup>
                  </xref>
                  <xref ref-type="corresp" rid="cor1">
                    <sup>*</sup>
                  </xref>
                </contrib>
                <contrib contrib-type="author" equal-contrib="yes">
                  <name>
                    <surname>Cecere</surname>
                    <given-names>Roberto</given-names>
                  </name>
                  <xref ref-type="aff" rid="aff2">
                    <sup>2</sup>
                  </xref>
                  <xref ref-type="aff" rid="aff3">
                    <sup>3</sup>
                  </xref>
                </contrib>
                <contrib contrib-type="author">
                  <name>
                    <surname>Cullen</surname>
                    <given-names>Harriet</given-names>
                  </name>
                  <xref ref-type="aff" rid="aff2">
                    <sup>2</sup>
                  </xref>
                </contrib>
                <contrib contrib-type="author">
                  <name>
                    <surname>Driver</surname>
                    <given-names>Jon</given-names>
                  </name>
                  <xref ref-type="aff" rid="aff1">
                    <sup>1</sup>
                  </xref>
                  <xref ref-type="aff" rid="aff2">
                    <sup>2</sup>
                  </xref>
                </contrib>
                <contrib contrib-type="author">
                  <name>
                    <surname>Romei</surname>
                    <given-names>Vincenzo</given-names>
                  </name>
                  <xref ref-type="aff" rid="aff1">
                    <sup>1</sup>
                  </xref>
                  <xref ref-type="aff" rid="aff2">
                    <sup>2</sup>
                  </xref>
                  <xref ref-type="corresp" rid="cor1">
                    <sup>*</sup>
                  </xref>
                </contrib>
              </contrib-group>
              <aff id="aff1">
                <label>1</label>
                <addr-line>Wellcome Trust Centre for Neuroimaging at UCL, University College London, London, United Kingdom</addr-line>
              </aff>
              <aff id="aff2">
                <label>2</label>
                <addr-line>UCL Institute of Cognitive Neuroscience, University College London, London, United Kingdom</addr-line>
              </aff>
              <aff id="aff3">
                <label>3</label>
                <addr-line>Centro studi di ricerche in Neuroscienze Cognitive, Alma Master Studiorum, Universita’ di Bologna, Bologna, Italy</addr-line>
              </aff>
              <contrib-group>
                <contrib contrib-type="editor">
                  <name>
                    <surname>de Lange</surname>
                    <given-names>Floris P.</given-names>
                  </name>
                  <role>Editor</role>
                  <xref ref-type="aff" rid="edit1"/>
                </contrib>
              </contrib-group>
              <aff id="edit1">
                <addr-line>Radboud University Nijmegen, The Netherlands</addr-line>
              </aff>
              <author-notes>
                <corresp id="cor1">* E-mail: <email>benjamin.haas.09@ucl.ac.uk</email> (BDH); <email>v.romei@ucl.ac.uk</email> (VR)</corresp>
                <fn fn-type="COI-statement">
                  <p><bold>Competing Interests: </bold>The authors have declared that no competing interests exist.</p>
                </fn>
                <fn fn-type="con">
                  <p>Conceived and designed the experiments: BDH VR JD RC. Performed the experiments: RC HC BdH. Analyzed the data: VR RC BDH. Wrote the paper: VR BDH RC HC.</p>
                </fn>
              </author-notes>
              <pub-date pub-type="collection">
                <year>2013</year>
              </pub-date>
              <pub-date pub-type="epub">
                <day>23</day>
                <month>1</month>
                <year>2013</year>
              </pub-date>
              <volume>8</volume>
              <issue>1</issue>
              <elocation-id>e54789</elocation-id>
              <history>
                <date date-type="received">
                  <day>18</day>
                  <month>8</month>
                  <year>2012</year>
                </date>
                <date date-type="accepted">
                  <day>14</day>
                  <month>12</month>
                  <year>2012</year>
                </date>
              </history>
              <permissions>
                <copyright-statement>© 2013 de Haas et al</copyright-statement>
                <copyright-year>2013</copyright-year>
                <copyright-holder>de Haas et al</copyright-holder>
                <license xlink:href="http://creativecommons.org/licenses/by/4.0/">
                  <license-p>This is an open-access article distributed under the terms of the Creative Commons Attribution License, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are properly credited.</license-p>
                </license>
              </permissions>
              <abstract>
                <sec>
                  <title>Background</title>
                  <p>The duration of sounds can affect the perceived duration of co-occurring visual stimuli. However, it is unclear whether this is limited to amodal processes of duration perception or affects other non-temporal qualities of visual perception.</p>
                </sec>
                <sec>
                  <title>Methodology/Principal Findings</title>
                  <p>Here, we tested the hypothesis that visual sensitivity - rather than only the perceived duration of visual stimuli - can be affected by the duration of co-occurring sounds. We found that visual detection sensitivity (d’) for unimodal stimuli was higher for stimuli of longer duration. Crucially, in a cross-modal condition, we replicated previous unimodal findings, observing that visual sensitivity was shaped by the duration of co-occurring sounds. When short visual stimuli (∼24 ms) were accompanied by sounds of matching duration, visual sensitivity was decreased relative to the unimodal visual condition. However, when the same visual stimuli were accompanied by longer auditory stimuli (∼60–96 ms), visual sensitivity was increased relative to the performance for ∼24 ms auditory stimuli. Across participants, this sensitivity enhancement was observed within a critical time window of ∼60–96 ms. Moreover, the amplitude of this effect correlated with visual sensitivity enhancement found for longer lasting visual stimuli across participants.</p>
                </sec>
                <sec>
                  <title>Conclusions/Significance</title>
                  <p>Our findings show that the duration of co-occurring sounds affects visual perception; it changes visual sensitivity in a similar way as altering the (actual) duration of the visual stimuli does.</p>
                </sec>
              </abstract>
              <funding-group>
                <funding-statement>BDH, VR and JD were supported by the Wellcome Trust. JD (recently deceased) was a Royal Society Anniversary Research Professor. The funders had no role in study design, data collection and analysis, decision to publish, or preparation of the manuscript.</funding-statement>
              </funding-group>
              <counts>
                <page-count count="8"/>
              </counts>
            </article-meta>
          </front>
          <body>
            <sec id="s1">
              <title>Introduction</title>
              <p>Time is a fundamental dimension across sensory modalities. Manipulating the temporal characteristics of a stimulus in one modality can affect time perception in other modalities causing a discrepancy between physical stimulus timing and its perception (cf. <xref rid="pone.0054789-Eagleman1" ref-type="bibr">[1]</xref>for a review). Changing the physical flutter rate of a clicking sound changes the apparent flicker rate of a flashing light (e.g. <xref rid="pone.0054789-Shipley1" ref-type="bibr">[2]</xref>, <xref rid="pone.0054789-Wada1" ref-type="bibr">[3]</xref>). More recently it has been shown that the perceived number of visual events in a rapid sequence can be biased towards the number of co-occurring sounds <xref rid="pone.0054789-Shams1" ref-type="bibr">[4]</xref>, that the timing of a static sound can determine the perceived direction of visual apparent motion <xref rid="pone.0054789-Freeman1" ref-type="bibr">[5]</xref> and that the perceived temporal closeness of visual events can be biased by temporally shifted auditory events <xref rid="pone.0054789-Burr1" ref-type="bibr">[6]</xref>.</p>
              <p>More specifically, several studies have described cross-modal effects on subjective duration perception (e.g., <xref rid="pone.0054789-Walker1" ref-type="bibr">[7]</xref>–<xref rid="pone.0054789-Klink1" ref-type="bibr">[10]</xref>). The first finding in this regard was that perceived auditory stimulus durations are expanded relative to perceived visual durations, and that the perceived duration of audiovisual stimuli is more similar to the one for auditory stimuli <xref rid="pone.0054789-Walker1" ref-type="bibr">[7]</xref>. This kind of auditory dominance is thought to reflect higher reliability of the auditory system for temporal judgments (e.g. <xref rid="pone.0054789-Chen1" ref-type="bibr">[9]</xref>). In line with, and extending, this hypothesis is the observation that this kind of auditory dominance can be reversed for a high ratio of visual to auditory stimulus reliability (<xref rid="pone.0054789-Wada1" ref-type="bibr">[3]</xref>, c.f. <xref rid="pone.0054789-Burr1" ref-type="bibr">[6]</xref>). The relative expansion of perceived auditory durations has been interpreted to reflect a faster auditory ‘pace maker’ mechanism for a presumed internal clock (c.f. e.g. <xref rid="pone.0054789-Chen1" ref-type="bibr">[9]</xref>). However, this could not explain all findings regarding cross-modal effects on duration perception. For instance, the perceived duration of visual flashes was found to critically depend on the duration of co-occurring sounds <xref rid="pone.0054789-Donovan1" ref-type="bibr">[8]</xref>, <xref rid="pone.0054789-Klink1" ref-type="bibr">[10]</xref>, <xref rid="pone.0054789-Romei1" ref-type="bibr">[11]</xref>. If brief flashes are accompanied by sounds, they can be perceived as temporally shorter or longer than a unimodal flash of same duration, depending on the duration of the co-occurring sound <xref rid="pone.0054789-Klink1" ref-type="bibr">[10]</xref>. This has been interpreted to reflect a ventriloquist-like capture of visual stimulus on- and offsets by sounds, which would translate to changes in the timing of ‘mode switch closures’ in the above mentioned model of an internal clock <xref rid="pone.0054789-Klink1" ref-type="bibr">[10]</xref>.</p>
              <p>In a recent study <xref rid="pone.0054789-Romei1" ref-type="bibr">[11]</xref> we replicated the effect of auditory stimulus duration on perceived visual duration. We asked participants to judge which of two brief flashes lasted longer. Stimulus durations were adjusted to a standard of 55 ms vs. the individual threshold for unimodal duration discrimination. We found that sensitivity was significantly enhanced when stimuli were accompanied by sounds of congruent durations. Audiovisually incongruent stimulus pairings led to significantly <italic>de</italic>creased performance (i.e. pairing the longer sound with the shorter flash and vice versa). These effects were abolished for asynchronous onsets of flashes and sounds and greatly reduced for sounds of much longer duration than the visual flashes. We interpreted this as evidence that the effect necessitates multisensory integration (rather than the participants simply ignoring the task demand to judge visual rather than auditory stimulus durations). We further speculated that if the duration of a sound affects the perceived duration of a concurrent visual stimulus, this might not be confined to duration ‘judgements’, but affect the actual duration of the visual perception itself, as if the visual representation was stretched in time by a longer lasting sound.</p>
              <p>Accordingly, here we test the hypothesis that the duration of co-occurring sounds affects visual perception itself by impacting objective visual performance for non-temporal visual stimulus qualities. If slightly longer sounds result in sustained perception for co-occurring peri-threshold visual events, this should not only affect duration judgments for these kinds of stimuli but also improve visual sensitivity for these events, similar to what one would expect for physical lengthening of the visual stimulus duration.</p>
              <p>In the present study our aim was to test whether pairing a visual stimulus with a longer lasting sound would yield prolonged perception of the visual stimulus. If so, sensitivity for the visual stimulus should be facilitated for longer sounds, similar to the visual detection improvement expected for a genuinely longer lasting visual stimulus.</p>
              <p>Furthermore we anticipated any such effect to be restricted to a critical time window of audiovisual integration. Previous studies point to the importance of cross-modal stimulus onsets falling within a time window of about 100 ms for audiovisual binding to occur <xref rid="pone.0054789-Bolognini1" ref-type="bibr">[12]</xref>, <xref rid="pone.0054789-Romei2" ref-type="bibr">[13]</xref>. As mentioned above, our recent findings point to a similar time window regarding the effect of prolonging sounds on visual stimulus duration judgments. If the duration of sounds is stretched too far beyond the visual stimulus offset, the effect disappears <xref rid="pone.0054789-Romei1" ref-type="bibr">[11]</xref>. We therefore aimed to parametrically vary the duration of co-occurring sounds up to about 100 ms and add an additional data point for a sound duration presumed to fall well beyond this temporal window of integration. Our hypothesis was that if there was an effect of sound duration on visual sensitivity, sensitivity would continuously increase with sound duration but fall back to baseline level for the longest sound duration purposefully chosen to fall beyond the window of audiovisual integration.</p>
            </sec>
            <sec sec-type="materials|methods" id="s2">
              <title>Materials and Methods</title>
              <sec id="s2a">
                <title>Ethics Statement</title>
                <p>All participants gave written informed consent to take part in this study, according to the Declaration of Helsinki. The study was approved by the UCL Research Ethics Committee, project ID no 1893/005.</p>
              </sec>
              <sec id="s2b">
                <title>Participants</title>
                <p>Twenty-eight participants were recruited for this experiment (mean age 25.1 years, range 25–30 years; 19 females, all right handed). All reported normal or corrected visual acuity and normal hearing. All participants were paid for their time.</p>
              </sec>
              <sec id="s2c">
                <title>Apparatus</title>
                <p>Stimuli were presented on a 21′CRT display (Sony GDM-F520) in a darkened room. Participants sat with their head in a chin rest at 65 cm viewing distance. Video resolution was 1600 × 1200, with a screen refresh rate of 85 Hz. Two small stereo PC speakers were placed on either side in front of the monitor. Stimulus control and data recording were implemented on a standard PC, running a MATLAB script using functions of Psychophysics Toolbox 3 <xref rid="pone.0054789-Kleiner1" ref-type="bibr">[14]</xref>. Un-speeded manual two-choice responses (see below) were given using a standard PC numeric pad.</p>
              </sec>
              <sec id="s2d">
                <title>Stimuli</title>
                <p>In each trial, a rectangle containing dynamic white noise (mean luminance: 4.8 cd/m<sup>2</sup>; size: 23.5×17.7 degrees of visual angle) was presented for two consecutive intervals, each lasting 1059 ms (i.e. 90 frames at a video refresh rate of 85 Hz), with a SOA (stimulus onset asynchrony) of 300 ms between the two displays. A fixation dot extending 0.22 degrees in visual angle was superimposed at the middle of the noise rectangle, which was centred on the screen. The fixation dot was visible throughout the whole experiment, and changed its colour from red to green as a ‘go’ signal for responses in between trials. The target visual stimulus was a transparent Gabor patch (alpha blending factor of.6) which was briefly flashed at 353 ms after the onset of the first or second dynamic noise rectangle. The Gabor patch was composed of a 2D sinusoidal luminance grating with spatial frequency of 2.69 cycles per degree visual angle within a Gaussian amplitude envelope of standard deviation 10. It was embedded in the white noise visual stimulation with its centre position 1.4 degrees visual angle below the fixation dot. The luminance amplitude of the Gabor patch was set to individual threshold and its duration varied with experimental conditions (see below).</p>
                <p>The auditory stimulus was a 400 Hz sinusoidal pure tone sampled at 44.1 kHz with 8 different durations (∼24, ∼36, ∼48, ∼60, ∼72, ∼84, ∼96 and ∼190 ms). Sound level was set to a ∼70 dB(A). In the audiovisual trials of the main experiment, sound stimuli of equal duration were presented at 353 ms after the onset of either of the white noise rectangles.</p>
              </sec>
              <sec id="s2e">
                <title>Procedure: Visual Titration</title>
                <p>Only visual stimuli were presented during this part of the experiment. In each trial a Gabor patch of ∼24 ms duration was embedded in one of two consecutive white noise displays with its midpoint at 1.4 degrees below fixation. In between trials the red fixation dot turned green, indicating participants should respond as to whether the Gabor patch was presented in the first or second white noise interval by pressing “1” or “2” on a keyboard. Across trials we pseudo-randomly varied the luminance amplitude of Gabor patches following a constant stimuli design (8 steps within a range of peak luminance measurements between 4.8 cd/m<sup>2</sup> to 6.3 cd/m<sup>2</sup>). This allowed us to identify the luminance threshold for each participant individually.Participants completed 2 blocks, each containing 14 trials of each of the 8 luminance amplitudes tested, i.e. 112 trials in total.</p>
                <p>The threshold was defined as the luminance amplitude allowing participants to correctly answer in 60% of the cases. In order to determine threshold luminance, we entered each individual visual titration curve into a sigmoid function and picked up the luminance value corresponding to the 60% accuracy on the sigmoid.</p>
              </sec>
              <sec id="s2f">
                <title>Procedure: Main Experiment</title>
                <p>In the main experiment, participants were again presented with a consecutive pair of dynamic white noise rectangles. Again, a Gabor patch was embedded in either the first or second white noise interval, and participants had to indicate whether they saw the flash in the first or second interval after each trial. The luminance amplitude of the target Gabor patch was set to a fixed value corresponding to the individual threshold, determined by the titration procedure for each participant. Trials in the main experiment fell in two conditions in pseudo-random order. In <italic>visual</italic> trials the flashing Gabor patch lasted for one of eight different durations (∼24, ∼36, ∼48, ∼60, ∼72, ∼84, ∼96, ∼190 ms), varying pseudo-randomly between trials. In <italic>audiovisual</italic> trials, the Gabor patch flash duration was fixed at ∼24 ms. While in visual trials no sound occurred, audiovisual trials additionally contained a pure tone auditory stimulus which was played twice with onsets at 353 ms after the first and second white noise interval respectively (i.e. synchronous with the Gabor patch onset in the target interval and at the matching time point during the non-target interval). The duration of tones pseudo-randomly varied between trials, corresponding to the same eight durations that the flashes could have in the unimodal visual condition (∼24, ∼36, ∼48, ∼60, ∼72, ∼84, ∼96 and ∼190 ms). Tone durations were always equal for the first and second interval of a given trial. Participants were instructed that auditory stimuli were irrelevant for the purpose of the task and therefore to ignore them. Participants completed 6 blocks of 80 trials each for a total of 30 stimuli per duration tested. The procedure is illustrated in <xref ref-type="fig" rid="pone-0054789-g001">Figure 1</xref>.</p>
                <fig id="pone-0054789-g001" orientation="portrait" position="float">
                  <object-id pub-id-type="doi">10.1371/journal.pone.0054789.g001</object-id>
                  <label>Figure 1</label>
                  <caption>
                    <title>Illustration of a trial in the main experiment.</title>
                    <p>Participants fixated a central red dot, while two consecutive intervals of dynamic white noise were presented on the screen. In either the first or second interval a Gabor patch was flashed for ∼24 ms at 1.4 degrees visual angle below fixation. The target flash appeared equiprobably in the first and second interval (each interval lasting 1059 ms, with the onset of the Gabor flash at 353 ms). In the example depicted the target flash appears in the first interval. Additionally, in both intervals a sound of variable duration (∼24 to ∼190 ms) was presented (sound onset 353 ms after interval onset for both intervals). In trials of a second, visual only, condition no sounds were played and the duration of Gabor patch flashes was variable (∼24 to ∼190 ms, matching sound durations in the audiovisual condition). After stimulus presentation the fixation dot turned green, indicating participants should report whether they perceived the Gabor patch in the first or second interval by button press.</p>
                  </caption>
                  <graphic xlink:href="pone.0054789.g001"/>
                </fig>
              </sec>
              <sec id="s2g">
                <title>Data Analysis</title>
                <p>For each participant we computed visual sensitivity (d’) for the visual detection task, we did this independently for each of the visual and corresponding auditory-visual durations using standard formulae <xref rid="pone.0054789-Macmillan1" ref-type="bibr">[15]</xref>.</p>
                <p>To address extreme cases (where false rates were zero) we adjusted all d’ values as suggested in <xref rid="pone.0054789-Snodgrass1" ref-type="bibr">[16]</xref>: false alarm rates were calculated as the number of false alarms +0.5, divided by the number of no-signal trials plus one (and, equally, hit rates as the number of hits +0.5, divided by the number of signal trials plus one; c.f. <xref rid="pone.0054789-Macmillan2" ref-type="bibr">[17]</xref>). d’ was analysed using repeated-measure analysis of variance (ANOVA), with Condition (visual only and audiovisual) and Duration (∼24, ∼36, ∼48, ∼60, ∼72, ∼84, ∼96 and ∼190 ms)as within subjects factors followed up by paired t-tests where appropriate.</p>
              </sec>
            </sec>
            <sec id="s3">
              <title>Results</title>
              <sec id="s3a">
                <title>Effects of Visual and Auditory Stimulus Duration on Visual Sensitivity</title>
                <p>The sensitivity (d’), group means and standard errors are shown in <xref ref-type="fig" rid="pone-0054789-g002">figure 2A</xref> (visual stimuli alone) and 2B (audio-visual stimuli). Note the increase in sensitivity in <xref ref-type="fig" rid="pone-0054789-g002">Figure 2A</xref> as a function of visual stimulus length and the corresponding increase in sensitivity in <xref ref-type="fig" rid="pone-0054789-g002">Figure 2B</xref> for auditory stimuli of ∼60, ∼72, ∼84 and ∼96 ms, before falling back towards baseline level at ∼190 ms. Please refer to <xref ref-type="table" rid="pone-0054789-t001">tables 1</xref> and <xref ref-type="table" rid="pone-0054789-t002">2</xref> for the complete set of signal detection results.</p>
                <fig id="pone-0054789-g002" orientation="portrait" position="float">
                  <object-id pub-id-type="doi">10.1371/journal.pone.0054789.g002</object-id>
                  <label>Figure 2</label>
                  <caption>
                    <title>Effect of stimulus durations on visual sensitivity.</title>
                    <p>Mean visual discrimination sensitivity (d’, SEM indicated) for varying visual stimulus durations (2A, upper panel) and for visual stimuli of fixed duration (∼24 ms), paired with auditory stimuli of varying durations (2B, lower panel). Asterisks indicate significant enhancement in visual sensitivity relative to the shortest (audio-) visual stimulus (leftmost data point in <xref ref-type="fig" rid="pone-0054789-g002">figures 2A and 2B</xref>) (* p&lt;.05, ** p&lt;.01 ***p&lt;.001; all Bonferroni corrected). The rightmost data point represents the average maximum of the visual sensitivity enhancement across participants in the visual task (namely ‘Ind. V max’, <xref ref-type="fig" rid="pone-0054789-g002">Figure 2A</xref>) and audiovisual task (namely ‘Ind. AV max’, <xref ref-type="fig" rid="pone-0054789-g002">Figure 2B</xref>).</p>
                  </caption>
                  <graphic xlink:href="pone.0054789.g002"/>
                </fig>
                <table-wrap id="pone-0054789-t001" orientation="portrait" position="float">
                  <object-id pub-id-type="doi">10.1371/journal.pone.0054789.t001</object-id>
                  <label>Table 1</label>
                  <caption>
                    <title>Hit rates (HIT), false alarm rates (FA) and criteria (c) for the visual condition.</title>
                  </caption>
                  <alternatives>
                    <graphic id="pone-0054789-t001-1" xlink:href="pone.0054789.t001"/>
                    <table frame="hsides" rules="groups">
                      <colgroup span="1">
                        <col align="left" span="1"/>
                        <col align="center" span="1"/>
                        <col align="center" span="1"/>
                        <col align="center" span="1"/>
                      </colgroup>
                      <thead>
                        <tr>
                          <td align="left" rowspan="1" colspan="1">Duration</td>
                          <td align="left" rowspan="1" colspan="1">HIT % (S.E.M.)</td>
                          <td align="left" rowspan="1" colspan="1">FA % (S.E.M.)</td>
                          <td align="left" rowspan="1" colspan="1">c (S.E.M.)</td>
                        </tr>
                      </thead>
                      <tbody>
                        <tr>
                          <td align="left" rowspan="1" colspan="1">
                            <bold>24</bold>
                            <bold>ms</bold>
                          </td>
                          <td align="left" rowspan="1" colspan="1">70% (±3%)</td>
                          <td align="left" rowspan="1" colspan="1">48% (±3%)</td>
                          <td align="left" rowspan="1" colspan="1">−0,21 (±0,07)</td>
                        </tr>
                        <tr>
                          <td align="left" rowspan="1" colspan="1">
                            <bold>36</bold>
                            <bold>ms</bold>
                          </td>
                          <td align="left" rowspan="1" colspan="1">76% (±3%)</td>
                          <td align="left" rowspan="1" colspan="1">34% (±3%)</td>
                          <td align="left" rowspan="1" colspan="1">−0,15 (±0,07)</td>
                        </tr>
                        <tr>
                          <td align="left" rowspan="1" colspan="1">
                            <bold>48</bold>
                            <bold>ms</bold>
                          </td>
                          <td align="left" rowspan="1" colspan="1">84% (±3%)</td>
                          <td align="left" rowspan="1" colspan="1">23% (±3%)</td>
                          <td align="left" rowspan="1" colspan="1">−0,16 (±0,07)</td>
                        </tr>
                        <tr>
                          <td align="left" rowspan="1" colspan="1">
                            <bold>60</bold>
                            <bold>ms</bold>
                          </td>
                          <td align="left" rowspan="1" colspan="1">88% (±2%)</td>
                          <td align="left" rowspan="1" colspan="1">11% (±2%)</td>
                          <td align="left" rowspan="1" colspan="1">0,009 (±0,06)</td>
                        </tr>
                        <tr>
                          <td align="left" rowspan="1" colspan="1">
                            <bold>72</bold>
                            <bold>ms</bold>
                          </td>
                          <td align="left" rowspan="1" colspan="1">94% (±1%)</td>
                          <td align="left" rowspan="1" colspan="1">9% (±2%)</td>
                          <td align="left" rowspan="1" colspan="1">−0,06 (±0,05)</td>
                        </tr>
                        <tr>
                          <td align="left" rowspan="1" colspan="1">
                            <bold>84</bold>
                            <bold>ms</bold>
                          </td>
                          <td align="left" rowspan="1" colspan="1">94% (±1%)</td>
                          <td align="left" rowspan="1" colspan="1">6% (±2%)</td>
                          <td align="left" rowspan="1" colspan="1">0,01 (±0,04)</td>
                        </tr>
                        <tr>
                          <td align="left" rowspan="1" colspan="1">
                            <bold>96</bold>
                            <bold>ms</bold>
                          </td>
                          <td align="left" rowspan="1" colspan="1">96% (±1%)</td>
                          <td align="left" rowspan="1" colspan="1">6% (±2%)</td>
                          <td align="left" rowspan="1" colspan="1">−0,05 (±0,04)</td>
                        </tr>
                        <tr>
                          <td align="left" rowspan="1" colspan="1">
                            <bold>190</bold>
                            <bold>ms</bold>
                          </td>
                          <td align="left" rowspan="1" colspan="1">99% (±1%)</td>
                          <td align="left" rowspan="1" colspan="1">3% (±1%)</td>
                          <td align="left" rowspan="1" colspan="1">−0,05 (±0,05)</td>
                        </tr>
                      </tbody>
                    </table>
                  </alternatives>
                  <table-wrap-foot>
                    <fn id="nt101">
                      <label/>
                      <p>Cells contain the mean and standard error of the mean (S.E.M.) across participants. Signal trials were defined as the ones in which the visual stimulus was displayed during the first interval.</p>
                    </fn>
                  </table-wrap-foot>
                </table-wrap>
                <table-wrap id="pone-0054789-t002" orientation="portrait" position="float">
                  <object-id pub-id-type="doi">10.1371/journal.pone.0054789.t002</object-id>
                  <label>Table 2</label>
                  <caption>
                    <title>Hit rates (HIT), false alarm rates (FA) and criteria (c) for the audiovisual condition.</title>
                  </caption>
                  <alternatives>
                    <graphic id="pone-0054789-t002-2" xlink:href="pone.0054789.t002"/>
                    <table frame="hsides" rules="groups">
                      <colgroup span="1">
                        <col align="left" span="1"/>
                        <col align="center" span="1"/>
                        <col align="center" span="1"/>
                        <col align="center" span="1"/>
                      </colgroup>
                      <thead>
                        <tr>
                          <td align="left" rowspan="1" colspan="1">Duration</td>
                          <td align="left" rowspan="1" colspan="1">HIT % (S.E.M.)</td>
                          <td align="left" rowspan="1" colspan="1">FA % (S.E.M.)</td>
                          <td align="left" rowspan="1" colspan="1">c (S.E.M.)</td>
                        </tr>
                      </thead>
                      <tbody>
                        <tr>
                          <td align="left" rowspan="1" colspan="1">
                            <bold>24</bold>
                            <bold>ms</bold>
                          </td>
                          <td align="left" rowspan="1" colspan="1">64% (±2%)</td>
                          <td align="left" rowspan="1" colspan="1">53% (±3%)</td>
                          <td align="left" rowspan="1" colspan="1">−0,21 (±0,07)</td>
                        </tr>
                        <tr>
                          <td align="left" rowspan="1" colspan="1">
                            <bold>36</bold>
                            <bold>ms</bold>
                          </td>
                          <td align="left" rowspan="1" colspan="1">65% (±4%)</td>
                          <td align="left" rowspan="1" colspan="1">50% (±3%)</td>
                          <td align="left" rowspan="1" colspan="1">−0,21 (±0,08)</td>
                        </tr>
                        <tr>
                          <td align="left" rowspan="1" colspan="1">
                            <bold>48</bold>
                            <bold>ms</bold>
                          </td>
                          <td align="left" rowspan="1" colspan="1">62% (±3%)</td>
                          <td align="left" rowspan="1" colspan="1">42% (±3%)</td>
                          <td align="left" rowspan="1" colspan="1">−0,06 (±0,07)</td>
                        </tr>
                        <tr>
                          <td align="left" rowspan="1" colspan="1">
                            <bold>60</bold>
                            <bold>ms</bold>
                          </td>
                          <td align="left" rowspan="1" colspan="1">65% (±3%)</td>
                          <td align="left" rowspan="1" colspan="1">44% (±3%)</td>
                          <td align="left" rowspan="1" colspan="1">−0,13 (±0,05)</td>
                        </tr>
                        <tr>
                          <td align="left" rowspan="1" colspan="1">
                            <bold>72</bold>
                            <bold>ms</bold>
                          </td>
                          <td align="left" rowspan="1" colspan="1">64% (±4%)</td>
                          <td align="left" rowspan="1" colspan="1">41% (±3%)</td>
                          <td align="left" rowspan="1" colspan="1">−0,06 (±0,08)</td>
                        </tr>
                        <tr>
                          <td align="left" rowspan="1" colspan="1">
                            <bold>84</bold>
                            <bold>ms</bold>
                          </td>
                          <td align="left" rowspan="1" colspan="1">68% (±3%)</td>
                          <td align="left" rowspan="1" colspan="1">45% (±3%)</td>
                          <td align="left" rowspan="1" colspan="1">−0,18 (±0,06)</td>
                        </tr>
                        <tr>
                          <td align="left" rowspan="1" colspan="1">
                            <bold>96</bold>
                            <bold>ms</bold>
                          </td>
                          <td align="left" rowspan="1" colspan="1">65% (±3%)</td>
                          <td align="left" rowspan="1" colspan="1">40% (±3%)</td>
                          <td align="left" rowspan="1" colspan="1">−0,06 (±0,06)</td>
                        </tr>
                        <tr>
                          <td align="left" rowspan="1" colspan="1">
                            <bold>190</bold>
                            <bold>ms</bold>
                          </td>
                          <td align="left" rowspan="1" colspan="1">62% (±4%)</td>
                          <td align="left" rowspan="1" colspan="1">49% (±3%)</td>
                          <td align="left" rowspan="1" colspan="1">−0,15 (±0,08)</td>
                        </tr>
                      </tbody>
                    </table>
                  </alternatives>
                  <table-wrap-foot>
                    <fn id="nt102">
                      <label/>
                      <p>Cells contain the mean and standard error of the mean (S.E.M.) across participants. Signal trials were defined as the ones in which the visual stimulus was displayed during the first interval.</p>
                    </fn>
                  </table-wrap-foot>
                </table-wrap>
                <p>The 2×8 ANOVA showed a main effect of Condition (F(1,27) = 563.73; p&lt;0.000), a main effect of Duration (F(7,189) = 60.7; p&lt;0.000) and a significant interaction between these two factors (F(7,189) = 46.92; p&lt;0.000). We broke down our analysis by the factor Condition, thus testing the factor Duration for visual only trials and audiovisual trials separately. For visual only trials we found a significant effect of Duration (F(7,189) = 92.17; p&lt;0.0000). Paired t-tests showed that compared to the shortest visual stimulus duration (∼24 ms, our baseline measure: BSL), all other visual stimulus durations enhanced visual sensitivity (all <italic>p</italic>s&lt;0.004, Bonferroni corrected). Crucially, also the repeated-measure ANOVA for audiovisual trials showed a significant effect of auditory stimulus Duration on visual sensitivity (F(7,189) = 2.29; p = 0.029).</p>
                <p>Paired t-tests showed that compared to the shortest, baseline audiovisual stimulus (∼24 ms), visual sensitivity was enhanced for auditory stimulus durations of ∼60 ms (t(27) = −3.08; p = 0.03, Bonferroni corrected), ∼72 ms (t(27) = −3.75; p = 0.006, Bonferroni corrected) ∼84 ms ((t(27) = −4.06; p = 0.005, Bonferroni corrected) and ∼96 ms (t(27) = −3.84; p = 0.005, Bonferroni corrected). All other auditory durations (∼36, ∼48 and ∼190 ms) did not significantly differ from our BSL (all ps&gt;0.23, Bonferroni corrected).The maximum of the auditory-duration induced visual sensitivity enhancement was smaller (∼0.6 d’; 7% increase relative to audiovisual BSL) than the enhancement induced by a genuinely longer lasting visual stimulus (∼3 d’; 35% increase relative to visual enhancement). Furthermore, sensitivity for the shortest (∼24 ms) audiovisual stimulus was significantly lower than for the shortest (∼24 ms) unimodal visual stimulus (t(27) = −3.36, p&lt;.05, Bonferroni corrected), while sensitivity for all other sound durations was not significantly different from sensitivity for the shortest unimodal visual stimulus (all p values &gt;.24).</p>
              </sec>
              <sec id="s3b">
                <title>Correlation between Effects of Visual and Auditory Stimulus Duration</title>
                <p>The magnitude of visual sensitivity enhancement for prolonged sounds relative to the shortest sound (i.e. BSL corrected values) varied considerably across participants (range: 0 to 2.35 d’, mean.93 d’; SD:.51 d’). The same was true with regard to the magnitude of enhancement for genuinely prolonged visual stimuli relative to the shortest visual stimulus, i.e. BSL corrected values (range: 0.86 to 3.84 d’, mean: 2.95 d’; SD:.69 d’). There were individual differences with regard to both processes: visual sensitivity enhancement by prolonging visual stimulus duration and by prolonging the duration of co-occurring sounds (cf. <xref rid="pone.0054789-deHaas1" ref-type="bibr">[18]</xref>–<xref rid="pone.0054789-Spence1" ref-type="bibr">[21]</xref> for individual differences in audiovisual integration). We reasoned that the size of these effects would be correlated across participants if they stem from similar mechanisms.</p>
                <p>Furthermore, participants also differed with regard to the particular sound duration for which they showed maximum visual sensitivity enhancement. We speculated that this reflected genuine trait-like differences between participants, such as the individual width of the multisensory window of integration (cf. <xref rid="pone.0054789-Stone1" ref-type="bibr">[20]</xref>, <xref rid="pone.0054789-Spence1" ref-type="bibr">[21]</xref>). Based on this assumption we reasoned that the maximum visual sensitivity enhancement (relative to BSL) for a given participant would be the best indicator for this participant’s effect size relative to other participants. We refer to this <italic>individual</italic> peak auditory enhancement as ‘the maximum audiovisual sensitivity enhancement’ (Ind. AV max).</p>
                <p>In a similar way we calculated the <italic>individual</italic> ‘maximum visual sensitivity enhancement’ (Ind. V max) in the unimodal condition. Note that the <italic>absolute</italic> size of both ‘Ind. V max’ and ‘Ind. AV max’ are likely to be inflated and thus non-informative per se. Still they appear as the ‘fairest’ way of quantifying <italic>relative</italic> individual effect sizes without biasing towards a particular window of integration.</p>
                <p>We therefore tested the correlation between ‘Ind. AV max’ and ‘Ind. V max’. The individual maxima in duration induced enhancement were significantly correlated between both conditions (r = .38, p&lt;.05; see <xref ref-type="fig" rid="pone-0054789-g003">Figure 3</xref>).</p>
                <fig id="pone-0054789-g003" orientation="portrait" position="float">
                  <object-id pub-id-type="doi">10.1371/journal.pone.0054789.g003</object-id>
                  <label>Figure 3</label>
                  <caption>
                    <title>Correlation between visual and audio-visual enhancement.</title>
                    <p>Correlation between individual peak auditory-induced enhancement (Ind. AV max) and peak visually-induced enhancement. Note that the maximum effective auditory and visual durations varied between participants and were thus determined on an individual basis (c.f. Methods for details).</p>
                  </caption>
                  <graphic xlink:href="pone.0054789.g003"/>
                </fig>
              </sec>
            </sec>
            <sec id="s4">
              <title>Discussion</title>
              <sec id="s4a">
                <title>Sound Duration and Visual Sensitivity</title>
                <p>We found that visual sensitivity depended on the duration of co-occurring auditory stimuli. Specifically, visual detection sensitivity (d’) for a ∼24 ms visual flash was significantly enhanced for auditory stimuli whose durations were between ∼60 and ∼96 ms, as compared to performance at baseline (matching auditory duration of ∼24 ms). However, no such visual sensitivity enhancement was found for an auditory stimulus lasting much longer than this critical time window (∼190 ms). A rather surprising aspect of our results is that the baseline level of performance in the audiovisual condition was significantly lower than the unimodal baseline performance. Participants’ detection performance became significantly worse when a ∼24 ms flash was accompanied by an auditory stimulus of matching duration (as compared to no accompanying sound).</p>
                <p>Our results are consistent with previous findings that auditory stimuli can bias duration judgments for co-occurring visual stimuli (e.g. <xref rid="pone.0054789-Walker1" ref-type="bibr">[7]</xref>–<xref rid="pone.0054789-Romei1" ref-type="bibr">[11]</xref>). Here, we show for the first time that the duration of auditory stimuli also impacts objective visual performance for non-temporal visual stimulus qualities.</p>
                <p>We previously proposed that effects of auditory duration on visual duration judgements reflect cross-modal binding processes <xref rid="pone.0054789-Romei1" ref-type="bibr">[11]</xref>. A visual event is perceived as longer when paired with a slightly longer lasting auditory event that is perceived as part of the same multisensory event <xref rid="pone.0054789-Romei1" ref-type="bibr">[11]</xref>. If this reflects a genuine effect of sustaining visual perception, it should affect duration judgements as well as non-temporal qualities of visual perception, including detection sensitivity for visual stimuli.</p>
                <p>Our current findings support this hypothesis and characterize the enhancement of visual sensitivity for sounds of longer duration within a restricted time window. This time window (∼60–96 ms) is consistent with previous findings regarding critical time windows for audiovisual integration (see e.g. <xref rid="pone.0054789-Bolognini1" ref-type="bibr">[12]</xref>, <xref rid="pone.0054789-Romei2" ref-type="bibr">[13]</xref>).</p>
              </sec>
              <sec id="s4b">
                <title>Lower Baseline Performance in the Audiovisual Condition</title>
                <p>Despite the predicted pattern of results within the audiovisual condition, a comparison across conditions yielded a surprising result in need of explanation. Lower baseline performance in the audiovisual condition was neither predicted by our hypotheses, nor the results of previous studies. Generally, the mere presentation of auditory stimuli during a visual task can modulate visual performance (e.g. <xref rid="pone.0054789-Shams1" ref-type="bibr">[4]</xref>, <xref rid="pone.0054789-Bolognini1" ref-type="bibr">[12]</xref>, <xref rid="pone.0054789-Vroomen1" ref-type="bibr">[22]</xref>–<xref rid="pone.0054789-Frassinetti1" ref-type="bibr">[29]</xref>) as well as responses in early visual areas <xref rid="pone.0054789-Romei2" ref-type="bibr">[13]</xref>, <xref rid="pone.0054789-Murray1" ref-type="bibr">[30]</xref>–<xref rid="pone.0054789-Romei3" ref-type="bibr">[35]</xref>. Even studies using very similar stimuli and paradigms found a detection sensitivity <italic>enhancement</italic> for audiovisual vs. visual stimuli, rather than a detrimental effect <xref rid="pone.0054789-Noesselt1" ref-type="bibr">[26]</xref>, <xref rid="pone.0054789-Chen2" ref-type="bibr">[36]</xref>.</p>
                <p>In Noesselt et al. <xref rid="pone.0054789-Noesselt1" ref-type="bibr">[26]</xref> participants had to decide in each trial, whether a Gabor patch was flashed in a cued peripheral region of interest or not. As in our experiment, flashes were quite brief (17 ms) and could be accompanied by a sound of matching duration. The presence or absence of sounds was not informative (sounds were as likely to be played in no-signal as in signal trials). Flash intensity was thresholded to 55–65% (low intensity) or 85–95% correct (high intensity) for unimodal visual stimuli. The presence of sounds yielded a significant detection sensitivity enhancement for low intensity stimuli. This condition was very similar to our experiment, with the only differences being that our stimuli were embedded in dynamic noise patterns and we used a two interval forced choice design, rather than a simple detection design. The latter difference could in theory be of importance – simple detection designs are more prone to biases in decision criterion and sound induced performance enhancements could thus be due to criterion shifts. Participants in this study <xref rid="pone.0054789-Noesselt1" ref-type="bibr">[26]</xref> showed indeed a conservative bias for low intensity stimuli, but the improvement in the audiovisual condition was in objective performance (d’) and not accompanied by a shift towards a more liberal criterion. The differences in results between this study and ours are thus unlikely to derive from the differences in task design.</p>
                <p>The design of Chen et al. <xref rid="pone.0054789-Chen2" ref-type="bibr">[36]</xref> was even closer to ours. Here, like in our study, participants had to decide in which of two intervals a Gabor patch was flashed for 17 ms, and stimuli were embedded in dynamic noise. The only difference was that in this study stimulus frames were interleaved with frames of noise, while our stimuli were superimposed with the noise mask (see above, Methods: Stimuli). Chen et al. <xref rid="pone.0054789-Chen2" ref-type="bibr">[36]</xref> measured stimulus intensity thresholds for fixed steps of noise intensities and compared the resulting threshold curves in the presence vs. absence of a non-informative, co-occurring sound with matching duration. The co-occurring sound led to significantly lowered detection thresholds, but this effect was restricted to one out of seven noise intensities. Further, an example set of psychometric functions provided (their <xref ref-type="fig" rid="pone-0054789-g002">Figure 2</xref>) points to the possibility that the sound-induced enhancement for stimulus intensities around threshold (i.e. 75% correct for unimodal stimuli) might be much less pronounced, or even reversed for lower stimulus intensities (yielding performance levels of 55–65% that we aimed for in our thresholding performance). Taken together, baseline performance enhancement for audiovisual stimuli – in the particular design we used – seems to be rather subtle and dependent on specific combinations of performance level, signal intensity and noise intensity.</p>
                <p>Still, to our knowledge, we are the first ones to report a significant detrimental effect of co-occurring sounds. Further research is needed to investigate this effect. One might speculate that very short sound transients can have a detrimental effect on visual sensitivity due to modality specific latencies in neural processing and the tendency of the subjective point of audiovisual synchrony to be shifted towards a visual lead <xref rid="pone.0054789-Vroomen3" ref-type="bibr">[37]</xref>. Depending on the nature of cross-modal interactions, a sound-induced boost in visual neural activity might precede the onset of visually evoked activity. This in turn could lower the signal to noise ratio of visual stimulus evoked activityHowever both of the studies discussed above (<xref rid="pone.0054789-Noesselt1" ref-type="bibr">[26]</xref> and <xref rid="pone.0054789-Chen2" ref-type="bibr">[36]</xref>) found sensitivity <italic>enhancement</italic> for even shorter audiovisual stimuli than ours. Further research could investigate this effect in a systematic way by parametrically varying perceptual performance levels, signal intensity and noise intensity. Crucially, future studies could also test for a potential role of modality specific processing latencies by introducing and parametrically varying a temporal offset between flashes and sounds.</p>
              </sec>
              <sec id="s4c">
                <title>Do Longer Sounds Affect Visual Sensitivity?</title>
                <p>The pattern of results we observed can be interpreted in two major ways. One interpretation, consistent with our initial hypothesis would suppose a process lowering overall visual sensitivity in the presence of co-occurring sounds, and a second, counter-acting process of visual sensitivity enhancement for longer lasting sounds. An alternative interpretation would suppose a process lowering visual sensitivity that is exclusive to a sound of short, matching duration and would suppose no effect on visual sensitivity whatsoever for longer sounds. Although the latter interpretation appears simpler, we think our data are more in line with the first interpretation.</p>
                <p>There are two aspects of our data that are hard to reconcile with the view that only the shortest sound duration had an effect on visual sensitivity. The first is the shape of the curve for visual sensitivity vs. sound duration (<xref ref-type="fig" rid="pone-0054789-g002">Figure 2</xref> B). Just as we predicted, visual sensitivity gradually increased for longer durations, but fell back to baseline level for a duration purposefully chosen to fall outside the temporal window of integration (e.g. <xref rid="pone.0054789-Bolognini1" ref-type="bibr">[12]</xref>, c.f. <xref ref-type="sec" rid="s1">Introduction</xref>). This drop to baseline level is expected under the hypothesis of prolonged sounds <italic>within</italic> the temporal window of integration enhancing visual sensitivity. But it is unexpected and hard to explain under the assumption that only sounds of matching duration had an effect on visual sensitivity. It would be interesting for future experiments to test visual sensitivity between 96 and 190 ms (for which we have no data). It would be particularly interesting to see whether visual sensitivity rises above unimodal baseline level before it drops off again.</p>
                <p>The second aspect of our data supporting an enhancement of visual sensitivity due to prolonged sounds is the observed correlation between conditions. Across participants the maximum difference between visual baseline level and performance for prolonged visual stimuli correlated with the maximum difference between audiovisual baseline and performance for prolonged sounds. This correlation would fit with the hypothesis that prolonged sounds yield a sustained visual neural activity or visual perception. We expect participants gaining more from physically prolonged visual stimulus durations to equally gain more from cross-modally induced sustain of visual representations. In contrast, there is no explanation for this correlation under the assumption that only the shortest sound duration had an effect on visual sensitivity. Taken together, we view the first of our proposed interpretations to be the more likely one for the pattern of data we observed. But if auditory stimulus duration influences visual sensitivity, how does it do so?</p>
              </sec>
              <sec id="s4d">
                <title>Potential Mechanisms for Sound Duration Dependent Visual Sensitivity Enhancement</title>
                <p>As noted above, the mere presentation of auditory stimuli during a visual task can modulate visual performance (e.g. <xref rid="pone.0054789-Shams1" ref-type="bibr">[4]</xref>, <xref rid="pone.0054789-Bolognini1" ref-type="bibr">[12]</xref>, <xref rid="pone.0054789-Vroomen1" ref-type="bibr">[22]</xref>–<xref rid="pone.0054789-Kim1" ref-type="bibr">[28]</xref>) as well as responses in early visual areas <xref rid="pone.0054789-Romei2" ref-type="bibr">[13]</xref>, <xref rid="pone.0054789-Murray1" ref-type="bibr">[30]</xref>–<xref rid="pone.0054789-Romei3" ref-type="bibr">[35]</xref>. In light of these findings it seems reasonable to interpret our results as representing sustained visual activation corresponding to the duration of co-occurring auditory stimuli. A recent study by Romei et al. <xref rid="pone.0054789-Romei4" ref-type="bibr">[38]</xref> found that the presentation of a brief auditory stimulus can phase-align oscillatory activity in the alpha frequency band over occipito-parietal areas and consequently modulate perception. These findings suggest a role for alpha oscillations in determining cross-modal effects on visual cortex excitability that might apply to our results. A critical time window of ∼60–100 ms would indeed correspond to one full alpha cycle and is likely to represent the temporal window for binding crossmodal information. Furthermore, the observed inter-individual variability in optimal duration of auditory stimuli could correspond to individual differences in oscillatory alpha frequency. Future studies should ascertain whether and to what extent the effects of auditory stimulus duration on visual sensitivity and duration judgments are due to oscillatory phase reset.</p>
                <p>An alternative (but possibly compatible) mechanism mediating the effects of longer sounds on visual sensitivity would be sound-induced attention or arousal. The accumulated stimulus energy of a longer sound will exceed the one of a shorter sound and maybe this kind of stronger signal is better suited to guide temporal attention towards the visual stimulus. Note, however that this kind of temporal uncertainty reduction has been psychophysically tested and refuted as an explanation for visual sensitivity enhancement induced by the mere presence of co-occurring sounds <xref rid="pone.0054789-Chen2" ref-type="bibr">[36]</xref>. Nevertheless, it could play a role in the duration dependent effects described here. If temporal attention guidance is (at least part of) the mechanism behind our findings, it would show interesting features of cross-modal integration. Its effects would be restricted to a temporal window of integration and would take place <italic>after</italic> the visual stimulus offset (note that the physical offset of the visual stimulus always preceded any physical differences in sound durations). The latter point underscores that any such effect of attention would be hard to distinguish from a cross-modal sustain of visual representations. The distinction between an explanation involving temporal attention and cross-modal effects might be artificial after all. Specifically, effects of cross-modal phase reset and of attention might work hand in hand, as suggested by recent neurophysiological results <xref rid="pone.0054789-Lakatos1" ref-type="bibr">[32]</xref>.</p>
              </sec>
            </sec>
          </body>
          <back>
            <ref-list>
              <title>References</title>
              <ref id="pone.0054789-Eagleman1">
                <label>1</label>
                <mixed-citation publication-type="journal"><name><surname>Eagleman</surname><given-names>DM</given-names></name> (<year>2008</year>) <article-title>Human time perception and its illusions</article-title>. <source>Current Opinion in Neurobiology</source>
<volume>18</volume>: <fpage>131</fpage>–<lpage>136</lpage> Available: <ext-link ext-link-type="uri" xlink:href="http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=2866156&amp;tool=pmcentrez&amp;rendertype=abstract">http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=2866156&amp;tool=pmcentrez&amp;rendertype=abstract</ext-link>.<pub-id pub-id-type="pmid">18639634</pub-id></mixed-citation>
              </ref>
              <ref id="pone.0054789-Shipley1">
                <label>2</label>
                <mixed-citation publication-type="other">Shipley T (1964) Auditory Flutter-Driving of Visual Flicker. Science 145: 1328–1330. Available: <ext-link ext-link-type="uri" xlink:href="http://www.sciencemag.org/cgi/content/abstract/145/3638/1328">http://www.sciencemag.org/cgi/content/abstract/145/3638/1328</ext-link>. Accessed 2012 Dec 27.</mixed-citation>
              </ref>
              <ref id="pone.0054789-Wada1">
                <label>3</label>
                <mixed-citation publication-type="journal"><name><surname>Wada</surname><given-names>Y</given-names></name>, <name><surname>Kitagawa</surname><given-names>N</given-names></name>, <name><surname>Noguchi</surname><given-names>K</given-names></name> (<year>2003</year>) <article-title>Audio-visual integration in temporal perception</article-title>. <source>International journal of psychophysiology?: official journal of the International Organization of Psychophysiology</source>
<volume>50</volume>: <fpage>117</fpage>–<lpage>124</lpage> Available: <ext-link ext-link-type="uri" xlink:href="http://www.ncbi.nlm.nih.gov/pubmed/14511840">http://www.ncbi.nlm.nih.gov/pubmed/14511840</ext-link>.<pub-id pub-id-type="pmid">14511840</pub-id></mixed-citation>
              </ref>
              <ref id="pone.0054789-Shams1">
                <label>4</label>
                <mixed-citation publication-type="other">Shams L, Kamitani Y, Shimojo S (2000) Illusions. What you see is what you hear. Nature 408: 788. Available: <ext-link ext-link-type="uri" xlink:href="http://www.ncbi.nlm.nih.gov/pubmed/11130706">http://www.ncbi.nlm.nih.gov/pubmed/11130706</ext-link>. Accessed 18 April 2012.</mixed-citation>
              </ref>
              <ref id="pone.0054789-Freeman1">
                <label>5</label>
                <mixed-citation publication-type="journal"><name><surname>Freeman</surname><given-names>E</given-names></name>, <name><surname>Driver</surname><given-names>J</given-names></name> (<year>2008</year>) <article-title>Direction of visual apparent motion driven solely by timing of a static sound</article-title>. <source>Current Biology</source>
<volume>18</volume>: <fpage>1262</fpage>–<lpage>1266</lpage>.<pub-id pub-id-type="pmid">18718760</pub-id></mixed-citation>
              </ref>
              <ref id="pone.0054789-Burr1">
                <label>6</label>
                <mixed-citation publication-type="journal"><name><surname>Burr</surname><given-names>D</given-names></name>, <name><surname>Banks</surname><given-names>MS</given-names></name>, <name><surname>Morrone</surname><given-names>MC</given-names></name> (<year>2009</year>) <article-title>Auditory dominance over vision in the perception of interval duration</article-title>. <source>Experimental brain research Experimentelle Hirnforschung Expérimentation cérébrale</source>
<volume>198</volume>: <fpage>49</fpage>–<lpage>57</lpage> Available: <ext-link ext-link-type="uri" xlink:href="http://www.ncbi.nlm.nih.gov/pubmed/19597804">http://www.ncbi.nlm.nih.gov/pubmed/19597804</ext-link>.<pub-id pub-id-type="pmid">19597804</pub-id></mixed-citation>
              </ref>
              <ref id="pone.0054789-Walker1">
                <label>7</label>
                <mixed-citation publication-type="journal"><name><surname>Walker</surname><given-names>JT</given-names></name>, <name><surname>Scott</surname><given-names>KJ</given-names></name> (<year>1981</year>) <article-title>Auditory-visual conflicts in the perceived duration of lights, tones and gaps</article-title>. <source>Journal of experimental psychology Human perception and performance</source>
<volume>7</volume>: <fpage>1327</fpage>–<lpage>1339</lpage> Available: <ext-link ext-link-type="uri" xlink:href="http://www.ncbi.nlm.nih.gov/pubmed/6458656">http://www.ncbi.nlm.nih.gov/pubmed/6458656</ext-link>.<pub-id pub-id-type="pmid">6458656</pub-id></mixed-citation>
              </ref>
              <ref id="pone.0054789-Donovan1">
                <label>8</label>
                <mixed-citation publication-type="journal"><name><surname>Donovan</surname><given-names>C-L</given-names></name>, <name><surname>Lindsay</surname><given-names>DS</given-names></name>, <name><surname>Kingstone</surname><given-names>A</given-names></name> (<year>2004</year>) <article-title>Flexible and abstract resolutions to crossmodal conflicts</article-title>. <source>Brain and Cognition</source>
<volume>56</volume>: <fpage>1</fpage>–<lpage>4</lpage> Available: <ext-link ext-link-type="uri" xlink:href="http://www.ncbi.nlm.nih.gov/pubmed/15380869">http://www.ncbi.nlm.nih.gov/pubmed/15380869</ext-link>.<pub-id pub-id-type="pmid">15380869</pub-id></mixed-citation>
              </ref>
              <ref id="pone.0054789-Chen1">
                <label>9</label>
                <mixed-citation publication-type="journal"><name><surname>Chen</surname><given-names>K-M</given-names></name>, <name><surname>Yeh</surname><given-names>S-L</given-names></name> (<year>2009</year>) <article-title>Asymmetric cross-modal effects in time perception</article-title>. <source>Acta psychologica</source>
<volume>130</volume>: <fpage>225</fpage>–<lpage>234</lpage> Available: <ext-link ext-link-type="uri" xlink:href="http://www.ncbi.nlm.nih.gov/pubmed/19195633">http://www.ncbi.nlm.nih.gov/pubmed/19195633</ext-link>.<pub-id pub-id-type="pmid">19195633</pub-id></mixed-citation>
              </ref>
              <ref id="pone.0054789-Klink1">
                <label>10</label>
                <mixed-citation publication-type="journal"><name><surname>Klink</surname><given-names>PC</given-names></name>, <name><surname>Montijn</surname><given-names>JS</given-names></name>, <name><surname>Van Wezel</surname><given-names>RJA</given-names></name> (<year>2011</year>) <article-title>Crossmodal duration perception involves perceptual grouping, temporal ventriloquism, and variable internal clock rates</article-title>. <source>Attention perception psychophysics</source>
<volume>73</volume>: <fpage>219</fpage>–<lpage>236</lpage> Available: <ext-link ext-link-type="uri" xlink:href="http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=3025116&amp;tool=pmcentrez&amp;rendertype=abstract">http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=3025116&amp;tool=pmcentrez&amp;rendertype=abstract</ext-link>.</mixed-citation>
              </ref>
              <ref id="pone.0054789-Romei1">
                <label>11</label>
                <mixed-citation publication-type="journal"><name><surname>Romei</surname><given-names>V</given-names></name>, <name><surname>De Haas</surname><given-names>B</given-names></name>, <name><surname>Mok</surname><given-names>RM</given-names></name>, <name><surname>Driver</surname><given-names>J</given-names></name> (<year>2011</year>) <article-title>Auditory Stimulus Timing Influences Perceived duration of Co-Occurring Visual Stimuli</article-title>. <source>Frontiers in psychology</source>
<volume>2</volume>: <fpage>8</fpage>.<pub-id pub-id-type="pmid">21713182</pub-id></mixed-citation>
              </ref>
              <ref id="pone.0054789-Bolognini1">
                <label>12</label>
                <mixed-citation publication-type="journal"><name><surname>Bolognini</surname><given-names>N</given-names></name>, <name><surname>Frassinetti</surname><given-names>F</given-names></name>, <name><surname>Serino</surname><given-names>A</given-names></name>, <name><surname>Làdavas</surname><given-names>E</given-names></name> (<year>2005</year>) <article-title>“Acoustical vision” of below threshold stimuli: interaction among spatially converging audiovisual inputs</article-title>. <source>Experimental brain research Experimentelle Hirnforschung Expérimentation cérébrale</source>
<volume>160</volume>: <fpage>273</fpage>–<lpage>282</lpage> Available: <ext-link ext-link-type="uri" xlink:href="http://www.ncbi.nlm.nih.gov/pubmed/15551091">http://www.ncbi.nlm.nih.gov/pubmed/15551091</ext-link>.<pub-id pub-id-type="pmid">15551091</pub-id></mixed-citation>
              </ref>
              <ref id="pone.0054789-Romei2">
                <label>13</label>
                <mixed-citation publication-type="journal"><name><surname>Romei</surname><given-names>V</given-names></name>, <name><surname>Murray</surname><given-names>MM</given-names></name>, <name><surname>Merabet</surname><given-names>LB</given-names></name>, <name><surname>Thut</surname><given-names>G</given-names></name> (<year>2007</year>) <article-title>Occipital transcranial magnetic stimulation has opposing effects on visual and auditory stimulus detection: implications for multisensory interactions</article-title>. <source>The Journal of neuroscience?</source>
<volume>27</volume>: <fpage>11465</fpage>–<lpage>11472</lpage> Available: <ext-link ext-link-type="uri" xlink:href="http://www.ncbi.nlm.nih.gov/pubmed/17959789">http://www.ncbi.nlm.nih.gov/pubmed/17959789</ext-link>.<pub-id pub-id-type="pmid">17959789</pub-id></mixed-citation>
              </ref>
              <ref id="pone.0054789-Kleiner1">
                <label>14</label>
                <mixed-citation publication-type="other">Kleiner M, Brainard D, Pelli D (2007) What’s new in Psychtoolbox-3? Perception 36: 14. Available: <ext-link ext-link-type="uri" xlink:href="http://www.perceptionweb.com/abstract.cgi?id=v070821">http://www.perceptionweb.com/abstract.cgi?id=v070821</ext-link>. Accessed 2012 Dec 27.</mixed-citation>
              </ref>
              <ref id="pone.0054789-Macmillan1">
                <label>15</label>
                <mixed-citation publication-type="journal"><name><surname>Macmillan</surname><given-names>NA</given-names></name>, <name><surname>Creelman</surname><given-names>CD</given-names></name> (<year>1997</year>) <article-title>d’plus: A program to calculate accuracy and bias measures from detection and discrimination data</article-title>. <source>Spatial Vision</source>
<volume>11</volume>: <fpage>141</fpage>–<lpage>143</lpage>.<pub-id pub-id-type="pmid">18095396</pub-id></mixed-citation>
              </ref>
              <ref id="pone.0054789-Snodgrass1">
                <label>16</label>
                <mixed-citation publication-type="journal"><name><surname>Snodgrass</surname><given-names>JG</given-names></name>, <name><surname>Corwin</surname><given-names>J</given-names></name> (<year>1988</year>) <article-title>Pragmatics of measuring recognition memory: applications to dementia and amnesia</article-title>. <source>Journal of experimental psychology General</source>
<volume>117</volume>: <fpage>34</fpage>–<lpage>50</lpage> Available: <ext-link ext-link-type="uri" xlink:href="http://www.ncbi.nlm.nih.gov/pubmed/2966230">http://www.ncbi.nlm.nih.gov/pubmed/2966230</ext-link>.<pub-id pub-id-type="pmid">2966230</pub-id></mixed-citation>
              </ref>
              <ref id="pone.0054789-Macmillan2">
                <label>17</label>
                <mixed-citation publication-type="other">Macmillan NA, Creelman CD (2004) Detection Theory: A User’s Guide. Psychology Press.</mixed-citation>
              </ref>
              <ref id="pone.0054789-deHaas1">
                <label>18</label>
                <mixed-citation publication-type="other">de Haas B, Kanai R, Jalkanen L, Rees G (2012) Grey matter volume in early human visual cortex predicts proneness to the sound-induced flash illusion. Proceedings Biological sciences/The Royal Society. Available: <ext-link ext-link-type="uri" xlink:href="http://www.ncbi.nlm.nih.gov/pubmed/23097516">http://www.ncbi.nlm.nih.gov/pubmed/23097516</ext-link>.</mixed-citation>
              </ref>
              <ref id="pone.0054789-Nath1">
                <label>19</label>
                <mixed-citation publication-type="journal"><name><surname>Nath</surname><given-names>AR</given-names></name>, <name><surname>Beauchamp</surname><given-names>MS</given-names></name> (<year>2012</year>) <article-title>A neural basis for interindividual differences in the McGurk effect, a multisensory speech illusion</article-title>. <source>NeuroImage</source>
<volume>59</volume>: <fpage>781</fpage>–<lpage>787</lpage> Available: <ext-link ext-link-type="uri" xlink:href="http://www.ncbi.nlm.nih.gov/pubmed/21787869">http://www.ncbi.nlm.nih.gov/pubmed/21787869</ext-link>.<pub-id pub-id-type="pmid">21787869</pub-id></mixed-citation>
              </ref>
              <ref id="pone.0054789-Stone1">
                <label>20</label>
                <mixed-citation publication-type="journal"><name><surname>Stone</surname><given-names>JV</given-names></name>, <name><surname>Hunkin</surname><given-names>NM</given-names></name>, <name><surname>Porrill</surname><given-names>J</given-names></name>, <name><surname>Wood</surname><given-names>R</given-names></name>, <name><surname>Keeler</surname><given-names>V</given-names></name>, <etal>et al</etal> (<year>2001</year>) <article-title>When is now? Perception of simultaneity</article-title>. <source>Proceedings Biological sciences/The Royal Society</source>
<volume>268</volume>: <fpage>31</fpage>–<lpage>38</lpage> Available: <ext-link ext-link-type="uri" xlink:href="http://rspb.royalsocietypublishing.org/content/268/1462/31">http://rspb.royalsocietypublishing.org/content/268/1462/31</ext-link>.</mixed-citation>
              </ref>
              <ref id="pone.0054789-Spence1">
                <label>21</label>
                <mixed-citation publication-type="journal"><name><surname>Spence</surname><given-names>C</given-names></name>, <name><surname>Squire</surname><given-names>S</given-names></name> (<year>2003</year>) <article-title>Multisensory integration: maintaining the perception of synchrony</article-title>. <source>Current Biology?</source>
<volume>13</volume>: <fpage>R519</fpage>–<lpage>21</lpage> Available: <ext-link ext-link-type="uri" xlink:href="http://www.ncbi.nlm.nih.gov/pubmed/12842029">http://www.ncbi.nlm.nih.gov/pubmed/12842029</ext-link>.<pub-id pub-id-type="pmid">12842029</pub-id></mixed-citation>
              </ref>
              <ref id="pone.0054789-Vroomen1">
                <label>22</label>
                <mixed-citation publication-type="journal"><name><surname>Vroomen</surname><given-names>J</given-names></name>, <name><surname>De Gelder</surname><given-names>B</given-names></name> (<year>2000</year>) <article-title>Sound enhances visual perception: cross-modal effects of auditory organization on vision</article-title>. <source>Journal of Experimental Psychology: Human Perception and Performance</source>
<volume>26</volume>: <fpage>1583</fpage>–<lpage>1590</lpage> Available: <ext-link ext-link-type="uri" xlink:href="http://www.ncbi.nlm.nih.gov/pubmed/11039486">http://www.ncbi.nlm.nih.gov/pubmed/11039486</ext-link>.<pub-id pub-id-type="pmid">11039486</pub-id></mixed-citation>
              </ref>
              <ref id="pone.0054789-Spence2">
                <label>23</label>
                <mixed-citation publication-type="other">Spence C, Driver J (2004) Crossmodal space and crossmodal attention. Spence C, Driver J, editors OUP.</mixed-citation>
              </ref>
              <ref id="pone.0054789-Vroomen2">
                <label>24</label>
                <mixed-citation publication-type="journal"><name><surname>Vroomen</surname><given-names>J</given-names></name>, <name><surname>Keetels</surname><given-names>M</given-names></name> (<year>2009</year>) <article-title>Sounds change four-dot masking</article-title>. <source>Acta Psychologica</source>
<volume>130</volume>: <fpage>58</fpage>–<lpage>63</lpage> Available: <ext-link ext-link-type="uri" xlink:href="http://www.ncbi.nlm.nih.gov/pubmed/19012870">http://www.ncbi.nlm.nih.gov/pubmed/19012870</ext-link>.<pub-id pub-id-type="pmid">19012870</pub-id></mixed-citation>
              </ref>
              <ref id="pone.0054789-Cappe1">
                <label>25</label>
                <mixed-citation publication-type="journal"><name><surname>Cappe</surname><given-names>C</given-names></name>, <name><surname>Thut</surname><given-names>G</given-names></name>, <name><surname>Romei</surname><given-names>V</given-names></name>, <name><surname>Murray</surname><given-names>MM</given-names></name> (<year>2009</year>) <article-title>Selective integration of auditory-visual looming cues by humans</article-title>. <source>Neuropsychologia</source>
<volume>47</volume>: <fpage>1045</fpage>–<lpage>1052</lpage> Available: <ext-link ext-link-type="uri" xlink:href="http://www.ncbi.nlm.nih.gov/pubmed/19041883">http://www.ncbi.nlm.nih.gov/pubmed/19041883</ext-link>.<pub-id pub-id-type="pmid">19041883</pub-id></mixed-citation>
              </ref>
              <ref id="pone.0054789-Noesselt1">
                <label>26</label>
                <mixed-citation publication-type="journal"><name><surname>Noesselt</surname><given-names>T</given-names></name>, <name><surname>Tyll</surname><given-names>S</given-names></name>, <name><surname>Boehler</surname><given-names>CN</given-names></name>, <name><surname>Budinger</surname><given-names>E</given-names></name>, <name><surname>Heinze</surname><given-names>H-J</given-names></name>, <etal>et al</etal> (<year>2010</year>) <article-title>Sound-Induced Enhancement of Low-Intensity Vision: Multisensory Influences on Human Sensory-Specific Cortices and Thalamic Bodies Relate to Perceptual Enhancement of Visual Detection Sensitivity</article-title>. <source>Journal of Neuroscience</source>
<volume>30</volume>: <fpage>13609</fpage>–<lpage>13623</lpage> Available: <ext-link ext-link-type="uri" xlink:href="http://www.jneurosci.org/cgi/doi/10.1523/JNEUROSCI.4524-09.2010">http://www.jneurosci.org/cgi/doi/10.1523/JNEUROSCI.4524-09.2010</ext-link>.<pub-id pub-id-type="pmid">20943902</pub-id></mixed-citation>
              </ref>
              <ref id="pone.0054789-Leo1">
                <label>27</label>
                <mixed-citation publication-type="journal"><name><surname>Leo</surname><given-names>F</given-names></name>, <name><surname>Romei</surname><given-names>V</given-names></name>, <name><surname>Freeman</surname><given-names>E</given-names></name>, <name><surname>Ladavas</surname><given-names>E</given-names></name>, <name><surname>Driver</surname><given-names>J</given-names></name> (<year>2011</year>) <article-title>Looming sounds enhance orientation sensitivity for visual stimuli on the same side as such sounds</article-title>. <source>Experimental Brain Research</source>
<volume>213</volume>: <fpage>193</fpage>–<lpage>201</lpage>.<pub-id pub-id-type="pmid">21643714</pub-id></mixed-citation>
              </ref>
              <ref id="pone.0054789-Kim1">
                <label>28</label>
                <mixed-citation publication-type="journal"><name><surname>Kim</surname><given-names>R</given-names></name>, <name><surname>Peters</surname><given-names>MAK</given-names></name>, <name><surname>Shams</surname><given-names>L</given-names></name> (<year>2011</year>) <article-title>0+1&gt;1: How Adding Noninformative Sound Improves Performance on a Visual Task</article-title>. <source>Psychological Science</source>
<volume>32</volume>: <fpage>6</fpage>–<lpage>12</lpage> Available: <ext-link ext-link-type="uri" xlink:href="http://www.ncbi.nlm.nih.gov/pubmed/22127367">http://www.ncbi.nlm.nih.gov/pubmed/22127367</ext-link>.</mixed-citation>
              </ref>
              <ref id="pone.0054789-Frassinetti1">
                <label>29</label>
                <mixed-citation publication-type="journal"><name><surname>Frassinetti</surname><given-names>F</given-names></name>, <name><surname>Bolognini</surname><given-names>N</given-names></name>, <name><surname>Làdavas</surname><given-names>E</given-names></name> (<year>2002</year>) <article-title>Enhancement of visual perception by crossmodal visuo-auditory interaction</article-title>. <source>Experimental brain research Experimentelle Hirnforschung Expérimentation cérébrale</source>
<volume>147</volume>: <fpage>332</fpage>–<lpage>343</lpage> Available: <ext-link ext-link-type="uri" xlink:href="http://www.springerlink.com/content/md0ef3xvfeaq3ylj/">http://www.springerlink.com/content/md0ef3xvfeaq3ylj/</ext-link>.<pub-id pub-id-type="pmid">12428141</pub-id></mixed-citation>
              </ref>
              <ref id="pone.0054789-Murray1">
                <label>30</label>
                <mixed-citation publication-type="other">Murray MM, Cappe C, Romei V, Martuzzi R, Thut G (2012) Auditory-visual multisensory interactions in humans: synthesis and controversies. In: Stein B, editor. The New Handbook of Multisensory Processing. MIT press.</mixed-citation>
              </ref>
              <ref id="pone.0054789-Wang1">
                <label>31</label>
                <mixed-citation publication-type="journal"><name><surname>Wang</surname><given-names>Y</given-names></name>, <name><surname>Celebrini</surname><given-names>S</given-names></name>, <name><surname>Trotter</surname><given-names>Y</given-names></name>, <name><surname>Barone</surname><given-names>P</given-names></name> (<year>2008</year>) <article-title>Visuo-auditory interactions in the primary visual cortex of the behaving monkey: electrophysiological evidence</article-title>. <source>BMC neuroscience</source>
<volume>9</volume>: <fpage>79</fpage> Available: <ext-link ext-link-type="uri" xlink:href="http://www.ncbi.nlm.nih.gov/pubmed/18699988">http://www.ncbi.nlm.nih.gov/pubmed/18699988</ext-link>.<pub-id pub-id-type="pmid">18699988</pub-id></mixed-citation>
              </ref>
              <ref id="pone.0054789-Lakatos1">
                <label>32</label>
                <mixed-citation publication-type="journal"><name><surname>Lakatos</surname><given-names>P</given-names></name>, <name><surname>O’Connell</surname><given-names>MN</given-names></name>, <name><surname>Barczak</surname><given-names>A</given-names></name>, <name><surname>Mills</surname><given-names>A</given-names></name>, <name><surname>Javitt</surname><given-names>DC</given-names></name>, <etal>et al</etal> (<year>2009</year>) <article-title>The leading sense: supramodal control of neurophysiological context by attention</article-title>. <source>Neuron</source>
<volume>64</volume>: <fpage>419</fpage>–<lpage>430</lpage> Available: <ext-link ext-link-type="uri" xlink:href="http://www.ncbi.nlm.nih.gov/pubmed/19914189">http://www.ncbi.nlm.nih.gov/pubmed/19914189</ext-link>.<pub-id pub-id-type="pmid">19914189</pub-id></mixed-citation>
              </ref>
              <ref id="pone.0054789-Cappe2">
                <label>33</label>
                <mixed-citation publication-type="journal"><name><surname>Cappe</surname><given-names>C</given-names></name>, <name><surname>Thut</surname><given-names>G</given-names></name>, <name><surname>Romei</surname><given-names>V</given-names></name>, <name><surname>Murray</surname><given-names>MM</given-names></name> (<year>2010</year>) <article-title>C., Thut, G., Romei, V., Murray MM. Auditory-visual multisensory interactions in humans: timing, topography, directionality, and sources</article-title>. <source>Journal of Neuroscience</source>
<volume>30</volume>: <fpage>12572</fpage>–<lpage>12580</lpage>.<pub-id pub-id-type="pmid">20861363</pub-id></mixed-citation>
              </ref>
              <ref id="pone.0054789-Cappe3">
                <label>34</label>
                <mixed-citation publication-type="journal"><name><surname>Cappe</surname><given-names>C</given-names></name>, <name><surname>Thelen</surname><given-names>A</given-names></name>, <name><surname>Romei</surname><given-names>V</given-names></name>, <name><surname>Thut</surname><given-names>G</given-names></name>, <name><surname>Murray</surname><given-names>M</given-names></name> (<year>2012</year>) <article-title>Looming signals reveal synergistic principles of multisensory integration</article-title>. <source>Journal of Neuroscience</source>
<volume>32</volume>: <fpage>1171</fpage>–<lpage>1182</lpage>.<pub-id pub-id-type="pmid">22279203</pub-id></mixed-citation>
              </ref>
              <ref id="pone.0054789-Romei3">
                <label>35</label>
                <mixed-citation publication-type="journal"><name><surname>Romei</surname><given-names>V</given-names></name>, <name><surname>Murray</surname><given-names>MM</given-names></name>, <name><surname>Cappe</surname><given-names>C</given-names></name>, <name><surname>Thut</surname><given-names>G</given-names></name> (<year>2009</year>) <article-title>Preperceptual and stimulus-selective enhancement of low-level human visual cortex excitability by sounds</article-title>. <source>Current biology?: CB</source>
<volume>19</volume>: <fpage>1799</fpage>–<lpage>1805</lpage> Available: <ext-link ext-link-type="uri" xlink:href="http://www.ncbi.nlm.nih.gov/pubmed/19836243">http://www.ncbi.nlm.nih.gov/pubmed/19836243</ext-link>.<pub-id pub-id-type="pmid">19836243</pub-id></mixed-citation>
              </ref>
              <ref id="pone.0054789-Chen2">
                <label>36</label>
                <mixed-citation publication-type="journal"><name><surname>Chen</surname><given-names>Y-C</given-names></name>, <name><surname>Huang</surname><given-names>P-C</given-names></name>, <name><surname>Yeh</surname><given-names>S-L</given-names></name>, <name><surname>Spence</surname><given-names>C</given-names></name> (<year>2011</year>) <article-title>Synchronous sounds enhance visual sensitivity without reducing target uncertainty</article-title>. <source>Seeing and perceiving</source>
<volume>24</volume>: <fpage>623</fpage>–<lpage>638</lpage> Available: <ext-link ext-link-type="uri" xlink:href="http://www.ncbi.nlm.nih.gov/pubmed/22353539">http://www.ncbi.nlm.nih.gov/pubmed/22353539</ext-link>.<pub-id pub-id-type="pmid">22353539</pub-id></mixed-citation>
              </ref>
              <ref id="pone.0054789-Vroomen3">
                <label>37</label>
                <mixed-citation publication-type="journal"><name><surname>Vroomen</surname><given-names>J</given-names></name>, <name><surname>Keetels</surname><given-names>M</given-names></name> (<year>2010</year>) <article-title>Perception of intersensory synchrony: a tutorial review</article-title>. <source>Attention, perception &amp; psychophysics</source>
<volume>72</volume>: <fpage>871</fpage>–<lpage>884</lpage> Available: <ext-link ext-link-type="uri" xlink:href="http://www.ncbi.nlm.nih.gov/pubmed/20436185">http://www.ncbi.nlm.nih.gov/pubmed/20436185</ext-link>.</mixed-citation>
              </ref>
              <ref id="pone.0054789-Romei4">
                <label>38</label>
                <mixed-citation publication-type="journal"><name><surname>Romei</surname><given-names>V</given-names></name>, <name><surname>Gross</surname><given-names>J</given-names></name>, <name><surname>Thut</surname><given-names>G</given-names></name> (<year>2012</year>) <article-title>Sounds reset rhythms of visual cortex and corresponding human visual perception</article-title>. <source>Current Biology</source>
<volume>22</volume>: <fpage>807</fpage>–<lpage>813</lpage>.<pub-id pub-id-type="pmid">22503499</pub-id></mixed-citation>
              </ref>
            </ref-list>
          </back>
        </article>
      </metadata>
    </record>
  </GetRecord>
</OAI-PMH>
