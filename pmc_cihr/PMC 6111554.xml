<?xml version="1.0" encoding="UTF-8"?>
<OAI-PMH xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/ http://www.openarchives.org/OAI/2.0/OAI-PMH.xsd">
  <responseDate>2024-05-03T08:39:30Z</responseDate>
  <request verb="GetRecord" identifier="oai:pubmedcentral.nih.gov:6111554" metadataPrefix="pmc">https:/www.ncbi.nlm.nih.gov/pmc/oai/oai.cgi</request>
  <GetRecord>
    <record>
      <header>
        <identifier>oai:pubmedcentral.nih.gov:6111554</identifier>
        <datestamp>2018-08-30</datestamp>
        <setSpec>sensors</setSpec>
        <setSpec>pmc-open</setSpec>
      </header>
      <metadata>
        <article xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" xsi:schemaLocation="https://jats.nlm.nih.gov/ns/archiving/1.3/ https://jats.nlm.nih.gov/archiving/1.3/xsd/JATS-archivearticle1-3.xsd" article-type="research-article">
          <front>
            <journal-meta>
              <journal-id journal-id-type="nlm-ta">Sensors (Basel)</journal-id>
              <journal-id journal-id-type="iso-abbrev">Sensors (Basel)</journal-id>
              <journal-id journal-id-type="publisher-id">sensors</journal-id>
              <journal-title-group>
                <journal-title>Sensors (Basel, Switzerland)</journal-title>
              </journal-title-group>
              <issn pub-type="epub">1424-8220</issn>
              <publisher>
                <publisher-name>MDPI</publisher-name>
              </publisher>
            </journal-meta>
            <article-meta>
              <article-id pub-id-type="accession">PMC6111554</article-id>
              <article-id pub-id-type="pmcid">PMC6111554</article-id>
              <article-id pub-id-type="pmc-uid">6111554</article-id>
              <article-id pub-id-type="pmid">30065177</article-id>
              <article-id pub-id-type="doi">10.3390/s18082474</article-id>
              <article-id pub-id-type="publisher-id">sensors-18-02474</article-id>
              <article-categories>
                <subj-group subj-group-type="heading">
                  <subject>Article</subject>
                </subj-group>
              </article-categories>
              <title-group>
                <article-title>Detection of Talking in Respiratory Signals: A Feasibility Study Using Machine Learning and Wearable Textile-Based Sensors</article-title>
              </title-group>
              <contrib-group>
                <contrib contrib-type="author">
                  <contrib-id contrib-id-type="orcid" authenticated="true">https://orcid.org/0000-0002-1334-8984</contrib-id>
                  <name>
                    <surname>Ejupi</surname>
                    <given-names>Andreas</given-names>
                  </name>
                </contrib>
                <contrib contrib-type="author">
                  <name>
                    <surname>Menon</surname>
                    <given-names>Carlo</given-names>
                  </name>
                  <xref rid="c1-sensors-18-02474" ref-type="corresp">*</xref>
                </contrib>
              </contrib-group>
              <aff id="af1-sensors-18-02474">Menrva Research Group, Schools of Mechatronic Systems &amp; Engineering Science at Simon Fraser University (SFU), Burnaby, BC V5A 1S6, Canada; <email>andreas@ejupi.at</email></aff>
              <author-notes>
                <corresp id="c1-sensors-18-02474"><label>*</label>Correspondence: <email>cmenon@sfu.ca</email></corresp>
              </author-notes>
              <pub-date pub-type="epub">
                <day>31</day>
                <month>7</month>
                <year>2018</year>
              </pub-date>
              <pub-date pub-type="collection">
                <month>8</month>
                <year>2018</year>
              </pub-date>
              <volume>18</volume>
              <issue>8</issue>
              <elocation-id>2474</elocation-id>
              <history>
                <date date-type="received">
                  <day>27</day>
                  <month>6</month>
                  <year>2018</year>
                </date>
                <date date-type="accepted">
                  <day>25</day>
                  <month>7</month>
                  <year>2018</year>
                </date>
              </history>
              <permissions>
                <copyright-statement>© 2018 by the authors.</copyright-statement>
                <copyright-year>2018</copyright-year>
                <license license-type="open-access">
                  <license-p>Licensee MDPI, Basel, Switzerland. This article is an open access article distributed under the terms and conditions of the Creative Commons Attribution (CC BY) license (<ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/">http://creativecommons.org/licenses/by/4.0/</ext-link>).</license-p>
                </license>
              </permissions>
              <abstract>
                <p>Social isolation and loneliness are major health concerns in young and older people. Traditional approaches to monitor the level of social interaction rely on self-reports. The goal of this study was to investigate if wearable textile-based sensors can be used to accurately detect if the user is talking as a future indicator of social interaction. In a laboratory study, fifteen healthy young participants were asked to talk while performing daily activities such as sitting, standing and walking. It is known that the breathing pattern differs significantly between normal and speech breathing (i.e., talking). We integrated resistive stretch sensors into wearable elastic bands, with a future integration into clothing in mind, to record the expansion and contraction of the chest and abdomen while breathing. We developed an algorithm incorporating machine learning and evaluated its performance in distinguishing between periods of talking and non-talking. In an intra-subject analysis, our algorithm detected talking with an average accuracy of 85%. The highest accuracy of 88% was achieved during sitting and the lowest accuracy of 80.6% during walking. Complete segments of talking were correctly identified with 96% accuracy. From the evaluated machine learning algorithms, the random forest classifier performed best on our dataset. We demonstrate that wearable textile-based sensors in combination with machine learning can be used to detect when the user is talking. In the future, this approach may be used as an indicator of social interaction to prevent social isolation and loneliness.</p>
              </abstract>
              <kwd-group>
                <kwd>wearable sensors</kwd>
                <kwd>machine learning</kwd>
                <kwd>smart textiles</kwd>
                <kwd>healthcare</kwd>
                <kwd>talking detection</kwd>
              </kwd-group>
            </article-meta>
          </front>
          <body>
            <sec id="sec1-sensors-18-02474">
              <title>1. Introduction</title>
              <p>Social isolation and loneliness are important health risk factors and known to negatively influence wellbeing. It has been reported that up to 50% of older people suffer from a low level of social interaction [<xref rid="B1-sensors-18-02474" ref-type="bibr">1</xref>]. The causes can be diverse including general health issues, disabilities and certain life events such as the loss of a spouse or a change in residence [<xref rid="B2-sensors-18-02474" ref-type="bibr">2</xref>,<xref rid="B3-sensors-18-02474" ref-type="bibr">3</xref>]. On a positive note, research has shown that social isolation and loneliness can be prevented. Intervention programs such as in-person support activities or phone-mediated groups have shown promising results [<xref rid="B3-sensors-18-02474" ref-type="bibr">3</xref>]. However, due to the limited health care resources, it would be warranted to accurately identify people who are in need of targeted interventions. Traditional approaches rely on the self-reports using questionnaires to assess the daily level of social interaction. Self-reports are often described as subjective and influenced by a recall bias [<xref rid="B4-sensors-18-02474" ref-type="bibr">4</xref>].</p>
              <p>One alternative approach could be to automatically identify people with a low level of social interaction by using technology. Previous work in this area has mainly focused on audio-based systems using a microphone to capture talking throughout the day [<xref rid="B5-sensors-18-02474" ref-type="bibr">5</xref>,<xref rid="B6-sensors-18-02474" ref-type="bibr">6</xref>]. Previous work has also investigated the use of video-based systems to monitor mouth movements as an indicator of social interaction [<xref rid="B7-sensors-18-02474" ref-type="bibr">7</xref>,<xref rid="B8-sensors-18-02474" ref-type="bibr">8</xref>]. Both methods look promising in terms of accuracy. However, user acceptance and portability might be a challenge [<xref rid="B9-sensors-18-02474" ref-type="bibr">9</xref>].</p>
              <p>There is a need for more unobtrusive and portable solutions. We propose to detect if someone is talking by using wearable textile-based sensors, which can be directly integrated into everyday clothing. Our approach does not rely on audio or video recordings; instead, it aims to detect talking by monitoring changes in the respiratory (i.e., breathing) patterns.</p>
              <sec id="sec1dot1-sensors-18-02474">
                <title>1.1. Detection of Talking (Speech Breathing)</title>
                <p>Generally, breathing results in an expansion and a contraction of the chest and abdominal region. It has been found that the breathing pattern differs significantly between normal and speech breathing (i.e., talking), with the respiration more rhythmic during normal breathing [<xref rid="B10-sensors-18-02474" ref-type="bibr">10</xref>,<xref rid="B11-sensors-18-02474" ref-type="bibr">11</xref>]. It has been also reported that the inhalation duration and the ratio between the inhalation and exhalation time are good discriminatory indicators [<xref rid="B12-sensors-18-02474" ref-type="bibr">12</xref>,<xref rid="B13-sensors-18-02474" ref-type="bibr">13</xref>].</p>
                <p>To date, only a few studies have investigated the use of wearable sensors to detect if someone is talking based on respiratory markers [<xref rid="B10-sensors-18-02474" ref-type="bibr">10</xref>,<xref rid="B12-sensors-18-02474" ref-type="bibr">12</xref>,<xref rid="B14-sensors-18-02474" ref-type="bibr">14</xref>]. These studies used inductive plethysmography sensors, which consist of electrical wires embedded in elastic bands usually attached to the chest and abdominal region. By generating a magnetic field and passing it through a sinusoidal arrangement of electrical wires, the self-conductance of the coils, which is proportional to the cross-sectional area surrounded by the band, can be measured [<xref rid="B12-sensors-18-02474" ref-type="bibr">12</xref>]. However, these sensors are primarily designed for clinical settings and mainly used for short duration recordings.</p>
              </sec>
              <sec id="sec1dot2-sensors-18-02474">
                <title>1.2. Textile-Based Sensors</title>
                <p>In this paper, we investigate the feasibility of wearable textile-based sensors. In particular, we focus on resistive stretch sensors, which are made by a mixture of polymer (e.g., silicone, rubber) and a conductive material (e.g., carbon black). These resistive sensors act like a resistor, which means that any elongation results in a measurable change in electrical resistance. Related work in this field has investigated the use of textile-based stretch sensors in several human applications. For example, Tognetti et al. [<xref rid="B15-sensors-18-02474" ref-type="bibr">15</xref>] investigated a textile-based sensor for posture monitoring. Similarly, Mattman et al. [<xref rid="B16-sensors-18-02474" ref-type="bibr">16</xref>] integrated sensors into tight-fitting clothing to classify between various body postures. Papi et al. [<xref rid="B17-sensors-18-02474" ref-type="bibr">17</xref>] explored the feasibility to discriminate between daily activities (i.e., walking, running, stair climbing) by using a stretch sensor attached to the knee. These studies suggest the preliminary feasibility of textile-based stretch sensors to monitor human motions. To the best of our knowledge, our study is the first to use this type of sensor to detect talking in respiratory signals.</p>
                <p>The main aims of this study were to (1) investigate the feasibility of textile-based stretch sensors to monitor breathing patterns, (2) develop an algorithm using machine learning to accurately detect talking and (3) evaluate its performance in a study with 15 participants.</p>
              </sec>
            </sec>
            <sec id="sec2-sensors-18-02474">
              <title>2. Materials and Methods</title>
              <sec id="sec2dot1-sensors-18-02474">
                <title>2.1. Stretch Sensor</title>
                <p>In this paper, we investigated the feasibility of a wearable textile-based stretch sensor to detect if someone is talking. The stretch sensor has been fabricated in our research lab (Menrva) at Simon Fraser University, Canada [<xref rid="B18-sensors-18-02474" ref-type="bibr">18</xref>], using a mixture of polymer and conductive carbon black. The sensor shows similar properties as the commercially available sensors from Adafruit (New York, NY, USA) [<xref rid="B19-sensors-18-02474" ref-type="bibr">19</xref>] and Image SI (Staten Island, NY, USA) [<xref rid="B20-sensors-18-02474" ref-type="bibr">20</xref>], but only has a diameter of 0.4 mm, which makes it suitable to integrate into garments (<xref ref-type="fig" rid="sensors-18-02474-f001">Figure 1</xref>). Previous work has shown good results in using machine learning to obtain accurate measurements from these textile-based stretch sensors [<xref rid="B21-sensors-18-02474" ref-type="bibr">21</xref>,<xref rid="B22-sensors-18-02474" ref-type="bibr">22</xref>] and using them for the monitoring of human movements [<xref rid="B15-sensors-18-02474" ref-type="bibr">15</xref>,<xref rid="B16-sensors-18-02474" ref-type="bibr">16</xref>,<xref rid="B23-sensors-18-02474" ref-type="bibr">23</xref>].</p>
              </sec>
              <sec id="sec2dot2-sensors-18-02474">
                <title>2.2. Chest and Abdominal Bands</title>
                <p>The approach was to detect talking based on changes in the breathing pattern. As is known from the literature, we can differentiate between chest and abdominal breathing [<xref rid="B24-sensors-18-02474" ref-type="bibr">24</xref>,<xref rid="B25-sensors-18-02474" ref-type="bibr">25</xref>]. Chest breathing can be described as the drawing of air into the chest area by using the intercostal muscles. This type of breathing is more common during states of exertion. In contrast, abdominal breathing is the expansion of the belly by contracting the diaphragm. This type of breathing is common during phases of relaxation [<xref rid="B26-sensors-18-02474" ref-type="bibr">26</xref>].</p>
                <p>However, breathing can be quite diverse between people. Some people are more heavily chest breathers, whereas others are more so abdominal breathers [<xref rid="B25-sensors-18-02474" ref-type="bibr">25</xref>]. To capture the expansion and contraction of the full torso, we designed three elastic bands with the stretch sensor integrated and positioned them at the abdominal, lower and upper chest region for our study (<xref ref-type="fig" rid="sensors-18-02474-f002">Figure 2</xref>). In the future, the sensor might be directly integrated into the clothing.</p>
                <p>The bands were made out of two materials. The back and side part were made out of a synthetic knit with medium elasticity. The front piece and attachment of the sensor were made of a fleece material with high elasticity. The intention was to concentrate the stretch during breathing (and talking) primarily on the sensor. Three pieces of the Menrva stretch sensor with a length of 10 cm each were integrated into the front piece of the bands (<xref ref-type="fig" rid="sensors-18-02474-f002">Figure 2</xref>). Sensors were laid out straight and secured with an elastic stitch on top. The wires were connected on both sides with a mixture of rubber glue and conductive ink.</p>
              </sec>
              <sec id="sec2dot3-sensors-18-02474">
                <title>2.3. Data Acquisition Hardware</title>
                <p>The three bands were connected to a data acquisition system (Model NI-USB-6009, National Instruments, Austin, TX, USA) using a voltage divider circuit to measure their electrical response by connecting a 5 V DC voltage source and a resistor in series to the sensors. The resistor value was selected to match the base resistance of the stretch sensor (20 k<inline-formula><mml:math id="mm1"><mml:mrow><mml:mi mathvariant="normal">Ω</mml:mi></mml:mrow></mml:math></inline-formula>). All data were captured with a sampling rate of 100 Hz.</p>
              </sec>
              <sec id="sec2dot4-sensors-18-02474">
                <title>2.4. Study Protocol</title>
                <p>The study protocol included three main parts with a total duration of 1.5 h per participant including the setup time. Participants were asked to wear the three custom-made sensors to monitor the expansion and contraction of the torso while talking. Sensor bands were tightly fitted, but still comfortable, for each participant. The tightness was adjusted based on the user’s feedback by explaining that the bands should be similarly tight and comfortable as, for example, a tight-fitting t-shirt, usually used for exercising. Participants were asked to talk while sitting, standing and walking. We selected these activities because they are the most common activities in which people talk in daily life. Each activity lasted for 20 min and included 5 trials with 2 min of non-talking and 2 min of talking. The order of the activities was randomized. To capture sufficient data of talking during each period and activity, we asked the participants to read out the text of a news article. The article included general information about the city of Vancouver, Canada. For the walking part, participants were asked to walk on a treadmill. We used a treadmill for convenience due to the limited length of the wires, which connected the bands with the data acquisition hardware. Talking while walking usually occurs at slower speed, and therefore, we selected 2 mph for this test.</p>
              </sec>
              <sec sec-type="subjects" id="sec2dot5-sensors-18-02474">
                <title>2.5. Participants</title>
                <p>Fifteen young adults were asked to participate in this study. Participants were between 19 and 30 years old and were students at Simon Fraser University (SFU), Canada. <xref rid="sensors-18-02474-t001" ref-type="table">Table 1</xref> shows the participant characteristics. Written informed consent was obtained from all participants prior to data collection. The study was approved by the Research Ethics Board of SFU.</p>
              </sec>
              <sec id="sec2dot6-sensors-18-02474">
                <title>2.6. Talking Detection Algorithm</title>
                <p>Our main aim was to detect talking based on changes in the respiratory signals. Before talking, air usually gets inhaled fast and then exhaled slowly while talking. This results in a specific breathing pattern when compared to normal breathing (<xref ref-type="fig" rid="sensors-18-02474-f003">Figure 3</xref>). Our algorithm utilizes this information to detect talking.</p>
                <p>Our algorithm is based on the following steps of data processing and analysis (<xref ref-type="fig" rid="sensors-18-02474-f004">Figure 4</xref>):<list list-type="bullet"><list-item><p>Data input: The input data to our algorithm were the raw sensor signals (sampled with 100 Hz) of the three bands, which we converted from voltage to resistance values.</p></list-item><list-item><p>Signal filtering: A healthy adult usually breathes between 12 and 18 times per minute at rest. For older adults, the breathing can vary between 12 and 30 times per minute [<xref rid="B27-sensors-18-02474" ref-type="bibr">27</xref>]. We filtered the sensor signals accordingly with a bandpass filter (4th order Butterworth, lower cut-off frequency of 0.1 Hz and higher cut-off of 1.5 Hz) to account for possible drift and reduce the overall level of noise in the sensor signals.</p></list-item><list-item><p>Breathing detection: Any inhalation of air and consequent expansion of the torso results in a peak of the stretch sensor signal. Our algorithm detects these peaks using MATLAB’s peak detection algorithm with an empirically-defined parameter of 5 for the minimum peak prominence setting. The prominence of a peak measures how much the peak stands out due to its intrinsic height and its location relative to other peaks.</p></list-item><list-item><p>Feature extraction: The detection of a peak triggers the feature extraction process. The algorithm centres a window with an empirically-found length of 3 s on each detected peak. From this time window, a set of predefined features get extracted and used as the input to a machine learning classifier.</p></list-item><list-item><p>Classification of talking: A machine learning classifier has been trained to detect speech breathing (i.e., talking) based on the extracted features.</p></list-item></list></p>
              </sec>
              <sec id="sec2dot7-sensors-18-02474">
                <title>2.7. Machine Learning Approach</title>
                <p>In the first part of the analysis, we were focused on identifying which machine learning algorithm, hyper parameters and features would generally perform best in the task of detecting talking using this kind of technology. In the second part of the analysis, we applied the selected model and calculated the accuracy for each participant.</p>
                <sec id="sec2dot7dot1-sensors-18-02474">
                  <title>2.7.1. Model Selection</title>
                  <p>Four machine learning algorithms have been selected to investigate their feasibility in detecting talking based on our collected data. We have selected these four algorithms because they have been commonly used in health-related machine learning tasks and have achieved promising results in the past. First, random forest is an ensemble method that operates by constructing multiple decision trees at training time and then uses the mean prediction of individual trees to estimate the target values [<xref rid="B28-sensors-18-02474" ref-type="bibr">28</xref>]. Second, neural network is a method inspired by the biological neural network system using layers and a number of interconnected nodes to make a prediction [<xref rid="B29-sensors-18-02474" ref-type="bibr">29</xref>]. Third, support vector machine operates by constructing a set of hyperplanes in a high- or infinite-dimensional space to estimate the target value [<xref rid="B30-sensors-18-02474" ref-type="bibr">30</xref>]. Fourth, linear discriminant analysis uses a linear decision boundary and has been proven to work well in practice due to its low computational costs [<xref rid="B31-sensors-18-02474" ref-type="bibr">31</xref>].</p>
                  <p>The hyper parameters for the machine learning classifiers were empirically identified. To calculate the performance of each model and select the best performing hyper parameters, we used 15-fold cross-validation. This was done on a training dataset that consisted of the first 70% of data of each participant. The model performance was evaluated using the receiver operating characteristics curve (ROC) and the associated area under the curve (AUC) metric.</p>
                  <p>For the random forest classifier, the best performance was achieved using 200 as the parameter for the number of trees (values tested between 10 and 200). For the support vector machine, the best performance was achieved with gamma set to 0.01 (tested between 0.001 and 1) and C set to 10 (tested between 1 and 100). For the neural network classifier, the best performance was achieved with a network structure of 2 hidden layers (tested from 1 to 2) and 30 neurons in the hidden layers.</p>
                </sec>
                <sec id="sec2dot7dot2-sensors-18-02474">
                  <title>2.7.2. Feature Extraction and Selection</title>
                  <p>Features were extracted with an automated feature extraction approach. Therefore, we used the Python library tsfresh [<xref rid="B32-sensors-18-02474" ref-type="bibr">32</xref>], which calculates and tests more than 100 predefined time and frequency-domain features with various parameters. Using this approach, we extracted features from the raw and first derivate of the sensor signals of all three bands. Features were extracted using a sliding window (size of 3 s) approach. For the feature selection, we also applied 15-fold cross-validation and used the same training dataset as for the hyper parameter tuning. A tree-based approach was used to rank the best performing features based on their relevance (i.e., Gini importance [<xref rid="B33-sensors-18-02474" ref-type="bibr">33</xref>]) for each run. Only the top 10% features among all runs were selected for the final algorithm to reduce complexity and computation time. For a detailed description of the calculation of these features, see [<xref rid="B32-sensors-18-02474" ref-type="bibr">32</xref>]. The majority of significant features were based on the sensor signals of the upper chest and lower chest band. The features included in our final model were:<list list-type="bullet"><list-item><p>Ratio beyond sigma: the ratio of values that are more than <inline-formula><mml:math id="mm2"><mml:mrow><mml:mrow><mml:mi>r</mml:mi><mml:mo>×</mml:mo><mml:mi>s</mml:mi><mml:mi>t</mml:mi><mml:mi>d</mml:mi><mml:mo>(</mml:mo><mml:mi>x</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> away from the mean of <italic>x</italic> (with <inline-formula><mml:math id="mm3"><mml:mrow><mml:mrow><mml:mi>r</mml:mi><mml:mo>=</mml:mo><mml:mo>{</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mn>2</mml:mn><mml:mo>}</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>).</p></list-item><list-item><p>Symmetry looking: the Boolean variable denoting if the distribution of <italic>x</italic> looks symmetric.</p></list-item><list-item><p>Continues Wavelet Transform peaks: the number of peaks of the continuous wavelet transform using a Mexican hat wavelet [<xref rid="B34-sensors-18-02474" ref-type="bibr">34</xref>].</p></list-item><list-item><p>Skewness: the sample skewness of <italic>x</italic> (calculated with the adjusted Fisher–Pearson standardized moment coefficient G1).</p></list-item><list-item><p>Energy ratio by chunks: the sum of squares of chunk <italic>i</italic> out of <italic>N</italic> chunks expressed as a ratio with the sum of squares over the whole (with <inline-formula><mml:math id="mm4"><mml:mrow><mml:mrow><mml:mi>N</mml:mi><mml:mo>=</mml:mo><mml:mn>10</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula>).</p></list-item><list-item><p>Augmented Dickey–Fuller: the hypothesis test that checks whether a unit root is present in <italic>x</italic> [<xref rid="B35-sensors-18-02474" ref-type="bibr">35</xref>].</p></list-item><list-item><p>Count above mean: the number of values in <italic>x</italic> that are higher than the mean of <italic>x</italic>.</p></list-item><list-item><p>Count below mean: the number of values in <italic>x</italic> that are lower than the mean of <italic>x</italic>.</p></list-item><list-item><p>Number of crossings: the number of crossings of <italic>x</italic> on <italic>m</italic> (with <inline-formula><mml:math id="mm5"><mml:mrow><mml:mrow><mml:mi>m</mml:mi><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula>).</p></list-item><list-item><p>Fourier coefficients: the coefficients of the one-dimensional discrete Fourier transform [<xref rid="B36-sensors-18-02474" ref-type="bibr">36</xref>].</p></list-item><list-item><p>Welch’s spectral density: the cross power spectral density of <italic>x</italic> [<xref rid="B37-sensors-18-02474" ref-type="bibr">37</xref>].</p></list-item><list-item><p>Sample entropy: the sample entropy of <italic>x</italic>.</p></list-item><list-item><p>Autoregressive coefficients: the fit of the unconditional maximum likelihood of an autoregressive <inline-formula><mml:math id="mm6"><mml:mrow><mml:mrow><mml:mi>A</mml:mi><mml:mi>R</mml:mi><mml:mo>(</mml:mo><mml:mi>k</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> process.</p></list-item></list></p>
                </sec>
                <sec id="sec2dot7dot3-sensors-18-02474">
                  <title>2.7.3. Performance Evaluation</title>
                  <p>We integrated the best performing machine learning model, features and parameters into our algorithm and evaluated its performance in detecting talking in an intra-subject analysis. The data of each participant were split into the activities of sitting, standing and walking. For each activity, we trained a model separately and evaluated it using cross-validation.</p>
                  <p>As sample-based performance metrics, accuracy (<inline-formula><mml:math id="mm7"><mml:mrow><mml:mrow><mml:mi>A</mml:mi><mml:mi>C</mml:mi><mml:mi>C</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula>), true positive rate (<inline-formula><mml:math id="mm8"><mml:mrow><mml:mrow><mml:mi>T</mml:mi><mml:mi>P</mml:mi><mml:mi>R</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula>) and false positive rate (<inline-formula><mml:math id="mm9"><mml:mrow><mml:mrow><mml:mi>F</mml:mi><mml:mi>P</mml:mi><mml:mi>R</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula>) were selected. <inline-formula><mml:math id="mm10"><mml:mrow><mml:mrow><mml:mi>T</mml:mi><mml:mi>P</mml:mi><mml:mi>R</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> has been defined as the percentage of correctly identified speech breathing patterns. <inline-formula><mml:math id="mm11"><mml:mrow><mml:mrow><mml:mi>F</mml:mi><mml:mi>P</mml:mi><mml:mi>R</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> has been defined as the percentage of incorrectly identified speech breathing patterns among all other breathing patterns.</p>
                  <disp-formula id="FD1-sensors-18-02474">
                    <label>(1)</label>
                    <mml:math id="mm12">
                      <mml:mrow>
                        <mml:mrow>
                          <mml:mi>A</mml:mi>
                          <mml:mi>C</mml:mi>
                          <mml:mi>C</mml:mi>
                          <mml:mo>=</mml:mo>
                          <mml:mfrac>
                            <mml:mrow>
                              <mml:mi>T</mml:mi>
                              <mml:mi>P</mml:mi>
                              <mml:mo>+</mml:mo>
                              <mml:mi>T</mml:mi>
                              <mml:mi>N</mml:mi>
                            </mml:mrow>
                            <mml:mrow>
                              <mml:mi>T</mml:mi>
                              <mml:mi>P</mml:mi>
                              <mml:mo>+</mml:mo>
                              <mml:mi>T</mml:mi>
                              <mml:mi>N</mml:mi>
                              <mml:mo>+</mml:mo>
                              <mml:mi>F</mml:mi>
                              <mml:mi>P</mml:mi>
                              <mml:mo>+</mml:mo>
                              <mml:mi>F</mml:mi>
                              <mml:mi>N</mml:mi>
                            </mml:mrow>
                          </mml:mfrac>
                        </mml:mrow>
                      </mml:mrow>
                    </mml:math>
                  </disp-formula>
                  <disp-formula id="FD2-sensors-18-02474">
                    <label>(2)</label>
                    <mml:math id="mm13">
                      <mml:mrow>
                        <mml:mrow>
                          <mml:mi>T</mml:mi>
                          <mml:mi>P</mml:mi>
                          <mml:mi>R</mml:mi>
                          <mml:mo>=</mml:mo>
                          <mml:mfrac>
                            <mml:mrow>
                              <mml:mi>T</mml:mi>
                              <mml:mi>P</mml:mi>
                            </mml:mrow>
                            <mml:mrow>
                              <mml:mi>T</mml:mi>
                              <mml:mi>P</mml:mi>
                              <mml:mo>+</mml:mo>
                              <mml:mi>F</mml:mi>
                              <mml:mi>N</mml:mi>
                            </mml:mrow>
                          </mml:mfrac>
                        </mml:mrow>
                      </mml:mrow>
                    </mml:math>
                  </disp-formula>
                  <disp-formula id="FD3-sensors-18-02474">
                    <label>(3)</label>
                    <mml:math id="mm14">
                      <mml:mrow>
                        <mml:mrow>
                          <mml:mi>F</mml:mi>
                          <mml:mi>P</mml:mi>
                          <mml:mi>R</mml:mi>
                          <mml:mo>=</mml:mo>
                          <mml:mfrac>
                            <mml:mrow>
                              <mml:mi>F</mml:mi>
                              <mml:mi>P</mml:mi>
                            </mml:mrow>
                            <mml:mrow>
                              <mml:mi>F</mml:mi>
                              <mml:mi>P</mml:mi>
                              <mml:mo>+</mml:mo>
                              <mml:mi>T</mml:mi>
                              <mml:mi>N</mml:mi>
                            </mml:mrow>
                          </mml:mfrac>
                        </mml:mrow>
                      </mml:mrow>
                    </mml:math>
                  </disp-formula>
                  <p>Furthermore, the number of correctly identified talking segments was calculated. A talking segment was classified correctly if the majority of prediction labels in this segment predicted talking.</p>
                  <disp-formula id="FD4-sensors-18-02474">
                    <label>(4)</label>
                    <mml:math id="mm15">
                      <mml:mrow>
                        <mml:mrow>
                          <mml:mi>A</mml:mi>
                          <mml:mi>C</mml:mi>
                          <mml:msub>
                            <mml:mi>C</mml:mi>
                            <mml:mrow>
                              <mml:mi>s</mml:mi>
                              <mml:mi>e</mml:mi>
                              <mml:mi>g</mml:mi>
                            </mml:mrow>
                          </mml:msub>
                          <mml:mo>=</mml:mo>
                          <mml:mfrac>
                            <mml:mrow>
                              <mml:mi>c</mml:mi>
                              <mml:mi>o</mml:mi>
                              <mml:mi>r</mml:mi>
                              <mml:mi>r</mml:mi>
                              <mml:mi>e</mml:mi>
                              <mml:mi>c</mml:mi>
                              <mml:mi>t</mml:mi>
                              <mml:mi>l</mml:mi>
                              <mml:mi>y</mml:mi>
                              <mml:mo>_</mml:mo>
                              <mml:mi>c</mml:mi>
                              <mml:mi>l</mml:mi>
                              <mml:mi>a</mml:mi>
                              <mml:mi>s</mml:mi>
                              <mml:mi>s</mml:mi>
                              <mml:mi>i</mml:mi>
                              <mml:mi>f</mml:mi>
                              <mml:mi>i</mml:mi>
                              <mml:mi>e</mml:mi>
                              <mml:mi>d</mml:mi>
                              <mml:mo>_</mml:mo>
                              <mml:mi>t</mml:mi>
                              <mml:mi>a</mml:mi>
                              <mml:mi>l</mml:mi>
                              <mml:mi>k</mml:mi>
                              <mml:mi>i</mml:mi>
                              <mml:mi>n</mml:mi>
                              <mml:mi>g</mml:mi>
                              <mml:mo>_</mml:mo>
                              <mml:mi>s</mml:mi>
                              <mml:mi>e</mml:mi>
                              <mml:mi>g</mml:mi>
                              <mml:mi>m</mml:mi>
                              <mml:mi>e</mml:mi>
                              <mml:mi>n</mml:mi>
                              <mml:mi>t</mml:mi>
                              <mml:mi>s</mml:mi>
                            </mml:mrow>
                            <mml:mrow>
                              <mml:mi>t</mml:mi>
                              <mml:mi>o</mml:mi>
                              <mml:mi>t</mml:mi>
                              <mml:mi>a</mml:mi>
                              <mml:mi>l</mml:mi>
                              <mml:mo>_</mml:mo>
                              <mml:mi>n</mml:mi>
                              <mml:mi>u</mml:mi>
                              <mml:mi>m</mml:mi>
                              <mml:mi>b</mml:mi>
                              <mml:mi>e</mml:mi>
                              <mml:mi>r</mml:mi>
                              <mml:mo>_</mml:mo>
                              <mml:mi>o</mml:mi>
                              <mml:mi>f</mml:mi>
                              <mml:mo>_</mml:mo>
                              <mml:mi>t</mml:mi>
                              <mml:mi>a</mml:mi>
                              <mml:mi>l</mml:mi>
                              <mml:mi>k</mml:mi>
                              <mml:mi>i</mml:mi>
                              <mml:mi>n</mml:mi>
                              <mml:mi>g</mml:mi>
                              <mml:mo>_</mml:mo>
                              <mml:mi>s</mml:mi>
                              <mml:mi>e</mml:mi>
                              <mml:mi>g</mml:mi>
                              <mml:mi>m</mml:mi>
                              <mml:mi>e</mml:mi>
                              <mml:mi>n</mml:mi>
                              <mml:mi>t</mml:mi>
                              <mml:mi>s</mml:mi>
                            </mml:mrow>
                          </mml:mfrac>
                        </mml:mrow>
                      </mml:mrow>
                    </mml:math>
                  </disp-formula>
                </sec>
              </sec>
              <sec id="sec2dot8-sensors-18-02474">
                <title>2.8. Software</title>
                <p>MATLAB (R2016b) was used for data acquisition, processing of the sensor data and algorithm development. The Python package scikit-learn [<xref rid="B38-sensors-18-02474" ref-type="bibr">38</xref>] was used to train and evaluate the machine learning models. The Python package tsfresh [<xref rid="B32-sensors-18-02474" ref-type="bibr">32</xref>] was used for automated feature extraction.</p>
              </sec>
            </sec>
            <sec id="sec3-sensors-18-02474">
              <title>3. Results</title>
              <p>One hour of sensor data was recorded from each participant with a recording time of 30 min of talking. The entire dataset included 11,924 detected breathings, which were used for further classification. We observed significant differences between normal and speech breathing in the activities of sitting, standing and walking (<xref ref-type="fig" rid="sensors-18-02474-f005">Figure 5</xref>). During the phases of talking, the breathing is less rhythmic with faster inhalations and slower exhalations.</p>
              <sec id="sec3dot1-sensors-18-02474">
                <title>3.1. Model Selection</title>
                <p>Among all tested machine learning algorithms, the random forest (and support vector machine) classifier performed best on our dataset with an AUC value of 0.90, which was slightly higher compared to the performance of the neural network classifier (AUC = 0.89) and linear discriminant analysis (AUC = 0.87) (<xref ref-type="fig" rid="sensors-18-02474-f006">Figure 6</xref>).</p>
              </sec>
              <sec id="sec3dot2-sensors-18-02474">
                <title>3.2. Accuracy of Talking Detection Algorithm</title>
                <p>Among all participants, our algorithm utilizing the random forest classifier detected talking with an average <italic>ACC</italic> of 85% (<italic>TPR</italic>: 81.3%, <italic>FPR</italic>: 12.8%) (<xref rid="sensors-18-02474-t002" ref-type="table">Table 2</xref>). The highest <italic>ACC</italic> of 88% was achieved in the sitting task and the lowest <italic>ACC</italic> of 80.6% in walking. <xref rid="sensors-18-02474-t003" ref-type="table">Table 3</xref> shows the results for each participant in detail with the accuracy ranging from 68.8% to 97.5%. Furthermore, segments of talking have been correctly classified with an <italic>ACC</italic><inline-formula><mml:math id="mm16"><mml:mrow><mml:msub><mml:mrow/><mml:mrow><mml:mi>s</mml:mi><mml:mi>e</mml:mi><mml:mi>g</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> of 96.3%. <xref ref-type="fig" rid="sensors-18-02474-f007">Figure 7</xref> illustrates the exemplary prediction accuracy of our algorithm on the data of participant P10. The number of misclassifications increased from sitting, standing to walking.</p>
              </sec>
            </sec>
            <sec id="sec4-sensors-18-02474">
              <title>4. Discussion</title>
              <p>We developed an algorithm that can detect if the user is talking based on respiratory markers. In contrast to previous work, we used textile-based stretch sensors to monitor the expansion and contraction of the torso and achieved a reasonable accuracy by incorporating machine learning into our algorithm.</p>
              <p>Previous studies have relied on either audio or video recordings to detect talking. Besides the technical challenges of these approaches, there might be also privacy concerns [<xref rid="B9-sensors-18-02474" ref-type="bibr">9</xref>]. The aim of this study was to develop a system that is unobtrusive and portable. We selected a wearable approach, as it would allow quantifying talking throughout the day independent of the user’s location. This is in alignment with a recent trend in the development of the wearable technologies for various health applications [<xref rid="B39-sensors-18-02474" ref-type="bibr">39</xref>,<xref rid="B40-sensors-18-02474" ref-type="bibr">40</xref>].</p>
              <p>Our approach uses wearable textile-based sensors to monitor breathing and as a consequence detect if someone is talking. Although there were some studies that have investigated the feasibility of detecting respiratory events in the past, only a few studies have focused on the detection of talking in respiratory signals [<xref rid="B10-sensors-18-02474" ref-type="bibr">10</xref>,<xref rid="B12-sensors-18-02474" ref-type="bibr">12</xref>,<xref rid="B14-sensors-18-02474" ref-type="bibr">14</xref>]. These studies have used inductive plethysmograph sensors. Conventional inductive plethysmograph sensors are primarily designed for the clinical setting and short-term recordings with possible limitations in the size of the electronics and number of sensors that can be used at the same time [<xref rid="B41-sensors-18-02474" ref-type="bibr">41</xref>].</p>
              <p>In terms of accuracy, Rahman et al. [<xref rid="B10-sensors-18-02474" ref-type="bibr">10</xref>] (and Bari et al. [<xref rid="B42-sensors-18-02474" ref-type="bibr">42</xref>]) reported 82 to 87% in speech/non-speech classification using inductive plethysmograph sensors. The reported accuracy is in alignment with what we have achieved in this study.</p>
              <p>What differentiates this work is the use of textile-based stretch sensors in combination with the developed machine learning-based algorithm. The sensor we used is flexible with a diameter of only 0.4 mm and acts like a resistor, which makes it easy to integrate into garments and to acquire measurements. We proposed an algorithm suitable to detect talking including a comprehensive identified and discriminative set of features upon which future work can build.</p>
              <p>What we have observed is that breathing and the corresponding patterns were quite heterogeneous between participants. Breathing was either shallow, normal or deep, and for some participants, the chest expansion was more noticeable, whereas for others, the abdominal region expanded more. We compensated for this behaviour by training our algorithm individually for each participant. In practice, this would suggest that a calibration phase might be needed before the system can be used by an individual. Another factor that might have influenced the accuracy was the sensitivity of the technology to noise due to body movements. Breathing and the corresponding expansion of the torso result in a relatively small elongation of the stretch sensor. What we have observed is that rotational and bending movements of the upper body influenced the measurements. This was especially noticeable in the task of walking, which might explain the lower accuracy in this task. Future work might combine our approach with an accelerometer to filter out the noise due to body movements.</p>
              <p>Considering the advantages of the technology, this approach might be suitable for the daily life setting. A future application could be the integration of the sensor (or a series of sensors) into a tight-fitting undershirt. In addition to the sensor, a circuit board and battery would be required. Preliminary results show that the sensor has a power draw of about 1.25 mW (as used in this study). This would allow the monitoring of the user’s level of talking throughout the day, and furthermore, this measurement could be used as an indicator of social interaction. Such a system might be used in older adults where social isolation and loneliness are common concerns [<xref rid="B1-sensors-18-02474" ref-type="bibr">1</xref>,<xref rid="B3-sensors-18-02474" ref-type="bibr">3</xref>]. For example, in an institutionalized setting, such a system could provide the staff daily feedback about the level of social interaction of each resident. Once a significant change in behaviour has been detected, targeted interventions could be started. Similarly, this technology could be used in older people living in the community where a low level of social interaction can lead to more frequent home visits by the healthcare professionals.</p>
              <p>We acknowledge certain study limitations. Data were collected in the laboratory setting under fairly controlled conditions with young and healthy adults. Participants were asked to read a text out loud, which might be different from conversational speaking. Future studies are warranted to determine whether this approach can be used in a daily life setting and to investigate the accuracy and user acceptance of this system in the older population.</p>
              <p>In summary, we have demonstrated that wearable textile-based sensors in combination with a machine learning-based algorithm can be used to detect when the user is talking. In future, this approach may be used to unobtrusively quantify talking as an indicator of social interaction, and consequently may prevent social isolation and loneliness.</p>
            </sec>
          </body>
          <back>
            <ack>
              <title>Acknowledgments</title>
              <p>The authors would like to thank all participants and individuals who contributed to the data collection.</p>
            </ack>
            <notes>
              <title>Author Contributions</title>
              <p>All authors read and approved the manuscript. Conceptualization, A.E. and C.M. Software, A.E. Validation, A.E. Formal analysis, A.E. Investigation, A.E. Resources, A.E. and C.M. Data curation, A.E. and C.M.; Writing—Original Draft Preparation, A.E.; Writing—Review &amp; Editing, A.E. and C.M. Visualization, A.E. Supervision, C.M. Project administration, C.M. Funding acquisition, C.M.</p>
            </notes>
            <notes>
              <title>Funding</title>
              <p>This study was supported by operating grants from the Natural Sciences and Engineering Research Council of Canada (NSERC), the Canadian Institutes of Health Research (CIHR) and the Canada Research Chair (CRC) program.</p>
            </notes>
            <notes notes-type="COI-statement">
              <title>Conflicts of Interest</title>
              <p>The authors declare no conflict of interest.</p>
            </notes>
            <ref-list>
              <title>References</title>
              <ref id="B1-sensors-18-02474">
                <label>1.</label>
                <element-citation publication-type="journal">
                  <person-group person-group-type="author">
                    <name>
                      <surname>Grenade</surname>
                      <given-names>L.</given-names>
                    </name>
                    <name>
                      <surname>Boldy</surname>
                      <given-names>D.</given-names>
                    </name>
                  </person-group>
                  <article-title>Social isolation and loneliness among older people: Issues and future challenges in community and residential settings</article-title>
                  <source>Aust. Health Rev.</source>
                  <year>2008</year>
                  <volume>32</volume>
                  <fpage>468</fpage>
                  <lpage>478</lpage>
                  <pub-id pub-id-type="doi">10.1071/AH080468</pub-id>
                  <?supplied-pmid 18666874?>
                  <pub-id pub-id-type="pmid">18666874</pub-id>
                </element-citation>
              </ref>
              <ref id="B2-sensors-18-02474">
                <label>2.</label>
                <element-citation publication-type="journal">
                  <person-group person-group-type="author">
                    <name>
                      <surname>Cattan</surname>
                      <given-names>M.</given-names>
                    </name>
                    <name>
                      <surname>White</surname>
                      <given-names>M.</given-names>
                    </name>
                    <name>
                      <surname>Bond</surname>
                      <given-names>J.</given-names>
                    </name>
                    <name>
                      <surname>Learmouth</surname>
                      <given-names>A.</given-names>
                    </name>
                  </person-group>
                  <article-title>Preventing social isolation and loneliness among older people: A systematic review of health promotion interventions</article-title>
                  <source>Ageing Soc.</source>
                  <year>2005</year>
                  <volume>25</volume>
                  <fpage>41</fpage>
                  <lpage>67</lpage>
                  <pub-id pub-id-type="doi">10.1017/S0144686X04002594</pub-id>
                </element-citation>
              </ref>
              <ref id="B3-sensors-18-02474">
                <label>3.</label>
                <element-citation publication-type="book">
                  <person-group person-group-type="author">
                    <collab>Health Quality Ontario</collab>
                  </person-group>
                  <source>Social Isolation in Community-Dwelling Seniors: An Evidence-Based Analysis</source>
                  <publisher-name>Health Quality Ontario</publisher-name>
                  <publisher-loc>Toronto, ON, Canada</publisher-loc>
                  <year>2008</year>
                  <volume>Volume 8</volume>
                  <fpage>1</fpage>
                  <lpage>49</lpage>
                </element-citation>
              </ref>
              <ref id="B4-sensors-18-02474">
                <label>4.</label>
                <element-citation publication-type="journal">
                  <person-group person-group-type="author">
                    <name>
                      <surname>Coughlin</surname>
                      <given-names>S.S.</given-names>
                    </name>
                  </person-group>
                  <article-title>Recall bias in epidemiologic studies</article-title>
                  <source>J. Clin. Epidemiol.</source>
                  <year>1990</year>
                  <volume>43</volume>
                  <fpage>87</fpage>
                  <lpage>91</lpage>
                  <pub-id pub-id-type="doi">10.1016/0895-4356(90)90060-3</pub-id>
                  <pub-id pub-id-type="pmid">2319285</pub-id>
                </element-citation>
              </ref>
              <ref id="B5-sensors-18-02474">
                <label>5.</label>
                <element-citation publication-type="confproc">
                  <person-group person-group-type="author">
                    <name>
                      <surname>Choudhury</surname>
                      <given-names>T.</given-names>
                    </name>
                    <name>
                      <surname>Pentland</surname>
                      <given-names>A.</given-names>
                    </name>
                  </person-group>
                  <article-title>Sensing and modeling human networks using the sociometer</article-title>
                  <source>Proceedings of the Seventh IEEE International Symposium on Wearable Computers</source>
                  <conf-loc>White Plains, NY, USA</conf-loc>
                  <conf-date>21–23 October 2003</conf-date>
                  <fpage>216</fpage>
                  <lpage>222</lpage>
                </element-citation>
              </ref>
              <ref id="B6-sensors-18-02474">
                <label>6.</label>
                <element-citation publication-type="journal">
                  <person-group person-group-type="author">
                    <name>
                      <surname>Wyatt</surname>
                      <given-names>D.</given-names>
                    </name>
                    <name>
                      <surname>Choudhury</surname>
                      <given-names>T.</given-names>
                    </name>
                    <name>
                      <surname>Bilmes</surname>
                      <given-names>J.</given-names>
                    </name>
                    <name>
                      <surname>Kitts</surname>
                      <given-names>J.A.</given-names>
                    </name>
                  </person-group>
                  <article-title>Inferring colocation and conversation networks from privacy-sensitive audio with implications for computational social science</article-title>
                  <source>ACM Trans. Intell. Syst. Technol.</source>
                  <year>2011</year>
                  <volume>2</volume>
                  <fpage>1</fpage>
                  <lpage>41</lpage>
                  <pub-id pub-id-type="doi">10.1145/1889681.1889688</pub-id>
                </element-citation>
              </ref>
              <ref id="B7-sensors-18-02474">
                <label>7.</label>
                <element-citation publication-type="confproc">
                  <person-group person-group-type="author">
                    <name>
                      <surname>Cristani</surname>
                      <given-names>M.</given-names>
                    </name>
                    <name>
                      <surname>Pesarin</surname>
                      <given-names>A.</given-names>
                    </name>
                    <name>
                      <surname>Vinciarelli</surname>
                      <given-names>A.</given-names>
                    </name>
                    <name>
                      <surname>Crocco</surname>
                      <given-names>M.</given-names>
                    </name>
                    <name>
                      <surname>Murino</surname>
                      <given-names>V.</given-names>
                    </name>
                  </person-group>
                  <article-title>Look at who’s talking: Voice Activity Detection by Automated Gesture Analysis</article-title>
                  <source>Proceedings of the European Conference on Ambient Intelligence</source>
                  <conf-loc>Amsterdam, The Netherlands</conf-loc>
                  <conf-date>16–18 November 2011</conf-date>
                  <fpage>72</fpage>
                  <lpage>80</lpage>
                </element-citation>
              </ref>
              <ref id="B8-sensors-18-02474">
                <label>8.</label>
                <element-citation publication-type="confproc">
                  <person-group person-group-type="author">
                    <name>
                      <surname>Rao</surname>
                      <given-names>R.R.</given-names>
                    </name>
                  </person-group>
                  <article-title>Cross-modal prediction in audio-visual communication</article-title>
                  <source>Proceedings of the 1996 IEEE International Conference on Acoustics, Speech, and Signal Processing</source>
                  <conf-loc>Atlanta, GA, USA</conf-loc>
                  <conf-date>9 May 1996</conf-date>
                </element-citation>
              </ref>
              <ref id="B9-sensors-18-02474">
                <label>9.</label>
                <element-citation publication-type="journal">
                  <person-group person-group-type="author">
                    <name>
                      <surname>Klasnja</surname>
                      <given-names>P.</given-names>
                    </name>
                    <name>
                      <surname>Consolvo</surname>
                      <given-names>S.</given-names>
                    </name>
                    <name>
                      <surname>Choudhury</surname>
                      <given-names>T.</given-names>
                    </name>
                    <name>
                      <surname>Beckwith</surname>
                      <given-names>R.</given-names>
                    </name>
                    <name>
                      <surname>Hightower</surname>
                      <given-names>J.</given-names>
                    </name>
                  </person-group>
                  <article-title>Exploring privacy concerns about personal sensing</article-title>
                  <source>Lect. Notes Comput. Sci.</source>
                  <year>2009</year>
                  <volume>5538</volume>
                  <fpage>176</fpage>
                  <lpage>183</lpage>
                </element-citation>
              </ref>
              <ref id="B10-sensors-18-02474">
                <label>10.</label>
                <element-citation publication-type="confproc">
                  <person-group person-group-type="author">
                    <name>
                      <surname>Rahman</surname>
                      <given-names>M.</given-names>
                    </name>
                    <name>
                      <surname>Ahsan</surname>
                      <given-names>A.</given-names>
                    </name>
                    <name>
                      <surname>Plarre</surname>
                      <given-names>K.</given-names>
                    </name>
                    <name>
                      <surname>al’Absi</surname>
                      <given-names>M.</given-names>
                    </name>
                    <name>
                      <surname>Ertin</surname>
                      <given-names>E.</given-names>
                    </name>
                    <name>
                      <surname>Kumar</surname>
                      <given-names>S.</given-names>
                    </name>
                  </person-group>
                  <article-title>mConverse: Inferring Conversation Episodes from Respiratory Measurements Collected in the Field</article-title>
                  <source>Proceedings of the 2nd Conference on Wireless Health</source>
                  <conf-loc>San Diego, CA, USA</conf-loc>
                  <conf-date>10–13 October 2011</conf-date>
                </element-citation>
              </ref>
              <ref id="B11-sensors-18-02474">
                <label>11.</label>
                <element-citation publication-type="confproc">
                  <person-group person-group-type="author">
                    <name>
                      <surname>Fuchs</surname>
                      <given-names>S.</given-names>
                    </name>
                    <name>
                      <surname>Reichel</surname>
                      <given-names>U.D.</given-names>
                    </name>
                    <name>
                      <surname>Rochet-Capellan</surname>
                      <given-names>A.</given-names>
                    </name>
                  </person-group>
                  <article-title>Changes in speech and breathing rate while speaking and biking</article-title>
                  <source>Proceedings of the 18th International Congress of Phonetic Sciences (ICPhS 2015)</source>
                  <conf-loc>Glasgow, UK</conf-loc>
                  <conf-date>10–14 August 2015</conf-date>
                </element-citation>
              </ref>
              <ref id="B12-sensors-18-02474">
                <label>12.</label>
                <element-citation publication-type="journal">
                  <person-group person-group-type="author">
                    <name>
                      <surname>Wilhelm</surname>
                      <given-names>F.H.</given-names>
                    </name>
                    <name>
                      <surname>Handke</surname>
                      <given-names>E.</given-names>
                    </name>
                    <name>
                      <surname>Roth</surname>
                      <given-names>W.</given-names>
                    </name>
                  </person-group>
                  <article-title>Detection of speaking with a new respiratory inductive plethysmography system</article-title>
                  <source>Biomed. Sci. Instrum.</source>
                  <year>2003</year>
                  <volume>39</volume>
                  <fpage>136</fpage>
                  <lpage>141</lpage>
                  <?supplied-pmid 12724882?>
                  <pub-id pub-id-type="pmid">12724882</pub-id>
                </element-citation>
              </ref>
              <ref id="B13-sensors-18-02474">
                <label>13.</label>
                <element-citation publication-type="book">
                  <person-group person-group-type="author">
                    <name>
                      <surname>Haugh</surname>
                      <given-names>M.</given-names>
                    </name>
                  </person-group>
                  <article-title>Conversational Interaction</article-title>
                  <source>The Cambridge Handbook of Pragmatics</source>
                  <publisher-name>Cambridge University Press</publisher-name>
                  <publisher-loc>Cambridge, UK</publisher-loc>
                  <year>2012</year>
                  <fpage>251</fpage>
                  <lpage>274</lpage>
                </element-citation>
              </ref>
              <ref id="B14-sensors-18-02474">
                <label>14.</label>
                <element-citation publication-type="confproc">
                  <person-group person-group-type="author">
                    <name>
                      <surname>Ramos-Garcia</surname>
                      <given-names>R.I.</given-names>
                    </name>
                    <name>
                      <surname>Tiffany</surname>
                      <given-names>S.</given-names>
                    </name>
                    <name>
                      <surname>Sazonov</surname>
                      <given-names>E.</given-names>
                    </name>
                  </person-group>
                  <article-title>Using respiratory signals for the recognition of human activities</article-title>
                  <source>Proceedings of the 2016 38th Annual International Conference of the IEEE Engineering in Medicine and Biology Society (EMBC)</source>
                  <conf-loc>Orlando, FL, USA</conf-loc>
                  <conf-date>16–20 August 2016</conf-date>
                  <fpage>173</fpage>
                  <lpage>176</lpage>
                </element-citation>
              </ref>
              <ref id="B15-sensors-18-02474">
                <label>15.</label>
                <element-citation publication-type="journal">
                  <person-group person-group-type="author">
                    <name>
                      <surname>Tognetti</surname>
                      <given-names>A.</given-names>
                    </name>
                    <name>
                      <surname>Bartalesi</surname>
                      <given-names>R.</given-names>
                    </name>
                    <name>
                      <surname>Lorussi</surname>
                      <given-names>F.</given-names>
                    </name>
                    <name>
                      <surname>De Rossi</surname>
                      <given-names>D.</given-names>
                    </name>
                  </person-group>
                  <article-title>Body segment position reconstruction and posture classification by smart textiles</article-title>
                  <source>Trans. Inst. Meas. Control</source>
                  <year>2007</year>
                  <volume>29</volume>
                  <fpage>215</fpage>
                  <lpage>253</lpage>
                  <pub-id pub-id-type="doi">10.1177/0142331207069487</pub-id>
                </element-citation>
              </ref>
              <ref id="B16-sensors-18-02474">
                <label>16.</label>
                <element-citation publication-type="confproc">
                  <person-group person-group-type="author">
                    <name>
                      <surname>Mattmann</surname>
                      <given-names>C.</given-names>
                    </name>
                    <name>
                      <surname>Amft</surname>
                      <given-names>O.</given-names>
                    </name>
                    <name>
                      <surname>Harms</surname>
                      <given-names>H.</given-names>
                    </name>
                    <name>
                      <surname>Troester</surname>
                      <given-names>G.</given-names>
                    </name>
                    <name>
                      <surname>Clemens</surname>
                      <given-names>F.</given-names>
                    </name>
                  </person-group>
                  <article-title>Recognizing upper body postures using textile strain sensors</article-title>
                  <source>Proceedings of the 2007 11th IEEE International Symposium on Wearable Computers (ISWC)</source>
                  <conf-loc>Boston, MA, USA</conf-loc>
                  <conf-date>11–13 October 2007</conf-date>
                  <fpage>29</fpage>
                  <lpage>36</lpage>
                </element-citation>
              </ref>
              <ref id="B17-sensors-18-02474">
                <label>17.</label>
                <element-citation publication-type="journal">
                  <person-group person-group-type="author">
                    <name>
                      <surname>Papi</surname>
                      <given-names>E.</given-names>
                    </name>
                    <name>
                      <surname>Spulber</surname>
                      <given-names>I.</given-names>
                    </name>
                    <name>
                      <surname>Kotti</surname>
                      <given-names>M.</given-names>
                    </name>
                    <name>
                      <surname>Georgiou</surname>
                      <given-names>P.</given-names>
                    </name>
                    <name>
                      <surname>McGregor</surname>
                      <given-names>A.H.</given-names>
                    </name>
                  </person-group>
                  <article-title>Smart sensing system for combined activity classification and estimation of knee range of motion</article-title>
                  <source>IEEE Sens. J.</source>
                  <year>2015</year>
                  <volume>15</volume>
                  <fpage>5535</fpage>
                  <lpage>5544</lpage>
                  <pub-id pub-id-type="doi">10.1109/JSEN.2015.2444441</pub-id>
                </element-citation>
              </ref>
              <ref id="B18-sensors-18-02474">
                <label>18.</label>
                <element-citation publication-type="confproc">
                  <person-group person-group-type="author">
                    <name>
                      <surname>Ferrone</surname>
                      <given-names>A.</given-names>
                    </name>
                    <name>
                      <surname>Maita</surname>
                      <given-names>F.</given-names>
                    </name>
                    <name>
                      <surname>Maiolo</surname>
                      <given-names>L.</given-names>
                    </name>
                    <name>
                      <surname>Arquilla</surname>
                      <given-names>M.</given-names>
                    </name>
                    <name>
                      <surname>Castiello</surname>
                      <given-names>A.</given-names>
                    </name>
                    <name>
                      <surname>Pecora</surname>
                      <given-names>A.</given-names>
                    </name>
                    <name>
                      <surname>Jiang</surname>
                      <given-names>X.</given-names>
                    </name>
                    <name>
                      <surname>Menon</surname>
                      <given-names>C.</given-names>
                    </name>
                    <name>
                      <surname>Ferrone</surname>
                      <given-names>A.</given-names>
                    </name>
                    <name>
                      <surname>Colace</surname>
                      <given-names>L.</given-names>
                    </name>
                    <etal/>
                  </person-group>
                  <article-title>Wearable band for hand gesture recognition based on strain sensors</article-title>
                  <source>Proceedings of the IEEE RAA/EMBS International Conference on Biomedical Robotics and Biomechatronics</source>
                  <conf-loc>Singapore</conf-loc>
                  <conf-date>26–29 June 2016</conf-date>
                  <fpage>4</fpage>
                  <lpage>7</lpage>
                </element-citation>
              </ref>
              <ref id="B19-sensors-18-02474">
                <label>19.</label>
                <element-citation publication-type="web">
                  <article-title>Adafruit</article-title>
                  <comment>Available online: <ext-link ext-link-type="uri" xlink:href="https://www.adafruit.com/">https://www.adafruit.com/</ext-link></comment>
                  <date-in-citation content-type="access-date" iso-8601-date="2018-06-10">(accessed on 10 June 2018)</date-in-citation>
                </element-citation>
              </ref>
              <ref id="B20-sensors-18-02474">
                <label>20.</label>
                <element-citation publication-type="web">
                  <article-title>Images SI</article-title>
                  <comment>Available online: <ext-link ext-link-type="uri" xlink:href="https://www.imagesco.com/">https://www.imagesco.com/</ext-link></comment>
                  <date-in-citation content-type="access-date" iso-8601-date="2018-06-10">(accessed on 10 June 2018)</date-in-citation>
                </element-citation>
              </ref>
              <ref id="B21-sensors-18-02474">
                <label>21.</label>
                <element-citation publication-type="confproc">
                  <person-group person-group-type="author">
                    <name>
                      <surname>Ejupi</surname>
                      <given-names>A.</given-names>
                    </name>
                    <name>
                      <surname>Ferrone</surname>
                      <given-names>A.</given-names>
                    </name>
                    <name>
                      <surname>Menon</surname>
                      <given-names>C.</given-names>
                    </name>
                  </person-group>
                  <article-title>Quantification of textile-based stretch sensors using machine learning: An exploratory study</article-title>
                  <source>Proceedings of the IEEE International Conference on Biomedical Robotics and Biomechatronics (BioRob)</source>
                  <conf-loc>Twente, The Netherlands</conf-loc>
                  <conf-date>26–29 August 2018</conf-date>
                </element-citation>
              </ref>
              <ref id="B22-sensors-18-02474">
                <label>22.</label>
                <element-citation publication-type="confproc">
                  <person-group person-group-type="author">
                    <name>
                      <surname>Gholami</surname>
                      <given-names>M.</given-names>
                    </name>
                    <name>
                      <surname>Ejupi</surname>
                      <given-names>A.</given-names>
                    </name>
                    <name>
                      <surname>Rezaei</surname>
                      <given-names>A.</given-names>
                    </name>
                    <name>
                      <surname>Ferrone</surname>
                      <given-names>A.</given-names>
                    </name>
                    <name>
                      <surname>Menon</surname>
                      <given-names>C.</given-names>
                    </name>
                  </person-group>
                  <article-title>Estimation of Knee Joint Angle using a Fabric-based Strain Sensor and Machine Learning: A Preliminary Investigation</article-title>
                  <source>Proceedings of the IEEE International Conference on Biomedical Robotics and Biomechatronics (BioRob)</source>
                  <conf-loc>Twente, The Netherlands</conf-loc>
                  <conf-date>26–29 August 2018</conf-date>
                </element-citation>
              </ref>
              <ref id="B23-sensors-18-02474">
                <label>23.</label>
                <element-citation publication-type="confproc">
                  <person-group person-group-type="author">
                    <name>
                      <surname>Rezaei</surname>
                      <given-names>A.</given-names>
                    </name>
                    <name>
                      <surname>Ejupi</surname>
                      <given-names>A.</given-names>
                    </name>
                    <name>
                      <surname>Gholami</surname>
                      <given-names>M.</given-names>
                    </name>
                    <name>
                      <surname>Ferrone</surname>
                      <given-names>A.</given-names>
                    </name>
                    <name>
                      <surname>Menon</surname>
                      <given-names>C.</given-names>
                    </name>
                  </person-group>
                  <article-title>Preliminary Investigation of Textile-Based Strain Sensors for the Detection of Human Gait Phases Using Machine Learning</article-title>
                  <source>Proceedings of the IEEE International Conference on Biomedical Robotics and Biomechatronics (BioRob)</source>
                  <conf-loc>Twente, The Netherlands</conf-loc>
                  <conf-date>26–29 August 2018</conf-date>
                </element-citation>
              </ref>
              <ref id="B24-sensors-18-02474">
                <label>24.</label>
                <element-citation publication-type="journal">
                  <person-group person-group-type="author">
                    <name>
                      <surname>Gilbert</surname>
                      <given-names>R.</given-names>
                    </name>
                    <name>
                      <surname>Auschincloss</surname>
                      <given-names>J.H.</given-names>
                    </name>
                    <name>
                      <surname>Peppi</surname>
                      <given-names>D.</given-names>
                    </name>
                  </person-group>
                  <article-title>Relationship of rib cage and abdomen motion to diaphragm function during quiet breathing</article-title>
                  <source>Chest</source>
                  <year>1981</year>
                  <volume>80</volume>
                  <fpage>607</fpage>
                  <lpage>612</lpage>
                  <pub-id pub-id-type="doi">10.1378/chest.80.5.607</pub-id>
                  <?supplied-pmid 7297153?>
                  <pub-id pub-id-type="pmid">7297153</pub-id>
                </element-citation>
              </ref>
              <ref id="B25-sensors-18-02474">
                <label>25.</label>
                <element-citation publication-type="journal">
                  <person-group person-group-type="author">
                    <name>
                      <surname>Kaneko</surname>
                      <given-names>H.</given-names>
                    </name>
                    <name>
                      <surname>Horie</surname>
                      <given-names>J.</given-names>
                    </name>
                  </person-group>
                  <article-title>Breathing movements of the chest and abdominal wall in healthy subjects</article-title>
                  <source>Respir. Care</source>
                  <year>2012</year>
                  <volume>57</volume>
                  <fpage>1442</fpage>
                  <lpage>1451</lpage>
                  <pub-id pub-id-type="doi">10.4187/respcare.01655</pub-id>
                  <?supplied-pmid 22348414?>
                  <pub-id pub-id-type="pmid">22348414</pub-id>
                </element-citation>
              </ref>
              <ref id="B26-sensors-18-02474">
                <label>26.</label>
                <element-citation publication-type="journal">
                  <person-group person-group-type="author">
                    <name>
                      <surname>Grimby</surname>
                      <given-names>G.</given-names>
                    </name>
                    <name>
                      <surname>Bunn</surname>
                      <given-names>J.</given-names>
                    </name>
                    <name>
                      <surname>Mead</surname>
                      <given-names>J.</given-names>
                    </name>
                  </person-group>
                  <article-title>Relative contribution of rib cage and abdomen to ventilation during exercise</article-title>
                  <source>J. Appl. Physiol.</source>
                  <year>1968</year>
                  <volume>24</volume>
                  <fpage>159</fpage>
                  <lpage>166</lpage>
                  <pub-id pub-id-type="doi">10.1152/jappl.1968.24.2.159</pub-id>
                  <?supplied-pmid 5637678?>
                  <pub-id pub-id-type="pmid">5637678</pub-id>
                </element-citation>
              </ref>
              <ref id="B27-sensors-18-02474">
                <label>27.</label>
                <element-citation publication-type="journal">
                  <person-group person-group-type="author">
                    <name>
                      <surname>McFadden</surname>
                      <given-names>J.P.</given-names>
                    </name>
                    <name>
                      <surname>Price</surname>
                      <given-names>R.</given-names>
                    </name>
                    <name>
                      <surname>Eastwood</surname>
                      <given-names>H.D.</given-names>
                    </name>
                    <name>
                      <surname>Briggs</surname>
                      <given-names>R.</given-names>
                    </name>
                  </person-group>
                  <article-title>Raised respiratory rate in elderly patients: A valuable physical sign</article-title>
                  <source>Br. Med. J. (Clin. Res. Ed.)</source>
                  <year>1982</year>
                  <volume>284</volume>
                  <fpage>626</fpage>
                  <lpage>627</lpage>
                  <pub-id pub-id-type="doi">10.1136/bmj.284.6316.626</pub-id>
                </element-citation>
              </ref>
              <ref id="B28-sensors-18-02474">
                <label>28.</label>
                <element-citation publication-type="journal">
                  <person-group person-group-type="author">
                    <name>
                      <surname>Liaw</surname>
                      <given-names>A.</given-names>
                    </name>
                    <name>
                      <surname>Wiener</surname>
                      <given-names>M.</given-names>
                    </name>
                  </person-group>
                  <article-title>Classification and Regression by randomForest</article-title>
                  <source>R News</source>
                  <year>2002</year>
                  <volume>2</volume>
                  <fpage>18</fpage>
                  <lpage>22</lpage>
                </element-citation>
              </ref>
              <ref id="B29-sensors-18-02474">
                <label>29.</label>
                <element-citation publication-type="journal">
                  <person-group person-group-type="author">
                    <name>
                      <surname>Specht</surname>
                      <given-names>D.F.</given-names>
                    </name>
                  </person-group>
                  <article-title>A general regression neural network</article-title>
                  <source>IEEE Trans. Neural Netw.</source>
                  <year>1991</year>
                  <volume>2</volume>
                  <fpage>568</fpage>
                  <lpage>576</lpage>
                  <pub-id pub-id-type="doi">10.1109/72.97934</pub-id>
                  <?supplied-pmid 18282872?>
                  <pub-id pub-id-type="pmid">18282872</pub-id>
                </element-citation>
              </ref>
              <ref id="B30-sensors-18-02474">
                <label>30.</label>
                <element-citation publication-type="journal">
                  <person-group person-group-type="author">
                    <name>
                      <surname>Drucker</surname>
                      <given-names>H.</given-names>
                    </name>
                    <name>
                      <surname>Burges</surname>
                      <given-names>C.J.C.</given-names>
                    </name>
                    <name>
                      <surname>Kaufman</surname>
                      <given-names>L.</given-names>
                    </name>
                    <name>
                      <surname>Smola</surname>
                      <given-names>A.</given-names>
                    </name>
                    <name>
                      <surname>Vapnik</surname>
                      <given-names>V.</given-names>
                    </name>
                  </person-group>
                  <article-title>Support vector regression machines</article-title>
                  <source>Adv. Neural Inf. Process. Syst.</source>
                  <year>1997</year>
                  <volume>1</volume>
                  <fpage>155</fpage>
                  <lpage>161</lpage>
                </element-citation>
              </ref>
              <ref id="B31-sensors-18-02474">
                <label>31.</label>
                <element-citation publication-type="confproc">
                  <person-group person-group-type="author">
                    <name>
                      <surname>Mika</surname>
                      <given-names>S.</given-names>
                    </name>
                    <name>
                      <surname>Ratsch</surname>
                      <given-names>G.</given-names>
                    </name>
                    <name>
                      <surname>Weston</surname>
                      <given-names>J.</given-names>
                    </name>
                    <name>
                      <surname>Schölkopf</surname>
                      <given-names>B.</given-names>
                    </name>
                    <name>
                      <surname>Muller</surname>
                      <given-names>K.R.</given-names>
                    </name>
                  </person-group>
                  <article-title>Fisher discriminant analysis with kernels</article-title>
                  <source>Proceedings of the 1999 IEEE Signal Processing Society Workshop (Cat. No. 98TH8468)</source>
                  <conf-loc>Madison, WI, USA</conf-loc>
                  <conf-date>25 August 1999</conf-date>
                  <fpage>41</fpage>
                  <lpage>48</lpage>
                </element-citation>
              </ref>
              <ref id="B32-sensors-18-02474">
                <label>32.</label>
                <element-citation publication-type="web">
                  <article-title>tsfresh</article-title>
                  <comment>Available online: <ext-link ext-link-type="uri" xlink:href="http://tsfresh.readthedocs.io/">http://tsfresh.readthedocs.io/</ext-link></comment>
                  <date-in-citation content-type="access-date" iso-8601-date="2018-06-10">(accessed on 10 June 2018)</date-in-citation>
                </element-citation>
              </ref>
              <ref id="B33-sensors-18-02474">
                <label>33.</label>
                <element-citation publication-type="book">
                  <person-group person-group-type="author">
                    <name>
                      <surname>Breiman</surname>
                      <given-names>L.</given-names>
                    </name>
                    <name>
                      <surname>Friedman</surname>
                      <given-names>J.H.</given-names>
                    </name>
                    <name>
                      <surname>Olshen</surname>
                      <given-names>R.A.</given-names>
                    </name>
                    <name>
                      <surname>Stone</surname>
                      <given-names>C.</given-names>
                    </name>
                  </person-group>
                  <source>Classification and Regression Trees</source>
                  <publisher-name>Routledge</publisher-name>
                  <publisher-loc>Abingdon, UK</publisher-loc>
                  <year>1984</year>
                  <volume>Volume 1</volume>
                </element-citation>
              </ref>
              <ref id="B34-sensors-18-02474">
                <label>34.</label>
                <element-citation publication-type="journal">
                  <person-group person-group-type="author">
                    <name>
                      <surname>Grossmann</surname>
                      <given-names>A.</given-names>
                    </name>
                    <name>
                      <surname>Morlet</surname>
                      <given-names>J.</given-names>
                    </name>
                  </person-group>
                  <article-title>Decomposition of hardy functions into square integrable wavelets of constant shape</article-title>
                  <source>J. Math. Anal.</source>
                  <year>1984</year>
                  <volume>15</volume>
                  <fpage>723</fpage>
                  <lpage>736</lpage>
                  <pub-id pub-id-type="doi">10.1137/0515056</pub-id>
                </element-citation>
              </ref>
              <ref id="B35-sensors-18-02474">
                <label>35.</label>
                <element-citation publication-type="journal">
                  <person-group person-group-type="author">
                    <name>
                      <surname>Cheung</surname>
                      <given-names>Y.W.</given-names>
                    </name>
                    <name>
                      <surname>La</surname>
                      <given-names>K.S.</given-names>
                    </name>
                  </person-group>
                  <article-title>Lag order and critical values of the augmented dickey-fuller test</article-title>
                  <source>J. Bus. Econ. Stat.</source>
                  <year>1995</year>
                  <volume>13</volume>
                  <fpage>277</fpage>
                  <lpage>280</lpage>
                </element-citation>
              </ref>
              <ref id="B36-sensors-18-02474">
                <label>36.</label>
                <element-citation publication-type="journal">
                  <person-group person-group-type="author">
                    <name>
                      <surname>Cooley</surname>
                      <given-names>J.W.</given-names>
                    </name>
                    <name>
                      <surname>Lewis</surname>
                      <given-names>P.A.W.</given-names>
                    </name>
                    <name>
                      <surname>Welch</surname>
                      <given-names>P.D.</given-names>
                    </name>
                  </person-group>
                  <article-title>The fast fourier transform and its applications</article-title>
                  <source>IEEE Trans. Educ.</source>
                  <year>1969</year>
                  <volume>12</volume>
                  <fpage>27</fpage>
                  <lpage>34</lpage>
                  <pub-id pub-id-type="doi">10.1109/TE.1969.4320436</pub-id>
                </element-citation>
              </ref>
              <ref id="B37-sensors-18-02474">
                <label>37.</label>
                <element-citation publication-type="journal">
                  <person-group person-group-type="author">
                    <name>
                      <surname>Welch</surname>
                      <given-names>P.D.</given-names>
                    </name>
                  </person-group>
                  <article-title>The use of fast fourier transform for the estimation of power spectra: A method based on time averaging over short, modified periodograms</article-title>
                  <source>IEEE Trans. Audio Electroacoust.</source>
                  <year>1967</year>
                  <volume>15</volume>
                  <fpage>70</fpage>
                  <lpage>73</lpage>
                  <pub-id pub-id-type="doi">10.1109/TAU.1967.1161901</pub-id>
                </element-citation>
              </ref>
              <ref id="B38-sensors-18-02474">
                <label>38.</label>
                <element-citation publication-type="journal">
                  <person-group person-group-type="author">
                    <name>
                      <surname>Pedregosa</surname>
                      <given-names>F.</given-names>
                    </name>
                    <name>
                      <surname>Varoquaux</surname>
                      <given-names>G.</given-names>
                    </name>
                  </person-group>
                  <article-title>Scikit-Learn: Machine Learning in Python</article-title>
                  <source>J. Mach. Learn. Res.</source>
                  <year>2011</year>
                  <volume>12</volume>
                  <fpage>2825</fpage>
                  <lpage>2830</lpage>
                </element-citation>
              </ref>
              <ref id="B39-sensors-18-02474">
                <label>39.</label>
                <element-citation publication-type="journal">
                  <person-group person-group-type="author">
                    <name>
                      <surname>Ejupi</surname>
                      <given-names>A.</given-names>
                    </name>
                    <name>
                      <surname>Lord</surname>
                      <given-names>S.R.</given-names>
                    </name>
                    <name>
                      <surname>Delbaere</surname>
                      <given-names>K.</given-names>
                    </name>
                  </person-group>
                  <article-title>New methods for fall risk prediction</article-title>
                  <source>Curr. Opin. Clin. Nutr. Metab. Care</source>
                  <year>2014</year>
                  <volume>17</volume>
                  <fpage>407</fpage>
                  <lpage>411</lpage>
                  <pub-id pub-id-type="doi">10.1097/MCO.0000000000000081</pub-id>
                  <?supplied-pmid 24992225?>
                  <pub-id pub-id-type="pmid">24992225</pub-id>
                </element-citation>
              </ref>
              <ref id="B40-sensors-18-02474">
                <label>40.</label>
                <element-citation publication-type="journal">
                  <person-group person-group-type="author">
                    <name>
                      <surname>Mukhopadhyay</surname>
                      <given-names>S.C.</given-names>
                    </name>
                  </person-group>
                  <article-title>Wearable sensors for human activity monitoring: A review</article-title>
                  <source>IEEE Sens. J.</source>
                  <year>2014</year>
                  <volume>15</volume>
                  <fpage>1321</fpage>
                  <lpage>1330</lpage>
                  <pub-id pub-id-type="doi">10.1109/JSEN.2014.2370945</pub-id>
                </element-citation>
              </ref>
              <ref id="B41-sensors-18-02474">
                <label>41.</label>
                <element-citation publication-type="journal">
                  <person-group person-group-type="author">
                    <name>
                      <surname>Zhang</surname>
                      <given-names>C.</given-names>
                    </name>
                    <name>
                      <surname>Tian</surname>
                      <given-names>Y.</given-names>
                    </name>
                  </person-group>
                  <article-title>RGB-D camera-based daily living activity recognition</article-title>
                  <source>J. Comput. Vis. Image Process.</source>
                  <year>2012</year>
                  <volume>2</volume>
                  <fpage>12</fpage>
                </element-citation>
              </ref>
              <ref id="B42-sensors-18-02474">
                <label>42.</label>
                <element-citation publication-type="journal">
                  <person-group person-group-type="author">
                    <name>
                      <surname>Bari</surname>
                      <given-names>R.</given-names>
                    </name>
                    <name>
                      <surname>Adams</surname>
                      <given-names>R.J.</given-names>
                    </name>
                    <name>
                      <surname>Rahman</surname>
                      <given-names>M.</given-names>
                    </name>
                    <name>
                      <surname>Parsons</surname>
                      <given-names>M.B.</given-names>
                    </name>
                    <name>
                      <surname>Buder</surname>
                      <given-names>E.H.</given-names>
                    </name>
                    <name>
                      <surname>Kumar</surname>
                      <given-names>S.</given-names>
                    </name>
                  </person-group>
                  <article-title>rConverse: Moment by moment conversation detection using a mobile respiration sensor</article-title>
                  <source>Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.</source>
                  <year>2018</year>
                  <volume>2</volume>
                  <fpage>2:1</fpage>
                  <lpage>2:27</lpage>
                  <pub-id pub-id-type="doi">10.1145/3191734</pub-id>
                </element-citation>
              </ref>
            </ref-list>
          </back>
          <floats-group>
            <fig id="sensors-18-02474-f001" orientation="portrait" position="float">
              <label>Figure 1</label>
              <caption>
                <p>Comparison between the Adafruit, Image SI and Menrva sensors.</p>
              </caption>
              <graphic xlink:href="sensors-18-02474-g001"/>
            </fig>
            <fig id="sensors-18-02474-f002" orientation="portrait" position="float">
              <label>Figure 2</label>
              <caption>
                <p>Three custom-made sensor bands to monitor the expansion and contraction of the torso while breathing (and talking). The red dashed line shows the positioning of the sensor.</p>
              </caption>
              <graphic xlink:href="sensors-18-02474-g002"/>
            </fig>
            <fig id="sensors-18-02474-f003" orientation="portrait" position="float">
              <label>Figure 3</label>
              <caption>
                <p>Changes in the air volume while talking.</p>
              </caption>
              <graphic xlink:href="sensors-18-02474-g003"/>
            </fig>
            <fig id="sensors-18-02474-f004" orientation="portrait" position="float">
              <label>Figure 4</label>
              <caption>
                <p>Design of the talking detection algorithm incorporating machine learning.</p>
              </caption>
              <graphic xlink:href="sensors-18-02474-g004"/>
            </fig>
            <fig id="sensors-18-02474-f005" orientation="portrait" position="float">
              <label>Figure 5</label>
              <caption>
                <p>Comparison of the raw sensor signals (upper chest band) between quiet and speech breathing (i.e., talking) for: (<bold>a</bold>) sitting; (<bold>b</bold>) standing; and (<bold>c</bold>) walking.</p>
              </caption>
              <graphic xlink:href="sensors-18-02474-g005"/>
            </fig>
            <fig id="sensors-18-02474-f006" orientation="portrait" position="float">
              <label>Figure 6</label>
              <caption>
                <p>Comparison of the ROC curves (and the associated AUC metric) among the tested machine learning algorithms.</p>
              </caption>
              <graphic xlink:href="sensors-18-02474-g006"/>
            </fig>
            <fig id="sensors-18-02474-f007" orientation="portrait" position="float">
              <label>Figure 7</label>
              <caption>
                <p>Exemplary detection of talking for participant P10 in activities: (<bold>a</bold>) sitting; (<bold>b</bold>) standing; and (<bold>c</bold>) walking.</p>
              </caption>
              <graphic xlink:href="sensors-18-02474-g007"/>
            </fig>
            <table-wrap id="sensors-18-02474-t001" orientation="portrait" position="float">
              <object-id pub-id-type="pii">sensors-18-02474-t001_Table 1</object-id>
              <label>Table 1</label>
              <caption>
                <p>Participant characteristics.</p>
              </caption>
              <table frame="hsides" rules="groups">
                <thead>
                  <tr>
                    <th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">
</th>
                    <th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Study Participants (<italic>n</italic> = 15)</th>
                  </tr>
                </thead>
                <tbody>
                  <tr>
                    <td align="center" valign="middle" rowspan="1" colspan="1">Age (years)</td>
                    <td align="center" valign="middle" rowspan="1" colspan="1">23 (3.8)</td>
                  </tr>
                  <tr>
                    <td align="center" valign="middle" rowspan="1" colspan="1">Gender (F/M)</td>
                    <td align="center" valign="middle" rowspan="1" colspan="1">6/9</td>
                  </tr>
                  <tr>
                    <td align="center" valign="middle" rowspan="1" colspan="1">Height (cm)</td>
                    <td align="center" valign="middle" rowspan="1" colspan="1">169.8 (8.9)</td>
                  </tr>
                  <tr>
                    <td align="center" valign="middle" rowspan="1" colspan="1">Weight (kg)</td>
                    <td align="center" valign="middle" rowspan="1" colspan="1">68.5 (12.1)</td>
                  </tr>
                  <tr>
                    <td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">BMI (kg/m<inline-formula><mml:math id="mm17"><mml:mrow><mml:msup><mml:mrow/><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:math></inline-formula>)</td>
                    <td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">23.6 (3.1)</td>
                  </tr>
                </tbody>
              </table>
            </table-wrap>
            <table-wrap id="sensors-18-02474-t002" orientation="portrait" position="float">
              <object-id pub-id-type="pii">sensors-18-02474-t002_Table 2</object-id>
              <label>Table 2</label>
              <caption>
                <p>Average performance of our algorithm in detecting talking among all participants.</p>
              </caption>
              <table frame="hsides" rules="groups">
                <thead>
                  <tr>
                    <th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">
</th>
                    <th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Average <italic>ACC</italic></th>
                    <th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Average <italic>TPR</italic></th>
                    <th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Average <italic>FPR</italic></th>
                  </tr>
                </thead>
                <tbody>
                  <tr>
                    <td align="center" valign="middle" rowspan="1" colspan="1">
                      <bold>Sitting</bold>
                    </td>
                    <td align="center" valign="middle" rowspan="1" colspan="1">88.0 (5.4)</td>
                    <td align="center" valign="middle" rowspan="1" colspan="1">88.0 (6.1)</td>
                    <td align="center" valign="middle" rowspan="1" colspan="1">12.6 (6.9)</td>
                  </tr>
                  <tr>
                    <td align="center" valign="middle" rowspan="1" colspan="1">
                      <bold>Standing</bold>
                    </td>
                    <td align="center" valign="middle" rowspan="1" colspan="1">86.3 (7.3)</td>
                    <td align="center" valign="middle" rowspan="1" colspan="1">84.2 (8.6)</td>
                    <td align="center" valign="middle" rowspan="1" colspan="1">12.5 (7.6)</td>
                  </tr>
                  <tr>
                    <td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
                      <bold>Walking</bold>
                    </td>
                    <td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">80.6 (7.7)</td>
                    <td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">71.8 (12.1)</td>
                    <td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">13.3 (6.5)</td>
                  </tr>
                  <tr>
                    <td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
                      <bold>Average</bold>
                    </td>
                    <td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">85.0 (6.8)</td>
                    <td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">81.3 (8.9)</td>
                    <td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">12.8 (7.0)</td>
                  </tr>
                </tbody>
              </table>
            </table-wrap>
            <table-wrap id="sensors-18-02474-t003" orientation="portrait" position="float">
              <object-id pub-id-type="pii">sensors-18-02474-t003_Table 3</object-id>
              <label>Table 3</label>
              <caption>
                <p>Performance results of our algorithm in detecting talking for each participant (P).</p>
              </caption>
              <table frame="hsides" rules="groups">
                <thead>
                  <tr>
                    <th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">
</th>
                    <th colspan="3" align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1">P01</th>
                    <th colspan="3" align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1">P02</th>
                    <th colspan="3" align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1">P03</th>
                    <th colspan="3" align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1">P04</th>
                    <th colspan="3" align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1">P05</th>
                  </tr>
                </thead>
                <tbody>
                  <tr>
                    <td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
</td>
                    <td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
                      <bold>
                        <italic>ACC</italic>
                      </bold>
                    </td>
                    <td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
                      <bold>
                        <italic>TPR</italic>
                      </bold>
                    </td>
                    <td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
                      <bold>
                        <italic>FPR</italic>
                      </bold>
                    </td>
                    <td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
                      <bold>
                        <italic>ACC</italic>
                      </bold>
                    </td>
                    <td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
                      <bold>
                        <italic>TPR</italic>
                      </bold>
                    </td>
                    <td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
                      <bold>
                        <italic>FPR</italic>
                      </bold>
                    </td>
                    <td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
                      <bold>
                        <italic>ACC</italic>
                      </bold>
                    </td>
                    <td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
                      <bold>
                        <italic>TPR</italic>
                      </bold>
                    </td>
                    <td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
                      <bold>
                        <italic>FPR</italic>
                      </bold>
                    </td>
                    <td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
                      <bold>
                        <italic>ACC</italic>
                      </bold>
                    </td>
                    <td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
                      <bold>
                        <italic>TPR</italic>
                      </bold>
                    </td>
                    <td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
                      <bold>
                        <italic>FPR</italic>
                      </bold>
                    </td>
                    <td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
                      <bold>
                        <italic>ACC</italic>
                      </bold>
                    </td>
                    <td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
                      <bold>
                        <italic>TPR</italic>
                      </bold>
                    </td>
                    <td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
                      <bold>
                        <italic>FPR</italic>
                      </bold>
                    </td>
                  </tr>
                  <tr>
                    <td align="center" valign="middle" rowspan="1" colspan="1">
                      <bold>Sitting</bold>
                    </td>
                    <td align="center" valign="middle" rowspan="1" colspan="1">94.3</td>
                    <td align="center" valign="middle" rowspan="1" colspan="1">92.3</td>
                    <td align="center" valign="middle" rowspan="1" colspan="1">3.9</td>
                    <td align="center" valign="middle" rowspan="1" colspan="1">94.8</td>
                    <td align="center" valign="middle" rowspan="1" colspan="1">93.5</td>
                    <td align="center" valign="middle" rowspan="1" colspan="1">4.2</td>
                    <td align="center" valign="middle" rowspan="1" colspan="1">97.5</td>
                    <td align="center" valign="middle" rowspan="1" colspan="1">94.9</td>
                    <td align="center" valign="middle" rowspan="1" colspan="1">0.9</td>
                    <td align="center" valign="middle" rowspan="1" colspan="1">84.9</td>
                    <td align="center" valign="middle" rowspan="1" colspan="1">84.9</td>
                    <td align="center" valign="middle" rowspan="1" colspan="1">15.0</td>
                    <td align="center" valign="middle" rowspan="1" colspan="1">82.9</td>
                    <td align="center" valign="middle" rowspan="1" colspan="1">81.2</td>
                    <td align="center" valign="middle" rowspan="1" colspan="1">15.6</td>
                  </tr>
                  <tr>
                    <td align="center" valign="middle" rowspan="1" colspan="1">
                      <bold>Standing</bold>
                    </td>
                    <td align="center" valign="middle" rowspan="1" colspan="1">93.9</td>
                    <td align="center" valign="middle" rowspan="1" colspan="1">92.2</td>
                    <td align="center" valign="middle" rowspan="1" colspan="1">4.4</td>
                    <td align="center" valign="middle" rowspan="1" colspan="1">90.5</td>
                    <td align="center" valign="middle" rowspan="1" colspan="1">90.0</td>
                    <td align="center" valign="middle" rowspan="1" colspan="1">9.1</td>
                    <td align="center" valign="middle" rowspan="1" colspan="1">94.2</td>
                    <td align="center" valign="middle" rowspan="1" colspan="1">91.5</td>
                    <td align="center" valign="middle" rowspan="1" colspan="1">3.7</td>
                    <td align="center" valign="middle" rowspan="1" colspan="1">76.1</td>
                    <td align="center" valign="middle" rowspan="1" colspan="1">73.1</td>
                    <td align="center" valign="middle" rowspan="1" colspan="1">21.3</td>
                    <td align="center" valign="middle" rowspan="1" colspan="1">86.0</td>
                    <td align="center" valign="middle" rowspan="1" colspan="1">79.1</td>
                    <td align="center" valign="middle" rowspan="1" colspan="1">9.9</td>
                  </tr>
                  <tr>
                    <td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
                      <bold>Walking</bold>
                    </td>
                    <td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">79.6</td>
                    <td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">76.4</td>
                    <td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">17.6</td>
                    <td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">85.2</td>
                    <td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">81.0</td>
                    <td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">11.6</td>
                    <td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">90.2</td>
                    <td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">82.4</td>
                    <td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">4.4</td>
                    <td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">68.8</td>
                    <td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">55.3</td>
                    <td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">20.7</td>
                    <td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">87.2</td>
                    <td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">83.6</td>
                    <td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">9.8</td>
                  </tr>
                  <tr>
                    <td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
</td>
                    <td colspan="3" align="center" valign="middle" style="border-bottom:solid thin" rowspan="1">
                      <bold>P06</bold>
                    </td>
                    <td colspan="3" align="center" valign="middle" style="border-bottom:solid thin" rowspan="1">
                      <bold>P07</bold>
                    </td>
                    <td colspan="3" align="center" valign="middle" style="border-bottom:solid thin" rowspan="1">
                      <bold>P08</bold>
                    </td>
                    <td colspan="3" align="center" valign="middle" style="border-bottom:solid thin" rowspan="1">
                      <bold>P09</bold>
                    </td>
                    <td colspan="3" align="center" valign="middle" style="border-bottom:solid thin" rowspan="1">
                      <bold>P10</bold>
                    </td>
                  </tr>
                  <tr>
                    <td align="center" valign="middle" rowspan="1" colspan="1">
                      <bold>Sitting</bold>
                    </td>
                    <td align="center" valign="middle" rowspan="1" colspan="1">81.6</td>
                    <td align="center" valign="middle" rowspan="1" colspan="1">87.7</td>
                    <td align="center" valign="middle" rowspan="1" colspan="1">24.2</td>
                    <td align="center" valign="middle" rowspan="1" colspan="1">88.6</td>
                    <td align="center" valign="middle" rowspan="1" colspan="1">95.1</td>
                    <td align="center" valign="middle" rowspan="1" colspan="1">20.2</td>
                    <td align="center" valign="middle" rowspan="1" colspan="1">93.7</td>
                    <td align="center" valign="middle" rowspan="1" colspan="1">93.1</td>
                    <td align="center" valign="middle" rowspan="1" colspan="1">5.9</td>
                    <td align="center" valign="middle" rowspan="1" colspan="1">89.3</td>
                    <td align="center" valign="middle" rowspan="1" colspan="1">93.3</td>
                    <td align="center" valign="middle" rowspan="1" colspan="1">14.6</td>
                    <td align="center" valign="middle" rowspan="1" colspan="1">92.0</td>
                    <td align="center" valign="middle" rowspan="1" colspan="1">90.9</td>
                    <td align="center" valign="middle" rowspan="1" colspan="1">7.3</td>
                  </tr>
                  <tr>
                    <td align="center" valign="middle" rowspan="1" colspan="1">
                      <bold>Standing</bold>
                    </td>
                    <td align="center" valign="middle" rowspan="1" colspan="1">82.5</td>
                    <td align="center" valign="middle" rowspan="1" colspan="1">79.2</td>
                    <td align="center" valign="middle" rowspan="1" colspan="1">14.7</td>
                    <td align="center" valign="middle" rowspan="1" colspan="1">94.0</td>
                    <td align="center" valign="middle" rowspan="1" colspan="1">94.9</td>
                    <td align="center" valign="middle" rowspan="1" colspan="1">7.1</td>
                    <td align="center" valign="middle" rowspan="1" colspan="1">95.1</td>
                    <td align="center" valign="middle" rowspan="1" colspan="1">92.5</td>
                    <td align="center" valign="middle" rowspan="1" colspan="1">2.9</td>
                    <td align="center" valign="middle" rowspan="1" colspan="1">93.6</td>
                    <td align="center" valign="middle" rowspan="1" colspan="1">96.1</td>
                    <td align="center" valign="middle" rowspan="1" colspan="1">8.9</td>
                    <td align="center" valign="middle" rowspan="1" colspan="1">87.0</td>
                    <td align="center" valign="middle" rowspan="1" colspan="1">81.4</td>
                    <td align="center" valign="middle" rowspan="1" colspan="1">9.1</td>
                  </tr>
                  <tr>
                    <td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
                      <bold>Walking</bold>
                    </td>
                    <td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">71.1</td>
                    <td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">70.5</td>
                    <td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">28.5</td>
                    <td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">87.8</td>
                    <td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">84.7</td>
                    <td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">9.3</td>
                    <td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">95.6</td>
                    <td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">90.8</td>
                    <td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">1.4</td>
                    <td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">79.7</td>
                    <td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">75.4</td>
                    <td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">17.0</td>
                    <td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">81.1</td>
                    <td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">68.2</td>
                    <td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">11.9</td>
                  </tr>
                  <tr>
                    <td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
</td>
                    <td colspan="3" align="center" valign="middle" style="border-bottom:solid thin" rowspan="1">
                      <bold>P11</bold>
                    </td>
                    <td colspan="3" align="center" valign="middle" style="border-bottom:solid thin" rowspan="1">
                      <bold>P12</bold>
                    </td>
                    <td colspan="3" align="center" valign="middle" style="border-bottom:solid thin" rowspan="1">
                      <bold>P13</bold>
                    </td>
                    <td colspan="3" align="center" valign="middle" style="border-bottom:solid thin" rowspan="1">
                      <bold>P14</bold>
                    </td>
                    <td colspan="3" align="center" valign="middle" style="border-bottom:solid thin" rowspan="1">
                      <bold>P15</bold>
                    </td>
                  </tr>
                  <tr>
                    <td align="center" valign="middle" rowspan="1" colspan="1">
                      <bold>Sitting</bold>
                    </td>
                    <td align="center" valign="middle" rowspan="1" colspan="1">82.7</td>
                    <td align="center" valign="middle" rowspan="1" colspan="1">82.0</td>
                    <td align="center" valign="middle" rowspan="1" colspan="1">16.7</td>
                    <td align="center" valign="middle" rowspan="1" colspan="1">89.8</td>
                    <td align="center" valign="middle" rowspan="1" colspan="1">87.0</td>
                    <td align="center" valign="middle" rowspan="1" colspan="1">7.9</td>
                    <td align="center" valign="middle" rowspan="1" colspan="1">83.5</td>
                    <td align="center" valign="middle" rowspan="1" colspan="1">89.3</td>
                    <td align="center" valign="middle" rowspan="1" colspan="1">22.7</td>
                    <td align="center" valign="middle" rowspan="1" colspan="1">84.3</td>
                    <td align="center" valign="middle" rowspan="1" colspan="1">79.1</td>
                    <td align="center" valign="middle" rowspan="1" colspan="1">12.6</td>
                    <td align="center" valign="middle" rowspan="1" colspan="1">79.3</td>
                    <td align="center" valign="middle" rowspan="1" colspan="1">75.0</td>
                    <td align="center" valign="middle" rowspan="1" colspan="1">16.7</td>
                  </tr>
                  <tr>
                    <td align="center" valign="middle" rowspan="1" colspan="1">
                      <bold>Standing</bold>
                    </td>
                    <td align="center" valign="middle" rowspan="1" colspan="1">73.9</td>
                    <td align="center" valign="middle" rowspan="1" colspan="1">73.8</td>
                    <td align="center" valign="middle" rowspan="1" colspan="1">26.1</td>
                    <td align="center" valign="middle" rowspan="1" colspan="1">83.3</td>
                    <td align="center" valign="middle" rowspan="1" colspan="1">70.8</td>
                    <td align="center" valign="middle" rowspan="1" colspan="1">9.0</td>
                    <td align="center" valign="middle" rowspan="1" colspan="1">78.7</td>
                    <td align="center" valign="middle" rowspan="1" colspan="1">76.2</td>
                    <td align="center" valign="middle" rowspan="1" colspan="1">18.8</td>
                    <td align="center" valign="middle" rowspan="1" colspan="1">75.8</td>
                    <td align="center" valign="middle" rowspan="1" colspan="1">79.5</td>
                    <td align="center" valign="middle" rowspan="1" colspan="1">28.2</td>
                    <td align="center" valign="middle" rowspan="1" colspan="1">89.8</td>
                    <td align="center" valign="middle" rowspan="1" colspan="1">93.2</td>
                    <td align="center" valign="middle" rowspan="1" colspan="1">13.9</td>
                  </tr>
                  <tr>
                    <td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
                      <bold>Walking</bold>
                    </td>
                    <td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">68.3</td>
                    <td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">42.5</td>
                    <td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">14.8</td>
                    <td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">79.4</td>
                    <td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">69.4</td>
                    <td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">13.5</td>
                    <td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">73.0</td>
                    <td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">61.3</td>
                    <td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">18.6</td>
                    <td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">81.7</td>
                    <td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">68.2</td>
                    <td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">9.9</td>
                    <td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">80.0</td>
                    <td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">66.9</td>
                    <td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">9.9</td>
                  </tr>
                </tbody>
              </table>
            </table-wrap>
          </floats-group>
        </article>
      </metadata>
    </record>
  </GetRecord>
</OAI-PMH>
