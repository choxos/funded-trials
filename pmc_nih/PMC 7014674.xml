<?xml version="1.0" encoding="UTF-8"?>
<OAI-PMH xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/ http://www.openarchives.org/OAI/2.0/OAI-PMH.xsd">
  <responseDate>2024-05-03T03:03:18Z</responseDate>
  <request verb="GetRecord" identifier="oai:pubmedcentral.nih.gov:7014674" metadataPrefix="pmc">https:/www.ncbi.nlm.nih.gov/pmc/oai/oai.cgi</request>
  <GetRecord>
    <record>
      <header>
        <identifier>oai:pubmedcentral.nih.gov:7014674</identifier>
        <datestamp>2020-02-18</datestamp>
        <setSpec>bmcpedi</setSpec>
        <setSpec>pmc-open</setSpec>
      </header>
      <metadata>
        <article xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:mml="http://www.w3.org/1998/Math/MathML" xsi:schemaLocation="https://jats.nlm.nih.gov/ns/archiving/1.3/ https://jats.nlm.nih.gov/archiving/1.3/xsd/JATS-archivearticle1-3.xsd" article-type="research-article">
          <front>
            <journal-meta>
              <journal-id journal-id-type="nlm-ta">BMC Pediatr</journal-id>
              <journal-id journal-id-type="iso-abbrev">BMC Pediatr</journal-id>
              <journal-title-group>
                <journal-title>BMC Pediatrics</journal-title>
              </journal-title-group>
              <issn pub-type="epub">1471-2431</issn>
              <publisher>
                <publisher-name>BioMed Central</publisher-name>
                <publisher-loc>London</publisher-loc>
              </publisher>
            </journal-meta>
            <article-meta>
              <article-id pub-id-type="accession">PMC7014674</article-id>
              <article-id pub-id-type="pmcid">PMC7014674</article-id>
              <article-id pub-id-type="pmc-uid">7014674</article-id>
              <article-id pub-id-type="pmid">32046671</article-id>
              <article-id pub-id-type="publisher-id">1941</article-id>
              <article-id pub-id-type="doi">10.1186/s12887-020-1941-5</article-id>
              <article-categories>
                <subj-group subj-group-type="heading">
                  <subject>Study Protocol</subject>
                </subj-group>
              </article-categories>
              <title-group>
                <article-title>Protocol for Correcting Residual Errors with Spectral, ULtrasound, Traditional Speech therapy Randomized Controlled Trial (C-RESULTS RCT)</article-title>
              </title-group>
              <contrib-group>
                <contrib contrib-type="author">
                  <name>
                    <surname>McAllister</surname>
                    <given-names>Tara</given-names>
                  </name>
                  <xref ref-type="aff" rid="Aff1">1</xref>
                </contrib>
                <contrib contrib-type="author" corresp="yes">
                  <contrib-id contrib-id-type="orcid">http://orcid.org/0000-0001-9971-6321</contrib-id>
                  <name>
                    <surname>Preston</surname>
                    <given-names>Jonathan L.</given-names>
                  </name>
                  <address>
                    <email>jopresto@syr.edu</email>
                  </address>
                  <xref ref-type="aff" rid="Aff2">2</xref>
                </contrib>
                <contrib contrib-type="author">
                  <name>
                    <surname>Hitchcock</surname>
                    <given-names>Elaine R.</given-names>
                  </name>
                  <xref ref-type="aff" rid="Aff3">3</xref>
                </contrib>
                <contrib contrib-type="author">
                  <name>
                    <surname>Hill</surname>
                    <given-names>Jennifer</given-names>
                  </name>
                  <xref ref-type="aff" rid="Aff4">4</xref>
                </contrib>
                <aff id="Aff1"><label>1</label><institution-wrap><institution-id institution-id-type="ISNI">0000 0004 1936 8753</institution-id><institution-id institution-id-type="GRID">grid.137628.9</institution-id><institution>Department of Communicative Sciences and Disorders, </institution><institution>New York University, </institution></institution-wrap>New York, NY USA </aff>
                <aff id="Aff2"><label>2</label><institution-wrap><institution-id institution-id-type="ISNI">0000 0001 2189 1568</institution-id><institution-id institution-id-type="GRID">grid.264484.8</institution-id><institution>Department of Communication Sciences and Disorders, </institution><institution>Syracuse University, </institution></institution-wrap>621 Skytop Rd, Suite 1200, Syracuse, NY 13244 USA </aff>
                <aff id="Aff3"><label>3</label><institution-wrap><institution-id institution-id-type="ISNI">0000 0001 0745 9736</institution-id><institution-id institution-id-type="GRID">grid.260201.7</institution-id><institution>Department of Communication Sciences &amp; Disorders, </institution><institution>Montclair State University, </institution></institution-wrap>Bloomfield, NJ USA </aff>
                <aff id="Aff4"><label>4</label><institution-wrap><institution-id institution-id-type="ISNI">0000 0004 1936 8753</institution-id><institution-id institution-id-type="GRID">grid.137628.9</institution-id><institution>Department of Applied Statistics, Social Science, and the Humanities, </institution><institution>New York University, </institution></institution-wrap>New York, NY USA </aff>
              </contrib-group>
              <pub-date pub-type="epub">
                <day>11</day>
                <month>2</month>
                <year>2020</year>
              </pub-date>
              <pub-date pub-type="pmc-release">
                <day>11</day>
                <month>2</month>
                <year>2020</year>
              </pub-date>
              <pub-date pub-type="collection">
                <year>2020</year>
              </pub-date>
              <volume>20</volume>
              <elocation-id>66</elocation-id>
              <history>
                <date date-type="received">
                  <day>14</day>
                  <month>1</month>
                  <year>2020</year>
                </date>
                <date date-type="accepted">
                  <day>22</day>
                  <month>1</month>
                  <year>2020</year>
                </date>
              </history>
              <permissions>
                <copyright-statement>© The Author(s). 2020</copyright-statement>
                <license license-type="OpenAccess">
                  <license-p><bold>Open Access</bold>This article is distributed under the terms of the Creative Commons Attribution 4.0 International License (<ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/">http://creativecommons.org/licenses/by/4.0/</ext-link>), which permits unrestricted use, distribution, and reproduction in any medium, provided you give appropriate credit to the original author(s) and the source, provide a link to the Creative Commons license, and indicate if changes were made. The Creative Commons Public Domain Dedication waiver (<ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/publicdomain/zero/1.0/">http://creativecommons.org/publicdomain/zero/1.0/</ext-link>) applies to the data made available in this article, unless otherwise stated.</license-p>
                </license>
              </permissions>
              <abstract id="Abs1">
                <sec>
                  <title>Background</title>
                  <p id="Par1">Speech sound disorder in childhood poses a barrier to academic and social participation, with potentially lifelong consequences for educational and occupational outcomes. While most speech errors resolve by the late school-age years, between 2 and 5% of speakers exhibit residual speech errors (RSE) that persist through adolescence or even adulthood. Previous findings from small-scale studies suggest that interventions incorporating visual biofeedback can outperform traditional motor-based treatment approaches for children with RSE, but this question has not been investigated in a well-powered randomized controlled trial.</p>
                </sec>
                <sec>
                  <title>Methods/design</title>
                  <p id="Par2">This project, <italic>Correcting Residual Errors with Spectral, ULtrasound, Traditional Speech therapy Randomized Controlled Trial</italic> (<italic>C-RESULTS RCT</italic>), aims to treat 110 children in a parallel randomized controlled clinical trial comparing biofeedback and non-biofeedback interventions for RSE affecting the North American English rhotic sound /ɹ/. Eligible children will be American English speakers, aged 9–15 years, who exhibit RSE affecting /ɹ/ but otherwise show typical cognitive-linguistic and hearing abilities. Participants will be randomized, with stratification by site (Syracuse University or Montclair State University) and pre-treatment speech production ability, to receive either a motor-based treatment consistent with current best practices in speech therapy (40% of participants) or treatment incorporating visual biofeedback (60% of participants). Within the biofeedback condition, participants will be assigned in equal numbers to receive biofeedback in the form of a real-time visual display of the acoustic signal of speech or ultrasound imaging of the tongue during speech. The primary outcome measure will assess changes in the acoustics of children’s production of /ɹ/ during treatment, while a secondary outcome measure will use blinded listeners to evaluate changes in the perceived accuracy of /ɹ/ production after the completion of all treatment. These measures will allow the treatment conditions to be compared with respect to both efficacy and efficiency.</p>
                </sec>
                <sec>
                  <title>Discussion</title>
                  <p id="Par3">By conducting the first well-powered randomized controlled trial comparing treatment with and without biofeedback, this study aims to provide high-quality evidence to guide treatment decisions for children with RSE.</p>
                </sec>
                <sec>
                  <title>Trial registration</title>
                  <p id="Par4">ClinicalTrials.gov identifier <ext-link ext-link-type="uri" xlink:href="https://clinicaltrials.gov/ct2/show/NCT03737318?cond=NCT03737318&amp;draw=2&amp;rank=1">NCT03737318</ext-link>, November 9, 2018.</p>
                </sec>
              </abstract>
              <kwd-group xml:lang="en">
                <title>Keywords</title>
                <kwd>Speech sound disorder</kwd>
                <kwd>Randomized controlled trial</kwd>
                <kwd>Biofeedback</kwd>
              </kwd-group>
              <funding-group>
                <award-group>
                  <funding-source>
                    <institution-wrap>
                      <institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100000055</institution-id>
                      <institution>National Institute on Deafness and Other Communication Disorders</institution>
                    </institution-wrap>
                  </funding-source>
                  <award-id>R01DC017476</award-id>
                  <principal-award-recipient>
                    <name>
                      <surname>McAllister</surname>
                      <given-names>Tara</given-names>
                    </name>
                  </principal-award-recipient>
                </award-group>
              </funding-group>
              <custom-meta-group>
                <custom-meta>
                  <meta-name>issue-copyright-statement</meta-name>
                  <meta-value>© The Author(s) 2020</meta-value>
                </custom-meta>
              </custom-meta-group>
            </article-meta>
          </front>
          <body>
            <sec id="Sec1">
              <title>Background</title>
              <p id="Par18">Developmental speech sound disorder results in reduced speech intelligibility and poses a barrier to academic and social participation. The negative socioemotional ramifications of speech sound disorder have been well documented [<xref ref-type="bibr" rid="CR1">1</xref>, <xref ref-type="bibr" rid="CR2">2</xref>], and the impact on educational and occupational outcomes may be lifelong [<xref ref-type="bibr" rid="CR3">3</xref>, <xref ref-type="bibr" rid="CR4">4</xref>]. Most children with delayed speech go on to develop normal speech by 8–9 years old, but between 2 and 5% of speakers exhibit residual speech errors (RSE) that persist through adolescence or even adulthood [<xref ref-type="bibr" rid="CR5">5</xref>–<xref ref-type="bibr" rid="CR7">7</xref>]. RSE is known to be particularly challenging to treat, with the result that speech-language pathologists (SLPs) frequently discharge these clients with their errors uncorrected [<xref ref-type="bibr" rid="CR8">8</xref>]. In North American English, residual errors affecting the rhotic sound /ɹ/ (as in the beginning of the word <italic>red</italic>) are considered the most common form of RSE [<xref ref-type="bibr" rid="CR8">8</xref>]; in order to select a relatively homogeneous participant population, the present study focuses on this subset of individuals with RSE.</p>
              <p id="Par19">Recent evidence suggests that visual biofeedback, which uses instrumentation to provide real-time information about aspects of speech that are typically outside the speaker’s conscious control [<xref ref-type="bibr" rid="CR9">9</xref>], can be used to enhance intervention for RSE and other speech disorders. Visual biofeedback can incorporate various technologies. The focus in C-RESULTS RCT is on two specific technologies: <italic>ultrasound biofeedback</italic>, in which an ultrasound probe held beneath the chin is used to create a dynamic image of the shape and movements of the tongue, and <italic>visual-acoustic biofeedback</italic>, in which a microphone and software are used to generate a real-time display of the resonant frequencies of the vocal tract (formants). In either type of biofeedback, the learner is shown a model or template representing correct production of the target sound and is encouraged to adjust their own output to achieve a better match with the target in the real-time visual display.</p>
              <p id="Par20">A number of recent studies have documented positive responses to biofeedback treatment in individuals who showed minimal response to previous intervention. This is true of both visual-acoustic [<xref ref-type="bibr" rid="CR10">10</xref>–<xref ref-type="bibr" rid="CR12">12</xref>] and ultrasound [<xref ref-type="bibr" rid="CR13">13</xref>–<xref ref-type="bibr" rid="CR16">16</xref>] types of biofeedback. Many of these studies have used single-case experimental methods to compare gains in biofeedback-enhanced treatment versus traditional forms of intervention. In the context of visual-acoustic biofeedback, a study of 11 children who received an initial period of traditional motor-based treatment followed by biofeedback treatment found that significant improvements in perceptual and acoustic measures of /ɹ/ production occurred only after biofeedback treatment had begun for all but one participant. Another single-case experimental study of 11 children [<xref ref-type="bibr" rid="CR11">11</xref>] found a significant interaction between treatment condition and order, such that a period of visual-acoustic biofeedback followed by a period of traditional motor-based treatment was associated with significantly larger effect sizes than the same treatments provided in the reverse order. Finally, in a single-case randomization study [<xref ref-type="bibr" rid="CR12">12</xref>], three out of seven participants showed a statistically significant advantage for visual-acoustic biofeedback over traditional treatment, while none showed a significant advantage in the opposite direction.</p>
              <p id="Par21">In a recent systematic review of 28 ultrasound biofeedback treatment studies reporting on over 100 participants, Sugden et al. [<xref ref-type="bibr" rid="CR17">17</xref>] indicated that this approach resulted in positive outcomes in many, but not all, children. For example, a single-case study with 8 children [<xref ref-type="bibr" rid="CR16">16</xref>] found that participants who previously had not responded to as much as 11 years of traditional intervention achieved large average effect sizes in ultrasound biofeedback treatment. Another recent study [<xref ref-type="bibr" rid="CR15">15</xref>] reported that improvements in /ɹ/ production were significantly greater for six children who completed 8 sessions with ultrasound biofeedback therapy than for six children who had completed 8 sessions of traditional articulation treatment.</p>
              <p id="Par22">The main limitation of previous research is the small number of individuals tested in each study and the lack of studies using a randomized controlled trial (RCT) methodology. Specifically, to determine whether biofeedback is an operative component of treatment, it is important to compare similarly structured treatment programs that differ only in the presence of biofeedback. In this study, therefore, biofeedback is compared to a similarly structured non-biofeedback treatment, termed motor-based treatment (MBT). Another limitation of previous research pertains to the outcome measures used to evaluate treatment response. While most published studies have focused on evaluating <italic>generalization</italic> of treatment gains (e.g., improvement in untreated words or contexts), models of motor learning suggest that biofeedback, as a form of detailed qualitative feedback, should have its greatest impact in early phases of treatment (<italic>acquisition</italic>) [<xref ref-type="bibr" rid="CR18">18</xref>, <xref ref-type="bibr" rid="CR19">19</xref>]. For example, in one study [<xref ref-type="bibr" rid="CR14">14</xref>], participants produced correct trials at a rate of 229 trials per hour in an ultrasound treatment condition, which significantly exceeded the rate of 155 correct trials per hour in the non-biofeedback treatment condition. However, by the end of the study, both conditions yielded roughly similar levels of generalization. Thus, studies comparing biofeedback and non-biofeedback treatment should consider the possibility that the largest difference may lie in the area of <italic>efficiency</italic> rather than <italic>efficacy</italic>.</p>
              <p id="Par23">C-RESULTS RCT aims to address these limitations by conducting a well-powered RCT comparing biofeedback and non-biofeedback forms of intervention. Visual-acoustic and ultrasound biofeedback, which have comparable evidence of efficacy, will be represented equally in the biofeedback condition. The non-biofeedback treatment condition, MBT, also has documented evidence of efficacy [<xref ref-type="bibr" rid="CR20">20</xref>, <xref ref-type="bibr" rid="CR21">21</xref>]. With treatment protocols and outcome measures that have been refined over numerous small experimental studies, this RCT can be expected to provide interpretable evidence for or against our hypothesis of improved outcomes in biofeedback relative to non-biofeedback intervention. To assess the possibility of differences in either efficiency or efficacy, the present study will track two outcomes. Acquisition, or speech performance over the course of early treatment, will serve as the primary outcome measure. Generalization, or speech performance on untrained contexts without clinician support, will serve as a secondary measure. Based on previous findings, we hypothesize that both MBT and biofeedback groups will improve over the course of treatment, but we also predict a significant between-group difference whereby pre-post gains associated with biofeedback treatment will significantly exceed those observed in the non-biofeedback MBT condition. We hypothesize that these differences will affect measures of both acquisition and generalization; if we observe a difference in the former but not the latter, this will constitute evidence that biofeedback is more efficient, but not more effective, than non-biofeedback treatment.</p>
            </sec>
            <sec id="Sec2">
              <title>Methods and design</title>
              <p id="Par24">C-RESULTS RCT is a parallel-group randomized controlled trial measuring the efficiency and efficacy of intervention with and without biofeedback for children with RSE affecting /ɹ/. Participants will be assigned to receive a standard dose of intervention in a biofeedback treatment condition or in the MBT (non-biofeedback) condition. Individuals assigned to the biofeedback treatment condition will be sub-allocated to receive either visual-acoustic or ultrasound biofeedback treatment in equal numbers. A comparison of biofeedback types is proposed as part of a separate study but will not constitute a primary aim of C-RESULTS RCT. To maximize power for the separate study comparing biofeedback types, participants will be allocated to biofeedback versus MBT conditions in a 3:2 ratio. Participant allocation in each group will be further stratified by pre-treatment severity, since previous research has identified this variable as an important indicator of subsequent treatment response. Finally, allocation will additionally be stratified by site, as data collection will occur in two locations in the United States (Montclair, New Jersey; Syracuse, New York).</p>
              <p id="Par25">We plan to enroll a total of 110 children with RSE. Our previous work comparing biofeedback and non-biofeedback treatment found a median effect size (Cohen’s d) of .70 for MBT versus ultrasound treatment [<xref ref-type="bibr" rid="CR14">14</xref>] and .64 for MBT versus visual-acoustic biofeedback [<xref ref-type="bibr" rid="CR11">11</xref>]. Because both biofeedback types will be represented in the biofeedback treatment condition in C-RESULTS RCT, we use .67 as our best estimate of the likely effect size of the difference between biofeedback and MBT conditions. Calculations in G*Power [<xref ref-type="bibr" rid="CR22">22</xref>] indicate that our proposed sample of 110 children has &gt; 90% power to detect an effect of this magnitude. This power calculation was based on a fully randomized experimental design; however, our final design includes blocking on pre-treatment accuracy, which we expect to be a significant predictor of outcomes. Thus, the original power calculation can be assumed to represent a conservative estimate.</p>
              <sec id="Sec3">
                <title>Participants</title>
                <p id="Par26">Ethics approval for this study has been obtained through the Biomedical Research Association of New York (BRANY, protocol #18–10-393). C-RESULTS RCT is a multi-site study with two treatment sites (Montclair State University and Syracuse University) and a central site for data processing and analysis (New York University). All participants will provide written assent to take part in the study, and the parent/guardian will additionally provide written permission for participation. For secure management of potentially identifying information, including consent/assent and responses to questionnaires, all study data will be entered and verified in REDCap electronic data capture tools hosted at Syracuse University [<xref ref-type="bibr" rid="CR23">23</xref>, <xref ref-type="bibr" rid="CR24">24</xref>]. Participants will be recruited primarily through community speech-language pathologists (SLPs). Cooperating SLPs will be identified mainly through online postings to listservs, blogs, social media channels, alumni lists, and personal contacts. Other participants may be referred directly by their parents, who will be reached by announcements posted to parenting groups on listservs and social media, as well as flyers posted in public locations such as libraries, schools, and pediatricians’ offices. Participant enrollment began August 29, 2019.</p>
                <p id="Par27">All participants must speak English as a dominant language and must have begun learning English by age 3, per parent report on a questionnaire. They must have no history of sensorineural hearing loss, major neurobehavioral disorder, or developmental disability (e.g., autism spectrum disorder, Down Syndrome), as indicated by parent report. They must be between 9;0 and 15;11 years of age at the time of enrollment. Participants must also pass a pure-tone hearing screening at 20 dB HL and a brief examination of oral structure and function. To rule out nonverbal cognitive delays, participants must additionally demonstrate adequate nonverbal skills as defined by a T score &lt; 1.3 SD below the mean on the Wechsler Abbreviated Scale of Intelligence-2 (WASI-2) [<xref ref-type="bibr" rid="CR25">25</xref>] Matrix Reasoning. To rule out cases of language impairment, which could confound the interpretation of our results, all participants must have language skills broadly within normal limits as evidenced by a passing score on the Clinical Evaluation of Language Fundamentals-5 (CELF-5) [<xref ref-type="bibr" rid="CR26">26</xref>] screening measure, or a minimum standard score of 80 on the Core Language Index of the CELF-5 (see below). Finally, to select individuals with a relatively uniform level of baseline severity, participants must exhibit less than 30% accuracy, based on consensus across 2 trained listeners, on a 50-item probe list eliciting /ɹ/ in various words. They must also score no higher than the 8th percentile on the Goldman-Fristoe Test of Articulation-3 (GFTA-3) [<xref ref-type="bibr" rid="CR27">27</xref>].</p>
                <p id="Par28">To rule out childhood apraxia of speech (CAS), we will initially administer two tasks; participants who score in the range for childhood apraxia on both tasks will be automatically excluded. (1) The Syllable Repetition task will be used to determine the number of added segments, with a cutoff score of 4 or more additions reflecting likely CAS [<xref ref-type="bibr" rid="CR28">28</xref>]. (2) The multisyllabic word task of the LinguiSystems Articulation Test [<xref ref-type="bibr" rid="CR29">29</xref>] will be administered to identify the number of inconsistencies across repeated productions, with a cutoff score of three or more inconsistencies reflecting likely CAS. If participants pass one measure but fail the other, a maximum performance task will be administered as a tiebreaker measure. In the maximum performance task, children sustain phonemes (e.g., /a/, /s/), repeat single syllables (e.g., /papapa/), and repeat trisyllable sequences (/pataka/). Procedures outlined by Thoonen et al. [<xref ref-type="bibr" rid="CR30">30</xref>, <xref ref-type="bibr" rid="CR31">31</xref>] will be followed to identify suspected cases of CAS based on slow or inaccurate trisyllable production. Participants who do not achieve a passing score on two of the three measures will be excluded.</p>
                <p id="Par29">No participants will be excluded from the proposed study on the basis of sex/gender or racial/ethnic group. Both male and female children will be recruited, and the large proposed sample size will allow us to test for any influence of gender on either typical performance patterns or response to intervention. However, based on the general makeup of the population of children with speech sound disorders [<xref ref-type="bibr" rid="CR5">5</xref>], we anticipate that slightly more males will be referred than females.</p>
              </sec>
              <sec id="Sec4">
                <title>Assessment process</title>
                <p id="Par30">A phone screening will initially be conducted to identify any exclusionary criteria identifiable via parent report (e.g., outside of age range, not a native speaker of English, or diagnosis of developmental disability). A detailed description of study requirements will also be presented to parents at this time. Individuals who pass the phone screening will be invited to participate in a 1–2 h eligibility assessment. Consent and assent instruments will be administered at this time, as well as questionnaires collecting detailed information about participants’ health and language history. The following tasks will be administered: hearing screening, oral mechanism screening, WASI-2 Matrix Reasoning [<xref ref-type="bibr" rid="CR25">25</xref>], CELF-5 Screener [<xref ref-type="bibr" rid="CR26">26</xref>], GFTA-3 [<xref ref-type="bibr" rid="CR27">27</xref>], Syllable Repetition Task [<xref ref-type="bibr" rid="CR28">28</xref>], and Linguisystems Articulation Test Inconsistency Screener [<xref ref-type="bibr" rid="CR29">29</xref>]. Participants will also produce custom probes eliciting /ɹ/ at the syllable/disyllable level (stimulability probe [<xref ref-type="bibr" rid="CR32">32</xref>]) and word level. Individuals who pass all eligibility criteria will be asked to return for 1–2 additional testing sessions that will collect information about acuity in auditory and somatosensory domains, to be used as part of a separate analysis examining individual predictors of response to treatment. If the results of any preliminary testing were equivocal, additional eligibility testing will be carried out at this point. Specifically, the full CELF-5 Core Language Index will be administered if the CELF-5 screening measure was not passed, and the above-described maximum performance tasks will be administered if the participant passed one but not both of the standard screening measures for CAS.</p>
              </sec>
              <sec id="Sec5">
                <title>Group allocation</title>
                <p id="Par31">Following the initial evaluation visits, all participants will complete a Dynamic Assessment session (Phase 0) consisting of 2 h of non-biofeedback intervention, described in more detail below, in which the clinician will provide extensive cueing and feedback in an effort to elicit the client’s best attempts at /ɹ/ production. Based on the treating clinicians’ perceptual ratings of participants’ performance in Phase 0, participants will be categorized as high responders (&gt; 5% accuracy during Dynamic Assessment) or low responders (0–5% accuracy during Dynamic Assessment). The study statistician will generate confidential participant treatment assignments in batches of 10, where each batch of 10 corresponds to a combination of site (Montclair State University versus Syracuse University) and response category (high versus low). Random assignments will be generated to ensure the following distribution per batch: 3 individuals assigned to receive visual-acoustic biofeedback, 3 to receive ultrasound biofeedback, and 4 to receive MBT. The first 4 batches will correspond to the first 10 participants recruited in each of these site-by-category combinations. The second 4 batches will correspond to the same site-by-category combinations for the next time period. The PIs will monitor allocation of participants to response categories within each site to understand whether the distribution is reasonably balanced. If it is, the final 4 batches will be assigned similarly. If not, the treatment assignment may be altered.</p>
                <p id="Par32">In cases where a participant drops out prior to the completion of Phase 1 (see below), the next participant to enroll will be assigned to the same treatment condition as a replacement for the dropout. If a participant drops out after the completion of Phase 1, they will retain their random assignment and the next participant will receive a new assignment. Dropouts will be offered compensation to return for a follow-up assessment after the typical duration of treatment elapses, in order to measure their outcomes in the absence of treatment.</p>
              </sec>
              <sec id="Sec6">
                <title>Intervention delivery and dosage</title>
                <p id="Par33">All treatment will be provided in an individualized fashion by a certified SLP who will have completed a standard training process, detailed below. Treatment will take place in research space at Syracuse University or Montclair State University. While participating in the study intervention, participants who are enrolled in any outside speech therapy will be asked to discontinue treatment on /ɹ/ if possible.</p>
                <p id="Par34">The data collection schedule is outlined in Fig. <xref rid="Fig1" ref-type="fig">1</xref>. Independent of a participant’s assigned treatment condition, interventions will be divided into three phases, each with its own schedule and dosage of treatment. Prior to randomization, all participants will complete a single 90-min Dynamic Assessment session featuring non-biofeedback treatment (Phase 0). The goal of this session is to evaluate participants’ stimulability for /ɹ/ production and classify them into the high- and low-response categories used for stratification in randomization, as described above. The Dynamic Assessment session will feature approximately 25 min of detailed instruction on the articulatory characteristics of English /ɹ/, followed by 15–20 min of unstructured pre-practice aimed at eliciting /ɹ/ in various syllables using a standard set of non-biofeedback techniques (verbal models, phonetic placement cues, and shaping strategies). This will be followed by a period of structured practice, approximately 45 min in duration, eliciting up to 200 syllables. The session will be terminated after 200 syllables are produced or after the cumulative session duration reaches 90 min.
<fig id="Fig1"><label>Fig. 1</label><caption><p>Schedule of evaluation, allocation, treatment, and close-out activities for C-RESULTS RCT</p></caption><graphic xlink:href="12887_2020_1941_Fig1_HTML" id="MO1"/></fig></p>
                <p id="Par35">After randomization, participants will complete two additional phases of treatment in their allocated condition. Phase 1 (Acquisition) will consist of 1 week of high-intensity, highly interactive practice, while Phase 2 (Generalization) will involve a longer period of lower-intensity, more structured practice. Specifically, Phase 1 (Acquisition) will focus on eliciting /ɹ/ at the syllable level in three 90-min sessions delivered over the course of 1 week. The first 45 min of these three sessions will consist of relatively unstructured pre-practice, similar to that described above for Phase 0 (Dynamic Assessment), but with the addition of condition-specific prompts and feedback. In the first Acquisition session only, participants will receive detailed condition-specific instructions, described in detail below, for approximately 25 of the first 45 min. The second half of the session will consist of structured practice; as described above for Phase 0 (Dynamic Assessment), structured practice will terminate after 200 syllables are elicited or after cumulative session duration reaches 90 min.</p>
                <p id="Par36">Phase 2 (Generalization) will consist of 16 semiweekly sessions with a duration of 45 min to 1 h. Each session in Phase 2 will begin with up to 10 min of unstructured pre-practice similar to that provided during Phase 1. This will be followed by a period of structured practice that will terminate after 250 trials or after cumulative session duration reaches 1 h, whichever comes first. In an effort to maximize generalization of learning gains, the difficulty of practice in Phase 2 will be dynamically adjusted based on participant performance; we describe the procedure for adaptive difficulty in a subsequent section.</p>
              </sec>
              <sec id="Sec7">
                <title>Types of intervention</title>
                <sec id="Sec8">
                  <title>Motor-based treatment (MBT)</title>
                  <p id="Par37">Participants across all arms of the study will be exposed to MBT because it forms the basis of the pre-randomization Phase 0 (Dynamic Assessment) session. In this initial session, participants will receive their first introduction to articulatory anatomy and tongue shapes for /ɹ/, following a script made available through an Open Science Framework repository linked in the 'Availability of data and materials' section below; completing the script takes roughly 20–25 min. Images and diagrams will be used to teach participants to identify different components of the tongue (root, back, blade, tip), with the rationale that more precise language to talk about articulation can help make articulator placement cues more effective. They will then be familiarized, via pictures, with major characteristics of correct tongue shapes for /ɹ/. They will be told that various different tongue shapes are associated with successful /ɹ/ production, but a few characteristics are shared across tongue shapes: (a) retraction of the tongue root, (b) elevation of the tongue front (tip, blade, and/or anterior dorsum), and (c) bracing of the lateral margins of the tongue against the posterior palate, with a groove down the center of the tongue. They will be instructed to compare and contrast images with and without these articulatory components, followed by comprehension questions in which they will be asked to identify correct versus incorrect tongue shapes for /ɹ/.</p>
                  <p id="Par38">After the initial instructional period,<xref ref-type="fn" rid="Fn1">1</xref> the clinician will attempt to elicit correct /ɹ/, first in an unstructured pre-practice context and then in structured practice, as stated above. In pre-practice, the basic therapeutic exchange will involve providing auditory models, eliciting imitation, and offering feedback and cues to shape production into successively closer approximations of the adult target [<xref ref-type="bibr" rid="CR14">14</xref>, <xref ref-type="bibr" rid="CR16">16</xref>, <xref ref-type="bibr" rid="CR33">33</xref>, <xref ref-type="bibr" rid="CR34">34</xref>]. Shaping can involve verbal cues for articulator placement (e.g., “<italic>Pull the back of your tongue way into the back of your throat</italic>”; “<italic>Pretend you’re holding a marble between the tongue and the back of your throat”</italic>) or eliciting from facilitative contexts (e.g., pairing /ɹ/ with the low back vowel /ɑ/, which is characterized by a position of the tongue body and root that is compatible with the articulatory configuration for /ɹ/). Suggested cues are summarized in a standard list that is linked through Open Science Framework. Structured practice will focus on cueing repetitive motor practice in an effort to make improved production habitual, with verbal feedback to reinforce correct productions and reshape off-target productions.</p>
                  <p id="Par40">Across phases and treatment conditions, magnetic resonance (MR) images representing correct articulation of /ɹ/ will be made available to instruct the client on correct positioning of the articulators. The image used in a given session for each participant will be selected to highlight specific aspects of tongue shape, such as elevation of the tongue blade or lowering of the tongue dorsum, judged to be appropriate for improving that individual’s /ɹ/ production. (In the MBT and visual-acoustic biofeedback conditions, while the clinician lacks direct knowledge of the client’s articulator position, inferences can be drawn from the perceptual quality of their production.) Note that static images from various technologies (MR, electropalatography) are incorporated into all treatment conditions, including MBT; it is only real-time feedback that is limited to the biofeedback conditions.</p>
                </sec>
                <sec id="Sec9">
                  <title>Ultrasound biofeedback</title>
                  <p id="Par41">In the ultrasound biofeedback condition, the core elements of MBT (auditory models and verbal descriptions of articulator placement) will be supplemented with a real-time ultrasound display of the shape and movements of the tongue, which can be compared against a target representing correct production. Ultrasound biofeedback will be delivered using an EchoBlaster 128 or a MicroUS probe (Telemed Medical Systems). The ultrasound hardware will be paired with Echo Wave software operating on a personal computer.</p>
                  <p id="Par42">In the first session of Phase 1 (Acquisition), participants will receive an introduction to ultrasound biofeedback, roughly 20–25 min in duration, guided by a script that is available through the project’s Open Science Framework repository. Participants will be oriented to basic information about ultrasound technology (e.g., how the probe is held) and the ultrasound image (e.g., how the tongue appears in an ultrasound image, which side is the front versus the back of the tongue). They will also be taught to connect what they see in the ultrasound image with the articulatory information that was introduced in the previous session. For instance, they will identify the different parts of their tongue (tip, blade, dorsum, root) in the ultrasound image. They will discuss the major features of articulation of /ɹ/ as seen in ultrasound and review images of correct and incorrect tongue shapes for /ɹ/. As a comprehension check, participants will be asked to distinguish between ultrasound images of correct and incorrect /ɹ/.</p>
                  <p id="Par43">Pre-practice and structured practice in the ultrasound biofeedback condition will be much the same as in the non-biofeedback treatment condition; the same list of suggested cues described above for the MBT condition will be used. The main difference is that, when the clinician cues the child to do something with the tongue (e.g., retract the tongue root), the ultrasound image will provide direct evidence as to whether or not the child was successful in following that instruction. In addition, the clinician will cue the child to match a visible target. One or two target tongue shapes will be selected for each participant, and a trace of the selected target will be superimposed over the ultrasound screen. Participants will be cued to reshape the tongue to match this target during /ɹ/ production. Participants will not be locked into a single tongue shape throughout the duration of treatment: if there is a lack of progress, operationalized as failure to increase by one level of difficulty across three sessions, a contrasting tongue shape will be introduced in the next session. Conversely, if a participant achieves perceptually accurate production of /ɹ/ in the treatment setting, the standard tongue shape can be replaced with a trace of the participant’s own best approximation to date.</p>
                </sec>
                <sec id="Sec10">
                  <title>Visual-acoustic biofeedback</title>
                  <p id="Par44">In visual-acoustic biofeedback, again, the core elements of MBT remain unchanged, but practice is supplemented with a real-time display of the acoustic signal of speech, which can be compared against a target representing the acoustics of a correct /ɹ/. Visual-acoustic biofeedback will be delivered using the Sona-Match module of the PENTAXMedical Computerized Speech Lab (CSL) software, which will be paired with CSL hardware model 4500B.</p>
                  <p id="Par45">In the first Acquisition session, participants will receive an introduction to the CSL Sona-Match program used for visual-acoustic biofeedback. As with ultrasound biofeedback, this introduction will last roughly 20–25 min and will be guided by a standard script that has been uploaded to the Open Science Framework site linked below. The Sona-Match software presents a dynamic display of the speech signal in the form of a real-time LPC (Linear Predictive Coding) spectrum. It also allows the clinician to load a template representing an appropriate pattern of formant heights for a particular sound, which can be superimposed over the dynamic LPC spectrum of the child's speech. Participants will initially be familiarized with the technology by being encouraged to produce a variety of sounds and observe how the formants (“peaks” or “bumps”) move when different sounds are produced. They will then be familiarized with the concept of matching formant templates through an exercise in which the clinician presents a template for a vowel that the child can articulate correctly, then cues the child to try different vowel sounds and guess the target sound based on the closest match. Once a participant demonstrates comprehension of this procedure for matching a template, the target formant configuration for /ɹ/ will be introduced with static images and videos. Participants will be taught that correct /ɹ/ productions are characterized acoustically by a low frequency of the third formant, which creates proximity between the second and third formants. To check comprehension, participants will be asked to differentiate between correct and incorrect /ɹ/ as seen in the visual-acoustic display.</p>
                  <p id="Par46">As above, pre-practice and structured practice will have the same basic characteristics as in non-biofeedback treatment, with the exception that the clinician will cue the child to match the visual formant target. It is important to note that articulator placement cues (following the same list of cues as the MBT condition) will be made available during visual-acoustic biofeedback practice, even though the visual display does not provide direct information about articulation. When the clinician cues the child to do something with the tongue (e.g., retract the tongue root), the child and clinician can visually observe the acoustic consequences of that modification and judge whether it brought them closer to an acoustically correct /ɹ/. A formant template will be selected for each child to match during practice. Because formant heights are influenced by vocal tract size, a procedure will be followed to identify the best-matching option from a library of formant templates collected from typically developing children of different ages and heights. Because participants with RSE may not be able to produce any correct /ɹ/ sounds, this matching procedure will be performed based on productions of the vowel /i/, and participants will be assigned a target /ɹ/ template from the typically developing child whose /i/ production best matched their own. As was the case for ultrasound tongue shape templates, if the child starts to achieve perceptually accurate /ɹ/ in treatment, the template based on another child’s production can be replaced with a template representing a freeze-frame of the participant’s own best approximation of /ɹ/.</p>
                </sec>
              </sec>
              <sec id="Sec11">
                <title>Stimuli and scoring</title>
                <p id="Par47">The /ɹ/ sound has somewhat different articulatory and/or acoustic characteristics in different coarticulatory contexts; these contexts may be differentially affected across children with /ɹ/ misarticulation [<xref ref-type="bibr" rid="CR10">10</xref>]. In the present study, target variants will not be customized on a per-participant basis; instead, all sessions will feature stimuli selected to represent five major phonetic contexts. These contexts are the following: syllabic /ɝ/ as in <italic>bird</italic>, postvocalic /ɹ/ in a front vowel context as in <italic>deer</italic> or <italic>chair</italic>, postvocalic /ɹ/ in a back vowel context as in <italic>star</italic> or <italic>door</italic>, prevocalic /ɹ/ in a front vowel context as in <italic>red</italic> or <italic>ran</italic>, and prevocalic /ɹ/ in a back vowel context as in <italic>rob</italic> or <italic>rude</italic>.</p>
                <p id="Par48">In pre-practice for Phase 0 (Dynamic Assessment) and Phase 1 (Acquisition), stimuli will be drawn from a fixed list consisting of up to three syllables representing each of the five contexts identified above. One syllable will be randomly selected to represent each context. In Phase 2 (Generalization), stimuli for pre-practice will be drawn from the list of words to be targeted in structured practice.</p>
                <p id="Par49">During structured practice, all stimuli will be presented and clinician responses recorded using our custom open-source <italic>Challenge Point Program</italic> (CPP) software [<xref ref-type="bibr" rid="CR35">35</xref>]. The syllables or words targeted in each session will be randomly selected from a master list by the CPP software. In Phase 1 (Acquisition), the software will select one syllable to represent each of the five variants listed above, resulting in a total of five targets per session. In Phase 2 (Generalization), the software will select two syllables/words from each of the five variants listed above, resulting in a total of ten targets per session. Practice will occur in blocks of ten consecutive trials on the same syllable or word, after which a new syllable or word will be addressed (e.g., ten trials of /ɹa/ followed by ten trials of /ɹu/). If the participant moves up in the adaptive difficulty hierarchy detailed below, new items will be selected, potentially resulting in more than ten words or syllables per session. After the software presents a stimulus and the participant produces it, the clinician will score the response as 0 or 1 based on their clinical impression of an incorrect or correct production of /ɹ/. Only fully correct productions will be scored as 1; distorted productions will be scored as 0.</p>
                <p id="Par50">In Phase 2 (Generalization) only, following each block of ten trials, the software automatically tallies the scores entered by the clinician and uses them to make adaptive changes in task difficulty for the next block. This reflects a goal of keeping learners at a level of difficulty that is neither too hard nor too easy in order to maximize opportunities for learning during speech practice; this “challenge point” concept is drawn from previous motor learning research [<xref ref-type="bibr" rid="CR36">36</xref>, <xref ref-type="bibr" rid="CR37">37</xref>]. If a participant produces eight or more correct responses in a block, the next block will feature an increase in difficulty; five or fewer correct responses will prompt a decrease in difficulty; and six or seven correct responses will trigger no change in difficulty. At the end of a session, the parameter settings are saved, and the next session begins at a level determined by the participant’s performance at the end of the previous session. A total of 15 different levels of increasing difficulty are built into the program.</p>
                <p id="Par51">The parameters used to adjust task difficulty include the linguistic complexity of the string in which the /ɹ/ target is elicited, the frequency with which verbal feedback and/or biofeedback are made available, and the mode of elicitation. These parameters will be adjusted on a rotating basis, such that a participant’s first increase in difficulty will involve an increase in linguistic complexity, the next a reduction in feedback frequency, and the next a change in mode of elicitation. Manipulations of stimulus complexity involve changes in the number of syllables per word, the presence or absence of the competing phonemes /l/ and /w/, and the presence or absence of a carrier phrase or sentence context; at the highest level, participants are asked to formulate their own sentence containing a target word. Manipulations of feedback include a gradual reduction in the frequency with which KP and KR feedback are provided (see details in Verbal Feedback Provided by Clinicians section below); at the highest levels, participants are asked to self-evaluate instead of relying on the clinician’s feedback. In biofeedback treatment conditions, the frequency with which the biofeedback display is made available is decreased in tandem with the reductions in verbal feedback frequency (from 80% at the highest level to 0% at the lowest level). Biofeedback frequency reduction will be achieved by minimizing the Sona-Match software in the visual-acoustic biofeedback condition, or by turning the probe off in the ultrasound biofeedback condition. The final parameter to be adjusted is mode of elicitation. At the lowest level, participants will simply be prompted to read each stimulus word; at higher levels, participants will be asked to produce each word or phrase with a prosodic manipulation such as an interrogative or exclamatory intonation pattern. These prosodic manipulations will initially be applied in a blocked fashion (i.e., all items in a block share the same intonation contour) and then in a randomized fashion.</p>
                <p id="Par52">In addition to the within-session changes described above, the CPP allows for performance-based adjustments to the schedule of stimulus presentation that are applied on a between-session basis. Specifically, if the participant’s cumulative accuracy in structured practice in a given session equals or exceeds 80%, the next session will feature a change from fully blocked practice (i.e., each block elicits 10 trials of a single word, and all words representing a given phonetic context for /ɹ/ are elicited in sequence) to random-blocked practice (i.e., each block elicits 10 trials of a single word, but across blocks, different words and phonetic contexts for /ɹ/ can appear in random order). If the child again achieves at least 80% accuracy at the session level, the schedule will again change to feature fully random practice (i.e., different words and variants are represented within each block of 10 trials).</p>
              </sec>
              <sec id="Sec12">
                <title>Verbal feedback provided by clinicians</title>
                <p id="Par53">During practice, each trial is designated to be followed by no feedback, by verbal feedback on the accuracy of /ɹ/ production (i.e., Knowledge of Results feedback, or KR) or by qualitative feedback on /ɹ/ production (i.e., Knowledge of Performance feedback, or KP). To increase fidelity in treatment delivery, the CPP software will provide prompts indicating when and what type of feedback should be provided during structured practice components of treatment. In Phase 1 (Acquisition), KP feedback will be prompted after every other trial, such that the clinician will provide qualitative feedback appropriate for the participant’s treatment condition. In Phase 2 (Generalization), the CPP will prompt a mix of KP and KR feedback, with the frequency of each feedback type changing over the different levels in the CPP complexity hierarchy. When a trial is designated to receive KR feedback, the CPP will automatically display a feedback message (e.g., “Great job”/“Try again”) based on the score for that trial entered by the treating clinician. The clinician may additionally verbalize this feedback, but this is not required. When a trial is designated to receive KP feedback, the CPP will prompt the clinician to provide qualitative verbal feedback. For the purpose of the present study, KP feedback has been operationalized to include 2–3 specific components. First, the clinician must reference what the child is doing or should be doing with the articulators (e.g., “I like the way you kept the front of the tongue up” or “Remember that the back of your tongue should be pulled back for /ɹ/”). Second, if the participant is in one of the biofeedback treatment conditions, the KP feedback must also make reference to what is seen on the visual display. (To make this possible, the CPP software restricts prompts for KP feedback to occur only on trials in which the biofeedback display is supposed to be made available.) The final component of KP feedback is an explicit verbal model of correct production that the clinician provides as a prompt to encourage correct production in the next trial.</p>
              </sec>
              <sec id="Sec13">
                <title>Clinician training</title>
                <p id="Par54">To become familiar with critical information about the study and the technologies used in it, treating clinicians will initially review a series of informational modules in Powerpoint format generated by the study PIs. A total of ten separate modules cover a range of topics including how /ɹ/ is produced, an overview of ultrasound technology, an overview of visual-acoustic technology, and a guide to using the CPP software described above. Three of the modules provide detailed information about how to cue the /ɹ/ sound in each of the three treatment conditions investigated (MBT, cueing with ultrasound, and cueing with visual-acoustic biofeedback); for these three, the Powerpoint has been augmented into a screencast video with one of the PIs providing voiceover narration. Treating clinicians will meet individually with their local PI after completing the training modules so that any outstanding questions can be discussed and resolved.</p>
                <p id="Par55">To obtain hands-on practice in treatment delivery, new clinicians will be required to conduct at least 2 sessions in each treatment condition under the direct supervision of their local PI, who will provide feedback during and after each session. (Pilot participants or students posing as participants will act as the recipients of treatment for this purpose.) The other clinical site will perform fidelity review of at least one pilot session in each treatment condition to ensure that equivalent methods are being used across sites.</p>
                <p id="Par56">To minimize bias on the part of treating clinicians, all training materials have been reviewed to ensure that no treatment approach is advanced as more effective than other alternatives. The training materials accurately characterize MBT as an evidence-based approach that represents the current standard of care. The PI of each site will act to maintain equipoise in laboratory discussions of MBT and biofeedback treatment methods.</p>
              </sec>
              <sec id="Sec14">
                <title>Treatment fidelity</title>
                <p id="Par57">Treating clinicians’ fidelity in adhering to standard protocols will be evaluated through review of screen-recorded video and audio records from a subset of sessions from each participant in each phase of treatment. To preserve blinding at the central site, the two clinical sites will perform fidelity checks for one another. For Phase 0 (Dynamic Assessment), which consists of a single session, fidelity checks will be completed on 20% of participants per site. For Phase 1 (Acquisition), one of the three sessions will be randomly selected for fidelity checking for each participant. For Phase 2 (Generalization), one session will be randomly selected from the first half and one from the second half of treatment for each participant. Due to the lengthy nature of treatment sessions, fidelity checks will be performed on a 50-trial subset randomly selected from within the full session duration.</p>
                <p id="Par58">In each fidelity check, a trained research assistant will review the video record of the clinician’s behavior and compare it against an output record reflecting trial-by-trial prompts generated by the CPP software. For each trial, the CPP output records the following information: (1) whether biofeedback should have been made available or withheld, if applicable; (2) whether the clinician should have provided a verbal model before the trial; (3) whether KP feedback should have been provided or withheld, and (4) whether the client should have been prompted to self-evaluate their production. In each case, the research assistant will score whether the clinician’s behavior accords with the parameter setting indicated by the software. If KP feedback was provided, the research assistant will additionally score whether the clinician’s verbal feedback included the three components indicated above (reference to the observed or desired articulatory behavior, reference to the visual display for biofeedback trials, and provision of a verbal model).</p>
              </sec>
              <sec id="Sec15">
                <title>Recording and equipment</title>
                <p id="Par59">Each site will use 64-bit Dell PCs operating Windows 10 with relevant software for all treatment conditions and evaluation tasks. Equipment for obtaining audio recordings has been standardized across sites. All recordings will be obtained with a head-mounted microphone (AKG C520 Professional Head-Worn Condenser microphone) positioned so the microphone arm is perpendicular to the corner of the mouth. The audio signal from the head-mounted microphone will be routed through a Behringer UMC 404HD audio interface and then to a Marantz solid-state digital recorder; in the visual-acoustic biofeedback condition, an additional output of the Behringer will be connected via line in to the CSL model 4500B hardware used to generate the visual-acoustic biofeedback display. Gain settings on both the audio interface and the digital recorder are standardized across sites, using a predetermined range within which gain can be adjusted on a by-speaker basis to accommodate individual differences in loudness. As a backup, all study activities will also be captured in screenrecorded video. All recordings will be obtained at a 44,100 Hz sampling rate with 16-bit encoding.</p>
              </sec>
              <sec id="Sec16">
                <title>Outcomes measurement</title>
                <p id="Par60">For our primary outcome measure, we will assess acoustic change in children’s /ɹ/ production in a subset of treatment trials produced over the 3 days of Phase 1 (Acquisition). In each block of ten trials, the ninth trial will be elicited with no feedback (KR, KP, or visual biofeedback). These trials, which will be flagged in the acoustic record by a preceding pure-tone burst, will be manually marked with a textgrid in Praat [<xref ref-type="bibr" rid="CR38">38</xref>] and then submitted to a forced aligner. The resulting phoneme-level annotation will be verified and adjusted as needed by trained research assistants. An automated script [<xref ref-type="bibr" rid="CR39">39</xref>] will then be used to extract the second and third formant frequencies (F2 and F3) from a 50 ms window at the center of the interval identified as the phoneme /ɹ/. F3-F2 distance, which is smaller in correct /ɹ/, will serve as the primary acoustic correlate of accuracy. This measure was selected because it provides some degree of correction for differences in vocal tract size and has also been shown to correlate strongly with perceptual ratings of /ɹ/ production accuracy [<xref ref-type="bibr" rid="CR40">40</xref>, <xref ref-type="bibr" rid="CR41">41</xref>]. Research assistants conducting this analysis will be blind to treatment condition.</p>
                <p id="Par61">Our secondary outcome measure will evaluate /ɹ/ production accuracy in a standard probe (50 words containing /ɹ/ in various phonetic contexts) elicited before treatment and again in a post-treatment assessment visit administered within approximately 1 week of the end of Phase 2 (Generalization). To enhance the real-world relevance of this measure, we will use blinded naïve listeners’ ratings of /ɹ/ production accuracy. (Perceptual ratings are not feasible for Phase 1 because within-treatment trials may be prolonged or otherwise unnatural, posing a confound for accuracy ratings provided by untrained listeners.) Following protocols refined in our previous research [<xref ref-type="bibr" rid="CR42">42</xref>, <xref ref-type="bibr" rid="CR43">43</xref>], we will split each word probe recording into 50 word-level productions and pool these productions across speakers and time points. These stimuli will then be presented in a randomized order for online listeners who originate from US-based IP addresses and report speaking American English as their native language. These listeners will be blind to speaker identity and treatment condition, but they will see an orthographic representation of the word being produced in each trial; they will be instructed to rate the accuracy of the /ɹ/ sound in each word in a binary fashion (correct/incorrect) while disregarding other sounds in the word. Per previous research, ratings will be collected until at least 9 unique listeners have rated each token. The proportion of “correct” ratings out of the total number of ratings (p̂-correct), which correlates strongly with both expert listeners’ ratings and acoustic measures [<xref ref-type="bibr" rid="CR42">42</xref>, <xref ref-type="bibr" rid="CR43">43</xref>], will serve as our primary perceptual measure of /ɹ/ production accuracy.</p>
                <p id="Par62">Finally, a survey assessing the social, emotional, and academic impact of RSE will be collected from participants’ caregivers immediately before and after treatment [<xref ref-type="bibr" rid="CR1">1</xref>]. The survey consists of 11 items (e.g., “My child’s speech sounds different from the speech of other children his or her age”; “My child’s speech has an impact on his or her academic performance”) that are presented in connection with a 5-point Likert scale, where a score of 5 represents strong agreement and a score of 1 represents strong disagreement. Responses will be combined using a Generalized Partial Credit Model [<xref ref-type="bibr" rid="CR44">44</xref>], in which individual item scores are summed into an overall impact score, with weighting to reflect a stronger or weaker association between each item and the total score.</p>
              </sec>
              <sec id="Sec17">
                <title>Data analysis plan</title>
                <p id="Par63">All analyses will be conducted following the intent-to-treat principle, comparing participants based on the treatment assigned regardless of subsequent exposure. To maintain the full sample, missing data will be multiply imputed for participants who drop out of the study using the <italic>mice</italic> package [<xref ref-type="bibr" rid="CR45">45</xref>] in the R statistical software language [<xref ref-type="bibr" rid="CR46">46</xref>]. This approach will capitalize on information collected from these participants prior to their loss to follow-up.</p>
                <p id="Par64">To assess impact on our primary outcome, the acoustic accuracy measure of F3-F2 distance, we will fit a multilevel model using Phase 1 (Acquisition) data. The primary effect of interest is a comparison between MBT and biofeedback treatments, with visual-acoustic and ultrasound biofeedback types pooled for this analysis. This initial model will also adjust for site, performance response category (high responder versus low responder based on Dynamic Assessment), Acquisition day (1, 2, or 3), and pre-treatment accuracy (mean F3-F2 distance across /ɹ/ sounds in the word probe elicited in the first pre-treatment evaluation). If a within-session time trend is supported by the data, we will extend the model to account for time trends across trials within each day of treatment. Random intercepts will be included to reflect the fact that observations are nested within speakers and words, and random slopes that improve overall model fit (for speaker, acquisition day; for word, performance response category and pre-treatment accuracy) will be retained following model comparison using likelihood ratio tests.</p>
                <p id="Par65">Likelihood ratio tests will also be used to test the hypothesis of an advantage for biofeedback over MBT. Tests will be performed in two different model specifications. In one, the overall effect across days will be tested by focusing on the coefficient on the treatment variable. The other specification will interact the treatment variable with time in order to test whether the effect varies across days, since the hypothesized advantage for biofeedback over MBT could emerge as early as the first day or as late as the third day of Phase 1 (Acquisition). Although a positive effect of biofeedback is hypothesized, two-sided hypothesis tests will be used to be conservative.</p>
                <p id="Par66">Our second analysis will fit a linear mixed-effects model using word probe data collected in one immediate pre-treatment and one immediate post-treatment session. The outcome variable will reflect the proportion of naïve listeners who rated the /ɹ/ sound in a given production as correct. The model will include an indicator for time point (pre- versus post-treatment) and for treatment group (biofeedback versus MBT), as well as the interaction between them. As was the case for our primary measure, the model will adjust for site, performance response category based on Dynamic Assessment, and pre-treatment accuracy. Random intercepts for speaker, item (the 50 target words making up the probe measure), and listener will be included. Random slopes of response category and pre-treatment accuracy on word will be examined and included only as warranted by model comparison. As above, likelihood ratio tests will be used to assess the significance of the overall treatment effect. Again, while a positive effect of biofeedback is hypothesized, we will use a two-sided hypothesis test to be conservative.</p>
                <p id="Par67">For this second model, we will additionally test for the presence of a significant time-by-condition interaction in which the magnitude of improvement associated with biofeedback significantly is allowed to vary with time. If we observe a difference in our primary measure (Acquisition) but not our secondary measure (Generalization), this will be interpreted as evidence that biofeedback is more efficient, but not more effective, than MBT.</p>
                <p id="Par68">The final analysis will evaluate changes in impact scores from the 11-item survey assessing the social, emotional, and academic consequences of RSE from pre- to post-treatment time points. A mixed model similar to that used in the previous analyses will be fit including time (pre- versus post-treatment), treatment group (biofeedback versus MBT), and the time-treatment interaction as the primary predictors. The model will also adjust for site, performance response category based on Dynamic Assessment, and pre-treatment accuracy. We will again estimate the interaction between time and treatment group to evaluate whether the functional impact of treatment differs across biofeedback versus MBT conditions.</p>
              </sec>
            </sec>
            <sec id="Sec18">
              <title>Discussion</title>
              <sec id="Sec19">
                <title>Potential significance</title>
                <p id="Par69">When the proposed data collection is complete, we will have measured both short-term acquisition and longer-term generalization changes in /ɹ/ production accuracy in two groups of children randomly assigned to receive MBT or biofeedback treatment. Based on our previous findings, we hypothesize that both groups will improve over the course of treatment, but we also predict significant between-group differences. For our Acquisition measure, we predict that the mixed-effects models will show a significant advantage for biofeedback over MBT as early as the first day of Phase 1, and no later than the final day. For the Generalization measure, we predict a significant time-by-condition interaction in which pre-post gains associated with biofeedback significantly exceed those observed in MBT. To evaluate whether functional impact differs across treatment types, we will look for the same interaction in survey scores. If these hypotheses are supported, it will constitute high-quality evidence that biofeedback outperforms MBT in both efficiency and efficacy. If we observe a difference in Acquisition but not Generalization or functional impact, this will constitute evidence that biofeedback is more efficient, but not more effective, than MBT.</p>
                <p id="Par70">This research has the potential to be clinically significant because it will provide the high-quality data needed to make an evidence-based recommendation regarding the adoption of biofeedback for children with RSE, who occupy disproportionate resources on the SLP caseload. In the United States, 89% of SLPs in the school setting report working with children with speech sound disorder, and the average caseload includes 18 children with this condition [<xref ref-type="bibr" rid="CR47">47</xref>]. By identifying the most efficient treatment approach, this research could in principle help move children with RSE off the caseload faster, thus improving their social and academic outcomes, reducing treatment costs, and freeing up resources for other children in need of SLP services [<xref ref-type="bibr" rid="CR8">8</xref>]. If our hypothesis of an advantage for biofeedback is supported, we will follow up with larger studies evaluating the effectiveness of biofeedback treatment in real-world conditions, such as a school setting. Even if we find no significant advantage for biofeedback over MBT, this research still has the potential to have significant clinical impact. Because the proposed trial is well-powered, a null finding would support the clinically important conclusion that the additional expense associated with adopting biofeedback may not be justified. In addition, it is important to keep in mind that most participants with RSE have a history of months or years of non-response to traditional treatment methods [<xref ref-type="bibr" rid="CR13">13</xref>, <xref ref-type="bibr" rid="CR14">14</xref>, <xref ref-type="bibr" rid="CR16">16</xref>]. If traditional treatment as operationalized in our study (MBT) is shown to yield the moderate to large effect size we typically see for biofeedback, a publication describing the method would in itself be a meaningful addition to the literature.</p>
              </sec>
              <sec id="Sec20">
                <title>Potential limitations</title>
                <p id="Par71">The present study has a number of limitations. First, for our primary analysis, we plan to pool together ultrasound and visual-acoustic types of biofeedback, which have shown similar evidence of efficacy in previous literature. However, very few studies have directly compared the two types of biofeedback against each other, and it is possible that one type is simply more effective than the other. To address this possibility, we will evaluate the relative effect of visual-acoustic versus ultrasound biofeedback treatment in an exploratory analysis that could inform future studies comparing the two.</p>
                <p id="Par72">Second, we have made every effort to maintain equipoise in our operationalization and execution of treatment, but it is impossible to completely eliminate the possibility that either clinicians or participants/parents could carry their own preference for one treatment over another. The novelty of the technology used in biofeedback may be a source of motivation for participants. This could result in greater gains in the biofeedback condition that are not directly attributable to the visualization component of the treatment, but rather to the participant’s increased engagement in the therapy process. We do not have a definite solution for this issue. However, one approach we are taking is to examine not only whether biofeedback works, but why it works—specifically, by testing hypotheses about the interaction of participants’ profiles of sensory acuity with their response to different types of biofeedback treatment. This is outside of the scope of the present paper but could help address this limitation in our broader program of research.</p>
                <p id="Par73">Third, our primary outcome measure assesses learning on a very short time frame, namely by examining changes within the treatment session during the first phase of intervention. Even our longer-term generalization measure examines performance immediately after the conclusion of treatment; no longer-term follow-up measures are planned. In addition, our analyses focus on /ɹ/ production accuracy at the word rather than the sentence or conversational level. We are collecting sentence-level data and plan to examine these in a supplementary analysis. In general, though, the addition of longer-term follow-up measures and broader assessment of generalization represents an important goal for future research.</p>
                <p id="Par74">Finally, even if we find that our hypothesis of an advantage for biofeedback is supported, we still face a challenge in translating this finding into clinical practice due to barriers to adoption of biofeedback—notably the high cost of the equipment and the need for specialized training to use it correctly. Costs of equipment are declining as technology advances, but it remains a significant obstacle for the average clinician. One avenue we are pursuing to address this limitation is a separate program of research developing a low-cost app to deliver visual-acoustic biofeedback treatment [<xref ref-type="bibr" rid="CR48">48</xref>]. In addition, throughout this and other studies, we are working to develop training materials that can be shared with clinicians interested in adopting biofeedback, which could make access to training less of an obstacle.</p>
              </sec>
            </sec>
          </body>
          <back>
            <glossary>
              <title>Abbreviations</title>
              <def-list>
                <def-item>
                  <term>CAS</term>
                  <def>
                    <p id="Par5">Childhood Apraxia of Speech</p>
                  </def>
                </def-item>
                <def-item>
                  <term>CELF-5</term>
                  <def>
                    <p id="Par6">Clinical Evaluation of Language Fundamentals - Fifth Edition</p>
                  </def>
                </def-item>
                <def-item>
                  <term>CPP</term>
                  <def>
                    <p id="Par7">Challenge Point Program (software)</p>
                  </def>
                </def-item>
                <def-item>
                  <term>CSL</term>
                  <def>
                    <p id="Par8">Computerized Speech Lab (hardware/software)</p>
                  </def>
                </def-item>
                <def-item>
                  <term>GFTA-3</term>
                  <def>
                    <p id="Par9">Goldman Fristoe Test of Articulation - Third Edition</p>
                  </def>
                </def-item>
                <def-item>
                  <term>KP</term>
                  <def>
                    <p id="Par10">Knowledge of Performance (feedback)</p>
                  </def>
                </def-item>
                <def-item>
                  <term>KR</term>
                  <def>
                    <p id="Par11">Knowledge of Results (feedback)</p>
                  </def>
                </def-item>
                <def-item>
                  <term>LPC</term>
                  <def>
                    <p id="Par12">Linear Predictive Coding (spectrum)</p>
                  </def>
                </def-item>
                <def-item>
                  <term>MBT</term>
                  <def>
                    <p id="Par13">Motor-Based Treatment</p>
                  </def>
                </def-item>
                <def-item>
                  <term>MR</term>
                  <def>
                    <p id="Par14">Magnetic Resonance (imaging)</p>
                  </def>
                </def-item>
                <def-item>
                  <term>RSE</term>
                  <def>
                    <p id="Par15">Residual Speech Errors</p>
                  </def>
                </def-item>
                <def-item>
                  <term>SLP</term>
                  <def>
                    <p id="Par16">Speech-Language Pathologist</p>
                  </def>
                </def-item>
                <def-item>
                  <term>WASI-2</term>
                  <def>
                    <p id="Par17">Wechsler Abbreviated Scales of Intelligence – Second Edition</p>
                  </def>
                </def-item>
              </def-list>
            </glossary>
            <fn-group>
              <fn id="Fn1">
                <label>1</label>
                <p id="Par39">Across all treatment conditions, the first treatment session in Phase 1 (Acquisition) will feature roughly 25 min of additional condition-specific training. For the MBT condition, this will consist of revisiting the content introduced during Dynamic Assessment (tongue shapes for /ɹ/, etc.) in greater detail.</p>
              </fn>
              <fn>
                <p>
                  <bold>Publisher’s Note</bold>
                </p>
                <p>Springer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations.</p>
              </fn>
            </fn-group>
            <ack>
              <title>Acknowledgements</title>
              <p>The authors gratefully acknowledge the contributions of the following individuals involved in data collection and analysis: Megan Leece, Michelle Turner Swartz, Twylah Campbell, Laura Ochs, Heather Kabakoff, Nina Benway, Robbie Lazarus, Lynne Harris, Kelly Garcia, and Olesia Gritsyk. We also appreciate the contributions of study consultants Frank Guenther, Doug Shiller, and Jose Ortiz.</p>
            </ack>
            <notes notes-type="author-contribution">
              <title>Authors’ contributions</title>
              <p>This manuscript was drafted by TM with JLP and ERH contributing substantially to revisions. JH contributed revisions to the description of the statistical analysis. The study design was developed by all four authors. Operationalized protocols for evaluation and intervention, as well as clinician training materials, were developed by TM, JLP, and ERH. Clinical data collection will be overseen by JLP and ERH. Data analysis will be completed by all four authors. All authors approved of the final version of this manuscript.</p>
            </notes>
            <notes notes-type="funding-information">
              <title>Funding</title>
              <p>This project is funded by the National Institute on Deafness and Other Communication Disorders (NIH R01DC017476). NIH is the primary sponsor. The study was peer-reviewed by the Language and Communication study section at NIH. NIH had no role in the design, collection, management, analysis, or interpretation of data or in writing this manuscript.</p>
            </notes>
            <notes notes-type="data-availability">
              <title>Availability of data and materials</title>
              <p>Materials referenced in this protocol are available in an Open Science Framework repository at <ext-link ext-link-type="uri" xlink:href="https://osf.io/6qs4d/">https://osf.io/6qs4d/</ext-link>, DOI 10.17605/OSF.IO/6QS4D. After collection, de-identified participant-level data will be made available in the same location.</p>
            </notes>
            <notes>
              <title>Ethics approval and consent to participate</title>
              <p id="Par75">The Biomedical Research Association of New York (BRANY) approved this research protocol (protocol #18–10-393 encompasses the protocols for New York University, Montclair State University, and Syracuse University.) Written informed parent consent and participant assent will be obtained for all participating children.</p>
            </notes>
            <notes>
              <title>Consent for publication</title>
              <p id="Par76">Not applicable.</p>
            </notes>
            <notes notes-type="COI-statement">
              <title>Competing interests</title>
              <p id="Par77">The authors declare that they have no competing interests.</p>
            </notes>
            <ref-list id="Bib1">
              <title>References</title>
              <ref id="CR1">
                <label>1.</label>
                <element-citation publication-type="journal">
                  <person-group person-group-type="author">
                    <name>
                      <surname>Hitchcock</surname>
                      <given-names>E</given-names>
                    </name>
                    <name>
                      <surname>Harel</surname>
                      <given-names>D</given-names>
                    </name>
                    <name>
                      <surname>McAllister</surname>
                      <given-names>BT</given-names>
                    </name>
                  </person-group>
                  <article-title>Social, emotional, and academic impact of residual speech errors in school-aged children: a survey study</article-title>
                  <source>Semin Speech Lang</source>
                  <year>2015</year>
                  <volume>36</volume>
                  <issue>4</issue>
                  <fpage>283</fpage>
                  <lpage>293</lpage>
                  <pub-id pub-id-type="doi">10.1055/s-0035-1562911</pub-id>
                  <?supplied-pmid 26458203?>
                  <pub-id pub-id-type="pmid">26458203</pub-id>
                </element-citation>
              </ref>
              <ref id="CR2">
                <label>2.</label>
                <element-citation publication-type="journal">
                  <person-group person-group-type="author">
                    <name>
                      <surname>Crowe Hall</surname>
                      <given-names>BJ</given-names>
                    </name>
                  </person-group>
                  <article-title>Attitudes of fourth and sixth graders towards peers with mild articulation disorders</article-title>
                  <source>Lang Speech Hear Serv Sch</source>
                  <year>1991</year>
                  <volume>22</volume>
                  <issue>1</issue>
                  <fpage>334</fpage>
                  <lpage>340</lpage>
                  <pub-id pub-id-type="doi">10.1044/0161-1461.2201.334</pub-id>
                </element-citation>
              </ref>
              <ref id="CR3">
                <label>3.</label>
                <element-citation publication-type="journal">
                  <person-group person-group-type="author">
                    <name>
                      <surname>McCormack</surname>
                      <given-names>J</given-names>
                    </name>
                    <name>
                      <surname>McLeod</surname>
                      <given-names>S</given-names>
                    </name>
                    <name>
                      <surname>McAllister</surname>
                      <given-names>L</given-names>
                    </name>
                    <name>
                      <surname>Harrison</surname>
                      <given-names>LJ</given-names>
                    </name>
                  </person-group>
                  <article-title>A systematic review of the association between childhood speech impairment and participation across the lifespan</article-title>
                  <source>Int J Speech Lang Pathol.</source>
                  <year>2009</year>
                  <volume>11</volume>
                  <issue>2</issue>
                  <fpage>155</fpage>
                  <lpage>170</lpage>
                  <pub-id pub-id-type="doi">10.1080/17549500802676859</pub-id>
                </element-citation>
              </ref>
              <ref id="CR4">
                <label>4.</label>
                <element-citation publication-type="journal">
                  <person-group person-group-type="author">
                    <name>
                      <surname>Felsenfeld</surname>
                      <given-names>S</given-names>
                    </name>
                    <name>
                      <surname>Broen</surname>
                      <given-names>PA</given-names>
                    </name>
                    <name>
                      <surname>McGue</surname>
                      <given-names>M</given-names>
                    </name>
                  </person-group>
                  <article-title>A 28-year follow-up of adults with a history of moderate phonological disorder: linguistic and personality results</article-title>
                  <source>J Speech Lang Hear Res</source>
                  <year>1992</year>
                  <volume>35</volume>
                  <issue>5</issue>
                  <fpage>1114</fpage>
                  <lpage>1125</lpage>
                  <pub-id pub-id-type="doi">10.1044/jshr.3505.1114</pub-id>
                </element-citation>
              </ref>
              <ref id="CR5">
                <label>5.</label>
                <element-citation publication-type="book">
                  <person-group person-group-type="author">
                    <name>
                      <surname>Shriberg</surname>
                      <given-names>LD</given-names>
                    </name>
                  </person-group>
                  <person-group person-group-type="editor">
                    <name>
                      <surname>Paul</surname>
                      <given-names>R</given-names>
                    </name>
                    <name>
                      <surname>Flipsen</surname>
                      <given-names>P</given-names>
                    </name>
                  </person-group>
                  <article-title>Childhood speech sound disorders: from postbehaviorism to the postgenomic era</article-title>
                  <source>Speech sound disorders in children</source>
                  <year>2009</year>
                  <publisher-loc>San Diego</publisher-loc>
                  <publisher-name>Plural Publishing</publisher-name>
                </element-citation>
              </ref>
              <ref id="CR6">
                <label>6.</label>
                <element-citation publication-type="journal">
                  <person-group person-group-type="author">
                    <name>
                      <surname>Culton</surname>
                      <given-names>GL</given-names>
                    </name>
                  </person-group>
                  <article-title>Speech disorders among college freshmen: a 13-year survey</article-title>
                  <source>J Speech Hear Disord</source>
                  <year>1986</year>
                  <volume>51</volume>
                  <issue>1</issue>
                  <fpage>3</fpage>
                  <lpage>7</lpage>
                  <pub-id pub-id-type="doi">10.1044/jshd.5101.03</pub-id>
                  <?supplied-pmid 3945057?>
                  <pub-id pub-id-type="pmid">3945057</pub-id>
                </element-citation>
              </ref>
              <ref id="CR7">
                <label>7.</label>
                <element-citation publication-type="journal">
                  <person-group person-group-type="author">
                    <name>
                      <surname>Flipsen</surname>
                      <given-names>P</given-names>
                    </name>
                  </person-group>
                  <article-title>Emergence and prevalence of persistent and residual speech errors</article-title>
                  <source>Semin Speech Lang</source>
                  <year>2015</year>
                  <volume>36</volume>
                  <issue>4</issue>
                  <fpage>217</fpage>
                  <lpage>223</lpage>
                  <pub-id pub-id-type="doi">10.1055/s-0035-1562905</pub-id>
                  <?supplied-pmid 26458197?>
                  <pub-id pub-id-type="pmid">26458197</pub-id>
                </element-citation>
              </ref>
              <ref id="CR8">
                <label>8.</label>
                <element-citation publication-type="journal">
                  <person-group person-group-type="author">
                    <name>
                      <surname>Ruscello</surname>
                      <given-names>DM</given-names>
                    </name>
                  </person-group>
                  <article-title>Visual feedback in treatment of residual phonological disorders</article-title>
                  <source>J Commun Disord</source>
                  <year>1995</year>
                  <volume>28</volume>
                  <issue>4</issue>
                  <fpage>279</fpage>
                  <lpage>302</lpage>
                  <pub-id pub-id-type="doi">10.1016/0021-9924(95)00058-X</pub-id>
                  <?supplied-pmid 8576411?>
                  <pub-id pub-id-type="pmid">8576411</pub-id>
                </element-citation>
              </ref>
              <ref id="CR9">
                <label>9.</label>
                <element-citation publication-type="journal">
                  <person-group person-group-type="author">
                    <name>
                      <surname>Volin</surname>
                      <given-names>RA</given-names>
                    </name>
                  </person-group>
                  <article-title>A relationship between stimulability and the efficacy of visual biofeedback in the training of a respiratory control task</article-title>
                  <source>Am J Speech Lang Pathol.</source>
                  <year>1998</year>
                  <volume>7</volume>
                  <issue>1</issue>
                  <fpage>81</fpage>
                  <lpage>90</lpage>
                  <pub-id pub-id-type="doi">10.1044/1058-0360.0701.81</pub-id>
                </element-citation>
              </ref>
              <ref id="CR10">
                <label>10.</label>
                <element-citation publication-type="journal">
                  <person-group person-group-type="author">
                    <name>
                      <surname>McAllister Byun</surname>
                      <given-names>T</given-names>
                    </name>
                    <name>
                      <surname>Hitchcock</surname>
                      <given-names>ER</given-names>
                    </name>
                  </person-group>
                  <article-title>Investigating the use of traditional and spectral biofeedback approaches to intervention for /r/ misarticulation</article-title>
                  <source>Am J Speech Lang Pathol.</source>
                  <year>2012</year>
                  <volume>21</volume>
                  <issue>3</issue>
                  <fpage>207</fpage>
                  <lpage>221</lpage>
                  <pub-id pub-id-type="doi">10.1044/1058-0360(2012/11-0083)</pub-id>
                  <pub-id pub-id-type="pmid">22442281</pub-id>
                </element-citation>
              </ref>
              <ref id="CR11">
                <label>11.</label>
                <element-citation publication-type="journal">
                  <person-group person-group-type="author">
                    <name>
                      <surname>McAllister Byun</surname>
                      <given-names>T</given-names>
                    </name>
                    <name>
                      <surname>Campbell</surname>
                      <given-names>H</given-names>
                    </name>
                  </person-group>
                  <article-title>Differential effects of visual-acoustic biofeedback intervention for residual speech errors</article-title>
                  <source>Front Hum Neurosci</source>
                  <year>2016</year>
                  <volume>10</volume>
                  <issue>567</issue>
                  <fpage>1</fpage>
                  <lpage>17</lpage>
                  <pub-id pub-id-type="pmid">26858619</pub-id>
                </element-citation>
              </ref>
              <ref id="CR12">
                <label>12.</label>
                <element-citation publication-type="journal">
                  <person-group person-group-type="author">
                    <name>
                      <surname>McAllister</surname>
                      <given-names>BT</given-names>
                    </name>
                  </person-group>
                  <article-title>Efficacy of visual-acoustic biofeedback intervention for residual rhotic errors: a single-subject randomization study</article-title>
                  <source>J Speech Lang Hear Res</source>
                  <year>2017</year>
                  <volume>60</volume>
                  <issue>5</issue>
                  <fpage>1175</fpage>
                  <lpage>1193</lpage>
                  <pub-id pub-id-type="doi">10.1044/2016_JSLHR-S-16-0038</pub-id>
                  <pub-id pub-id-type="pmid">28389677</pub-id>
                </element-citation>
              </ref>
              <ref id="CR13">
                <label>13.</label>
                <element-citation publication-type="journal">
                  <person-group person-group-type="author">
                    <name>
                      <surname>Preston</surname>
                      <given-names>JL</given-names>
                    </name>
                    <name>
                      <surname>McCabe</surname>
                      <given-names>P</given-names>
                    </name>
                    <name>
                      <surname>Rivera-Campos</surname>
                      <given-names>A</given-names>
                    </name>
                    <name>
                      <surname>Whittle</surname>
                      <given-names>JL</given-names>
                    </name>
                    <name>
                      <surname>Landry</surname>
                      <given-names>E</given-names>
                    </name>
                    <name>
                      <surname>Maas</surname>
                      <given-names>E</given-names>
                    </name>
                  </person-group>
                  <article-title>Ultrasound visual feedback treatment and practice variability for residual speech sound errors</article-title>
                  <source>J Speech Lang Hear Res</source>
                  <year>2014</year>
                  <volume>57</volume>
                  <issue>6</issue>
                  <fpage>2102</fpage>
                  <lpage>2115</lpage>
                  <pub-id pub-id-type="doi">10.1044/2014_JSLHR-S-14-0031</pub-id>
                  <?supplied-pmid 25087938?>
                  <pub-id pub-id-type="pmid">25087938</pub-id>
                </element-citation>
              </ref>
              <ref id="CR14">
                <label>14.</label>
                <element-citation publication-type="journal">
                  <person-group person-group-type="author">
                    <name>
                      <surname>Preston</surname>
                      <given-names>JL</given-names>
                    </name>
                    <name>
                      <surname>Leece</surname>
                      <given-names>MC</given-names>
                    </name>
                    <name>
                      <surname>Maas</surname>
                      <given-names>E</given-names>
                    </name>
                  </person-group>
                  <article-title>Motor-based treatment with and without ultrasound feedback for residual speech sound errors</article-title>
                  <source>Int J Lang Commun Disord</source>
                  <year>2017</year>
                  <volume>52</volume>
                  <issue>1</issue>
                  <fpage>80</fpage>
                  <lpage>94</lpage>
                  <pub-id pub-id-type="doi">10.1111/1460-6984.12259</pub-id>
                  <?supplied-pmid 27296780?>
                  <pub-id pub-id-type="pmid">27296780</pub-id>
                </element-citation>
              </ref>
              <ref id="CR15">
                <label>15.</label>
                <element-citation publication-type="journal">
                  <person-group person-group-type="author">
                    <name>
                      <surname>Preston</surname>
                      <given-names>JL</given-names>
                    </name>
                    <name>
                      <surname>McAllister</surname>
                      <given-names>T</given-names>
                    </name>
                    <name>
                      <surname>Phillips</surname>
                      <given-names>E</given-names>
                    </name>
                    <name>
                      <surname>Boyce</surname>
                      <given-names>S</given-names>
                    </name>
                    <name>
                      <surname>Tiede</surname>
                      <given-names>M</given-names>
                    </name>
                    <name>
                      <surname>Kim</surname>
                      <given-names>JS</given-names>
                    </name>
                    <etal/>
                  </person-group>
                  <article-title>Remediating residual rhotic errors with traditional and ultrasound-enhanced treatment: a single-case experimental study</article-title>
                  <source>Am J Speech Lang Pathol</source>
                  <year>2019</year>
                  <volume>28</volume>
                  <issue>3</issue>
                  <fpage>1167</fpage>
                  <lpage>1183</lpage>
                  <pub-id pub-id-type="doi">10.1044/2019_AJSLP-18-0261</pub-id>
                  <?supplied-pmid 31170355?>
                  <pub-id pub-id-type="pmid">31170355</pub-id>
                </element-citation>
              </ref>
              <ref id="CR16">
                <label>16.</label>
                <element-citation publication-type="journal">
                  <person-group person-group-type="author">
                    <name>
                      <surname>McAllister Byun</surname>
                      <given-names>T</given-names>
                    </name>
                    <name>
                      <surname>Hitchcock</surname>
                      <given-names>ER</given-names>
                    </name>
                    <name>
                      <surname>Swartz</surname>
                      <given-names>MT</given-names>
                    </name>
                  </person-group>
                  <article-title>Retroflex versus bunched in treatment for rhotic misarticulation: evidence from ultrasound biofeedback intervention</article-title>
                  <source>J Speech Lang Hear Res</source>
                  <year>2014</year>
                  <volume>57</volume>
                  <issue>6</issue>
                  <fpage>2116</fpage>
                  <lpage>2130</lpage>
                  <pub-id pub-id-type="doi">10.1044/2014_JSLHR-S-14-0034</pub-id>
                  <pub-id pub-id-type="pmid">25088034</pub-id>
                </element-citation>
              </ref>
              <ref id="CR17">
                <label>17.</label>
                <element-citation publication-type="journal">
                  <person-group person-group-type="author">
                    <name>
                      <surname>Sugden</surname>
                      <given-names>E</given-names>
                    </name>
                    <name>
                      <surname>Lloyd</surname>
                      <given-names>S</given-names>
                    </name>
                    <name>
                      <surname>Lam</surname>
                      <given-names>J</given-names>
                    </name>
                    <name>
                      <surname>Cleland</surname>
                      <given-names>J</given-names>
                    </name>
                  </person-group>
                  <article-title>Systematic review of ultrasound visual biofeedback in intervention for speech sound disorders</article-title>
                  <source>Int J Lang Commun Disord</source>
                  <year>2019</year>
                  <volume>54</volume>
                  <issue>5</issue>
                  <fpage>705</fpage>
                  <lpage>728</lpage>
                  <pub-id pub-id-type="doi">10.1111/1460-6984.12478</pub-id>
                  <?supplied-pmid 31179581?>
                  <pub-id pub-id-type="pmid">31179581</pub-id>
                </element-citation>
              </ref>
              <ref id="CR18">
                <label>18.</label>
                <element-citation publication-type="journal">
                  <person-group person-group-type="author">
                    <name>
                      <surname>Hodges</surname>
                      <given-names>NJ</given-names>
                    </name>
                    <name>
                      <surname>Franks</surname>
                      <given-names>IM</given-names>
                    </name>
                  </person-group>
                  <article-title>Learning a coordination skill: interactive effects of instruction and feedback</article-title>
                  <source>Res Q Exerc Sport</source>
                  <year>2001</year>
                  <volume>72</volume>
                  <issue>2</issue>
                  <fpage>132</fpage>
                  <lpage>142</lpage>
                  <pub-id pub-id-type="doi">10.1080/02701367.2001.10608943</pub-id>
                  <?supplied-pmid 11393876?>
                  <pub-id pub-id-type="pmid">11393876</pub-id>
                </element-citation>
              </ref>
              <ref id="CR19">
                <label>19.</label>
                <element-citation publication-type="journal">
                  <person-group person-group-type="author">
                    <name>
                      <surname>Maas</surname>
                      <given-names>E</given-names>
                    </name>
                    <name>
                      <surname>Robin</surname>
                      <given-names>DA</given-names>
                    </name>
                    <name>
                      <surname>Austermann Hula</surname>
                      <given-names>SN</given-names>
                    </name>
                    <name>
                      <surname>Freedman</surname>
                      <given-names>SE</given-names>
                    </name>
                    <name>
                      <surname>Wulf</surname>
                      <given-names>G</given-names>
                    </name>
                    <name>
                      <surname>Ballard</surname>
                      <given-names>KJ</given-names>
                    </name>
                    <etal/>
                  </person-group>
                  <article-title>Principles of motor learning in treatment of motor speech disorders</article-title>
                  <source>Am J Speech Lang Pathol.</source>
                  <year>2008</year>
                  <volume>17</volume>
                  <issue>3</issue>
                  <fpage>277</fpage>
                  <lpage>298</lpage>
                  <pub-id pub-id-type="doi">10.1044/1058-0360(2008/025)</pub-id>
                  <?supplied-pmid 18663111?>
                  <pub-id pub-id-type="pmid">18663111</pub-id>
                </element-citation>
              </ref>
              <ref id="CR20">
                <label>20.</label>
                <mixed-citation publication-type="other">Van Riper C. Speech Correction: Principles and Methods, vol. 1. Englewood Cliffs: Prentice-Hall; 1939.</mixed-citation>
              </ref>
              <ref id="CR21">
                <label>21.</label>
                <mixed-citation publication-type="other">Van Riper C. Speech Correction: Principles and Methods, vol. 9. Englewood Cliffs: Prentice-Hall; 1996.</mixed-citation>
              </ref>
              <ref id="CR22">
                <label>22.</label>
                <element-citation publication-type="journal">
                  <person-group person-group-type="author">
                    <name>
                      <surname>Faul</surname>
                      <given-names>F</given-names>
                    </name>
                    <name>
                      <surname>Erdfelder</surname>
                      <given-names>E</given-names>
                    </name>
                    <name>
                      <surname>Lang</surname>
                      <given-names>A-G</given-names>
                    </name>
                    <name>
                      <surname>Buchner</surname>
                      <given-names>A</given-names>
                    </name>
                  </person-group>
                  <article-title>G*power 3: a flexible statistical power analysis program for the social, behavioral, and biomedical sciences</article-title>
                  <source>Behav Res Methods</source>
                  <year>2007</year>
                  <volume>39</volume>
                  <issue>2</issue>
                  <fpage>175</fpage>
                  <lpage>191</lpage>
                  <pub-id pub-id-type="doi">10.3758/BF03193146</pub-id>
                  <pub-id pub-id-type="pmid">17695343</pub-id>
                </element-citation>
              </ref>
              <ref id="CR23">
                <label>23.</label>
                <element-citation publication-type="journal">
                  <person-group person-group-type="author">
                    <name>
                      <surname>Harris</surname>
                      <given-names>PA</given-names>
                    </name>
                    <name>
                      <surname>Taylor</surname>
                      <given-names>R</given-names>
                    </name>
                    <name>
                      <surname>Thielke</surname>
                      <given-names>R</given-names>
                    </name>
                    <name>
                      <surname>Payne</surname>
                      <given-names>J</given-names>
                    </name>
                    <name>
                      <surname>Gonzalez</surname>
                      <given-names>N</given-names>
                    </name>
                    <name>
                      <surname>Conde</surname>
                      <given-names>JG</given-names>
                    </name>
                  </person-group>
                  <article-title>Research electronic data capture (REDCap)--a metadata-driven methodology and workflow process for providing translational research informatics support</article-title>
                  <source>J Biomed Inform</source>
                  <year>2009</year>
                  <volume>42</volume>
                  <issue>2</issue>
                  <fpage>377</fpage>
                  <lpage>381</lpage>
                  <pub-id pub-id-type="doi">10.1016/j.jbi.2008.08.010</pub-id>
                  <?supplied-pmid 18929686?>
                  <pub-id pub-id-type="pmid">18929686</pub-id>
                </element-citation>
              </ref>
              <ref id="CR24">
                <label>24.</label>
                <element-citation publication-type="journal">
                  <person-group person-group-type="author">
                    <name>
                      <surname>Harris</surname>
                      <given-names>PA</given-names>
                    </name>
                    <name>
                      <surname>Taylor</surname>
                      <given-names>R</given-names>
                    </name>
                    <name>
                      <surname>Minor</surname>
                      <given-names>BL</given-names>
                    </name>
                    <name>
                      <surname>Elliott</surname>
                      <given-names>V</given-names>
                    </name>
                    <name>
                      <surname>Fernandez</surname>
                      <given-names>M</given-names>
                    </name>
                    <name>
                      <surname>O’Neal</surname>
                      <given-names>L</given-names>
                    </name>
                    <etal/>
                  </person-group>
                  <article-title>The REDCap consortium: Building an international community of software platform partners</article-title>
                  <source>J Biomed Inform</source>
                  <year>2019</year>
                  <volume>95</volume>
                  <fpage>103208</fpage>
                  <pub-id pub-id-type="doi">10.1016/j.jbi.2019.103208</pub-id>
                  <?supplied-pmid 31078660?>
                  <pub-id pub-id-type="pmid">31078660</pub-id>
                </element-citation>
              </ref>
              <ref id="CR25">
                <label>25.</label>
                <element-citation publication-type="book">
                  <person-group person-group-type="author">
                    <name>
                      <surname>Wechsler</surname>
                      <given-names>D</given-names>
                    </name>
                  </person-group>
                  <source>Wechsler abbreviated scales of intelligence</source>
                  <year>2011</year>
                  <edition>2</edition>
                  <publisher-loc>San Antonio</publisher-loc>
                  <publisher-name>Pearson</publisher-name>
                </element-citation>
              </ref>
              <ref id="CR26">
                <label>26.</label>
                <element-citation publication-type="book">
                  <person-group person-group-type="author">
                    <name>
                      <surname>Wiig</surname>
                      <given-names>E</given-names>
                    </name>
                    <name>
                      <surname>Semel</surname>
                      <given-names>E</given-names>
                    </name>
                    <name>
                      <surname>Secord</surname>
                      <given-names>W</given-names>
                    </name>
                  </person-group>
                  <source>Clinical evaluation of language fundamentals—fifth edition (CELF-5)</source>
                  <year>2013</year>
                  <publisher-loc>Bloomington</publisher-loc>
                  <publisher-name>Pearson</publisher-name>
                </element-citation>
              </ref>
              <ref id="CR27">
                <label>27.</label>
                <element-citation publication-type="book">
                  <person-group person-group-type="author">
                    <name>
                      <surname>Goldman</surname>
                      <given-names>R</given-names>
                    </name>
                    <name>
                      <surname>Fristoe</surname>
                      <given-names>M</given-names>
                    </name>
                  </person-group>
                  <source>Goldman-Fristoe test of articulation</source>
                  <year>2015</year>
                  <edition>3</edition>
                  <publisher-loc>Bloomington</publisher-loc>
                  <publisher-name>Pearson</publisher-name>
                </element-citation>
              </ref>
              <ref id="CR28">
                <label>28.</label>
                <element-citation publication-type="journal">
                  <person-group person-group-type="author">
                    <name>
                      <surname>Shriberg</surname>
                      <given-names>LD</given-names>
                    </name>
                    <name>
                      <surname>Lohmeier</surname>
                      <given-names>HL</given-names>
                    </name>
                    <name>
                      <surname>Campbell</surname>
                      <given-names>TF</given-names>
                    </name>
                    <name>
                      <surname>Dollaghan</surname>
                      <given-names>CA</given-names>
                    </name>
                    <name>
                      <surname>Green</surname>
                      <given-names>JR</given-names>
                    </name>
                    <name>
                      <surname>Moore</surname>
                      <given-names>CA</given-names>
                    </name>
                  </person-group>
                  <article-title>A nonword repetition task for speakers with misarticulations: the syllable repetition task (SRT)</article-title>
                  <source>J Speech Lang Hear Res JSLHR</source>
                  <year>2009</year>
                  <volume>52</volume>
                  <issue>5</issue>
                  <fpage>1189</fpage>
                  <lpage>1212</lpage>
                  <pub-id pub-id-type="doi">10.1044/1092-4388(2009/08-0047)</pub-id>
                  <?supplied-pmid 19635944?>
                  <pub-id pub-id-type="pmid">19635944</pub-id>
                </element-citation>
              </ref>
              <ref id="CR29">
                <label>29.</label>
                <element-citation publication-type="book">
                  <person-group person-group-type="author">
                    <name>
                      <surname>Bowers</surname>
                      <given-names>L</given-names>
                    </name>
                    <name>
                      <surname>Huisingh</surname>
                      <given-names>R</given-names>
                    </name>
                  </person-group>
                  <source>Linguisystems articulation test</source>
                  <year>2011</year>
                  <publisher-loc>East Moline</publisher-loc>
                  <publisher-name>Linguisystems, Inc</publisher-name>
                </element-citation>
              </ref>
              <ref id="CR30">
                <label>30.</label>
                <element-citation publication-type="journal">
                  <person-group person-group-type="author">
                    <name>
                      <surname>Thoonen</surname>
                      <given-names>G</given-names>
                    </name>
                    <name>
                      <surname>Maassen</surname>
                      <given-names>B</given-names>
                    </name>
                    <name>
                      <surname>Gabreels</surname>
                      <given-names>F</given-names>
                    </name>
                    <name>
                      <surname>Schreuder</surname>
                      <given-names>R</given-names>
                    </name>
                    <name>
                      <surname>de Swart</surname>
                      <given-names>B</given-names>
                    </name>
                  </person-group>
                  <article-title>Towards a standardised assessment procedure for developmental apraxia of speech</article-title>
                  <source>Eur J Disord Commun</source>
                  <year>1997</year>
                  <volume>32</volume>
                  <issue>1</issue>
                  <fpage>37</fpage>
                  <lpage>60</lpage>
                  <pub-id pub-id-type="doi">10.3109/13682829709021455</pub-id>
                  <?supplied-pmid 9135712?>
                  <pub-id pub-id-type="pmid">9135712</pub-id>
                </element-citation>
              </ref>
              <ref id="CR31">
                <label>31.</label>
                <element-citation publication-type="journal">
                  <person-group person-group-type="author">
                    <name>
                      <surname>Thoonen</surname>
                      <given-names>G</given-names>
                    </name>
                    <name>
                      <surname>Maassen</surname>
                      <given-names>B</given-names>
                    </name>
                    <name>
                      <surname>Gabreels</surname>
                      <given-names>F</given-names>
                    </name>
                    <name>
                      <surname>Schreuder</surname>
                      <given-names>R</given-names>
                    </name>
                  </person-group>
                  <article-title>Validity of maximum performance tasks to diagnose motor speech disorders in children</article-title>
                  <source>Clin Linguist Phon</source>
                  <year>1999</year>
                  <volume>13</volume>
                  <issue>1</issue>
                  <fpage>1</fpage>
                  <lpage>23</lpage>
                  <pub-id pub-id-type="doi">10.1080/026992099299211</pub-id>
                </element-citation>
              </ref>
              <ref id="CR32">
                <label>32.</label>
                <element-citation publication-type="journal">
                  <person-group person-group-type="author">
                    <name>
                      <surname>Miccio</surname>
                      <given-names>AW</given-names>
                    </name>
                  </person-group>
                  <article-title>Clinical problem solving: assessment of phonological disorders</article-title>
                  <source>Am J Speech Lang Pathol</source>
                  <year>2002</year>
                  <volume>11</volume>
                  <issue>3</issue>
                  <fpage>221</fpage>
                  <lpage>229</lpage>
                  <pub-id pub-id-type="doi">10.1044/1058-0360(2002/023)</pub-id>
                </element-citation>
              </ref>
              <ref id="CR33">
                <label>33.</label>
                <element-citation publication-type="journal">
                  <person-group person-group-type="author">
                    <name>
                      <surname>Boyce</surname>
                      <given-names>SE</given-names>
                    </name>
                  </person-group>
                  <article-title>The articulatory phonetics of /r/ for residual speech errors</article-title>
                  <source>Semin Speech Lang</source>
                  <year>2015</year>
                  <volume>36</volume>
                  <issue>4</issue>
                  <fpage>257</fpage>
                  <lpage>270</lpage>
                  <pub-id pub-id-type="doi">10.1055/s-0035-1562909</pub-id>
                  <?supplied-pmid 26458201?>
                  <pub-id pub-id-type="pmid">26458201</pub-id>
                </element-citation>
              </ref>
              <ref id="CR34">
                <label>34.</label>
                <element-citation publication-type="journal">
                  <person-group person-group-type="author">
                    <name>
                      <surname>Klein</surname>
                      <given-names>HB</given-names>
                    </name>
                    <name>
                      <surname>McAllister Byun</surname>
                      <given-names>T</given-names>
                    </name>
                    <name>
                      <surname>Davidson</surname>
                      <given-names>L</given-names>
                    </name>
                    <name>
                      <surname>Grigos</surname>
                      <given-names>MI</given-names>
                    </name>
                  </person-group>
                  <article-title>A multidimensional investigation of children’s /r/ productions: perceptual, ultrasound, and acoustic measures</article-title>
                  <source>Am J Speech Lang Pathol.</source>
                  <year>2013</year>
                  <volume>22</volume>
                  <issue>3</issue>
                  <fpage>540</fpage>
                  <lpage>553</lpage>
                  <pub-id pub-id-type="doi">10.1044/1058-0360(2013/12-0137)</pub-id>
                  <?supplied-pmid 23813195?>
                  <pub-id pub-id-type="pmid">23813195</pub-id>
                </element-citation>
              </ref>
              <ref id="CR35">
                <label>35.</label>
                <element-citation publication-type="book">
                  <person-group person-group-type="author">
                    <name>
                      <surname>McAllister Byun</surname>
                      <given-names>T</given-names>
                    </name>
                    <name>
                      <surname>Hitchcock</surname>
                      <given-names>E</given-names>
                    </name>
                    <name>
                      <surname>Ortiz</surname>
                      <given-names>J</given-names>
                    </name>
                  </person-group>
                  <source>Challenge-R</source>
                  <year>2014</year>
                </element-citation>
              </ref>
              <ref id="CR36">
                <label>36.</label>
                <element-citation publication-type="journal">
                  <person-group person-group-type="author">
                    <name>
                      <surname>Guadagnoli</surname>
                      <given-names>MA</given-names>
                    </name>
                    <name>
                      <surname>Lee</surname>
                      <given-names>TD</given-names>
                    </name>
                  </person-group>
                  <article-title>Challenge point: a framework for conceptualizing the effects of various practice conditions in motor learning</article-title>
                  <source>J Mot Behav</source>
                  <year>2004</year>
                  <volume>36</volume>
                  <issue>2</issue>
                  <fpage>212</fpage>
                  <lpage>224</lpage>
                  <pub-id pub-id-type="doi">10.3200/JMBR.36.2.212-224</pub-id>
                  <?supplied-pmid 15130871?>
                  <pub-id pub-id-type="pmid">15130871</pub-id>
                </element-citation>
              </ref>
              <ref id="CR37">
                <label>37.</label>
                <element-citation publication-type="book">
                  <person-group person-group-type="author">
                    <name>
                      <surname>Rvachew</surname>
                      <given-names>S</given-names>
                    </name>
                    <name>
                      <surname>Brosseau-Lapré</surname>
                      <given-names>F</given-names>
                    </name>
                  </person-group>
                  <source>Developmental phonological disorders: foundations of clinical practice</source>
                  <year>2012</year>
                  <publisher-loc>San Diego</publisher-loc>
                  <publisher-name>Plural Pub</publisher-name>
                </element-citation>
              </ref>
              <ref id="CR38">
                <label>38.</label>
                <element-citation publication-type="book">
                  <person-group person-group-type="author">
                    <name>
                      <surname>Boersma</surname>
                      <given-names>P</given-names>
                    </name>
                    <name>
                      <surname>Weenink</surname>
                      <given-names>D</given-names>
                    </name>
                  </person-group>
                  <source>Praat: doing phonetics by computer. 6.1.06</source>
                  <year>2019</year>
                </element-citation>
              </ref>
              <ref id="CR39">
                <label>39.</label>
                <element-citation publication-type="book">
                  <person-group person-group-type="author">
                    <name>
                      <surname>Lennes</surname>
                      <given-names>M</given-names>
                    </name>
                  </person-group>
                  <source>Collect formant data from files [internet]</source>
                  <year>2003</year>
                </element-citation>
              </ref>
              <ref id="CR40">
                <label>40.</label>
                <element-citation publication-type="journal">
                  <person-group person-group-type="author">
                    <name>
                      <surname>Campbell</surname>
                      <given-names>Heather</given-names>
                    </name>
                    <name>
                      <surname>Harel</surname>
                      <given-names>Daphna</given-names>
                    </name>
                    <name>
                      <surname>Hitchcock</surname>
                      <given-names>Elaine</given-names>
                    </name>
                    <name>
                      <surname>McAllister Byun</surname>
                      <given-names>Tara</given-names>
                    </name>
                  </person-group>
                  <article-title>Selecting an acoustic correlate for automated measurement of American English rhotic production in children</article-title>
                  <source>International Journal of Speech-Language Pathology</source>
                  <year>2017</year>
                  <volume>20</volume>
                  <issue>6</issue>
                  <fpage>635</fpage>
                  <lpage>643</lpage>
                  <pub-id pub-id-type="doi">10.1080/17549507.2017.1359334</pub-id>
                  <?supplied-pmid 28795872?>
                  <pub-id pub-id-type="pmid">28795872</pub-id>
                </element-citation>
              </ref>
              <ref id="CR41">
                <label>41.</label>
                <element-citation publication-type="journal">
                  <person-group person-group-type="author">
                    <name>
                      <surname>Dugan</surname>
                      <given-names>SH</given-names>
                    </name>
                    <name>
                      <surname>Silbert</surname>
                      <given-names>N</given-names>
                    </name>
                    <name>
                      <surname>McAllister</surname>
                      <given-names>T</given-names>
                    </name>
                    <name>
                      <surname>Preston</surname>
                      <given-names>JL</given-names>
                    </name>
                    <name>
                      <surname>Sotto</surname>
                      <given-names>C</given-names>
                    </name>
                    <name>
                      <surname>Boyce</surname>
                      <given-names>SE</given-names>
                    </name>
                  </person-group>
                  <article-title>Modelling category goodness judgments in children with residual sound errors</article-title>
                  <source>Clin Linguist Phon</source>
                  <year>2019</year>
                  <volume>33</volume>
                  <issue>4</issue>
                  <fpage>295</fpage>
                  <lpage>315</lpage>
                  <pub-id pub-id-type="doi">10.1080/02699206.2018.1477834</pub-id>
                  <?supplied-pmid 29792525?>
                  <pub-id pub-id-type="pmid">29792525</pub-id>
                </element-citation>
              </ref>
              <ref id="CR42">
                <label>42.</label>
                <element-citation publication-type="journal">
                  <person-group person-group-type="author">
                    <name>
                      <surname>McAllister Byun</surname>
                      <given-names>T</given-names>
                    </name>
                    <name>
                      <surname>Halpin</surname>
                      <given-names>PF</given-names>
                    </name>
                    <name>
                      <surname>Szeredi</surname>
                      <given-names>D</given-names>
                    </name>
                  </person-group>
                  <article-title>Online crowdsourcing for efficient rating of speech: a validation study</article-title>
                  <source>J Commun Disord</source>
                  <year>2015</year>
                  <volume>53</volume>
                  <fpage>70</fpage>
                  <lpage>83</lpage>
                  <pub-id pub-id-type="doi">10.1016/j.jcomdis.2014.11.003</pub-id>
                  <?supplied-pmid 25578293?>
                  <pub-id pub-id-type="pmid">25578293</pub-id>
                </element-citation>
              </ref>
              <ref id="CR43">
                <label>43.</label>
                <element-citation publication-type="journal">
                  <person-group person-group-type="author">
                    <name>
                      <surname>McAllister Byun</surname>
                      <given-names>T</given-names>
                    </name>
                    <name>
                      <surname>Harel</surname>
                      <given-names>D</given-names>
                    </name>
                    <name>
                      <surname>Halpin</surname>
                      <given-names>PF</given-names>
                    </name>
                    <name>
                      <surname>Szeredi</surname>
                      <given-names>D</given-names>
                    </name>
                  </person-group>
                  <article-title>Deriving gradient measures of child speech from crowdsourced ratings</article-title>
                  <source>J Commun Disord</source>
                  <year>2016</year>
                  <volume>64</volume>
                  <fpage>91</fpage>
                  <lpage>102</lpage>
                  <pub-id pub-id-type="doi">10.1016/j.jcomdis.2016.07.001</pub-id>
                  <?supplied-pmid 27481555?>
                  <pub-id pub-id-type="pmid">27481555</pub-id>
                </element-citation>
              </ref>
              <ref id="CR44">
                <label>44.</label>
                <element-citation publication-type="book">
                  <person-group person-group-type="author">
                    <name>
                      <surname>Muraki</surname>
                      <given-names>Eiji</given-names>
                    </name>
                  </person-group>
                  <article-title>A Generalized Partial Credit Model</article-title>
                  <source>Handbook of Modern Item Response Theory</source>
                  <year>1997</year>
                  <publisher-loc>New York, NY</publisher-loc>
                  <publisher-name>Springer New York</publisher-name>
                  <fpage>153</fpage>
                  <lpage>164</lpage>
                </element-citation>
              </ref>
              <ref id="CR45">
                <label>45.</label>
                <element-citation publication-type="journal">
                  <person-group person-group-type="author">
                    <name>
                      <surname>van Buuren</surname>
                      <given-names>S</given-names>
                    </name>
                    <name>
                      <surname>Groothuis-Oudshoorn</surname>
                      <given-names>K</given-names>
                    </name>
                  </person-group>
                  <article-title>Mice: multivariate imputation by chained equations in R</article-title>
                  <source>J statistical software</source>
                  <year>2011</year>
                  <volume>45</volume>
                  <fpage>1</fpage>
                  <lpage>67</lpage>
                  <pub-id pub-id-type="doi">10.18637/jss.v045.i03</pub-id>
                </element-citation>
              </ref>
              <ref id="CR46">
                <label>46.</label>
                <element-citation publication-type="book">
                  <person-group person-group-type="author">
                    <collab>R Core Team</collab>
                  </person-group>
                  <source>R: a language and environment for statistical computing</source>
                  <year>2017</year>
                  <publisher-loc>Vienna</publisher-loc>
                  <publisher-name>R Foundation for Statistical Computing</publisher-name>
                </element-citation>
              </ref>
              <ref id="CR47">
                <label>47.</label>
                <element-citation publication-type="book">
                  <person-group person-group-type="author">
                    <collab>ASHA</collab>
                  </person-group>
                  <source>ASHA 2016 Schools survey: SLP caseload characteristics report [internet]</source>
                  <year>2016</year>
                </element-citation>
              </ref>
              <ref id="CR48">
                <label>48.</label>
                <element-citation publication-type="journal">
                  <person-group person-group-type="author">
                    <name>
                      <surname>McAllister Byun</surname>
                      <given-names>T</given-names>
                    </name>
                    <name>
                      <surname>Campbell</surname>
                      <given-names>H</given-names>
                    </name>
                    <name>
                      <surname>Carey</surname>
                      <given-names>H</given-names>
                    </name>
                    <name>
                      <surname>Liang</surname>
                      <given-names>W</given-names>
                    </name>
                    <name>
                      <surname>Park</surname>
                      <given-names>TH</given-names>
                    </name>
                    <name>
                      <surname>Svirsky</surname>
                      <given-names>M</given-names>
                    </name>
                  </person-group>
                  <article-title>Enhancing intervention for residual rhotic errors via app-delivered biofeedback: a case study</article-title>
                  <source>J Speech Lang Hear Res</source>
                  <year>2017</year>
                  <volume>60</volume>
                  <issue>6S</issue>
                  <fpage>1810</fpage>
                  <lpage>1817</lpage>
                  <pub-id pub-id-type="doi">10.1044/2017_JSLHR-S-16-0248</pub-id>
                  <pub-id pub-id-type="pmid">28655050</pub-id>
                </element-citation>
              </ref>
            </ref-list>
          </back>
        </article>
      </metadata>
    </record>
  </GetRecord>
</OAI-PMH>
