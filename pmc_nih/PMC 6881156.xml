<?xml version="1.0" encoding="UTF-8"?>
<OAI-PMH xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/ http://www.openarchives.org/OAI/2.0/OAI-PMH.xsd">
  <responseDate>2024-05-03T05:11:25Z</responseDate>
  <request verb="GetRecord" identifier="oai:pubmedcentral.nih.gov:6881156" metadataPrefix="pmc">https:/www.ncbi.nlm.nih.gov/pmc/oai/oai.cgi</request>
  <GetRecord>
    <record>
      <header>
        <identifier>oai:pubmedcentral.nih.gov:6881156</identifier>
        <datestamp>2019-12-05</datestamp>
        <setSpec>sciadv</setSpec>
        <setSpec>pmc-open</setSpec>
      </header>
      <metadata>
        <article xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" xsi:schemaLocation="https://jats.nlm.nih.gov/ns/archiving/1.3/ https://jats.nlm.nih.gov/archiving/1.3/xsd/JATS-archivearticle1-3.xsd" article-type="research-article">
          <front>
            <journal-meta>
              <journal-id journal-id-type="nlm-ta">Sci Adv</journal-id>
              <journal-id journal-id-type="iso-abbrev">Sci Adv</journal-id>
              <journal-id journal-id-type="publisher-id">SciAdv</journal-id>
              <journal-id journal-id-type="hwp">advances</journal-id>
              <journal-title-group>
                <journal-title>Science Advances</journal-title>
              </journal-title-group>
              <issn pub-type="epub">2375-2548</issn>
              <publisher>
                <publisher-name>American Association for the Advancement of Science</publisher-name>
              </publisher>
            </journal-meta>
            <article-meta>
              <article-id pub-id-type="accession">PMC6881156</article-id>
              <article-id pub-id-type="pmcid">PMC6881156</article-id>
              <article-id pub-id-type="pmc-uid">6881156</article-id>
              <article-id pub-id-type="pmid">31807706</article-id>
              <article-id pub-id-type="publisher-id">aax8783</article-id>
              <article-id pub-id-type="doi">10.1126/sciadv.aax8783</article-id>
              <article-categories>
                <subj-group subj-group-type="article-type">
                  <subject>Research Article</subject>
                </subj-group>
                <subj-group subj-group-type="heading">
                  <subject>Research Articles</subject>
                </subj-group>
                <subj-group subj-group-type="legacy-article-type">
                  <subject>SciAdv r-articles</subject>
                </subj-group>
                <subj-group subj-group-type="field">
                  <subject>Cognitive Neuroscience</subject>
                  <subject>Cognitive Neuroscience</subject>
                </subj-group>
                <subj-group subj-group-type="overline">
                  <subject>Cognitive Neuroscience</subject>
                </subj-group>
              </article-categories>
              <title-group>
                <article-title>Modeling other minds: Bayesian inference explains human choices in group decision-making</article-title>
              </title-group>
              <contrib-group>
                <contrib contrib-type="author">
                  <contrib-id contrib-id-type="orcid" authenticated="true">http://orcid.org/0000-0002-5986-0924</contrib-id>
                  <name>
                    <surname>Khalvati</surname>
                    <given-names>Koosha</given-names>
                  </name>
                  <xref ref-type="aff" rid="aff1">
                    <sup>1</sup>
                  </xref>
                </contrib>
                <contrib contrib-type="author">
                  <contrib-id contrib-id-type="orcid" authenticated="true">http://orcid.org/0000-0002-8361-2277</contrib-id>
                  <name>
                    <surname>Park</surname>
                    <given-names>Seongmin A.</given-names>
                  </name>
                  <xref ref-type="aff" rid="aff2">
                    <sup>2</sup>
                  </xref>
                  <xref ref-type="aff" rid="aff3">
                    <sup>3</sup>
                  </xref>
                </contrib>
                <contrib contrib-type="author">
                  <contrib-id contrib-id-type="orcid" authenticated="true">http://orcid.org/0000-0002-0994-3932</contrib-id>
                  <name>
                    <surname>Mirbagheri</surname>
                    <given-names>Saghar</given-names>
                  </name>
                  <xref ref-type="aff" rid="aff4">
                    <sup>4</sup>
                  </xref>
                </contrib>
                <contrib contrib-type="author">
                  <contrib-id contrib-id-type="orcid" authenticated="true">http://orcid.org/0000-0003-2894-7688</contrib-id>
                  <name>
                    <surname>Philippe</surname>
                    <given-names>Remi</given-names>
                  </name>
                  <xref ref-type="aff" rid="aff3">
                    <sup>3</sup>
                  </xref>
                </contrib>
                <contrib contrib-type="author">
                  <contrib-id contrib-id-type="orcid" authenticated="true">http://orcid.org/0000-0002-7045-6850</contrib-id>
                  <name>
                    <surname>Sestito</surname>
                    <given-names>Mariateresa</given-names>
                  </name>
                  <xref ref-type="aff" rid="aff3">
                    <sup>3</sup>
                  </xref>
                </contrib>
                <contrib contrib-type="author">
                  <name>
                    <surname>Dreher</surname>
                    <given-names>Jean-Claude</given-names>
                  </name>
                  <xref ref-type="award" rid="award489764"/>
                  <xref ref-type="aff" rid="aff3">
                    <sup>3</sup>
                  </xref>
                  <xref ref-type="author-notes" rid="afn1">*</xref>
                </contrib>
                <contrib contrib-type="author">
                  <contrib-id contrib-id-type="orcid" authenticated="true">http://orcid.org/0000-0003-0682-8952</contrib-id>
                  <name>
                    <surname>Rao</surname>
                    <given-names>Rajesh P. N.</given-names>
                  </name>
                  <xref ref-type="award" rid="award489766"/>
                  <xref ref-type="award" rid="award489765"/>
                  <xref ref-type="award" rid="award489767"/>
                  <xref ref-type="aff" rid="aff1">
                    <sup>1</sup>
                  </xref>
                  <xref ref-type="aff" rid="aff5">
                    <sup>5</sup>
                  </xref>
                  <xref ref-type="author-notes" rid="afn1">*</xref>
                  <xref rid="cor1" ref-type="corresp">†</xref>
                </contrib>
                <aff id="aff1"><label>1</label>Paul G. Allen School of Computer Science and Engineering, University of Washington, Seattle, WA, USA.</aff>
                <aff id="aff2"><label>2</label>Center for Mind and Brain, University of California, Davis, Davis, CA, USA.</aff>
                <aff id="aff3"><label>3</label>Neuroeconomics Laboratory, Institut des Sciences Cognitives Marc Jeannerod, Lyon, France.</aff>
                <aff id="aff4"><label>4</label>Department of Psychology, New York University, New York, NY, USA.</aff>
                <aff id="aff5"><label>5</label>Center for Neurotechnology, University of Washington, Seattle, WA, USA.</aff>
              </contrib-group>
              <author-notes>
                <fn id="afn1">
                  <label>*</label>
                  <p>Joint senior authors.</p>
                </fn>
                <corresp id="cor1"><label>†</label>Corresponding author. Email: <email xlink:href="rao@cs.washington.edu">rao@cs.washington.edu</email></corresp>
              </author-notes>
              <pub-date pub-type="collection">
                <month>11</month>
                <year>2019</year>
              </pub-date>
              <pub-date pub-type="epub">
                <day>27</day>
                <month>11</month>
                <year>2019</year>
              </pub-date>
              <volume>5</volume>
              <issue>11</issue>
              <elocation-id>eaax8783</elocation-id>
              <history>
                <date date-type="received">
                  <day>03</day>
                  <month>5</month>
                  <year>2019</year>
                </date>
                <date date-type="accepted">
                  <day>19</day>
                  <month>9</month>
                  <year>2019</year>
                </date>
              </history>
              <permissions>
                <copyright-statement>Copyright © 2019 The Authors, some rights reserved; exclusive licensee American Association for the Advancement of Science. No claim to original U.S. Government Works. Distributed under a Creative Commons Attribution NonCommercial License 4.0 (CC BY-NC).</copyright-statement>
                <copyright-year>2019</copyright-year>
                <copyright-holder>The Authors</copyright-holder>
                <license license-type="open-access" xlink:href="http://creativecommons.org/licenses/by-nc/4.0/" id="p-2">
                  <license-p>This is an open-access article distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by-nc/4.0/">Creative Commons Attribution-NonCommercial license</ext-link>, which permits use, distribution, and reproduction in any medium, so long as the resultant use is <bold>not</bold> for commercial advantage and provided the original work is properly cited.</license-p>
                </license>
              </permissions>
              <abstract abstract-type="teaser">
                <p>A Bayesian model suggests that when interacting with a group, humans simulate the “mind of the group” to choose an action.</p>
              </abstract>
              <abstract>
                <p>To make decisions in a social context, humans have to predict the behavior of others, an ability that is thought to rely on having a model of other minds known as “theory of mind.” Such a model becomes especially complex when the number of people one simultaneously interacts with is large and actions are anonymous. Here, we present results from a group decision-making task known as the volunteer’s dilemma and demonstrate that a Bayesian model based on partially observable Markov decision processes outperforms existing models in quantitatively predicting human behavior and outcomes of group interactions. Our results suggest that in decision-making tasks involving large groups with anonymous members, humans use Bayesian inference to model the “mind of the group,” making predictions of others’ decisions while also simulating the effects of their own actions on the group’s dynamics in the future.</p>
              </abstract>
              <funding-group>
                <award-group id="award489766">
                  <funding-source>
                    <institution-wrap>
                      <institution-id institution-id-type="doi">http://dx.doi.org/10.13039/100000001</institution-id>
                      <institution>National Science Foundation</institution>
                    </institution-wrap>
                  </funding-source>
                  <award-id>EEC-102872</award-id>
                </award-group>
                <award-group id="award489765">
                  <funding-source>
                    <institution-wrap>
                      <institution-id institution-id-type="doi">http://dx.doi.org/10.13039/100000025</institution-id>
                      <institution>National Institute of Mental Health</institution>
                    </institution-wrap>
                  </funding-source>
                  <award-id>R01MH112166- 03</award-id>
                </award-group>
                <award-group id="award489767">
                  <funding-source>
                    <institution-wrap>
                      <institution-id institution-id-type="doi">http://dx.doi.org/10.13039/501100011730</institution-id>
                      <institution>Templeton World Charity Foundation</institution>
                    </institution-wrap>
                  </funding-source>
                </award-group>
                <award-group id="award489764">
                  <funding-source>
                    <institution-wrap>
                      <institution>French National Research Agency (ANR)</institution>
                    </institution-wrap>
                  </funding-source>
                  <award-id>n◦16-NEUC</award-id>
                </award-group>
              </funding-group>
              <custom-meta-group>
                <custom-meta>
                  <meta-name>Copyeditor</meta-name>
                  <meta-value>Monica Bilog</meta-value>
                </custom-meta>
              </custom-meta-group>
            </article-meta>
          </front>
          <body>
            <sec sec-type="introduction" disp-level="1">
              <title>INTRODUCTION</title>
              <p>The importance of social decision-making in human behavior has spawned a large body of research in social neuroscience and decision-making (<xref rid="R1" ref-type="bibr"><italic>1</italic></xref>, <xref rid="R2" ref-type="bibr"><italic>2</italic></xref>). Human behavior relies heavily on predicting future states of the environment under uncertainty and choosing appropriate actions to achieve a goal. In a social context, the degree of uncertainty about the possible outcomes increases drastically as the behavior of others is much less predictable than the physics of the environment.</p>
              <p>One approach to handling uncertainty in social settings is to act based on a belief about others. This approach includes inferring the consequences of one’s own behavior under uncertainty as opposed to “belief-free” models (<xref rid="R3" ref-type="bibr"><italic>3</italic></xref>) that simply select the action that has been rewarding in the past, given current observations (<xref rid="R4" ref-type="bibr"><italic>4</italic></xref>, <xref rid="R5" ref-type="bibr"><italic>5</italic></xref>). The difference between “belief-based” and belief-free models in social decision-making is closely related to “model-based” and “model-free” approaches (<xref rid="R6" ref-type="bibr"><italic>6</italic></xref>, <xref rid="R7" ref-type="bibr"><italic>7</italic></xref>) in nonsocial decision-making but with a greater emphasis on uncertainty due to the greater unpredictability of human behavior in social tasks.</p>
              <p>In belief-based decision-making, the subject learns a model of the environment, updates the model based on observations and rewards, and chooses actions based on a probabilistic “belief” about the current state of the world (<xref rid="R5" ref-type="bibr"><italic>5</italic></xref>, <xref rid="R8" ref-type="bibr"><italic>8</italic></xref>, <xref rid="R9" ref-type="bibr"><italic>9</italic></xref>). As a result, the relationship of the current action with rewards received and current observations is indirect. Besides the history of rewards received and the current observation, the learned model can also include other factors such as potential future rewards and more general rules about the environment. Therefore, the belief-based (model-based) approach is more flexible than belief-free (model-free) decision-making (<xref rid="R10" ref-type="bibr"><italic>10</italic></xref>, <xref rid="R11" ref-type="bibr"><italic>11</italic></xref>). However, belief-based decision-making requires more cognitive resources, for example, for simulation of future events. Thus, there is an inherent trade-off between the two types of approaches, and determining which approach humans adopt for different situations is an important open area of research (<xref rid="R12" ref-type="bibr"><italic>12</italic></xref>).</p>
              <p>Several studies have presented evidence in favor of the belief-based approach by quantifying the similarity between probabilistic model-based methods and human behavior when the subject interacts with or reasons about another human (<xref rid="R5" ref-type="bibr"><italic>5</italic></xref>, <xref rid="R13" ref-type="bibr"><italic>13</italic></xref>–<xref rid="R18" ref-type="bibr"><italic>18</italic></xref>). Compared to reasoning about a single person, decision-making in a group with a large number of members can get complicated. On the one hand, having more group members disproportionately increases the cognitive cost of tracking minds compared to the cost of only tracking the reward history of each action given the current observations. On the other hand, consistent with the importance that human society places on group decisions, a belief-based approach might be the optimal strategy.</p>
              <p>How does one extend a belief-based approach for reasoning about a single person to the case of decision-making within a large group? Group decision-making becomes even more challenging when the actions of others in the group are anonymous (e.g., voting as part of a jury) (<xref rid="R19" ref-type="bibr"><italic>19</italic></xref>, <xref rid="R20" ref-type="bibr"><italic>20</italic></xref>). In such situations, reasoning about the state of mind of individual group members is not possible but the dynamics of group decisions do depend on each individual’s actions.</p>
              <p>To investigate these complexities that arise in group decision-making, we focused on the volunteer’s dilemma task, wherein a few individuals endure some costs to benefit the whole group (<xref rid="R21" ref-type="bibr"><italic>21</italic></xref>). Examples of the task include guarding duty, blood donation, and stepping forward to stop an act of violence in a public place (<xref rid="R22" ref-type="bibr"><italic>22</italic></xref>). To mimic the volunteer’s dilemma in a laboratory setting, we used the thresholded binary version of a multiround public goods game (PGG) where the actions of each individual are hidden from others (<xref rid="R21" ref-type="bibr"><italic>21</italic></xref>, <xref rid="R23" ref-type="bibr"><italic>23</italic></xref>).</p>
              <p>Using an optimal Bayesian framework based on partially observable Markov decision processes (POMDPs) (<xref rid="R24" ref-type="bibr"><italic>24</italic></xref>), we propose that in group decision-making, humans simulate the “mind of the group” by modeling an average group member’s mind when making their current choices. Our model incorporates prior knowledge, current observations, and a simulation of the future based on the current actions for modeling human decisions within a group. We compared our model to a model-free reinforcement learning approach based on the reward history of each action as well as a previous descriptive method for fitting human behavior in the PGG. Our model predicts human behavior significantly better than the model-free reinforcement learning and descriptive approaches. Furthermore, by leveraging the interpretable nature of our model, we are able to show a potential underlying computational mechanism for the group decision-making process.</p>
            </sec>
            <sec sec-type="results" disp-level="1">
              <title>RESULTS</title>
              <sec disp-level="2">
                <title>Human behavior in a binary PGG</title>
                <p>The participants were 29 adults (mean age, 22.97 years old ± 0.37; 14 women). We analyzed the behavioral data of 12 PGGs in which participants played 15 rounds of the game within the same group of <italic>N</italic> players (<italic>N</italic> = 5).</p>
                <p>At the beginning of each round, 1 monetary unit (MU) was endowed (E) to each player. In each round, a player could choose between two options: contribute or free-ride. Contribution had a cost of C = 1 MU, implying that the player could choose between keeping their initial endowment or giving it up. In contrast to the classical PGG where the group reward is a linear function of total contributions (<xref rid="R25" ref-type="bibr"><italic>25</italic></xref>), in our PGG, public goods were produced as a group reward (G = 2 MU to each player) if and only if at least <italic>k</italic> players each contributed 1 MU. <italic>k</italic> was set to two or four randomly for each session and conveyed to group members before the start of the session. The resultant amount after one round is therefore E − C + G = 2 MU for the contributor and E + G = 3 MU for the free-rider when public goods were produced (the round was a SUCCESS). On the other hand, the contributor has E − C = 0 MU and the free-rider has E = 1 MU when no public goods were produced (the round was a FAILURE).</p>
                <p><xref rid="F1" ref-type="fig">Figure 1</xref> depicts one round of the PGG task. After the subject acts, the total number of contributions, free-rides, and the overall outcome of the round is revealed (success or failure in securing the 2 MU group reward), but each individual player’s actions remained unknown. In addition, as shown in the figure, the value of <italic>k</italic> for the current session was always presented on the screen to ensure that the subjects had it in mind when making decisions. Although subjects were told that they were playing with other humans, in reality, they were playing with a computer that generated the actions of all the other <italic>N</italic> − 1 = 4 players using an algorithm based on human data (see Methods). In each session, the subject played with a different group of players.</p>
                <fig id="F1" fig-type="figure" orientation="portrait" position="float">
                  <label>Fig. 1</label>
                  <caption>
                    <title>Multiround PGG.</title>
                    <p>The figure depicts the sequence of computer screens a subject sees in one round of the PGG. The subject is assigned four other players as partners, and each round requires the subject to make a decision: Keep 1 MU (i.e., free-ride) or contribute 1 MU. The subject knows whether the threshold to generate public goods (reward of 2 MU for each player) is two or four contributions (from the five players). After the subject acts, the total number of contributions and overall outcome of the round (success or failure) are revealed.</p>
                  </caption>
                  <graphic xlink:href="aax8783-F1"/>
                </fig>
                <p>As shown in <xref rid="F2" ref-type="fig">Fig. 2A</xref>, subjects contributed significantly more when the number of required volunteers was higher with an average contribution rate of 55% (SD = 0.31) for <italic>k</italic> = 4 in comparison to 33% (SD = 0.18) for <italic>k</italic> = 2 {two-tailed paired sample <italic>t</italic> test, <italic>t</italic>(28) = 3.94, <italic>P</italic> = 5.0 × 10<sup>−4</sup>, 95% confidence interval (CI) difference = [0.11,0.33]}. In addition, <xref rid="F2" ref-type="fig">Fig. 2B</xref> shows that the probability of generating public good was significantly higher when <italic>k</italic> = 2 with a success rate of 87% (SD = 0.09) compared to 36% (SD = 0.29) when <italic>k</italic> = 4 {two-tailed paired sample <italic>t</italic> test, <italic>t</italic>(28) = 10.08, <italic>P</italic> = 8.0 × 10<sup>−11</sup>, 95% CI difference = [0.40,0.60]}. All but six of the subjects contributed more when <italic>k</italic> = 4 (<xref rid="F2" ref-type="fig">Fig. 2C</xref>). Of these six players, five chose to free-ride more than 95% of the time. In addition, success rate was higher when <italic>k</italic> = 2 for all players (<xref rid="F2" ref-type="fig">Fig. 2D</xref>).</p>
                <fig id="F2" fig-type="figure" orientation="portrait" position="float">
                  <label>Fig. 2</label>
                  <caption>
                    <title>Human behavior in the PGG Task.</title>
                    <p>(<bold>A</bold>) Average contribution probability across subjects is significantly higher when the task requires more volunteers (<italic>k</italic>) to generate the group reward. (<bold>B</bold>) Average probability of success across all subjects in generating the group reward is significantly higher when <italic>k</italic> is lower. Error bars indicate within-subject SE (<xref rid="R52" ref-type="bibr"><italic>52</italic></xref>). (<bold>C</bold>) Average probability of contribution for each subject for <italic>k</italic> = 2 versus <italic>k</italic> = 4. Each point represents a subject. Subjects tend to contribute more often when the task requires more volunteers. (<bold>D</bold>) Average success rate for each subject was higher for <italic>k</italic> = 2 versus <italic>k</italic> = 4. <bold>(E)</bold> Average probability of contribution across subjects decreases throughout a game, especially for <italic>k</italic> = 4. Dotted lines are linear functions showing this trend for each <italic>k</italic>. (<bold>F</bold>) Average contribution probability across subjects as a function of number of games played. The contribution probability does not change significantly as subjects play more games.</p>
                  </caption>
                  <graphic xlink:href="aax8783-F2"/>
                </fig>
                <p>The contribution rate of the subjects dropped during the course of the trial on average, especially for <italic>k</italic> = 4, but remained above zero. <xref rid="F2" ref-type="fig">Figure 2E</xref> shows the average contribution rate across all subjects as a function of round number (1 to 15). We also compared the average contribution for the first five rounds with that for the last five rounds. For <italic>k</italic> = 4, the average contribution probability across all subjects for the first five rounds was 0.6 (SD = 0.20) and significantly higher than that for the last five rounds (average across subjects = 0.49, SD = 0.19) {two-tailed paired sample <italic>t</italic> test, <italic>t</italic>(28) = 3.65, <italic>P</italic> = 0.001, 95% CI difference = [0.05,0.17]}. For <italic>k</italic> = 2, the difference between the first five rounds (average = 0.53, SD = 0.32) and the last five rounds (average = 0.50, SD = 0.33) was insignificant {two-tailed paired sample <italic>t</italic> test, <italic>t</italic>(28) = 1.51, <italic>P</italic> = 0.14, 95% CI difference = [ − 0.01,0.06]}.</p>
                <p>The average contribution probability did not change significantly as subjects played more games (<xref rid="F2" ref-type="fig">Fig. 2F</xref>). In each condition, most of the players played at least five games (27 players for <italic>k</italic> = 2 and 26 for <italic>k</italic> = 4). For <italic>k</italic> = 2, in their first game, the average contribution rate of players was 0.37 (SD = 0.25), while in their fifth game, it was 0.30 (SD = 0.24) {two-tailed paired sample <italic>t</italic> test, <italic>t</italic>(26) = 1.34, <italic>P</italic> = 0.19, 95% CI difference = [ − 0.03,0.17]}. When <italic>k</italic> = 4, the average contribution rate was 0.57 (SD = 0.30) in the first game and 0.61 (SD = 0.35) in the fifth game {two-tailed paired sample <italic>t</italic> test, <italic>t</italic>(25) = − 0.69, <italic>P</italic> = 0.50, 95% CI difference = [ − 0.16,0.08]}.</p>
              </sec>
              <sec disp-level="2">
                <title>Probabilistic model of theory of mind for the group in the PGG</title>
                <p>Consider one round of the PGG task. A player can be expected to choose an action (contribute or free-ride) based on the number of contributions they anticipate the others to make in that round. Because the actions of individual players remain unknown through the game, the only observable parameter is the total number of contributions. One can therefore model this situation using a single random variable θ, denoting the average probability of contribution by any group member. With this definition, the total number of contributions by all the other members of the group can be expressed as a binomial distribution. Specifically, if θ is the probability of contribution by each group member, the probability of observing <italic>m</italic> contributions from the <italic>N</italic> − 1 others in a group of <italic>N</italic> people is<disp-formula id="E1"><mml:math id="m1"><mml:mrow><mml:mrow><mml:mi>P</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>m</mml:mi><mml:mo>∣</mml:mo><mml:mi mathvariant="normal">θ</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mo stretchy="true">(</mml:mo><mml:mtable><mml:mtr><mml:mtd><mml:mrow><mml:mi>N</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mi>m</mml:mi></mml:mtd></mml:mtr></mml:mtable><mml:mo stretchy="true">)</mml:mo><mml:msup><mml:mi mathvariant="normal">θ</mml:mi><mml:mi>m</mml:mi></mml:msup><mml:msup><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:mi mathvariant="normal">θ</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mrow><mml:mi>N</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:mi>m</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:mrow></mml:math><label>(1)</label></disp-formula></p>
                <p>Using this probability, a player can calculate the expected number of contributions from the others, compare it with <italic>k</italic>, and decide whether to contribute or free-ride accordingly. For example, if θ is very low, there is not a high probability of observing <italic>k</italic> − 1 contributions by the others, implying that free-riding is the best strategy.</p>
                <p>There are two important facts that make this decision-making more complex. First, the player does not know θ. θ must be estimated from the behavior of the group members. Second, other group members also have a theory of mind. Therefore, they can be expected to change their strategy based on the actions of others. Because of this ability in other group members, each player needs to simulate the effect of their action on the group’s behavior in the future.</p>
                <p>To model the uncertainty in θ, we assume that a probability distribution over θ is maintained in the player’s mind, representing their belief about the cooperativeness of the group. Each player starts with an initial probability distribution, called the prior belief about θ, and updates this belief over successive rounds based on the actions of the others. The prior belief may be based on the prior life experience of the player, or what they believe others would do through fictitious play (<xref rid="R26" ref-type="bibr"><italic>26</italic></xref>). For example, the player may start with a prior belief that the group will be a cooperative one but change this belief after observing low numbers of contributions by the others. Such belief updates can be performed using Bayes’ rule to invert the probabilistic relationship between θ and the number of contributions given by <xref ref-type="disp-formula" rid="E1">Eq. 1</xref>.</p>
                <p>A suitable prior probability distribution for estimating the parameter θ of a binomial distribution is the beta distribution, which is itself determined by two (hyper) parameters α and β<disp-formula id="E2"><mml:math id="m2"><mml:mrow><mml:mrow><mml:mtable><mml:mtr columnalign="left"><mml:mtd><mml:mrow><mml:mi mathvariant="normal">θ</mml:mi><mml:mo>∼</mml:mo><mml:mtext>Beta</mml:mtext><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="normal">α</mml:mi><mml:mo>,</mml:mo><mml:mi mathvariant="normal">β</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mtd></mml:mtr><mml:mtr columnalign="left"><mml:mtd><mml:mrow><mml:mtext>Beta</mml:mtext><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="normal">α</mml:mi><mml:mo>,</mml:mo><mml:mi mathvariant="normal">β</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>:</mml:mo><mml:mi>P</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo>∣</mml:mo><mml:mi mathvariant="normal">α</mml:mi><mml:mo>,</mml:mo><mml:mi mathvariant="normal">β</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>∝</mml:mo><mml:msup><mml:mi>x</mml:mi><mml:mrow><mml:mi mathvariant="normal">α</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:msup><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:mi>x</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mrow><mml:mi mathvariant="normal">β</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:mrow></mml:math><label>(2)</label></disp-formula></p>
                <p>Starting with a prior probability Beta(α<sub>1</sub>, β<sub>1</sub>) for θ, the player updates their belief about θ after observing the number of contributions from the others in each round through Bayes’ rule. This updated belief is called the posterior probability of θ. The posterior probability of θ in each round serves as the prior for the next round.</p>
                <p>In economics, the ability to infer the belief of others is sometimes called sophistication (<xref rid="R27" ref-type="bibr"><italic>27</italic></xref>, <xref rid="R28" ref-type="bibr"><italic>28</italic></xref>). Here, we consider a simple form of sophistication: We assume that each player thinks other group members have the same model as themselves (α and β). This is justifiable due to computational efficiency and more importantly anonymity of players. As a result, with a prior of Beta(α<italic><sub>t</sub></italic>, β<italic><sub>t</sub></italic>) after observing <italic>c</italic> contributions (including one’s own when made) in round <italic>t</italic>, the posterior probability of θ for the subject becomes Beta(α<sub><italic>t</italic> + 1</sub>, β<sub><italic>t</italic> + 1</sub>), where α<sub><italic>t</italic> + 1</sub> = α<italic><sub>t</sub></italic> + <italic>c</italic> and β<sub><italic>t</italic> + 1</sub> = β<italic><sub>t</sub></italic> + <italic>N</italic> − <italic>c</italic>. Technically, this follows because the beta distribution is conjugate to the binomial distribution (<xref rid="R29" ref-type="bibr"><italic>29</italic></xref>). Note that we include one’s own action in the update of the belief because one’s own action can change the future contribution level of the others.</p>
                <p>Intuitively, α represents the number of contributions made thus far, and β represents the number of free-rides. α<sub>1</sub> and β<sub>1</sub> (that define prior belief) represent the player’s a priori expectation about the relative number of contributions versus free-rides, respectively, before the session begins. For example, when α<sub>1</sub> is larger than β<sub>1</sub>, the player starts the task with the belief that people will contribute more than free-ride. Large values of α<sub>1</sub> and β<sub>1</sub> imply that the subject thinks that the average contribution probability will not change significantly after one round of the game when updated with the relatively small number <italic>c</italic> as above.</p>
                <p>Decision making in the PGG task is also made complex by the fact that the actual cooperativeness of the group itself (not just the player’s belief about it) may change from one round to the next: Players observe the contributions of the others and may change their own strategy for the next round. For example, players may start the game making contributions but change their strategy to free-riding if they observe a large number of contributions by the others. We model this phenomenon using a parameter 0 ≤ γ ≤ 1, which serves as a decay rate: The prior probability for round <italic>t</italic> is modeled as Beta(γα<italic><sub>t</sub></italic>, γβ<italic><sub>t</sub></italic>), which allows recent observations about the contributions of other players to be given more importance than observations from the more distant past. Thus, in a round with <italic>c</italic> total contributions (including the subject’s own contribution when made), the subject’s belief about the cooperativeness of the group as a whole changes from Beta(α<italic><sub>t</sub></italic>, β<italic><sub>t</sub></italic>) to Beta(α<sub><italic>t</italic> + 1</sub>, β<sub><italic>t</italic> + 1</sub>) where α<sub><italic>t</italic> + 1</sub> = γα<italic><sub>t</sub></italic> + <italic>c</italic> and β<sub><italic>t</italic> + 1</sub> = γβ<italic><sub>t</sub></italic> + <italic>N</italic> − <italic>c</italic>.</p>
              </sec>
              <sec disp-level="2">
                <title>Action selection</title>
                <p>How should a player decide whether to contribute or free-ride in each round? One possible strategy is to maximize the reward for the current round by calculating the expected number of contributions by the others based on the current belief. Using <xref ref-type="disp-formula" rid="E1">Eq. 1</xref> and the prior probability distribution over θ, the probability of seeing <italic>m</italic> contributions by the others when the belief about the cooperativeness of the group is Beta(α, β) is given by<disp-formula id="E3"><mml:math id="m3"><mml:mrow><mml:mrow><mml:mtable><mml:mtr columnalign="left"><mml:mtd><mml:mrow><mml:mi>P</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>m</mml:mi><mml:mo>∣</mml:mo><mml:mi mathvariant="normal">α</mml:mi><mml:mo>,</mml:mo><mml:mi mathvariant="normal">β</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mstyle displaystyle="true"><mml:msubsup><mml:mo stretchy="true">∫</mml:mo><mml:mn>0</mml:mn><mml:mn>1</mml:mn></mml:msubsup></mml:mstyle><mml:mi>P</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>m</mml:mi><mml:mo>∣</mml:mo><mml:mi mathvariant="normal">θ</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mi>P</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="normal">θ</mml:mi><mml:mo>∣</mml:mo><mml:mi mathvariant="normal">α</mml:mi><mml:mo>,</mml:mo><mml:mi mathvariant="normal">β</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mi>d</mml:mi><mml:mi mathvariant="normal">θ</mml:mi></mml:mrow></mml:mtd></mml:mtr><mml:mtr columnalign="left"><mml:mtd><mml:mrow><mml:mo>∝</mml:mo><mml:mstyle displaystyle="true"><mml:msubsup><mml:mo stretchy="true">∫</mml:mo><mml:mn>0</mml:mn><mml:mn>1</mml:mn></mml:msubsup></mml:mstyle><mml:mo stretchy="true">(</mml:mo><mml:mtable><mml:mtr><mml:mtd><mml:mrow><mml:mi>N</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mi>m</mml:mi></mml:mtd></mml:mtr></mml:mtable><mml:mo stretchy="true">)</mml:mo><mml:msup><mml:mi mathvariant="normal">θ</mml:mi><mml:mi>m</mml:mi></mml:msup><mml:msup><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:mi mathvariant="normal">θ</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mrow><mml:mi>N</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:mi>m</mml:mi></mml:mrow></mml:msup><mml:msup><mml:mi mathvariant="normal">θ</mml:mi><mml:mrow><mml:mi mathvariant="normal">α</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:msup><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:mi mathvariant="normal">θ</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mrow><mml:mi mathvariant="normal">β</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:mi>d</mml:mi><mml:mi mathvariant="normal">θ</mml:mi></mml:mrow></mml:mtd></mml:mtr><mml:mtr columnalign="left"><mml:mtd><mml:mrow><mml:mo>∝</mml:mo><mml:mo stretchy="true">(</mml:mo><mml:mtable><mml:mtr><mml:mtd><mml:mrow><mml:mi>N</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mi>m</mml:mi></mml:mtd></mml:mtr></mml:mtable><mml:mo stretchy="true">)</mml:mo><mml:mstyle displaystyle="true"><mml:msubsup><mml:mo stretchy="true">∫</mml:mo><mml:mn>0</mml:mn><mml:mn>1</mml:mn></mml:msubsup></mml:mstyle><mml:msup><mml:mi mathvariant="normal">θ</mml:mi><mml:mrow><mml:mi mathvariant="normal">α</mml:mi><mml:mo>+</mml:mo><mml:mi>m</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:msup><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:mi mathvariant="normal">θ</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mrow><mml:mi mathvariant="normal">β</mml:mi><mml:mo>+</mml:mo><mml:mi>N</mml:mi><mml:mo>−</mml:mo><mml:mi>m</mml:mi><mml:mo>−</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mi>d</mml:mi><mml:mi mathvariant="normal">θ</mml:mi></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:mrow></mml:math><label>(3)</label></disp-formula></p>
                <p>One can calculate the expected reward for the contribute versus free-ride actions in the current round based on the above equation. Maximizing this reward, however, is not the best strategy. As alluded to earlier, the actions of each player can change the behavior of other group members in future rounds. Specifically, our model assumes that its own contribution in the current round increases the average contribution rate of the group in the future rounds. <xref ref-type="disp-formula" rid="E10">Equation 10</xref> in Methods shows the exact assumptions of our model (with updates of α<sub><italic>t</italic> + 1</sub> = γα<italic><sub>t</sub></italic> + <italic>c</italic> and β<sub><italic>t</italic> + 1</sub> = γβ<italic><sub>t</sub></italic> + <italic>N</italic> − <italic>c</italic> for its belief) about the dynamics of the actual (hidden) state of the environment. The optimal strategy therefore is to calculate the cooperativeness of the group through the end of the session and consider the reward over all future rounds in the session before selecting the current action. Thus, an optimal agent would contribute for two reasons. First, contributing could enable the group to reach at least <italic>k</italic> volunteers in the current round. Second, contributing encourages other members to contribute in future rounds. Specifically, a contribution by the subject increases the average contribution rate for the next round by increasing α in the next round (see the transition function in Methods).</p>
                <p>Long-term reward maximization (as discussed above) based on probabilistic inference of hidden state in an environment (here, θ, the probability of contribution of group members) can be modeled using the framework of POMDPs (<xref rid="R24" ref-type="bibr"><italic>24</italic></xref>). Further details can be found in Methods, but briefly, to maximize the total expected reward, our model starts from the last round, the reward is calculated for each action and state, and then the model steps back one time step to find the optimal action for each state in that round. This process is repeated in a recursive fashion. <xref rid="F3" ref-type="fig">Figure 3A</xref> shows a schematic of the PGG experiment modeled using a POMDP, and <xref rid="F3" ref-type="fig">Fig. 3B</xref> illustrates the mechanism of action selection in this model.</p>
                <fig id="F3" fig-type="figure" orientation="portrait" position="float">
                  <label>Fig. 3</label>
                  <caption>
                    <title>POMDP model of the multiround PGG.</title>
                    <p>(<bold>A</bold>) Model: The subject does not know the average probability of contribution of the group. The POMDP model assumes that the subject maintains a probability distribution (“belief,” denoted by <italic>b<sub>t</sub></italic>) about the group’s average probability of contribution (denoted by θ<italic><sub>t</sub></italic>) and updates this belief after observing the outcome <italic>c<sub>t</sub></italic> (contribution by others) in each round. (<bold>B</bold>) Action selection: The POMDP model chooses an action (<italic>a<sub>t</sub></italic>) that maximizes the expected total reward (∑<italic>r<sub>i</sub></italic>) across all rounds based on the current belief and the consequence of the action (contribution “c” or free-ride “f”) on group behavior in future rounds.</p>
                  </caption>
                  <graphic xlink:href="aax8783-F3"/>
                </fig>
                <p>As an example of the POMDP model’s ability to select actions for the PGG task, <xref rid="F4" ref-type="fig">Fig. 4 (A and B)</xref> shows the best actions for a given round (here, round 9) as prescribed by the POMDP model for <italic>k</italic> = 2 and <italic>k</italic> = 4, respectively (the number of minimum volunteers needed). The best actions are shown as a function of different belief states the subject may have, expressed in terms of the different values possible for belief parameters α<italic><sub>t</sub></italic> and β<italic><sub>t</sub></italic>. This mapping from beliefs to actions is called a policy.</p>
                <fig id="F4" fig-type="figure" orientation="portrait" position="float">
                  <label>Fig. 4</label>
                  <caption>
                    <title>Optimal actions prescribed by the POMDP policy as a function of belief state.</title>
                    <p>Plot (<bold>A</bold>) shows the policy for <italic>k</italic> = 2 and plot (<bold>B</bold>) for <italic>k</italic> = 4. The purple regions represent those belief states (defined by α<italic><sub>t</sub></italic> and β<italic><sub>t</sub></italic>) for which free-riding is the optimal action; the yellow regions represent belief states for which the optimal action is contributing. These plots confirm that the optimal policy depends highly on <italic>k</italic>, the number of required volunteers. For the two plots, the decay rate was 1 and <italic>t</italic> was 9.</p>
                  </caption>
                  <graphic xlink:href="aax8783-F4"/>
                </fig>
                <p>Our simulations using the POMDP model showed that considering a much longer horizon (e.g., 50 rounds) instead of just 15 rounds gave a better fit to the subjects’ behavior, suggesting that human subjects may be inclined to use long horizons for group decision-making tasks (see Discussion). Such a long horizon for determining the optimal policy makes the model similar to an infinite horizon POMDP model (<xref rid="R30" ref-type="bibr"><italic>30</italic></xref>). As a result, the optimal policy for all rounds in our model is very similar to the policy for round 9 shown in <xref rid="F4" ref-type="fig">Fig. 4 (A and B)</xref>.</p>
                <p>In summary, the POMDP model performs two computations simultaneously. The first computation is probabilistic estimation of the (hidden) average contribution rate through belief updates. The average contribution rate changes during the course of the game as players interact with each other. The second computation involves selecting actions to influence this average contribution rate and to maximize total expected reward. This is the action selection component, which is performed by backward reasoning from the last round.</p>
              </sec>
              <sec disp-level="2">
                <title>POMDP model predicts human behavior in volunteer’s dilemma task</title>
                <p>The POMDP model has three parameters, α<sub>1</sub>, β<sub>1</sub>, and γ, which determine the subject’s actions and belief in each round. We fit these parameters to the subject’s actions by minimizing the error, i.e., the difference between the POMDP model’s predicted action and the subject’s action in each round. The average percentage error across all rounds is then the percentage of rounds that the model incorrectly predicts (contribute instead of free-ride or vice versa). We defined accuracy as the percentage of the rounds that the model predicts correctly.</p>
                <p>We also calculated the leave-one-out cross-validated (LOOCV) accuracy of our fits (<xref rid="R29" ref-type="bibr"><italic>29</italic></xref>), where each “left out” data point is one whole game and the parameters were fit to the other 11 games of the subject. Note that our LOOCV accuracy is a prediction of the subject’s behavior in a game without any parameter tuning based on this game. In addition, while different rounds of each game are highly correlated, the games of each subject are independent from each other (given the parameters of that subject) as the other group members change in each game.</p>
                <p>We found that the POMDP model had an average fitting accuracy across subjects of 84% (SD = 0.06), while the average LOOCV accuracy was 77% (SD = 0.08). <xref rid="F5" ref-type="fig">Figure 5A</xref> compares the average fitting and LOOCV accuracies of the POMDP model with two other models. The first is a model-free reinforcement learning model known as Q-learning: Actions are chosen on the basis of their rewards in previous rounds (<xref rid="R31" ref-type="bibr"><italic>31</italic></xref>), with the utility of group reward, initial values, and learning rate as free parameters (five parameters per subject; see Methods).</p>
                <fig id="F5" fig-type="figure" orientation="portrait" position="float">
                  <label>Fig. 5</label>
                  <caption>
                    <title>POMDP model’s performance and predictions.</title>
                    <p>(<bold>A</bold>) Average fitting and LOOCV accuracy across all models. The POMDP model has significantly higher accuracy compared to the other models (*<italic>P</italic> &lt; 0.05 and ***<italic>P</italic> &lt; 0.001). Error bars indicate within-subject SE (<xref rid="R52" ref-type="bibr"><italic>52</italic></xref>). (<bold>B</bold>) POMDP model’s prediction of a subject’s probability of contribution compared to experimental data for the two <italic>k</italic> values [black circles: same data as in <xref rid="F2" ref-type="fig">Fig. 2C</xref>). (<bold>C</bold>) Same data as (B) but the POMDP model’s prediction and the experimental data are shown for each <italic>k</italic> separately (blue for <italic>k</italic> = 2 and orange for <italic>k</italic> = 4]. (<bold>D</bold>) POMDP model’s prediction (blue circles) of a subject’s belief about group success in each round (on average) compared to actual data (black circles, same data as in <xref rid="F2" ref-type="fig">Fig. 2D</xref>). (<bold>E</bold>) Same data as (D), but the POMDP model’s prediction and actual data are shown for each <italic>k</italic> separately (blue for <italic>k</italic> = 2 and orange for <italic>k</italic> = 4). (<bold>F</bold>) Same data as (B) and (C) but with the data points binned on the basis of round of the game. (<bold>G</bold>) Same data as (D) and (E) but with the data points binned based on round of the game.</p>
                  </caption>
                  <graphic xlink:href="aax8783-F5"/>
                </fig>
                <p>The average fitting accuracy of the Q-learning model was 79% (SD = 0.07), which is significantly worse than the POMDP model’s fitting accuracy given above {two-tailed paired <italic>t</italic> test, <italic>t</italic>(28) = −6.75, <italic>P</italic> = 2.52 × 10<sup>−7</sup>, 95% CI difference = [−0.06, −0.03]}. In addition, the average LOOCV accuracy of the POMDP model was significantly higher than the average LOOCV accuracy of Q-learning, which was 73% (SD = 0.09) {two-tailed paired <italic>t</italic> test, <italic>t</italic>(28) = 2.20, <italic>P</italic> = 0.037, 95% CI difference =[0.004,0.08]}.</p>
                <p>We additionally tested a previously explored descriptive model in the PGG literature known as the linear two-factor model (<xref rid="R32" ref-type="bibr"><italic>32</italic></xref>), which predicts the current action of each player based on the player’s own action and contributions by the others in the previous round (this model has three free parameters per subject; see Methods). The average fitting accuracy of the two-factor model was 78% (SD = 0.09), which is significantly lower than the POMDP model’s fitting accuracy {two-tailed paired <italic>t</italic> test, <italic>t</italic>(28) = −4.86, <italic>P</italic> = 4.1 × 10<sup>−5</sup>, 95% CI difference = [−0.08, −0.03]}. Moreover, the LOOCV accuracy of the two-factor model was 47% (SD = 20), significantly lower than the POMDP model {two-tailed paired <italic>t</italic> test, <italic>t</italic>(28) = −7.61, <italic>P</italic> = 2.7 × 10<sup>−8</sup>, 95% CI difference = [−0.38, −0.22]}. The main reason for this result, especially the lower LOOCV accuracy, is that group success also depends on the required number of volunteers (<italic>k</italic>). This value is automatically incorporated in the POMDP’s calculation of expected reward. Also, reinforcement learning works directly with rewards and therefore does not need explicit knowledge of <italic>k</italic> (however, a separate parameter for each <italic>k</italic> is needed in the initial value function for Q-learning; see Methods). Given that the number of free parameters for the descriptive and model-free approaches is greater than or equal to the number of free parameters in the POMDP model, the higher accuracy of POMDP is notable in terms of model comparison.</p>
                <p>We tested the POMDP model’s predictions of contribution probability for each subject for the two <italic>k</italic> values with experimental data (same data as in <xref rid="F2" ref-type="fig">Fig. 2C</xref>; see Methods). As shown in <xref rid="F5" ref-type="fig">Fig. 5 (B and C)</xref>, the POMDP model’s predictions match the pattern of distribution of actual data from the experiments.</p>
                <p>The POMDP model, when fit to a subject’s actions, can also explain other events during the PGG task in contrast to the other models described above. For example, based on <xref ref-type="disp-formula" rid="E3">Eq. 3</xref> and the action chosen by the POMDP model, one can predict the subject’s belief about the probability of success in the current round. This prediction cannot be directly validated, but it can be compared to actual success. If we consider actual success as the ground truth, the average accuracy of the POMDP model’s prediction of success probability across subjects was 71% (SD = 0.07). Moreover, the predictions matched the pattern of success rate data from the experiment (<xref rid="F5" ref-type="fig">Fig. 5, D and E</xref>). The other models presented above are not capable of making such a prediction.</p>
                <p>The POMDP model’s predictions also match experimental data when the data points are binned on the basis of round of the game. The model correctly predicts a decrease in contribution for <italic>k</italic> = 4 and lack of significant change in contribution rate on average for <italic>k</italic> = 2 (<xref rid="F5" ref-type="fig">Fig. 5F</xref>). Moreover, the model’s prediction of a subject’s belief about group success matches the actual data round by round (<xref rid="F5" ref-type="fig">Fig. 5G</xref>). Further comparisons to other models, such as the interactive-POMDP model (<xref rid="R33" ref-type="bibr"><italic>33</italic></xref>), are provided in the Supplementary Materials.</p>
              </sec>
              <sec disp-level="2">
                <title>Distribution of POMDP parameters</title>
                <p>We can gain insights into the subject’s behavior by interpreting the parameters of our POMDP model in the context of the task. As alluded to above, the prior parameters α<sub>1</sub> and β<sub>1</sub> represent the subject’s prior expectations of contributions and free-rides, respectively. Therefore, the ratio α<sub>1</sub>/β<sub>1</sub> characterizes the subject’s expectation of contributions by group members, while the average of these parameters, (α<sub>1</sub> + β<sub>1</sub>)/2, indicates the weight the subject gives to prior experience with similar groups before the start of the game. The decay rate γ determines the weight given to past observations compared to new ones: The smaller the decay rate, the more weight the subject gives to new observations.</p>
                <p>We examined the distribution of these parameter values for our subjects after fitting the POMDP model to their behavior (<xref rid="F6" ref-type="fig">Fig. 6, A and B</xref>). The ratio α<sub>1</sub>/β<sub>1</sub> was in the reasonable range of 0.5 to 2 for almost all subjects (<xref rid="F6" ref-type="fig">Fig. 6C</xref>; in our algorithm, the ratio can be as high as 200 or as low as 1/200; see Methods). The value of (α<sub>1</sub> + β<sub>1</sub>)/2 across subjects was mostly between 40 to 120 (<xref rid="F6" ref-type="fig">Fig. 6D</xref>), suggesting that prior belief about groups did have a significant role in players’ strategy, but it was not the only factor because observations over multiple rounds can still alter this initial belief. To confirm the effect of actions during the game, we performed a comparison with a POMDP model that does not update α and β over time and only uses its prior. The accuracy of this modified POMDP model was 66% (SD = 0.17), significantly lower than our original model {two-tailed paired <italic>t</italic> test, <italic>t</italic>(28) = − 5.47, <italic>P</italic> = 7.64 × 10<sup>−6</sup>, 95% CI difference =[ − 0.23, − 0.11]}. The average α<italic><sub>t</sub></italic> and β<italic><sub>t</sub></italic> for each of the 15 rounds, as well as distributions of their difference with the prior values α<sub>1</sub> and β<sub>1</sub> are presented in the Supplementary Materials.</p>
                <fig id="F6" fig-type="figure" orientation="portrait" position="float">
                  <label>Fig. 6</label>
                  <caption>
                    <title>Distribution of POMDP parameters across subjects.</title>
                    <p>(<bold>A</bold>) Histogram of α<sub>1</sub> across all subjects. (<bold>B</bold>) Histogram of β<sub>1</sub> across all subjects. (<bold>C</bold>) Histogram of the ratio α<sub>1</sub>/β<sub>1</sub> shows a value between 0.5 and 2 for almost all subjects. (<bold>D</bold>) Histogram of (α<sub>1</sub> + β<sub>1</sub>)/2. For most subjects, this value is between 40 and 120. (<bold>E</bold>) Histogram of prior belief Beta(α<sub>1</sub>, β<sub>1</sub>) translated into expected contribution by the others in the first round. Note that the values, after fitting to the subjects’ behavior, are mostly between 2 and 3. (<bold>F</bold>) When <italic>k</italic> = 2, all subjects expected a high probability of group success in the first round (before making any observations about the group). (<bold>G</bold>) When <italic>k</italic> = 4, almost all subjects assigned a chance of less than 60% to group success in the first round. (<bold>H</bold>) Box plot of decay rate γ across subjects shows that this value is almost always above 0.95. The median is 0.97 (orange line) and the mean is 0.93 (green line).</p>
                  </caption>
                  <graphic xlink:href="aax8783-F6"/>
                </fig>
                <p>We also calculated the expected value of contribution by the others in the first round, which is between 0 and <italic>N</italic> − 1 = 4, based on the values of α<sub>1</sub> and β<sub>1</sub> for the subjects. For almost all subjects, this expected value was between two and three (<xref rid="F6" ref-type="fig">Fig. 6E</xref>).</p>
                <p>In addition, we calculated each subject’s prior belief about group success (probability of success in the first round) based on α<sub>1</sub>, β<sub>1</sub>, and the subject’s POMDP policy in the first round. As group success depends on the required number of volunteers (<italic>k</italic>), probability of success is different for <italic>k</italic> = 2 and <italic>k</italic> = 4 even with the same α<sub>1</sub> and β<sub>1</sub>. <xref rid="F6" ref-type="fig">Figure 6 (F and G)</xref> shows the distribution of this prior probability of success across all subjects for <italic>k</italic> = 2 and <italic>k</italic> = 4. For <italic>k</italic> = 2, all subjects expected a high probability of success in the first round, whereas most of the subjects expected less than 60% chance for success when <italic>k</italic> = 4. While these beliefs cannot be directly validated, the results point to the importance of the required number of volunteers in shaping the subjects’ behavior.</p>
                <p>Additionally, the decay rate γ, which determines the weight accorded to the prior and previous observations compared to the most recent observation, was almost always above 0.95, with a mean of 0.93 and a median of 0.97 (<xref rid="F6" ref-type="fig">Fig. 6H</xref>). Only three subjects had a decay rate less than 0.95 (not shown in the figure), suggesting that almost all subjects relied on observations made across multiple rounds when computing their beliefs rather than reasoning based solely on the current or most recent observations.</p>
              </sec>
            </sec>
            <sec sec-type="discussion" disp-level="1">
              <title>DISCUSSION</title>
              <p>We introduced a normative model based on POMDPs for explaining human behavior in a group decision-making task. Our model combines probabilistic reasoning about the group with long-term reward maximization by simulating the effect of each action on the future behavior of the group. The greater accuracy of our model in explaining and predicting the subjects’ behavior compared to the other models suggests that humans make decisions in group settings by reasoning about the group as a whole. This mechanism is analogous to maintaining a theory of mind about another person, except that the theory of mind pertains to a group member on average.</p>
              <p>This is the first time, to our knowledge, that a normative model has been proposed for a group decision-making task. Existing models to explain human behavior in the PGG, for example, are descriptive and do not provide insights into the computational mechanisms underlying the decisions (<xref rid="R32" ref-type="bibr"><italic>32</italic></xref>). While the regression-based descriptive method we compared our POMDP model to can potentially be seen as a “learned” model-free approach to mapping observations to choice in the next round, our model was also able to outperform this method.</p>
              <p>In addition to providing a better fit and prediction of the subject’s behavior, our model, when fit to the subject’s actions, can predict success rate in each round without being explicitly trained for such predictions, in contrast to the other methods. In addition, as alluded to in <xref rid="F6" ref-type="fig">Fig. 6 (C, D, and H)</xref>, when fit to the subjects’ actions, the parameters were all within a reasonable range, showing the importance of prior knowledge and multiple observations in decision-making. The POMDP model is normative and strictly constrained by probability theory and optimal control theory. The beta distribution is used because it is the conjugate prior of the binomial distribution (<xref rid="R29" ref-type="bibr"><italic>29</italic></xref>) and not due to better fits compared to other distributions.</p>
              <p>The POMDP policy aligns with our intuition about action selection in the volunteer’s dilemma task. A player chooses to free-ride for two reasons: (i) when the cooperativeness of the group is low and therefore there is no benefit in contributing, and (ii) when the player knows there are already enough volunteers and contributing leads to a waste of resources. The two purple areas of <xref rid="F4" ref-type="fig">Fig. 4A</xref> represent these two conditions for <italic>k</italic> = 2. The upper left part represents large α<italic><sub>t</sub></italic> and small β<italic><sub>t</sub></italic>, implying a high contribution rate, while the bottom right part represents small α<italic><sub>t</sub></italic> and large β<italic><sub>t</sub></italic>, implying a low contribution rate. When <italic>k</italic> = 4, all but one of the five players must contribute for group success—this causes a significant difference in the optimal POMDP policy compared to the <italic>k</italic> = 2 condition. As seen in <xref rid="F4" ref-type="fig">Fig. 4B</xref>, there is only a single region of belief space for which free-riding is the best strategy, namely, when the player does not expect contributions by enough players (relatively large β<italic><sub>t</sub></italic>). On the other hand, as expected, this region is much larger compared to the same region for <italic>k</italic> = 2 (see <xref rid="F4" ref-type="fig">Fig. 4A</xref>). The POMDP model predicts that free-riding is not a viable action in the <italic>k</italic> = 4 case (<xref rid="F4" ref-type="fig">Fig. 4B</xref>) because not only does this action require all the other four players to contribute to generate the group reward in the current round but also such an action increases the chances that the group contribution will be lower in the next round, resulting in lesser expected reward in future rounds. The opposite situation can also occur especially when <italic>k</italic> = 2. A player may contribute not to gain the group reward in the current round but to encourage others to contribute in the next rounds. When an optimal player chooses free-riding due to low cooperativeness of the group, the estimated average contribution is so low that the group is not likely to get the group reward in the next rounds even with an increase in the average contribution due to the player’s contribution. On the other hand, when an optimal player chooses to free-ride due to high cooperativeness of the group, the estimated average contribution rate is so high that the chance of success remains high in future rounds even with a decrease in average contribution rate due the player free-riding in the current round.</p>
              <p>In a game with a predetermined and known number of rounds, even if the player considers the future, one might expect the most rewarding action in the last rounds to be free-riding as there is little or no future to consider. However, our experimental data did not support this conclusion. Our model is able to explain these data using the hypothesis that subjects may use a longer horizon than the exact number of rounds in a game. Such a strategy provides a significant computational benefit by making the policies for different rounds similar to each other, avoiding recalculation of a policy for each single round. Recent studies in human decision-making have demonstrated that humans may use such minimal modifications of model-based policies for efficiency (<xref rid="R34" ref-type="bibr"><italic>34</italic></xref>, <xref rid="R35" ref-type="bibr"><italic>35</italic></xref>). More broadly, group decision-making occurs among groups of humans (and animals) that live together. Thus, any group decision-making involves practically an infinite horizon, i.e., there is always a future interaction even after the current task has ended, justifying the use of long horizons.</p>
              <p>In the volunteer’s dilemma, not only is the common goal not reached when there are not enough volunteers but also having more than the required number of volunteers leads to a waste of resources. As a result, an accurate prediction of others’ intentions based on one’s beliefs is crucial to make accurate decisions. This gives the model-based approach a huge advantage over model-free methods in terms of reward gathering, thus making it more beneficial for the brain to endure the extra cognitive cost. It is possible that in simpler tasks where the accurate prediction of minds is less crucial, the brain adopts a model-free approach.</p>
              <p>Our model was based on the binomial and beta distributions for binary values due to the nature of the task, but it can be easily extended to the more general case of a discrete set of actions using multinomial and Dirichlet distributions (<xref rid="R36" ref-type="bibr"><italic>36</italic></xref>). In addition, the model can be extended to multivariate states, e.g., when the players are no longer anonymous. In such cases, the belief can be modeled as a joint probability distribution over all parameters of the state. This, however, incurs a significant computational cost. An interesting area for future research is investigating whether, under some circumstances, humans model group members with similar behavior as one subgroup to reduce the number of minds one should reason about.</p>
              <p>Our POMDP framework assumes that each subject starts with the same prior about average group member contribution probability at the beginning of each game. However, subjects might try to estimate this prior for a new group in the first few rounds, i.e., “explore” their new environment before seeking to maximize their reward (“exploit”) based on this prior (<xref rid="R5" ref-type="bibr"><italic>5</italic></xref>). Such an “active inference” approach has been studied in two-person interactions (<xref rid="R15" ref-type="bibr"><italic>15</italic></xref>, <xref rid="R16" ref-type="bibr"><italic>16</italic></xref>) and is an interesting direction of research in group decision-making.</p>
              <p>Mimicking human behavior does not guarantee that a POMDP model (or any model) is being implemented in the brain. However, the POMDP model’s generalizability and the interpretability of its components, such as existence of a prior or simulation of the future, make it a useful tool for understanding the decision-making process.</p>
              <p>The POMDP framework can model social tasks beyond economic decision-making, such as prediction of others’ intentions and actions in everyday situations (<xref rid="R37" ref-type="bibr"><italic>37</italic></xref>). In these cases, we would need to modify the model’s definition of the state of other minds to include dimensions such as valence, competence, and social impact instead of propensity to contribute monetary units as in the PGG task (<xref rid="R38" ref-type="bibr"><italic>38</italic></xref>).</p>
              <p>The interpretability of the POMDP framework offers an opportunity to study the neurocognitive mechanisms of group decision-making in healthy and diseased brains. POMDPs and similar Bayesian models have previously proved useful in understanding neural responses in sensory decision-making (<xref rid="R39" ref-type="bibr"><italic>39</italic></xref>–<xref rid="R41" ref-type="bibr"><italic>41</italic></xref>) and in tasks involving interactions with a single individual (<xref rid="R13" ref-type="bibr"><italic>13</italic></xref>, <xref rid="R17" ref-type="bibr"><italic>17</italic></xref>, <xref rid="R18" ref-type="bibr"><italic>18</italic></xref>). We believe that the POMDP model we have proposed can likewise prove useful in interpreting neural responses and data from neuroimaging studies of group decision-making tasks. In addition, the model can be used for Bayesian theory-driven investigations in the field of computational psychiatry (<xref rid="R42" ref-type="bibr"><italic>42</italic></xref>). For example, theory of mind deficits are a key feature of autism spectrum disorder (<xref rid="R43" ref-type="bibr"><italic>43</italic></xref>), but it is unclear what computational components are impaired and how they are affected. The POMDP model may provide a new avenue for computational studies of such neuropsychiatric disorders (<xref rid="R44" ref-type="bibr"><italic>44</italic></xref>).</p>
            </sec>
            <sec sec-type="methods" disp-level="1">
              <title>METHODS</title>
              <sec disp-level="2">
                <title>Experiment</title>
                <p>Thirty right-handed students at the University of Parma were recruited for this study. One of them aborted the experiment due to anxiety. Data from the other 29 participants were collected, analyzed, and reported. On the basis of self-reported questionnaires, none of the participants had a history of neurological or psychiatric disorders. This study was approved by the Institutional Review Board of the local ethics committee from Parma University (IRB no. A13-37030), which was carried out according to the ethical standards of the 2013 Declaration of Helsinki. All participants gave their informed written consent. As mentioned in Results, each subject played 14 sessions of the PGG (i.e., the volunteer’s dilemma), each containing 15 rounds. In the first two sessions, subjects received no feedback about the result of each round. However, in the following 12 sessions, social and monetary feedback were provided to the subject. The feedback included the number of contributors and free-riders, and the subject’s reward in that round. Each individual player’s action, however, remained unknown to the others. Therefore, individual players could not be tracked. We present analyses from the games with feedback.</p>
                <p>In each round (see <xref rid="F1" ref-type="fig">Fig. 1</xref>), the participant had to make a decision within 3 s by pressing a key; otherwise, the round was repeated. After the action selection (2.5 to 4 s), the outcome of the round was shown to the subject for 4 s. Then, players evaluated the outcome of the round before the next round started. Subjects were told that they were playing with 19 other participants located in other rooms. Overall, 20 players were playing the PGG in four different groups simultaneously. These groups were randomly chosen by a computer at the beginning of each session. In reality, subjects were playing with a computer. In other words, a computer algorithm was generating all the actions of others for each subject. Each subject got a final monetary reward equal to the result of one PGG randomly selected by the computer at the end of the study.</p>
                <p>In a PGG with <italic>N</italic> = 5 players, we denote the action of player <italic>i</italic> in round <italic>t</italic> with the binary value of <inline-formula><mml:math id="m4"><mml:mrow><mml:mrow><mml:msubsup><mml:mi>a</mml:mi><mml:mi>i</mml:mi><mml:mi>t</mml:mi></mml:msubsup><mml:mo> </mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo>≤</mml:mo><mml:mi>i</mml:mi><mml:mo>≤</mml:mo><mml:mi>N</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>,</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> with <inline-formula><mml:math id="m5"><mml:mrow><mml:mrow><mml:msubsup><mml:mi>a</mml:mi><mml:mi>i</mml:mi><mml:mi>t</mml:mi></mml:msubsup><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula> representing contribution and <inline-formula><mml:math id="m6"><mml:mrow><mml:mrow><mml:msubsup><mml:mi>a</mml:mi><mml:mi>i</mml:mi><mml:mi>t</mml:mi></mml:msubsup><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula> representing free-riding. The human subject is assumed to be player 1. We define the average contribution rate of others <inline-formula><mml:math id="m7"><mml:mrow><mml:mrow><mml:msubsup><mml:mover accent="true"><mml:mi>a</mml:mi><mml:mo stretchy="true">¯</mml:mo></mml:mover><mml:mrow><mml:mn>2</mml:mn><mml:mo>:</mml:mo><mml:mi>N</mml:mi></mml:mrow><mml:mi>t</mml:mi></mml:msubsup><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:msubsup><mml:mo>∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>2</mml:mn></mml:mrow><mml:mi>N</mml:mi></mml:msubsup><mml:msubsup><mml:mi>a</mml:mi><mml:mi>i</mml:mi><mml:mi>t</mml:mi></mml:msubsup></mml:mrow><mml:mrow><mml:mi>N</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mfrac></mml:mrow></mml:mrow></mml:math></inline-formula> and generate each of the <italic>N</italic> − 1 actions of others in round <italic>t</italic> using the following probabilistic function<disp-formula id="E4"><mml:math id="m8"><mml:mrow><mml:mrow><mml:mtext>logit</mml:mtext><mml:mo stretchy="false">(</mml:mo><mml:msubsup><mml:mover accent="true"><mml:mi>a</mml:mi><mml:mo stretchy="true">¯</mml:mo></mml:mover><mml:mrow><mml:mn>2</mml:mn><mml:mo>:</mml:mo><mml:mi>N</mml:mi></mml:mrow><mml:mi>t</mml:mi></mml:msubsup><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:msub><mml:mi>e</mml:mi><mml:mn>0</mml:mn></mml:msub><mml:msubsup><mml:mi>a</mml:mi><mml:mn>1</mml:mn><mml:mrow><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msubsup><mml:mo>+</mml:mo><mml:msub><mml:mi>e</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo stretchy="true">(</mml:mo><mml:msup><mml:mrow><mml:mo stretchy="true">(</mml:mo><mml:mfrac><mml:mrow><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:msup><mml:mi>K</mml:mi><mml:mrow><mml:mi>T</mml:mi><mml:mo>−</mml:mo><mml:mi>t</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup></mml:mrow><mml:mrow><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:mi>K</mml:mi></mml:mrow></mml:mfrac><mml:mo stretchy="true">)</mml:mo></mml:mrow><mml:msub><mml:mi>e</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:msup><mml:msubsup><mml:mover accent="true"><mml:mi>a</mml:mi><mml:mo stretchy="true">¯</mml:mo></mml:mover><mml:mrow><mml:mn>2</mml:mn><mml:mo>:</mml:mo><mml:mi>N</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msubsup><mml:mo>−</mml:mo><mml:mi>K</mml:mi><mml:mo stretchy="true">)</mml:mo></mml:mrow></mml:mrow></mml:math><label>(4)</label></disp-formula>where <italic>K</italic> = <italic>k</italic>/<italic>N</italic>, in which <italic>k</italic> is the required number of contributors.</p>
                <p>This model has three free parameters: <italic>e</italic><sub>0</sub>, <italic>e</italic><sub>1</sub>, and <italic>e</italic><sub>2</sub>. These were obtained by fitting the above function to the actual actions of subjects in another PGG study (<xref rid="R45" ref-type="bibr"><italic>45</italic></xref>), making this function a simulation of human behavior in the PGG task. Specifically, to generate the actions of others, we fixed <italic>e</italic><sub>2</sub> to 1 for all games. <italic>e</italic><sub>0</sub> was drawn randomly from the range of [0.15,0.35] for each game, and <italic>e</italic><sub>1</sub> was set to 1 − <italic>e</italic><sub>0</sub>. This combination and the random sampling of <italic>e</italic><sub>0</sub> in each game simulated different response strategies for the others in each game, simulating new sets of group members. Higher values of <italic>e</italic><sub>0</sub> make the algorithm more likely to choose its next action based on the result of the group interaction in the previous round (especially the action of the subject). On the other hand, lower values of <italic>e</italic><sub>0</sub> make the algorithm more likely to stick to its previous action. For the first round of each game, we used the mean contribution rate of each subject as their fellow members’ decision.</p>
              </sec>
              <sec disp-level="2">
                <title>Markov decision processes</title>
                <p>A Markov decision process (MDP) is a tuple (<italic>S</italic>, <italic>A</italic>, <italic>T</italic>, <italic>R</italic>), where <italic>S</italic> represents the set of states of the environment, <italic>A</italic> is the set of actions, <italic>T</italic> is the transition function <italic>S</italic> × <italic>S</italic> × <italic>A</italic> → [0,1] that determines the probability of the next state given the current state and action, i.e., <italic>T</italic>(<italic>s</italic><sup>′</sup>, <italic>s</italic>, <italic>a</italic>) = <italic>P</italic>(<italic>s</italic><sup>′</sup> ∣ <italic>s</italic>, <italic>a</italic>), and <italic>R</italic> is the reward function <italic>S</italic> × <italic>A</italic> → <italic>R</italic> representing the reward associated with each state and action (<xref rid="R30" ref-type="bibr"><italic>30</italic></xref>). In an MDP with horizon <italic>H</italic> (total number of performed actions), given the initial state <italic>s</italic><sub>1</sub>, the goal is to choose a sequence of actions that maximizes the total expected reward<disp-formula id="E5"><mml:math id="m9"><mml:mrow><mml:mrow><mml:msup><mml:mi mathvariant="normal">π</mml:mi><mml:mo>*</mml:mo></mml:msup><mml:mo>=</mml:mo><mml:munder><mml:mrow><mml:mtext>arg</mml:mtext><mml:mo> </mml:mo><mml:mtext>max</mml:mtext></mml:mrow><mml:mrow><mml:msub><mml:mi>a</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>a</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:mo>…</mml:mo><mml:mo>,</mml:mo><mml:msub><mml:mi>a</mml:mi><mml:mi>H</mml:mi></mml:msub></mml:mrow></mml:munder><mml:mstyle displaystyle="true"><mml:munderover><mml:mo movablelimits="false" stretchy="true">∑</mml:mo><mml:mrow><mml:mi>t</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>H</mml:mi></mml:munderover></mml:mstyle><mml:msub><mml:mi>E</mml:mi><mml:msub><mml:mi>s</mml:mi><mml:mi>t</mml:mi></mml:msub></mml:msub><mml:mo stretchy="false">[</mml:mo><mml:mi>R</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>s</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>a</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:mrow></mml:math><label>(5)</label></disp-formula></p>
                <p>This sequence, called the optimal policy, can be found using the technique of dynamic programming (<xref rid="R30" ref-type="bibr"><italic>30</italic></xref>). For an MDP with time horizon <italic>H</italic>, the <italic>Q</italic> value, value function <italic>V</italic>, and action function <italic>U</italic> at the last time step <italic>t</italic> = <italic>H</italic> are defined as<disp-formula id="E6"><mml:math id="m10"><mml:mrow><mml:mrow><mml:mo>∀</mml:mo><mml:mi>s</mml:mi><mml:mo>∈</mml:mo><mml:mi>S</mml:mi><mml:mo>:</mml:mo><mml:mo stretchy="true">{</mml:mo><mml:mtable><mml:mtr columnalign="left"><mml:mtd><mml:mrow><mml:msup><mml:mi>Q</mml:mi><mml:mi>H</mml:mi></mml:msup><mml:mo stretchy="false">(</mml:mo><mml:mi>s</mml:mi><mml:mo>,</mml:mo><mml:mi>a</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>←</mml:mo><mml:mi>R</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>s</mml:mi><mml:mo>,</mml:mo><mml:mi>a</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mtd></mml:mtr><mml:mtr columnalign="left"><mml:mtd><mml:mrow><mml:msup><mml:mi>V</mml:mi><mml:mi>H</mml:mi></mml:msup><mml:mo stretchy="false">(</mml:mo><mml:mi>s</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>←</mml:mo><mml:msub><mml:mtext>max</mml:mtext><mml:mi>a</mml:mi></mml:msub><mml:msup><mml:mi>Q</mml:mi><mml:mi>H</mml:mi></mml:msup><mml:mo stretchy="false">(</mml:mo><mml:mi>s</mml:mi><mml:mo>,</mml:mo><mml:mi>a</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mtd></mml:mtr><mml:mtr columnalign="left"><mml:mtd><mml:mrow><mml:msup><mml:mi>U</mml:mi><mml:mi>H</mml:mi></mml:msup><mml:mo stretchy="false">(</mml:mo><mml:mi>s</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>←</mml:mo><mml:mtext>arg</mml:mtext><mml:mo> </mml:mo><mml:msub><mml:mtext>max</mml:mtext><mml:mi>a</mml:mi></mml:msub><mml:msup><mml:mi>Q</mml:mi><mml:mi>H</mml:mi></mml:msup><mml:mo stretchy="false">(</mml:mo><mml:mi>s</mml:mi><mml:mo>,</mml:mo><mml:mi>a</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:mrow></mml:math><label>(6)</label></disp-formula></p>
                <p>For any <italic>t</italic> from 1 to <italic>H</italic> − 1, the value function <italic>V<sup>t</sup></italic> and action function <italic>U<sup>t</sup></italic> are defined recursively as<disp-formula id="E7"><mml:math id="m11"><mml:mrow><mml:mrow><mml:mo stretchy="true">{</mml:mo><mml:mtable><mml:mtr columnalign="left"><mml:mtd><mml:mrow><mml:msup><mml:mi>Q</mml:mi><mml:mi>t</mml:mi></mml:msup><mml:mo stretchy="false">(</mml:mo><mml:mi>s</mml:mi><mml:mo>,</mml:mo><mml:mi>a</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>←</mml:mo><mml:mi>R</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>s</mml:mi><mml:mo>,</mml:mo><mml:mi>a</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>+</mml:mo><mml:msub><mml:mo stretchy="true">∑</mml:mo><mml:mrow><mml:mi>s</mml:mi><mml:mo>′</mml:mo><mml:mo>∈</mml:mo><mml:mi>S</mml:mi></mml:mrow></mml:msub><mml:mi>T</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>s</mml:mi><mml:mo>′</mml:mo><mml:mo>,</mml:mo><mml:mi>s</mml:mi><mml:mo>,</mml:mo><mml:mi>a</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:msup><mml:mi>V</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:mo stretchy="false">(</mml:mo><mml:mi>s</mml:mi><mml:mo>′</mml:mo><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mtd></mml:mtr><mml:mtr columnalign="left"><mml:mtd><mml:mrow><mml:msup><mml:mi>V</mml:mi><mml:mi>t</mml:mi></mml:msup><mml:mo stretchy="false">(</mml:mo><mml:mi>s</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>←</mml:mo><mml:msub><mml:mtext>max</mml:mtext><mml:mi>a</mml:mi></mml:msub><mml:msup><mml:mi>Q</mml:mi><mml:mi>t</mml:mi></mml:msup><mml:mo stretchy="false">(</mml:mo><mml:mi>s</mml:mi><mml:mo>,</mml:mo><mml:mi>a</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mtd></mml:mtr><mml:mtr columnalign="left"><mml:mtd><mml:mrow><mml:msup><mml:mi>U</mml:mi><mml:mi>t</mml:mi></mml:msup><mml:mo stretchy="false">(</mml:mo><mml:mi>s</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>←</mml:mo><mml:mtext>arg</mml:mtext><mml:mo> </mml:mo><mml:msub><mml:mtext>max</mml:mtext><mml:mi>a</mml:mi></mml:msub><mml:msup><mml:mi>Q</mml:mi><mml:mi>t</mml:mi></mml:msup><mml:mo stretchy="false">(</mml:mo><mml:mi>s</mml:mi><mml:mo>,</mml:mo><mml:mi>a</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:mrow></mml:math><label>(7)</label></disp-formula></p>
                <p>Starting from the initial state <italic>s</italic><sub>1</sub> at time 1, the action chosen by the optimal policy π* at time step <italic>t</italic> is <italic>U<sup>t</sup></italic>(<italic>s<sub>t</sub></italic>).</p>
                <p>When the state of the environment is hidden, the MDP turns into a partially observable MDP (POMDP) where the state is estimated probabilistically from observations or measurements from sensors. Formally, a POMDP is defined as (<italic>S</italic>, <italic>A</italic>, <italic>Z</italic>, <italic>T</italic>, <italic>O</italic>, <italic>R</italic>), where <italic>S</italic>, <italic>A</italic>, <italic>T</italic>, and <italic>R</italic> are defined as in the case of MDPs, <italic>Z</italic> is the set of possible observations, and <italic>O</italic> is the observation function <italic>Z</italic> × <italic>S</italic> → [0,1] that determines the probability of any observation <italic>z</italic> given a state <italic>s</italic>, i.e., <italic>O</italic>(<italic>z</italic>, <italic>s</italic>) = <italic>P</italic>(<italic>z</italic>∣<italic>s</italic>). To find the optimal policy, the POMDP model uses the posterior probability of states, known as the belief state, where <italic>b<sub>t</sub></italic>(<italic>s</italic>) = <italic>P</italic>(<italic>s</italic>∣<italic>z</italic><sub>1</sub>, <italic>a</italic><sub>1</sub>, <italic>z</italic><sub>2</sub>, …, <italic>a</italic><sub><italic>t</italic> − 1</sub>). Belief states can be computed recursively as follows<disp-formula id="E8"><mml:math id="M1"><mml:mrow><mml:mrow><mml:mo>∀</mml:mo><mml:mi>s</mml:mi><mml:mo>∈</mml:mo><mml:mi>S</mml:mi><mml:mo>:</mml:mo><mml:msub><mml:mi>b</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>s</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>∝</mml:mo><mml:mi>O</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>z</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:mi>s</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:munder><mml:mo>∑</mml:mo><mml:mrow><mml:mi>s</mml:mi><mml:mo>′</mml:mo><mml:mo>∈</mml:mo><mml:mi>S</mml:mi></mml:mrow></mml:munder><mml:mi>T</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>s</mml:mi><mml:mo>,</mml:mo><mml:mi>s</mml:mi><mml:mo>′</mml:mo><mml:mo>,</mml:mo><mml:msub><mml:mi>a</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:msub><mml:mi>b</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>s</mml:mi><mml:mo>′</mml:mo><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math><label>(8)</label></disp-formula></p>
                <p>If we define <italic>R</italic>(<italic>b<sub>t</sub></italic>, <italic>a<sub>t</sub></italic>) as the expected reward of <italic>a<sub>t</sub></italic>, i.e., <italic>E<sub>s<sub>t</sub></sub></italic>[<italic>R</italic>(<italic>s<sub>t</sub></italic>, <italic>a<sub>t</sub></italic>)], starting from initial belief state, <italic>b</italic><sub>1</sub>, the optimal policy for the POMDP is given by<disp-formula id="E9"><mml:math id="m13"><mml:mrow><mml:mrow><mml:msup><mml:mi mathvariant="normal">π</mml:mi><mml:mo>*</mml:mo></mml:msup><mml:mo>=</mml:mo><mml:munder><mml:mrow><mml:mtext>arg</mml:mtext><mml:mo> </mml:mo><mml:mtext>max</mml:mtext></mml:mrow><mml:mrow><mml:msub><mml:mi>a</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>a</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:mo>…</mml:mo><mml:mo>,</mml:mo><mml:msub><mml:mi>a</mml:mi><mml:mi>H</mml:mi></mml:msub></mml:mrow></mml:munder><mml:mstyle displaystyle="true"><mml:munderover><mml:mo movablelimits="false" stretchy="true">∑</mml:mo><mml:mrow><mml:mi>t</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>H</mml:mi></mml:munderover></mml:mstyle><mml:msub><mml:mi>E</mml:mi><mml:msub><mml:mi>s</mml:mi><mml:mi>t</mml:mi></mml:msub></mml:msub><mml:mo stretchy="false">[</mml:mo><mml:mi>R</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>b</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>a</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:mrow></mml:math><label>(9)</label></disp-formula></p>
                <p>A POMDP can be considered an MDP whose states are belief states. This belief state space, however, is exponentially larger than the underlying state space. Therefore, solving a POMDP optimally is computationally expensive, unless the belief state can be represented by a few parameters as in our case (<xref rid="R30" ref-type="bibr"><italic>30</italic></xref>). For solving larger POMDP problems, various approximation and learning algorithms have been proposed. We refer the reader to the growing literature on this topic (<xref rid="R46" ref-type="bibr"><italic>46</italic></xref>–<xref rid="R48" ref-type="bibr"><italic>48</italic></xref>).</p>
              </sec>
              <sec disp-level="2">
                <title>POMDP for binary PGG</title>
                <p>The state of the environment is represented by the average cooperativeness of the group or, equivalently, the average probability θ of contribution by a group member. Because θ is not observable, the task is a POMDP, and one must maintain a probability distribution (belief) over θ. The beta distribution, represented by two free parameters (α and β), is the conjugate prior for binomial distribution (<xref rid="R29" ref-type="bibr"><italic>29</italic></xref>). Therefore, when performing Bayesian inference to obtain the belief state over θ, combining the beta distribution as the prior belief and the binomial distribution as the likelihood results in another beta distribution as the posterior belief. Using the beta distribution for the belief state, our POMDP turns into an MDP with a two-dimensional state space represented by α and β. Starting from an initial belief state Beta(α<sub>1</sub>, β<sub>1</sub>) and with an additional free parameter γ, the next belief states are determined by the actions of all players at each round as described in Results. For the reward function, we used the monetary reward function of the PGG. Therefore, the elements of our new MDP derived from the PGG POMDP are as follows</p>
                <p>• <italic>S</italic> = (α, β)</p>
                <p>• <italic>A</italic> = {<italic>c</italic>, <italic>f</italic>}</p>
                <p>• <inline-formula><mml:math id="m14"><mml:mrow><mml:mrow><mml:mi>T</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>s</mml:mi><mml:mo>′</mml:mo><mml:mo>,</mml:mo><mml:mi>s</mml:mi><mml:mo>,</mml:mo><mml:mi>a</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>:</mml:mo><mml:mo stretchy="true">{</mml:mo><mml:mtable columnalign="left"><mml:mtr><mml:mtd><mml:mrow><mml:mi>P</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="normal">γ</mml:mi><mml:mi mathvariant="normal">α</mml:mi><mml:mo>+</mml:mo><mml:mi>k</mml:mi><mml:mo>′</mml:mo><mml:mo>+</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mi mathvariant="normal">γ</mml:mi><mml:mi mathvariant="normal">β</mml:mi><mml:mo>+</mml:mo><mml:mi>N</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:mi>k</mml:mi><mml:mo>′</mml:mo><mml:mo stretchy="false">)</mml:mo><mml:mo>∣</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="normal">α</mml:mi><mml:mo>,</mml:mo><mml:mi mathvariant="normal">β</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>,</mml:mo><mml:mi>c</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mo stretchy="true">(</mml:mo><mml:mtable><mml:mtr><mml:mtd><mml:mrow><mml:mi>N</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mrow><mml:mi>k</mml:mi><mml:mo>′</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable><mml:mo stretchy="true">)</mml:mo><mml:mfrac><mml:mrow><mml:mi>B</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="normal">γ</mml:mi><mml:mi mathvariant="normal">α</mml:mi><mml:mo>+</mml:mo><mml:mi>k</mml:mi><mml:mo>′</mml:mo><mml:mo>,</mml:mo><mml:mi mathvariant="normal">γ</mml:mi><mml:mi mathvariant="normal">β</mml:mi><mml:mo>+</mml:mo><mml:mi>N</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:mi>k</mml:mi><mml:mo>′</mml:mo><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mrow><mml:mi>B</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="normal">γ</mml:mi><mml:mi mathvariant="normal">α</mml:mi><mml:mo>,</mml:mo><mml:mi mathvariant="normal">γ</mml:mi><mml:mi mathvariant="normal">β</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mfrac></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mrow><mml:mi>P</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="normal">γ</mml:mi><mml:mi mathvariant="normal">α</mml:mi><mml:mo>+</mml:mo><mml:mi>k</mml:mi><mml:mo>′</mml:mo><mml:mo>,</mml:mo><mml:mi mathvariant="normal">γ</mml:mi><mml:mi mathvariant="normal">β</mml:mi><mml:mo>+</mml:mo><mml:mi>N</mml:mi><mml:mo>−</mml:mo><mml:mi>k</mml:mi><mml:mo>′</mml:mo><mml:mo stretchy="false">)</mml:mo><mml:mo>∣</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="normal">α</mml:mi><mml:mo>,</mml:mo><mml:mi mathvariant="normal">β</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>,</mml:mo><mml:mi>f</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mo stretchy="true">(</mml:mo><mml:mtable><mml:mtr><mml:mtd><mml:mrow><mml:mi>N</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mrow><mml:mi>k</mml:mi><mml:mo>′</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable><mml:mo stretchy="true">)</mml:mo><mml:mfrac><mml:mrow><mml:mi>B</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="normal">γ</mml:mi><mml:mi mathvariant="normal">α</mml:mi><mml:mo>+</mml:mo><mml:mi>k</mml:mi><mml:mo>′</mml:mo><mml:mo>,</mml:mo><mml:mi mathvariant="normal">γ</mml:mi><mml:mi mathvariant="normal">β</mml:mi><mml:mo>+</mml:mo><mml:mi>N</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:mi>k</mml:mi><mml:mo>′</mml:mo><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mrow><mml:mi>B</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="normal">γ</mml:mi><mml:mi mathvariant="normal">α</mml:mi><mml:mo>,</mml:mo><mml:mi mathvariant="normal">γ</mml:mi><mml:mi mathvariant="normal">β</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mfrac></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:mrow></mml:math></inline-formula></p>
                <p>• <inline-formula><mml:math id="m15"><mml:mrow><mml:mrow><mml:mi>R</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>s</mml:mi><mml:mo>,</mml:mo><mml:mi>a</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>:</mml:mo><mml:mo stretchy="true">{</mml:mo><mml:mtable columnalign="left"><mml:mtr><mml:mtd><mml:mrow><mml:mi>R</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="normal">α</mml:mi><mml:mo>,</mml:mo><mml:mi mathvariant="normal">β</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>,</mml:mo><mml:mi>c</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mi>E</mml:mi><mml:mo>−</mml:mo><mml:mi>C</mml:mi><mml:mo>+</mml:mo><mml:msubsup><mml:mo>∑</mml:mo><mml:mrow><mml:mi>k</mml:mi><mml:mo>′</mml:mo><mml:mo>=</mml:mo><mml:mi>k</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>N</mml:mi></mml:msubsup><mml:mo stretchy="true">(</mml:mo><mml:mtable><mml:mtr><mml:mtd><mml:mrow><mml:mi>N</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mrow><mml:mi>k</mml:mi><mml:mo>′</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable><mml:mo stretchy="true">)</mml:mo><mml:mfrac><mml:mrow><mml:mi>B</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="normal">α</mml:mi><mml:mo>+</mml:mo><mml:mi>k</mml:mi><mml:mo>′</mml:mo><mml:mo>,</mml:mo><mml:mi mathvariant="normal">β</mml:mi><mml:mo>+</mml:mo><mml:mi>N</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:mi>k</mml:mi><mml:mo>′</mml:mo><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mrow><mml:mi>B</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="normal">α</mml:mi><mml:mo>,</mml:mo><mml:mi mathvariant="normal">β</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mfrac><mml:mi>G</mml:mi></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mrow><mml:mi>R</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="normal">α</mml:mi><mml:mo>,</mml:mo><mml:mi mathvariant="normal">β</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>,</mml:mo><mml:mi>f</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mi>E</mml:mi><mml:mo>+</mml:mo><mml:msubsup><mml:mo>∑</mml:mo><mml:mrow><mml:mi>k</mml:mi><mml:mo>′</mml:mo><mml:mo>=</mml:mo><mml:mi>k</mml:mi></mml:mrow><mml:mi>N</mml:mi></mml:msubsup><mml:mo stretchy="true">(</mml:mo><mml:mtable><mml:mtr><mml:mtd><mml:mrow><mml:mi>N</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mrow><mml:mi>k</mml:mi><mml:mo>′</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable><mml:mo stretchy="true">)</mml:mo><mml:mfrac><mml:mrow><mml:mi>B</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="normal">α</mml:mi><mml:mo>+</mml:mo><mml:mi>k</mml:mi><mml:mo>′</mml:mo><mml:mo>,</mml:mo><mml:mi mathvariant="normal">β</mml:mi><mml:mo>+</mml:mo><mml:mi>N</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:mi>k</mml:mi><mml:mo>′</mml:mo><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mrow><mml:mi>B</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="normal">α</mml:mi><mml:mo>,</mml:mo><mml:mi mathvariant="normal">β</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mfrac><mml:mi>G</mml:mi></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:mrow></mml:math></inline-formula></p>
                <p>B(α, β) is the normalizing constant: <inline-formula><mml:math id="m16"><mml:mrow><mml:mrow><mml:mi mathvariant="normal">B</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="normal">α</mml:mi><mml:mo>,</mml:mo><mml:mi mathvariant="normal">β</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:msubsup><mml:mo>∫</mml:mo><mml:mn>0</mml:mn><mml:mn>1</mml:mn></mml:msubsup><mml:msup><mml:mi mathvariant="normal">θ</mml:mi><mml:mrow><mml:mi mathvariant="normal">α</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:msup><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:mi mathvariant="normal">θ</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mrow><mml:mi mathvariant="normal">β</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:mi>d</mml:mi><mml:mi mathvariant="normal">θ</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula>.</p>
                <p>The POMDP model above assumes that the hidden state, i.e. θ, is a random variable following a Bernoulli distribution, which changes with the actions of all players in each round. These actions serve as samples from this distribution, with α<sub>1</sub> and β<sub>1</sub> being the initial samples. Also, the decay rate γ controls the weights of previous samples. Using maximum likelihood estimation, for any <italic>t</italic>, θ<italic><sub>t</sub></italic> equals α<italic><sub>t</sub></italic>/(α<italic><sub>t</sub></italic> + β<italic><sub>t</sub></italic>). One can also estimate θ in a recursive fashion<disp-formula id="E10"><mml:math id="m17"><mml:mrow><mml:mrow><mml:msub><mml:mi mathvariant="normal">θ</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>←</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="normal">γ</mml:mi><mml:mi mathvariant="normal">α</mml:mi></mml:mrow><mml:mi>t</mml:mi></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="normal">γ</mml:mi><mml:mi mathvariant="normal">β</mml:mi></mml:mrow><mml:mi>t</mml:mi></mml:msub><mml:mo>+</mml:mo><mml:mi>N</mml:mi></mml:mrow></mml:mfrac><mml:mo stretchy="true">(</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="normal">γ</mml:mi><mml:mi mathvariant="normal">α</mml:mi></mml:mrow><mml:mi>t</mml:mi></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="normal">γ</mml:mi><mml:mi mathvariant="normal">β</mml:mi></mml:mrow><mml:mi>t</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:msub><mml:mi mathvariant="normal">θ</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo>+</mml:mo><mml:mstyle displaystyle="true"><mml:munderover><mml:mo movablelimits="false" stretchy="true">∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>N</mml:mi></mml:munderover></mml:mstyle><mml:msubsup><mml:mi>a</mml:mi><mml:mi>i</mml:mi><mml:mi>t</mml:mi></mml:msubsup><mml:mo stretchy="true">)</mml:mo></mml:mrow></mml:mrow></mml:math><label>(10)</label></disp-formula>where <inline-formula><mml:math id="m18"><mml:mrow><mml:mrow><mml:msubsup><mml:mi>a</mml:mi><mml:mi>i</mml:mi><mml:mi>t</mml:mi></mml:msubsup></mml:mrow></mml:mrow></mml:math></inline-formula> is the action of player <italic>i</italic> in round <italic>t</italic> (<inline-formula><mml:math id="m19"><mml:mrow><mml:mrow><mml:msubsup><mml:mi>a</mml:mi><mml:mi>i</mml:mi><mml:mi>t</mml:mi></mml:msubsup><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula> for contribution and 0 for free-ride).</p>
                <p>According to the experiment, the time horizon should be 15 time steps. However, we found that a longer horizon (<italic>H</italic> = 50) for all players provides a better fit to the subjects’ data, potentially reflecting an intrinsic bias in humans for using longer horizons for social decision-making. For each subject, we found α<sub>1</sub>, β<sub>1</sub>, and γ that made our POMDP’s optimal policy fit the subject’s actions as much as possible. For simplicity, we only considered integer values for states (integer α and β). The fitting process involved searching over integer values from 1 to 200 for α<sub>1</sub> and β<sub>1</sub> and values between 0 and 1 with a precision of 0.01 (0.01,0.02, …,0.99,1.0) for γ. The fitting criterion was round-by-round accuracy. For consistency with the descriptive model, the first round was not included (despite the POMDP model’s capability of predicting it). Because the utility value for public good for a subject can be higher than the monetary reward due to social or cultural reasons (<xref rid="R49" ref-type="bibr"><italic>49</italic></xref>), we investigated the effect of higher values for the group reward <italic>G</italic> in the reward function of the POMDP. This, however, did not improve the fit. A preliminary version of the above model but without the γ parameter was presented in (<xref rid="R50" ref-type="bibr"><italic>50</italic></xref>).</p>
                <p>As specified above, the best action for each state in round <italic>t</italic> is <italic>U<sup>t</sup></italic>(<italic>s</italic>). The probability of contribution (choice probability) can be calculated using a logit function: 1/(1 + <italic>exp</italic> (<italic>z</italic>(<italic>Q<sup>t</sup></italic>(<italic>s</italic>, <italic>f</italic>) − <italic>Q<sup>t</sup></italic>(<italic>s</italic>, <italic>c</italic>)) (<xref rid="R19" ref-type="bibr"><italic>19</italic></xref>). For each <italic>k</italic>, we used one free parameter <italic>z</italic> across all subjects to maximize the likelihood of contribution probability given the experimental data [implementation by scikit-learn (<xref rid="R51" ref-type="bibr"><italic>51</italic></xref>)]. Note that the parameter <italic>z</italic> does not affect the accuracy of fits and predictions because it does not affect the action with the maximum expected total reward.</p>
                <p>In round <italic>t</italic>, if the POMDP model selects the action “contribution,” the probability of success can be calculated as <inline-formula><mml:math id="m20"><mml:mrow><mml:mrow><mml:msup><mml:msub><mml:mo>∑</mml:mo><mml:mrow><mml:mi>m</mml:mi><mml:mo>=</mml:mo><mml:mi>k</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mrow><mml:mi>N</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:mi>P</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>m</mml:mi><mml:mo>∣</mml:mo><mml:msub><mml:mi mathvariant="normal">α</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi mathvariant="normal">β</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> (see <xref ref-type="disp-formula" rid="E3">Eq. 3</xref>). Otherwise, the probability of success is <inline-formula><mml:math id="m21"><mml:mrow><mml:mrow><mml:msup><mml:msub><mml:mo>∑</mml:mo><mml:mrow><mml:mi>m</mml:mi><mml:mo>=</mml:mo><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mi>N</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:mi>P</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>m</mml:mi><mml:mo>∣</mml:mo><mml:msub><mml:mi mathvariant="normal">α</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi mathvariant="normal">β</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>. This probability value was compared to the actual success and failure of each round to compute the accuracy of success prediction by the POMDP model.</p>
              </sec>
              <sec disp-level="2">
                <title>Model-free method: Q-learning</title>
                <p>We used Q-learning as our model-free approach. There are two <italic>Q</italic> values in the PGG task, one for each action, i.e., <italic>Q</italic>(c) and <italic>Q</italic>(f) for “contribute” and “free-ride,” respectively. At the beginning of each PGG, <italic>Q</italic>(c) and <italic>Q</italic>(f) are initialized to the expected reward for a subject for that action based on a free parameter <italic>p</italic>, which represents the prior probability of group success. As a result, we have<disp-formula id="E11"><mml:math id="m22"><mml:mrow><mml:mrow><mml:mo stretchy="true">{</mml:mo><mml:mtable><mml:mtr columnalign="left"><mml:mtd><mml:mrow><mml:msup><mml:mi>Q</mml:mi><mml:mn>1</mml:mn></mml:msup><mml:mo stretchy="false">(</mml:mo><mml:mi>c</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>←</mml:mo><mml:mi>p</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>E</mml:mi><mml:mo>−</mml:mo><mml:mi>C</mml:mi><mml:mo>+</mml:mo><mml:mi>G</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>+</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:mi>p</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mi>E</mml:mi><mml:mo>−</mml:mo><mml:mi>C</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mtd></mml:mtr><mml:mtr columnalign="left"><mml:mtd><mml:mrow><mml:msup><mml:mi>Q</mml:mi><mml:mn>1</mml:mn></mml:msup><mml:mo stretchy="false">(</mml:mo><mml:mi>f</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>←</mml:mo><mml:mi>p</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>E</mml:mi><mml:mo>+</mml:mo><mml:mi>G</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>+</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:mi>p</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mi>E</mml:mi></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:mrow></mml:math><label>(11)</label></disp-formula></p>
                <p>We customized the utility function for each subject by making the group reward <italic>G</italic> a free parameter to account for possible prosocial intent (<xref rid="R49" ref-type="bibr"><italic>49</italic></xref>). Moreover, as the probability of success is different for <italic>k</italic> = 2 and <italic>k</italic> = 4, we used two separate parameters <italic>p</italic><sub>2</sub> and <italic>p</italic><sub>4</sub> instead of <italic>p</italic>, depending on the value of <italic>k</italic> in the PGG.</p>
                <p>In each round of the game, the action with the maximum <italic>Q</italic> value was chosen. The <italic>Q</italic> value for that action was then updated on the basis of the subject’s action and group success/failure, with a learning rate η<italic><sup>t</sup></italic>. This learning rate was a function of the round number, i.e., <inline-formula><mml:math id="m23"><mml:mrow><mml:mrow><mml:msup><mml:mi mathvariant="normal">η</mml:mi><mml:mi>t</mml:mi></mml:msup><mml:mo>=</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mrow><mml:msub><mml:mi mathvariant="normal">λ</mml:mi><mml:mn>0</mml:mn></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi mathvariant="normal">λ</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mi>t</mml:mi></mml:mrow></mml:mfrac></mml:mrow></mml:mrow></mml:math></inline-formula>, where λ<sub>0</sub> and λ<sub>1</sub> are free parameters, and <italic>t</italic> is the number of the current round. Let the subject’s action in round <italic>t</italic> be <italic>a<sup>t</sup></italic>, the Q-learning model’s chosen action be <inline-formula><mml:math id="m24"><mml:mrow><mml:mrow><mml:msup><mml:mover accent="true"><mml:mi>a</mml:mi><mml:mo stretchy="true">^</mml:mo></mml:mover><mml:mi>t</mml:mi></mml:msup></mml:mrow></mml:mrow></mml:math></inline-formula>, and the reward obtained be <italic>r<sup>t</sup></italic>. We have<disp-formula id="E12"><mml:math id="m25"><mml:mrow><mml:mrow><mml:mn>1</mml:mn><mml:mo>≤</mml:mo><mml:mi>t</mml:mi><mml:mo>≤</mml:mo><mml:mn>15</mml:mn><mml:mo>:</mml:mo><mml:mo stretchy="true">{</mml:mo><mml:mtable columnalign="left"><mml:mtr><mml:mtd><mml:mrow><mml:msup><mml:mover accent="true"><mml:mi>a</mml:mi><mml:mo stretchy="true">^</mml:mo></mml:mover><mml:mi>t</mml:mi></mml:msup><mml:mo>=</mml:mo><mml:mtext>arg</mml:mtext><mml:mo> </mml:mo><mml:msub><mml:mtext>max</mml:mtext><mml:mrow><mml:mi>a</mml:mi><mml:mo>∈</mml:mo><mml:mo stretchy="false">{</mml:mo><mml:mi>c</mml:mi><mml:mo>,</mml:mo><mml:mi>f</mml:mi><mml:mo stretchy="false">}</mml:mo></mml:mrow></mml:msub><mml:msup><mml:mi>Q</mml:mi><mml:mi>t</mml:mi></mml:msup><mml:mo stretchy="false">(</mml:mo><mml:mi>a</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mrow><mml:msup><mml:mi>Q</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mi>a</mml:mi><mml:mi>t</mml:mi></mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mo>←</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:msup><mml:mi mathvariant="normal">η</mml:mi><mml:mi>t</mml:mi></mml:msup><mml:mo stretchy="false">)</mml:mo><mml:msup><mml:mi>Q</mml:mi><mml:mi>t</mml:mi></mml:msup><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mi>a</mml:mi><mml:mi>t</mml:mi></mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mo>+</mml:mo><mml:msup><mml:mi mathvariant="normal">η</mml:mi><mml:mi>t</mml:mi></mml:msup><mml:msup><mml:mi>r</mml:mi><mml:mi>t</mml:mi></mml:msup></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:mrow></mml:math><label>(12)</label></disp-formula></p>
                <p>For each subject, we searched for the values of λ<sub>0</sub>, λ<sub>1</sub>, the group reward <italic>G</italic>, and the probability of group success <italic>p</italic><sub>2</sub> or <italic>p</italic><sub>4</sub> that maximize the round-by-round accuracy of the Q-learning model. Similar to the other models, the first round was not included in this fitting process.</p>
              </sec>
              <sec disp-level="2">
                <title>Descriptive model</title>
                <p>Our descriptive model was based on a logistic regression [implementation by scikit-learn (<xref rid="R51" ref-type="bibr"><italic>51</italic></xref>)] that predicts the subject’s action in the current round based on their own previous action and the total number of contributions by the others in the previous round. As a result, this model has three free parameters (two features and a bias parameter). Let <inline-formula><mml:math id="m26"><mml:mrow><mml:mrow><mml:msubsup><mml:mi>a</mml:mi><mml:mn>1</mml:mn><mml:mi>t</mml:mi></mml:msubsup></mml:mrow></mml:mrow></mml:math></inline-formula> be the subject’s action in round <italic>t</italic> and <inline-formula><mml:math id="m27"><mml:mrow><mml:mrow><mml:msubsup><mml:mi>a</mml:mi><mml:mrow><mml:mn>2</mml:mn><mml:mo>:</mml:mo><mml:mi>N</mml:mi></mml:mrow><mml:mi>t</mml:mi></mml:msubsup></mml:mrow></mml:mrow></mml:math></inline-formula> be the actions of others in the same round. The subject’s predicted action in the next round <italic>t</italic> + 1 is then given by<disp-formula id="E13"><mml:math id="m28"><mml:mrow><mml:mrow><mml:msubsup><mml:mover accent="true"><mml:mi>a</mml:mi><mml:mo stretchy="true">^</mml:mo></mml:mover><mml:mn>1</mml:mn><mml:mrow><mml:mi>t</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:mo stretchy="true">{</mml:mo><mml:mtable columnalign="left"><mml:mtr><mml:mtd><mml:mi>c</mml:mi></mml:mtd><mml:mtd><mml:mrow><mml:msub><mml:mi mathvariant="normal">κ</mml:mi><mml:mn>0</mml:mn></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi mathvariant="normal">κ</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:msubsup><mml:mi>a</mml:mi><mml:mn>1</mml:mn><mml:mi>t</mml:mi></mml:msubsup><mml:mo>+</mml:mo><mml:msub><mml:mi mathvariant="normal">κ</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo stretchy="true">(</mml:mo><mml:mstyle displaystyle="true"><mml:msubsup><mml:mo stretchy="true">∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>2</mml:mn></mml:mrow><mml:mi>N</mml:mi></mml:msubsup></mml:mstyle><mml:msubsup><mml:mi>a</mml:mi><mml:mi>i</mml:mi><mml:mi>t</mml:mi></mml:msubsup><mml:mo stretchy="true">)</mml:mo><mml:mo>&gt;</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mi>f</mml:mi></mml:mtd><mml:mtd><mml:mtext>otherwise</mml:mtext></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:mrow></mml:math><label>(13)</label></disp-formula></p>
                <p>We used one separate regression model for each subject. As the model’s predicted action is based on the previous round’s actions, the subject’s action in the first round cannot be predicted by this model.</p>
              </sec>
              <sec disp-level="2">
                <title>Leave-one-out cross-validation</title>
                <p>For all three approaches, LOOCV was computed on the basis of the games played by each subject. For each subject, we set aside one game, fitted the parameters to the other 11 games, and computed the error of the model with fitted parameters on the game that was set aside. We repeated this for all games and reported the average of the 12 errors as LOOCV error for the subject.</p>
              </sec>
              <sec disp-level="2">
                <title>Static probability distribution and greedy strategy</title>
                <p>If a player does not consider the future and solely maximizes the expected reward in the current round (greedy strategy) or ignores the effect of an action on others, the optimal action is always free-riding independent of the average probability of contribution by a group member. This is because free-riding always results in one unit more monetary reward (3 MU for success or 1 MU for failure) compared to contribution (2 or 0 MU), except in the case where the total number of contributions by others is exactly <italic>k</italic> − 1. In the latter case, choosing contribution yields one unit more reward (2 MU) compared to free-riding (1 MU). This means that the expected reward for free-riding is always more than that for contribution unless the probability of observing exactly <italic>k</italic> − 1 contributions by others is greater than 0.5. We show that this is impossible for any value of θ. First, note that the probability of exactly <italic>k</italic> − 1 contributions from <italic>N</italic> − 1 players is maximized when θ = (<italic>k</italic> − 1)/(<italic>N</italic> − 1). Next, for any θ, the probability of <italic>k</italic> − 1 contributions from <italic>N</italic> − 1 players is<disp-formula id="E14"><mml:math id="m29"><mml:mrow><mml:mrow><mml:mi>P</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>k</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn><mml:mo>∣</mml:mo><mml:mi mathvariant="normal">θ</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mo stretchy="true">(</mml:mo><mml:mtable columnalign="center"><mml:mtr><mml:mtd><mml:mrow><mml:mi>N</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mrow><mml:mi>k</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mtd></mml:mtr></mml:mtable><mml:mo stretchy="true">)</mml:mo><mml:msup><mml:mi mathvariant="normal">θ</mml:mi><mml:mrow><mml:mi>k</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:msup><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:mi mathvariant="normal">θ</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mrow><mml:mi>N</mml:mi><mml:mo>−</mml:mo><mml:mi>k</mml:mi></mml:mrow></mml:msup><mml:mo>≤</mml:mo><mml:mo stretchy="true">(</mml:mo><mml:mtable columnalign="center" displaystyle="true"><mml:mtr><mml:mtd><mml:mrow><mml:mi>N</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mrow><mml:mi>k</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mtd></mml:mtr></mml:mtable><mml:mo stretchy="true">)</mml:mo><mml:msup><mml:mrow><mml:mo stretchy="true">(</mml:mo><mml:mfrac><mml:mrow><mml:mi>k</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>N</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mfrac><mml:mo stretchy="true">)</mml:mo></mml:mrow><mml:mrow><mml:mi>k</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:msup><mml:mrow><mml:mo stretchy="true">(</mml:mo><mml:mfrac><mml:mrow><mml:mi>N</mml:mi><mml:mo>−</mml:mo><mml:mi>k</mml:mi></mml:mrow><mml:mrow><mml:mi>N</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mfrac><mml:mo stretchy="true">)</mml:mo></mml:mrow><mml:mrow><mml:mi>N</mml:mi><mml:mo>−</mml:mo><mml:mi>k</mml:mi></mml:mrow></mml:msup><mml:mo>=</mml:mo><mml:msup><mml:mn>0.75</mml:mn><mml:mn>3</mml:mn></mml:msup><mml:mo>&lt;</mml:mo><mml:mn>0.5</mml:mn></mml:mrow></mml:mrow></mml:math><label>(14)</label></disp-formula>for <italic>N</italic> = 5 and for either <italic>k</italic> = 2 or <italic>k</italic> = 4.</p>
              </sec>
            </sec>
            <sec sec-type="supplementary-material">
              <title>Supplementary Material</title>
              <supplementary-material id="PMC_1" content-type="local-data">
                <caption>
                  <title>http://advances.sciencemag.org/cgi/content/full/5/11/eaax8783/DC1</title>
                </caption>
                <media mimetype="text" mime-subtype="html" xlink:href="supp_5_11_eaax8783__index.html"/>
              </supplementary-material>
              <supplementary-material id="PMC_2" content-type="local-data">
                <caption>
                  <title>Download PDF</title>
                </caption>
                <media mimetype="application" mime-subtype="pdf" xlink:href="aax8783_SM.pdf"/>
              </supplementary-material>
              <supplementary-material id="PMC_3" content-type="local-data">
                <caption>
                  <title>Modeling other minds: Bayesian inference explains human choices in group decision-making</title>
                </caption>
                <media mimetype="text" mime-subtype="html" xlink:href="supp_5_11_eaax8783__index.html"/>
              </supplementary-material>
            </sec>
          </body>
          <back>
            <ack>
              <title>Acknowledgments</title>
              <p><bold>Funding:</bold> This work was funded by the NSF-ANR Collaborative Research in Computational Neuroscience “CRCNS SOCIAL POMDP” <italic>n°</italic>16-NEUC to J.-C.D. and CRCNS NIMH grant no. 5R01MH112166-03, NSF grant no. EEC-1028725, and a Templeton World Charity Foundation grant to R.P.N.R. The experiments were performed within the framework of the Laboratory of Excellence “LABEX ANR-11-LABEX-0042” of Universite de Lyon, attributed to J.-C.D., within the program “Investissements d’Avenir” (ANR-11-IDEX-0007) operated by the French National Research Agency (ANR). J.-C.D. was also funded by the IDEX University Lyon 1 (project INDEPTH). <bold>Author contributions:</bold> R.P.N.R. and J.-C.D. developed the general research concept. S.A.P. designed and programmed the task under the supervision of J.-C.D., and M.S. ran the experiment under the supervision of J.-C.D. K.K. developed the model under the supervision of R.P.N.R., implemented the algorithms, and analyzed the data in collaboration with R.P.N.R. S.M. interpreted the computational results in the context of social neuroscience. K.K. developed the reinforcement learning model after discussions with R.P. K.K., S.M., and R.P.N.R. wrote the manuscript in collaboration with S.A.P., R.P., and J.-C.D. <bold>Competing interests:</bold> The authors declare that they have no competing interests. <bold>Data and materials availability:</bold> Both data and code are available upon request from the corresponding author. All data needed to evaluate the conclusions in the paper are present in the paper and/or the Supplementary Materials. Additional data related to this paper may be requested from the authors.</p>
            </ack>
            <app-group>
              <app>
                <title>SUPPLEMENTARY MATERIALS</title>
                <supplementary-material content-type="local-data">
                  <p>Supplementary material for this article is available at <ext-link ext-link-type="uri" xlink:href="http://advances.sciencemag.org/cgi/content/full/5/11/eaax8783/DC1">http://advances.sciencemag.org/cgi/content/full/5/11/eaax8783/DC1</ext-link></p>
                  <p>Supplementary Text</p>
                  <p>Fig. S1. Distribution and change in belief parameters over multiple rounds.</p>
                  <p>Fig. S2. Data generated by the POMDP model compared to experimental data.</p>
                </supplementary-material>
              </app>
              <app>
                <p><ext-link ext-link-type="uri" xlink:href="https://en.bio-protocol.org/rap.aspx?eid=10.1126/sciadv.aax8783">View/request a protocol for this paper from <italic>Bio-protocol</italic></ext-link>.</p>
              </app>
            </app-group>
            <ref-list>
              <title>REFERENCES AND NOTES</title>
              <ref id="R1">
                <label>1</label>
                <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Sanfey</surname><given-names>A. G.</given-names></name></person-group>, 
<article-title>Social decision-making: Insights from game theory and neuroscience</article-title>. <source>Science</source><volume>318</volume>, 
<fpage>598</fpage>–<lpage>602</lpage> (<year>2007</year>).<pub-id pub-id-type="pmid">17962552</pub-id></mixed-citation>
              </ref>
              <ref id="R2">
                <label>2</label>
                <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Joiner</surname><given-names>J.</given-names></name>, <name name-style="western"><surname>Piva</surname><given-names>M.</given-names></name>, <name name-style="western"><surname>Turrin</surname><given-names>C.</given-names></name>, <name name-style="western"><surname>Chang</surname><given-names>S. W. C.</given-names></name></person-group>, 
<article-title>Social learning through prediction error in the brain</article-title>. <source>npj Sci. Learn.</source><volume>2</volume>, 
<fpage>8</fpage> (<year>2017</year>).<pub-id pub-id-type="pmid">30631454</pub-id></mixed-citation>
              </ref>
              <ref id="R3">
                <label>3</label>
                <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Mookherjee</surname><given-names>D.</given-names></name>, <name name-style="western"><surname>Sopher</surname><given-names>B.</given-names></name></person-group>, 
<article-title>Learning and decision costs in experimental constant sum games</article-title>. <source>Games Econ. Behav.</source><volume>19</volume>, 
<fpage>97</fpage>–<lpage>132</lpage> (<year>1997</year>).</mixed-citation>
              </ref>
              <ref id="R4">
                <label>4</label>
                <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Camerer</surname><given-names>C. F.</given-names></name>, <name name-style="western"><surname>Ho</surname><given-names>T. H.</given-names></name></person-group>, 
<article-title>Experience-weighted attraction learning in normal form games</article-title>. <source>Econometrica</source><volume>67</volume>, 
<fpage>827</fpage>–<lpage>874</lpage> (<year>1999</year>).</mixed-citation>
              </ref>
              <ref id="R5">
                <label>5</label>
                <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Friston</surname><given-names>K.</given-names></name>, <name name-style="western"><surname>FitzGerald</surname><given-names>T.</given-names></name>, <name name-style="western"><surname>Rigoli</surname><given-names>F.</given-names></name>, <name name-style="western"><surname>Schwartenbeck</surname><given-names>P.</given-names></name>, <name name-style="western"><surname>O’Doherty</surname><given-names>J.</given-names></name>, <name name-style="western"><surname>Pezzulo</surname><given-names>G.</given-names></name></person-group>, 
<article-title>Active inference and learning</article-title>. <source>Neurosci. Biobehav. Rev.</source><volume>68</volume>, 
<fpage>862</fpage>–<lpage>879</lpage> (<year>2016</year>).<pub-id pub-id-type="pmid">27375276</pub-id></mixed-citation>
              </ref>
              <ref id="R6">
                <label>6</label>
                <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Dayan</surname><given-names>P.</given-names></name>, <name name-style="western"><surname>Daw</surname><given-names>N. D.</given-names></name></person-group>, 
<article-title>Decision theory, reinforcement learning, and the brain</article-title>. <source>Cogn. Affect. Behav. Neurosci.</source><volume>8</volume>, 
<fpage>429</fpage>–<lpage>453</lpage> (<year>2008</year>).<pub-id pub-id-type="pmid">19033240</pub-id></mixed-citation>
              </ref>
              <ref id="R7">
                <label>7</label>
                <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Daw</surname><given-names>N. D.</given-names></name>, <name name-style="western"><surname>Dayan</surname><given-names>P.</given-names></name></person-group>, 
<article-title>The algorithmic anatomy of model-based evaluation</article-title>. <source>Philos. Trans. R. Soc. B</source><volume>369</volume>, 
<fpage>20130478</fpage> (<year>2014</year>).</mixed-citation>
              </ref>
              <ref id="R8">
                <label>8</label>
                <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Daw</surname><given-names>N. D.</given-names></name>, <name name-style="western"><surname>Gershman</surname><given-names>S. J.</given-names></name>, <name name-style="western"><surname>Seymour</surname><given-names>B.</given-names></name>, <name name-style="western"><surname>Dayan</surname><given-names>P.</given-names></name>, <name name-style="western"><surname>Dolan</surname><given-names>R. J.</given-names></name></person-group>, 
<article-title>Model-based influences on humans’ choices and striatal prediction errors</article-title>. <source>Neuron</source><volume>69</volume>, 
<fpage>1204</fpage>–<lpage>1215</lpage> (<year>2011</year>).<pub-id pub-id-type="pmid">21435563</pub-id></mixed-citation>
              </ref>
              <ref id="R9">
                <label>9</label>
                <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Culbreth</surname><given-names>A. J.</given-names></name>, <name name-style="western"><surname>Westbrook</surname><given-names>A.</given-names></name>, <name name-style="western"><surname>Daw</surname><given-names>N. D.</given-names></name>, <name name-style="western"><surname>Botvinick</surname><given-names>M.</given-names></name>, <name name-style="western"><surname>Barch</surname><given-names>D. M.</given-names></name></person-group>, 
<article-title>Reduced model-based decision-making in schizophrenia</article-title>. <source>J. Abnorm. Psychol.</source><volume>125</volume>, 
<fpage>777</fpage>–<lpage>787</lpage> (<year>2016</year>).<pub-id pub-id-type="pmid">27175984</pub-id></mixed-citation>
              </ref>
              <ref id="R10">
                <label>10</label>
                <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Doll</surname><given-names>B. B.</given-names></name>, <name name-style="western"><surname>Simon</surname><given-names>D. A.</given-names></name>, <name name-style="western"><surname>Daw</surname><given-names>N. D.</given-names></name></person-group>, 
<article-title>The ubiquity of model-based reinforcement learning</article-title>. <source>Curr. Opin. Neurobiol.</source><volume>22</volume>, 
<fpage>1075</fpage>–<lpage>1081</lpage> (<year>2012</year>).<pub-id pub-id-type="pmid">22959354</pub-id></mixed-citation>
              </ref>
              <ref id="R11">
                <label>11</label>
                <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Dolan</surname><given-names>R. J.</given-names></name>, <name name-style="western"><surname>Dayan</surname><given-names>P.</given-names></name></person-group>, 
<article-title>Goals and habits in the brain</article-title>. <source>Neuron</source><volume>80</volume>, 
<fpage>312</fpage>–<lpage>325</lpage> (<year>2013</year>).<pub-id pub-id-type="pmid">24139036</pub-id></mixed-citation>
              </ref>
              <ref id="R12">
                <label>12</label>
                <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Charpentier</surname><given-names>C. J.</given-names></name>, <name name-style="western"><surname>ODoherty</surname><given-names>J. P.</given-names></name></person-group>, 
<article-title>The application of computational models to social neuroscience: Promises and pitfalls</article-title>. <source>Soc. Neurosci.</source><volume>13</volume>, 
<fpage>637</fpage>–<lpage>647</lpage> (<year>2018</year>).<pub-id pub-id-type="pmid">30173633</pub-id></mixed-citation>
              </ref>
              <ref id="R13">
                <label>13</label>
                <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Yoshida</surname><given-names>W.</given-names></name>, <name name-style="western"><surname>Seymour</surname><given-names>B.</given-names></name>, <name name-style="western"><surname>Friston</surname><given-names>K. J.</given-names></name>, <name name-style="western"><surname>Dolan</surname><given-names>R. J.</given-names></name></person-group>, 
<article-title>Neural mechanisms of belief inference during cooperative games</article-title>. <source>J. Neurosci.</source><volume>30</volume>, 
<fpage>10744</fpage>–<lpage>10751</lpage> (<year>2010</year>).<pub-id pub-id-type="pmid">20702705</pub-id></mixed-citation>
              </ref>
              <ref id="R14">
                <label>14</label>
                <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Xiang</surname><given-names>T.</given-names></name>, <name name-style="western"><surname>Ray</surname><given-names>D.</given-names></name>, <name name-style="western"><surname>Lohrenz</surname><given-names>T.</given-names></name>, <name name-style="western"><surname>Dayan</surname><given-names>P.</given-names></name>, <name name-style="western"><surname>Montague</surname><given-names>P. R.</given-names></name></person-group>, 
<article-title>Computational phenotyping of two-person interactions reveals differential neural response to depth-of-thought</article-title>. <source>PLOS Comput. Biol.</source><volume>8</volume>, 
<fpage>e1002841</fpage> (<year>2012</year>).<pub-id pub-id-type="pmid">23300423</pub-id></mixed-citation>
              </ref>
              <ref id="R15">
                <label>15</label>
                <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Moutoussis</surname><given-names>M.</given-names></name>, <name name-style="western"><surname>Fearon</surname><given-names>P.</given-names></name>, <name name-style="western"><surname>El-Deredy</surname><given-names>W.</given-names></name>, <name name-style="western"><surname>Dolan</surname><given-names>R. J.</given-names></name>, <name name-style="western"><surname>Friston</surname><given-names>K. J.</given-names></name></person-group>, 
<article-title>Bayesian inferences about the self (and others): A review</article-title>. <source>Conscious. Cogn.</source><volume>25</volume>, 
<fpage>67</fpage>–<lpage>76</lpage> (<year>2014</year>).<pub-id pub-id-type="pmid">24583455</pub-id></mixed-citation>
              </ref>
              <ref id="R16">
                <label>16</label>
                <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Moutoussis</surname><given-names>M.</given-names></name>, <name name-style="western"><surname>Trujillo-Barreto</surname><given-names>N. J.</given-names></name>, <name name-style="western"><surname>El-Deredy</surname><given-names>W.</given-names></name>, <name name-style="western"><surname>Dolan</surname><given-names>R. J.</given-names></name>, <name name-style="western"><surname>Friston</surname><given-names>K. J.</given-names></name></person-group>, 
<article-title>A formal model of interpersonal inference</article-title>. <source>Front. Hum. Neurosci.</source><volume>8</volume>, 
<fpage>160</fpage> (<year>2014</year>).<pub-id pub-id-type="pmid">24723872</pub-id></mixed-citation>
              </ref>
              <ref id="R17">
                <label>17</label>
                <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Hula</surname><given-names>A.</given-names></name>, <name name-style="western"><surname>Montague</surname><given-names>P. R.</given-names></name>, <name name-style="western"><surname>Dayan</surname><given-names>P.</given-names></name></person-group>, 
<article-title>Monte carlo planning method estimates planning horizons during interactive social exchange</article-title>. <source>PLoS Comput. Biol.</source><volume>11</volume>, 
<fpage>e1004254</fpage> (<year>2015</year>).<pub-id pub-id-type="pmid">26053429</pub-id></mixed-citation>
              </ref>
              <ref id="R18">
                <label>18</label>
                <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Baker</surname><given-names>C. L.</given-names></name>, <name name-style="western"><surname>Jara-Ettinger</surname><given-names>J.</given-names></name>, <name name-style="western"><surname>Saxe</surname><given-names>R.</given-names></name>, <name name-style="western"><surname>Tenenbaum</surname><given-names>J. B.</given-names></name></person-group>, 
<article-title>Rational quantitative attribution of beliefs, desires and percepts in human mentalizing</article-title>. <source>Nat. Hum. Behav.</source><volume>1</volume>, 
<fpage>0064</fpage> (<year>2017</year>).</mixed-citation>
              </ref>
              <ref id="R19">
                <label>19</label>
                <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Suzuki</surname><given-names>S.</given-names></name>, <name name-style="western"><surname>Adachi</surname><given-names>R.</given-names></name>, <name name-style="western"><surname>Dunne</surname><given-names>S.</given-names></name>, <name name-style="western"><surname>Bossaerts</surname><given-names>P.</given-names></name>, <name name-style="western"><surname>O’Doherty</surname><given-names>J. P.</given-names></name></person-group>, 
<article-title>Neural mechanisms underlying human consensus decision-making</article-title>. <source>Neuron</source><volume>86</volume>, 
<fpage>591</fpage>–<lpage>602</lpage> (<year>2015</year>).<pub-id pub-id-type="pmid">25864634</pub-id></mixed-citation>
              </ref>
              <ref id="R20">
                <label>20</label>
                <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Park</surname><given-names>S. A.</given-names></name>, <name name-style="western"><surname>Goïame</surname><given-names>S.</given-names></name>, <name name-style="western"><surname>O’Connor</surname><given-names>D. A.</given-names></name>, <name name-style="western"><surname>Dreher</surname><given-names>J.-C.</given-names></name></person-group>, 
<article-title>Integration of individual and social information for decision-making in groups of different sizes</article-title>. <source>PLOS Biol.</source><volume>15</volume>, 
<fpage>e2001958</fpage> (<year>2017</year>).<pub-id pub-id-type="pmid">28658252</pub-id></mixed-citation>
              </ref>
              <ref id="R21">
                <label>21</label>
                <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Diekmann</surname><given-names>D.</given-names></name></person-group>, 
<article-title>Volunteer’s dilemma</article-title>. <source>J. Confl. Resolut.</source><volume>29</volume>, 
<fpage>605</fpage>–<lpage>610</lpage> (<year>1985</year>).</mixed-citation>
              </ref>
              <ref id="R22">
                <label>22</label>
                <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Darley</surname><given-names>J. M.</given-names></name>, <name name-style="western"><surname>Latane</surname><given-names>B.</given-names></name></person-group>, 
<article-title>Bystander intervention in emergencies: Diffusion of responsibility</article-title>. <source>J. Pers. Soc. Psychol.</source><volume>8</volume>, 
<fpage>377</fpage>–<lpage>383</lpage> (<year>1968</year>).<pub-id pub-id-type="pmid">5645600</pub-id></mixed-citation>
              </ref>
              <ref id="R23">
                <label>23</label>
                <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Archetti</surname><given-names>M.</given-names></name>, <name name-style="western"><surname>Scheuring</surname><given-names>I.</given-names></name></person-group>, 
<article-title>Coexistence of cooperation and defection in public goods games</article-title>. <source>Evolution</source><volume>65</volume>, 
<fpage>1140</fpage>–<lpage>1148</lpage> (<year>2011</year>).<pub-id pub-id-type="pmid">21062277</pub-id></mixed-citation>
              </ref>
              <ref id="R24">
                <label>24</label>
                <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Kaelbling</surname><given-names>L. P.</given-names></name>, <name name-style="western"><surname>Littman</surname><given-names>M. L.</given-names></name>, <name name-style="western"><surname>Cassandra</surname><given-names>A. R.</given-names></name></person-group>, 
<article-title>Planning and acting in partially observable stochastic domains</article-title>. <source>Artif. Intell.</source><volume>101</volume>, 
<fpage>99</fpage>–<lpage>134</lpage> (<year>1998</year>).</mixed-citation>
              </ref>
              <ref id="R25">
                <label>25</label>
                <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Fehr</surname><given-names>E.</given-names></name>, <name name-style="western"><surname>Gachter</surname><given-names>S.</given-names></name></person-group>, 
<article-title>Cooperation and punishment in public goods experiments</article-title>. <source>Am. Econ. Rev.</source><volume>90</volume>, 
<fpage>980</fpage>–<lpage>994</lpage> (<year>2000</year>).</mixed-citation>
              </ref>
              <ref id="R26">
                <label>26</label>
                <mixed-citation publication-type="book">G. W. Brown, Iterative solution of games by fictitious play, in <italic>Activity Analysis of Production and Allocation</italic>, T. C. Koopmans, Ed. (Wiley, 1951), pp. 374–376.</mixed-citation>
              </ref>
              <ref id="R27">
                <label>27</label>
                <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Costa-Gomes</surname><given-names>M.</given-names></name>, <name name-style="western"><surname>Crawford</surname><given-names>V. P.</given-names></name>, <name name-style="western"><surname>Broseta</surname><given-names>B.</given-names></name></person-group>, 
<article-title>Cognition and behavior in normal-form games: An experimental study</article-title>. <source>Econometrica</source><volume>69</volume>, 
<fpage>1193</fpage>–<lpage>1235</lpage> (<year>2001</year>).</mixed-citation>
              </ref>
              <ref id="R28">
                <label>28</label>
                <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Devaine</surname><given-names>M.</given-names></name>, <name name-style="western"><surname>Hollard</surname><given-names>G.</given-names></name>, <name name-style="western"><surname>Daunizeau</surname><given-names>J.</given-names></name></person-group>, 
<article-title>Theory of mind: Did evolution fool us?</article-title><source>PLOS ONE</source><volume>9</volume>, 
<fpage>e87619</fpage> (<year>2014</year>).<pub-id pub-id-type="pmid">24505296</pub-id></mixed-citation>
              </ref>
              <ref id="R29">
                <label>29</label>
                <mixed-citation publication-type="book">K. P. Murphy, Machine learning: A probabilistic perspective, in <italic>Adaptive Computation and Machine Learning</italic> (MIT Press, 2012).</mixed-citation>
              </ref>
              <ref id="R30">
                <label>30</label>
                <mixed-citation publication-type="book">S. Thrun, W. Burgard, D. Fox, <italic>Probabilistic Robotics</italic> (MIT Press, 2005).</mixed-citation>
              </ref>
              <ref id="R31">
                <label>31</label>
                <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Tsitsiklis</surname><given-names>J. N.</given-names></name></person-group>, 
<article-title>Asynchronous stochastic approximation and Q-learning</article-title>. <source>Mach. Learn.</source><volume>16</volume>, 
<fpage>185</fpage>–<lpage>202</lpage> (<year>1994</year>).</mixed-citation>
              </ref>
              <ref id="R32">
                <label>32</label>
                <mixed-citation publication-type="book">M. Wunder, S. Suri, D. J. Watts. Empirical agent based models of cooperation in public goods games, in <italic>Proceedings of the Fourteenth ACM Conference on Electronic Commerce (EC)</italic> (ACM, 2013), pp. 891–908.</mixed-citation>
              </ref>
              <ref id="R33">
                <label>33</label>
                <mixed-citation publication-type="book">P. J. Gmytrasiewicz, P. Doshi, Interactive POMDPs: Properties and preliminary results, in <italic>Proceedings of the Third International Joint Conference on Autonomous Agents and Multiagent Systems-Volume 3</italic> (IEEE Computer Society, 2004), pp. 1374–1375.</mixed-citation>
              </ref>
              <ref id="R34">
                <label>34</label>
                <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Momennejad</surname><given-names>I.</given-names></name>, <name name-style="western"><surname>Russek</surname><given-names>E. M.</given-names></name>, <name name-style="western"><surname>Cheong</surname><given-names>J. H.</given-names></name>, <name name-style="western"><surname>Botvinick</surname><given-names>M. M.</given-names></name>, <name name-style="western"><surname>Daw</surname><given-names>N. D.</given-names></name>, <name name-style="western"><surname>Gershman</surname><given-names>S. J.</given-names></name></person-group>, 
<article-title>The successor representation in human reinforcement learning</article-title>. <source>Nat. Hum. Behav.</source><volume>1</volume>, 
<fpage>680</fpage>–<lpage>692</lpage> (<year>2017</year>).<pub-id pub-id-type="pmid">31024137</pub-id></mixed-citation>
              </ref>
              <ref id="R35">
                <label>35</label>
                <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Russek</surname><given-names>E. M.</given-names></name>, <name name-style="western"><surname>Momennejad</surname><given-names>I.</given-names></name>, <name name-style="western"><surname>Botvinick</surname><given-names>M. M.</given-names></name>, <name name-style="western"><surname>Gershman</surname><given-names>S. J.</given-names></name>, <name name-style="western"><surname>Daw</surname><given-names>N. D.</given-names></name></person-group>, 
<article-title>Predictive representations can link model-based reinforcement learning to model-free mechanisms</article-title>. <source>PLOS Comput. Biol.</source><volume>13</volume>, 
<fpage>e1005768</fpage> (<year>2017</year>).<pub-id pub-id-type="pmid">28945743</pub-id></mixed-citation>
              </ref>
              <ref id="R36">
                <label>36</label>
                <mixed-citation publication-type="confproc">H. Attias, Planning by probabilistic inference, in <italic>Proceedings of the 9th International Workshop on Artificial Intelligence and Statistics</italic>, Key West, FL, 3 to 6 January 2003.</mixed-citation>
              </ref>
              <ref id="R37">
                <label>37</label>
                <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Tamir</surname><given-names>D. I.</given-names></name>, <name name-style="western"><surname>Thornton</surname><given-names>M. A.</given-names></name></person-group>, 
<article-title>Modeling the predictive social mind</article-title>. <source>Trends Cogn. Sci.</source><volume>22</volume>, 
<fpage>201</fpage>–<lpage>212</lpage> (<year>2018</year>).<pub-id pub-id-type="pmid">29361382</pub-id></mixed-citation>
              </ref>
              <ref id="R38">
                <label>38</label>
                <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Tamir</surname><given-names>D. I.</given-names></name>, <name name-style="western"><surname>Thornton</surname><given-names>M. A.</given-names></name>, <name name-style="western"><surname>Contreras</surname><given-names>J. M.</given-names></name>, <name name-style="western"><surname>Mitchell</surname><given-names>J. P.</given-names></name></person-group>, 
<article-title>Neural evidence that three dimensions organize mental state representation: Rationality, social impact, and valence</article-title>. <source>Proc. Natl. Acad. Sci. U.S.A.</source><volume>113</volume>, 
<fpage>194</fpage>–<lpage>199</lpage> (<year>2016</year>).<pub-id pub-id-type="pmid">26621704</pub-id></mixed-citation>
              </ref>
              <ref id="R39">
                <label>39</label>
                <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Rao</surname><given-names>R. P. N.</given-names></name></person-group>, 
<article-title>Decision making under uncertainty: A neural model based on partially observable Markov decision processes</article-title>. <source>Front. Comput. Neurosci.</source><volume>4</volume>, 
<fpage>146</fpage> (<year>2010</year>).<pub-id pub-id-type="pmid">21152255</pub-id></mixed-citation>
              </ref>
              <ref id="R40">
                <label>40</label>
                <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Huang</surname><given-names>Y.</given-names></name>, <name name-style="western"><surname>Rao</surname><given-names>R. P. N.</given-names></name></person-group>, 
<article-title>Reward optimization in the primate brain: A probabilistic model of decision making under uncertainty</article-title>. <source>PLOS ONE</source><volume>8</volume>, 
<fpage>e53344</fpage> (<year>2013</year>).<pub-id pub-id-type="pmid">23349707</pub-id></mixed-citation>
              </ref>
              <ref id="R41">
                <label>41</label>
                <mixed-citation publication-type="confproc">K. Khalvati, R. P. Rao, A Bayesian framework for modeling confidence in perceptual decision making, in <italic>Advances in Neural Information Processing Systems</italic>, Montreal, Quebec, Canada, 7 to 12 December 2015, pp. 2413–2421.</mixed-citation>
              </ref>
              <ref id="R42">
                <label>42</label>
                <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Huys</surname><given-names>Q. J. M.</given-names></name>, <name name-style="western"><surname>Maia</surname><given-names>T. V.</given-names></name>, <name name-style="western"><surname>Frank</surname><given-names>M. J.</given-names></name></person-group>, 
<article-title>Computational psychiatry as a bridge from neuroscience to clinical applications</article-title>. <source>Nat. Neurosci.</source><volume>19</volume>, 
<fpage>404</fpage>–<lpage>413</lpage> (<year>2016</year>).<pub-id pub-id-type="pmid">26906507</pub-id></mixed-citation>
              </ref>
              <ref id="R43">
                <label>43</label>
                <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Baron-Cohen</surname><given-names>S.</given-names></name>, <name name-style="western"><surname>Wheelwright</surname><given-names>S.</given-names></name>, <name name-style="western"><surname>Hill</surname><given-names>J.</given-names></name>, <name name-style="western"><surname>Raste</surname><given-names>Y.</given-names></name>, <name name-style="western"><surname>Plumb</surname><given-names>I.</given-names></name></person-group>, 
<article-title>The “reading the mind in the eyes” test revised version: A study with normal adults, and adults with Asperger syndrome or high-functioning autism</article-title>. <source>J. Child Psychol. Psychiatry</source><volume>42</volume>, 
<fpage>241</fpage>–<lpage>251</lpage> (<year>2001</year>).<pub-id pub-id-type="pmid">11280420</pub-id></mixed-citation>
              </ref>
              <ref id="R44">
                <label>44</label>
                <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Schwartenbeck</surname><given-names>P.</given-names></name>, <name name-style="western"><surname>Friston</surname><given-names>K.</given-names></name></person-group>, 
<article-title>Computational phenotyping in psychiatry: A worked example</article-title>. <source>eNeuro</source><volume>3</volume>, 
<fpage>ENEURO.0049-16.2016</fpage> (<year>2016</year>).</mixed-citation>
              </ref>
              <ref id="R45">
                <label>45</label>
                <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Park</surname><given-names>S. A.</given-names></name>, <name name-style="western"><surname>Jeong</surname><given-names>S.</given-names></name>, <name name-style="western"><surname>Jeong</surname><given-names>J.</given-names></name></person-group>, 
<article-title>TV programs that denounce unfair advantage impact women’s sensitivity to defection in the public goods game</article-title>. <source>Soc. Neurosci.</source><volume>8</volume>, 
<fpage>568</fpage>–<lpage>582</lpage> (<year>2013</year>).<pub-id pub-id-type="pmid">24047315</pub-id></mixed-citation>
              </ref>
              <ref id="R46">
                <label>46</label>
                <mixed-citation publication-type="book">K. Khalvati, A. K. Mackworth, A fast pairwise heuristic for planning under uncertainty, in <italic>Proceedings of The Twenty-Seventh AAAI Conference on Artificial Intelligence</italic> (Association for the Advancement of Artificial Intelligence, 2013), pp. 187–193.</mixed-citation>
              </ref>
              <ref id="R47">
                <label>47</label>
                <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Shani</surname><given-names>G.</given-names></name>, <name name-style="western"><surname>Pineau</surname><given-names>J.</given-names></name>, <name name-style="western"><surname>Kaplow</surname><given-names>R.</given-names></name></person-group>, 
<article-title>A survey of point-based POMDP solvers</article-title>. <source>Auton. Agent. Multi-Agent Syst.</source><volume>27</volume>, 
<fpage>1</fpage>–<lpage>51</lpage> (<year>2013</year>).</mixed-citation>
              </ref>
              <ref id="R48">
                <label>48</label>
                <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Luo</surname><given-names>Y.</given-names></name>, <name name-style="western"><surname>Bai</surname><given-names>H.</given-names></name>, <name name-style="western"><surname>Hsu</surname><given-names>D.</given-names></name>, <name name-style="western"><surname>Lee</surname><given-names>W. S.</given-names></name></person-group>, 
<article-title>Importance sampling for online planning under uncertainty</article-title>. <source>Int. J. Robot. Res.</source><volume>38</volume>, 
<fpage>162</fpage>–<lpage>181</lpage> (<year>2018</year>).</mixed-citation>
              </ref>
              <ref id="R49">
                <label>49</label>
                <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Fehr</surname><given-names>E.</given-names></name>, <name name-style="western"><surname>Fischbacher</surname><given-names>U.</given-names></name>, <name name-style="western"><surname>Gächter</surname><given-names>S.</given-names></name></person-group>, 
<article-title>Strong reciprocity, human cooperation, and the enforcement of social norms</article-title>. <source>Hum. Nat.</source><volume>13</volume>, 
<fpage>1</fpage>–<lpage>25</lpage> (<year>2002</year>).<pub-id pub-id-type="pmid">26192593</pub-id></mixed-citation>
              </ref>
              <ref id="R50">
                <label>50</label>
                <mixed-citation publication-type="confproc">K. Khalvati, S. A. Park, J.-C. Dreher, R. P. Rao, A probabilistic model of social decision making based on reward maximization, in <italic>Advances in Neural Information Processing Systems</italic>, Barcelona, Spain, 5 to 10 December 2016, pp. 2901–2909.</mixed-citation>
              </ref>
              <ref id="R51">
                <label>51</label>
                <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Pedregosa</surname><given-names>F.</given-names></name>, <name name-style="western"><surname>Varoquaux</surname><given-names>G.</given-names></name>, <name name-style="western"><surname>Gramfort</surname><given-names>A.</given-names></name>, <name name-style="western"><surname>Michel</surname><given-names>V.</given-names></name>, <name name-style="western"><surname>Thirion</surname><given-names>B.</given-names></name>, <name name-style="western"><surname>Grisel</surname><given-names>O.</given-names></name>, <name name-style="western"><surname>Blondel</surname><given-names>M.</given-names></name>, <name name-style="western"><surname>Prettenhofer</surname><given-names>P.</given-names></name>, <name name-style="western"><surname>Weiss</surname><given-names>R.</given-names></name>, <name name-style="western"><surname>Dubourg</surname><given-names>V.</given-names></name>, <name name-style="western"><surname>Vanderplas</surname><given-names>J.</given-names></name>, <name name-style="western"><surname>Passos</surname><given-names>A.</given-names></name>, <name name-style="western"><surname>Cournapeau</surname><given-names>D.</given-names></name></person-group>, 
<article-title>Scikit-learn: Machine learning in python</article-title>. <source>J. Mach. Learn. Res.</source><volume>12</volume>, 
<fpage>2825</fpage>–<lpage>2830</lpage> (<year>2011</year>).</mixed-citation>
              </ref>
              <ref id="R52">
                <label>52</label>
                <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Cousineau</surname><given-names>D.</given-names></name></person-group>, 
<article-title>Confidence intervals in within-subject designs: A simpler solution to Loftus and Masson’s method</article-title>. <source>Tutor. Quant. Methods Psychol.</source><volume>1</volume>, 
<fpage>42</fpage>–<lpage>45</lpage> (<year>2005</year>).</mixed-citation>
              </ref>
            </ref-list>
          </back>
        </article>
      </metadata>
    </record>
  </GetRecord>
</OAI-PMH>
