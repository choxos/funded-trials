<?xml version="1.0" encoding="UTF-8"?>
<OAI-PMH xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/ http://www.openarchives.org/OAI/2.0/OAI-PMH.xsd">
  <responseDate>2024-05-03T05:05:04Z</responseDate>
  <request verb="GetRecord" identifier="oai:pubmedcentral.nih.gov:4624806" metadataPrefix="pmc">https:/www.ncbi.nlm.nih.gov/pmc/oai/oai.cgi</request>
  <GetRecord>
    <record>
      <header>
        <identifier>oai:pubmedcentral.nih.gov:4624806</identifier>
        <datestamp>2015-11-06</datestamp>
        <setSpec>plosone</setSpec>
        <setSpec>pmc-open</setSpec>
      </header>
      <metadata>
        <article xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" xsi:schemaLocation="https://jats.nlm.nih.gov/ns/archiving/1.3/ https://jats.nlm.nih.gov/archiving/1.3/xsd/JATS-archivearticle1-3.xsd" article-type="research-article">
          <front>
            <journal-meta>
              <journal-id journal-id-type="nlm-ta">PLoS One</journal-id>
              <journal-id journal-id-type="iso-abbrev">PLoS ONE</journal-id>
              <journal-id journal-id-type="publisher-id">plos</journal-id>
              <journal-id journal-id-type="pmc">plosone</journal-id>
              <journal-title-group>
                <journal-title>PLoS ONE</journal-title>
              </journal-title-group>
              <issn pub-type="epub">1932-6203</issn>
              <publisher>
                <publisher-name>Public Library of Science</publisher-name>
                <publisher-loc>San Francisco, CA USA</publisher-loc>
              </publisher>
            </journal-meta>
            <article-meta>
              <article-id pub-id-type="accession">PMC4624806</article-id>
              <article-id pub-id-type="pmcid">PMC4624806</article-id>
              <article-id pub-id-type="pmc-uid">4624806</article-id>
              <article-id pub-id-type="pmid">26509795</article-id>
              <article-id pub-id-type="pmid">26509795</article-id>
              <article-id pub-id-type="doi">10.1371/journal.pone.0141125</article-id>
              <article-id pub-id-type="publisher-id">PONE-D-15-07554</article-id>
              <article-categories>
                <subj-group subj-group-type="heading">
                  <subject>Research Article</subject>
                </subj-group>
              </article-categories>
              <title-group>
                <article-title>Audiovisual Delay as a Novel Cue to Visual Distance</article-title>
                <alt-title alt-title-type="running-head">Audiovisual Distance Cue</alt-title>
              </title-group>
              <contrib-group>
                <contrib contrib-type="author">
                  <name>
                    <surname>Jaekl</surname>
                    <given-names>Philip</given-names>
                  </name>
                  <xref ref-type="aff" rid="aff001">
                    <sup>1</sup>
                  </xref>
                  <xref rid="cor001" ref-type="corresp">*</xref>
                </contrib>
                <contrib contrib-type="author">
                  <name>
                    <surname>Seidlitz</surname>
                    <given-names>Jakob</given-names>
                  </name>
                  <xref ref-type="aff" rid="aff001">
                    <sup>1</sup>
                  </xref>
                  <xref ref-type="aff" rid="aff002">
                    <sup>2</sup>
                  </xref>
                </contrib>
                <contrib contrib-type="author">
                  <name>
                    <surname>Harris</surname>
                    <given-names>Laurence R.</given-names>
                  </name>
                  <xref ref-type="aff" rid="aff003">
                    <sup>3</sup>
                  </xref>
                </contrib>
                <contrib contrib-type="author">
                  <name>
                    <surname>Tadin</surname>
                    <given-names>Duje</given-names>
                  </name>
                  <xref ref-type="aff" rid="aff001">
                    <sup>1</sup>
                  </xref>
                  <xref ref-type="aff" rid="aff004">
                    <sup>4</sup>
                  </xref>
                </contrib>
              </contrib-group>
              <aff id="aff001">
                <label>1</label>
                <addr-line>Center for Visual Science and Department of Brain and Cognitive Sciences, University of Rochester, Rochester, New York, United States of America</addr-line>
              </aff>
              <aff id="aff002">
                <label>2</label>
                <addr-line>Laboratory of Brain and Cognition, National Institute of Mental Health, National Institutes of Health, Bethesda, Maryland, United States of America</addr-line>
              </aff>
              <aff id="aff003">
                <label>3</label>
                <addr-line>Centre for Vision Research and Departments of Psychology, Biology and Kinesiology, York University, Toronto, Ontario, Canada</addr-line>
              </aff>
              <aff id="aff004">
                <label>4</label>
                <addr-line>Department of Ophthalmology, University of Rochester School of Medicine, Rochester, New York, United States of America</addr-line>
              </aff>
              <contrib-group>
                <contrib contrib-type="editor">
                  <name>
                    <surname>Mamassian</surname>
                    <given-names>Pascal</given-names>
                  </name>
                  <role>Editor</role>
                  <xref ref-type="aff" rid="edit1"/>
                </contrib>
              </contrib-group>
              <aff id="edit1">
                <addr-line>Ecole Normale Supérieure &amp; CNRS, FRANCE</addr-line>
              </aff>
              <author-notes>
                <fn fn-type="COI-statement" id="coi001">
                  <p><bold>Competing Interests: </bold>The authors have declared that no competing interests exist.</p>
                </fn>
                <fn fn-type="con" id="contrib001">
                  <p>Conceived and designed the experiments: PJ LRH DT. Performed the experiments: PJ JS. Analyzed the data: PJ DT JS. Wrote the paper: PJ DT LRH JS.</p>
                </fn>
                <corresp id="cor001">* E-mail: <email>pjaekl@cvs.rochester.edu</email></corresp>
              </author-notes>
              <pub-date pub-type="epub">
                <day>28</day>
                <month>10</month>
                <year>2015</year>
              </pub-date>
              <pub-date pub-type="collection">
                <year>2015</year>
              </pub-date>
              <volume>10</volume>
              <issue>10</issue>
              <elocation-id>e0141125</elocation-id>
              <history>
                <date date-type="received">
                  <day>19</day>
                  <month>2</month>
                  <year>2015</year>
                </date>
                <date date-type="accepted">
                  <day>5</day>
                  <month>10</month>
                  <year>2015</year>
                </date>
              </history>
              <permissions>
                <copyright-statement>© 2015 Jaekl et al</copyright-statement>
                <copyright-year>2015</copyright-year>
                <copyright-holder>Jaekl et al</copyright-holder>
                <license xlink:href="http://creativecommons.org/licenses/by/4.0/">
                  <license-p>This is an open-access article distributed under the terms of the Creative Commons Attribution License, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are properly credited.</license-p>
                </license>
              </permissions>
              <self-uri content-type="pdf" xlink:type="simple" xlink:href="pone.0141125.pdf"/>
              <abstract>
                <p>For audiovisual sensory events, sound arrives with a delay relative to light that increases with event distance. It is unknown, however, whether humans can use these ubiquitous sound delays as an information source for distance computation. Here, we tested the hypothesis that audiovisual delays can both bias and improve human perceptual distance discrimination, such that visual stimuli paired with auditory delays are perceived as more distant and are thereby an ordinal distance cue. In two experiments, participants judged the relative distance of two repetitively displayed three-dimensional dot clusters, both presented with sounds of varying delays. In the first experiment, dot clusters presented with a sound delay were judged to be more distant than dot clusters paired with equivalent sound leads. In the second experiment, we confirmed that the presence of a sound delay was sufficient to cause stimuli to appear as more distant. Additionally, we found that ecologically congruent pairing of more distant events with a sound delay resulted in an increase in the precision of distance judgments. A control experiment determined that the sound delay duration influencing these distance judgments was not detectable, thereby eliminating decision-level influence. In sum, we present evidence that audiovisual delays can be an ordinal cue to visual distance.</p>
              </abstract>
              <funding-group>
                <funding-statement>Support was provided by the National Institutes of Health and the Center for Visual Science (University of Rochester) core grant: NEI P30 EY001319. The funders had no role in study design, data collection and analysis, decision to publish, or preparation of the manuscript.</funding-statement>
              </funding-group>
              <counts>
                <fig-count count="2"/>
                <table-count count="0"/>
                <page-count count="12"/>
              </counts>
              <custom-meta-group>
                <custom-meta id="data-availability">
                  <meta-name>Data Availability</meta-name>
                  <meta-value>Data files for all experiments in this study are available from the Figshare database. DOIs: Experiment 1: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.6084/m9.figshare.1494822">http://dx.doi.org/10.6084/m9.figshare.1494822</ext-link>; Experiment 2: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.6084/m9.figshare.1494822">http://dx.doi.org/10.6084/m9.figshare.1494822</ext-link>.</meta-value>
                </custom-meta>
              </custom-meta-group>
            </article-meta>
            <notes>
              <title>Data Availability</title>
              <p>Data files for all experiments in this study are available from the Figshare database. DOIs: Experiment 1: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.6084/m9.figshare.1494822">http://dx.doi.org/10.6084/m9.figshare.1494822</ext-link>; Experiment 2: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.6084/m9.figshare.1494822">http://dx.doi.org/10.6084/m9.figshare.1494822</ext-link>.</p>
            </notes>
          </front>
          <body>
            <sec sec-type="intro" id="sec001">
              <title>Introduction</title>
              <p>Perceiving the distance to objects is a key function of sensory systems. In humans, distance perception is thought to rely primarily on visual cues [<xref rid="pone.0141125.ref001" ref-type="bibr">1</xref>] while free field auditory distance estimation is regarded as considerably less reliable [<xref rid="pone.0141125.ref002" ref-type="bibr">2</xref>]. Moreover, distance estimation is most often conceptualized as a self-contained, unisensory process, ignoring the fact that many events provide both auditory and visual signals. In such cases, light arrives essentially instantaneously whereas sound arrives after a delay that varies with the distance between the observer and the source event. This crossmodal asynchrony has been considered an impediment to correctly perceiving ‘bound’ auditory and visual components caused by the same source event, requiring compensation for veridical perception. Some investigations have demonstrated that such compensation can occur [<xref rid="pone.0141125.ref003" ref-type="bibr">3</xref>–<xref rid="pone.0141125.ref005" ref-type="bibr">5</xref>]. Compensation does not occur when temporal delays are too small and/or visual distance information is not easily available [<xref rid="pone.0141125.ref006" ref-type="bibr">6</xref>–<xref rid="pone.0141125.ref008" ref-type="bibr">8</xref>]. However, regardless of whether the delay compensation takes place, asynchronies in audiovisual arrival times contain temporal information that can reliably signal the distance of the events.</p>
              <p>Previous findings demonstrating neural selectivity to audiovisual sound delays [<xref rid="pone.0141125.ref009" ref-type="bibr">9</xref>,<xref rid="pone.0141125.ref010" ref-type="bibr">10</xref>] are consistent with behavioral demonstrations that perception of audiovisual simultaneity at source can be robust to sound delays [<xref rid="pone.0141125.ref003" ref-type="bibr">3</xref>] and can even take into account the dependency of sound delays on event distance as derived by visual distance cues [<xref rid="pone.0141125.ref004" ref-type="bibr">4</xref>,<xref rid="pone.0141125.ref005" ref-type="bibr">5</xref>,<xref rid="pone.0141125.ref011" ref-type="bibr">11</xref>]. Here, we ask whether our sensory systems go beyond simply correcting for audiovisual delays and use such asynchronies as an information source for distance computation. This question was motivated by our recent results showing that audiovisual delays can affect perceived stimulus size, indirectly suggesting a role of audiovisual asynchrony in size/distance scaling [<xref rid="pone.0141125.ref012" ref-type="bibr">12</xref>]. In brief, we found that when a circle was shown in an otherwise dark room, it was perceived as being larger when presented with short sound delays. No effects of sound on perceived size were found for synchronous or leading sounds. One possible explanation of this finding is that sound delays cause objects to appear farther and thereby larger. Here, we aim to test this hypothesis and determine if sound delays can indeed influence perceived distance.</p>
              <p>Broadly, there are two different kinds of visual distance cues, based on the scale of information they provide: metric and ordinal. Metric cues, such as stereopsis, signal precise, interval-ratio type information about object distance. In contrast, ordinal cues, such as occlusion and aerial perspective, only provide ordinal or simply modulatory information regarding visual distance [<xref rid="pone.0141125.ref013" ref-type="bibr">13</xref>,<xref rid="pone.0141125.ref014" ref-type="bibr">14</xref>].</p>
              <p>There is evidence that some animals, notably bats and dolphins, can use sound delays as a metric distance cue. There are even documented cases of blind humans who exhibit coarse forms of echolocation [<xref rid="pone.0141125.ref015" ref-type="bibr">15</xref>–<xref rid="pone.0141125.ref017" ref-type="bibr">17</xref>]. However, it is unlikely that humans, with our less advanced auditory systems, can typically exhibit same levels of echolocating sophistication as animals whose predominant sensory system is echolocation [<xref rid="pone.0141125.ref018" ref-type="bibr">18</xref>]. Moreover, in free field environments sound delay varies with wind speed and direction, temperature and humidity [<xref rid="pone.0141125.ref019" ref-type="bibr">19</xref>]–factors further complicating the use sound delays as a precise, metric cue to distance in such environments (less open environments such as forests or caves may provide additional auditory distance cues via direct-to reverberant energy ratios [<xref rid="pone.0141125.ref020" ref-type="bibr">20</xref>]). This, however, does not preclude human utilization of audiovisual delays as ordinal information about event distance—a hypothesis tested in this study. As an ordinal distance cue, sound delays would not provide precise distance information but rather work in combination with other depth cues to help improve distance perception. Specifically, we tested the hypothesis that the presence of sound delays will affect the perceived visual distance of audiovisual events by effectively pulling the perceived stimulus location toward more distant locations indicated by sound delays. Moreover, if sound delays do play a role in human distance perception, the presence of sound delays that are congruent with other distance cues should increase the precision of distance estimation [<xref rid="pone.0141125.ref021" ref-type="bibr">21</xref>,<xref rid="pone.0141125.ref022" ref-type="bibr">22</xref>]. Note that here we are concerned with perceptual effects of relatively brief sound delays, on the order of tens of milliseconds. Considerably longer sound delays, such as those associated with lightning strikes, can also signal event distance, but such effects are cognitive in nature.</p>
            </sec>
            <sec id="sec002">
              <title>Experiment 1</title>
              <p>To explore whether audiovisual asynchronies can bias visual distance judgments, we asked participants to adjust the relative distance of two stereoscopically presented, three-dimensional random dot clusters (<xref rid="pone.0141125.g001" ref-type="fig">Fig 1A and 1B</xref>) to make them appear equidistant. The dot clusters were separated laterally and presented in a repetitive sequence (<xref rid="pone.0141125.g001" ref-type="fig">Fig 1A</xref>), alternating until participants were confident in each adjustment. Our method was partially inspired by Freeman and Driver [<xref rid="pone.0141125.ref023" ref-type="bibr">23</xref>], who used alternating audiovisual stimuli to influence spatio-temporal processing of visual motion direction. On every trial, each cluster was paired with a task-irrelevant sound. The onset of one cluster was paired with a sound delayed by between 0 and 100ms and the other was paired with an identical sound leading by the same asynchrony. We hypothesized that if the sound delay increases perceptual distance, the sound-delayed clusters would match the perceived distance of the sound-leading clusters at physically closer locations as indicated by stereo disparity (<xref rid="pone.0141125.g001" ref-type="fig">Fig 1A</xref>).</p>
              <fig id="pone.0141125.g001" orientation="portrait" position="float">
                <object-id pub-id-type="doi">10.1371/journal.pone.0141125.g001</object-id>
                <label>Fig 1</label>
                <caption>
                  <title>Experiment 1.</title>
                  <p>(A) Stimulus timeline. Right and left clusters alternated continuously, appearing for 225 ms with a 600 ms inter-stimulus interval. Sounds paired with one cluster preceded visual onset while sounds paired with the other cluster were delayed by an equal amount that ranged between 0 and 100 ms. Time one and time two are illustrated in a spatial manner in 1B. (B) Spatial arrangement of alternating dot clusters as conveyed by stereoscopic depth. At ‘Time 1,’ the right cluster was presented at a stereoscopically defined distance and paired with a sound delay. At ‘Time 2’ after an interstimulus interval, the left cluster was presented at a different distance (here shown as more distant) and paired with a sound lead. Participants adjusted the relative distance of two alternating dot clusters until they appeared to be at the same perceived distance. (C) Experimental rationale. If the presence of sound delays increases perceived visual distance, the cluster presented with a sound delay would need to be shown physically closer for the two clusters to appear equidistant. (D) Averaged median adjustments of stereo disparity. Positive biases indicate that dot clusters presented with sound delays were perceived as more distant than clusters with sound leads (**p = 0.015, *p = 0.048). Error bars are 95% confidence intervals from the bootstrap analysis as described in Materials and Methods.</p>
                </caption>
                <graphic xlink:href="pone.0141125.g001"/>
              </fig>
              <sec id="sec003" sec-type="materials|methods">
                <title>Materials and Methods</title>
                <sec id="sec004">
                  <title>Participants</title>
                  <p>Five participants (ages 21 to 35, two female and three male) completed the first experiment. All had normal or corrected-to-normal vision and were pre-screened to ensure they had acute stereopsis (&lt; 32 arcsec, as measured using a stereo acuity test (VAC Random Dot 2, Vision Assessment Corporation) and were able to fuse stereoscopic images in the display. The participants did not have any known hearing problems. Both Experiment 1 and 2 were conducted under a protocol approved by the University of Rochester Research Subjects Review Board and included written consent from participants.</p>
                </sec>
                <sec id="sec005">
                  <title>Apparatus</title>
                  <p>Visual stimuli were projected by a customized 120 Hz DLP projector (DepthQ WXGA-360) onto a projection screen in a dark room. The projector’s color wheel was removed, allowing the presentation of three gray-scale images per cycle at 120 Hz. Frame timing and duration were carefully verified with an oscilloscope. DLP projectors are inherently linear, and this was verified with a Minolta LS-110 photometer. The experiments were controlled by a Mac Pro 4.1 running Matlab 7.5 with the Psychophysics Toolbox [<xref rid="pone.0141125.ref024" ref-type="bibr">24</xref>,<xref rid="pone.0141125.ref025" ref-type="bibr">25</xref>].</p>
                  <p>Participants were seated at a distance of 2.4 m from the display with their heads stabilized on a chinrest. The display size was 24° wide by 14° high. Left and right-eye images were alternated by RealD CE4 (RealD, Beverly Hills, CA) liquid crystal shutter glasses at a rate of 120 Hz. Stimuli were only shown in the middle third of each 8.3 ms frame, leaving 5.6 ms of dark between each stimulus frame and enabling good stereo separation. Sounds were presented using two PC speakers symmetrically flanking the visual display, separated by 1 m. Thus, the horizontal and vertical coordinates of the perceived stereo sound location were designed to approximate the visual stimulus position—a stimulus property this is known to facilitate multisensory interactions [<xref rid="pone.0141125.ref026" ref-type="bibr">26</xref>].</p>
                </sec>
                <sec id="sec006">
                  <title>Stimuli and Procedure</title>
                  <p>Dots were shown in two stereoscopically arranged clusters. Both consisted of 50 non-overlapping white dots (dot size .07°, luminance 115 cd/m<sup>2</sup>) spread randomly in two cylindrical ‘volumes’ (diameter = 1.5°), centered 0.5° to the left and right of display center. When both clusters were shown at the same simulated distance, each cluster was shown at a 1.3° cyclopean reference disparity corresponding to a distance of about 2.7 m from the participant, varying slightly with interocular distance. The clusters spanned a range of about 3.25 cm depth (given by disparity). All stimuli were presented on a grey background (30 cd/m<sup>2</sup>). This large diffuse background made it difficult to use relative distance information from objects in the room as an aid to correct responses. Sounds consisted of 5 ms ‘click’ noises, presented at approximately 73 dB (<ext-link ext-link-type="uri" xlink:href="http://www.freesound.org/people/junggle/sounds/28812/">http://www.freesound.org/people/junggle/sounds/28812/</ext-link>). These 'click' sounds had multiple frequency peaks that declined in power nearly monotonically between 0 and 200 Hz.</p>
                  <p>The left and right clusters were presented alternately in a repetitive sequence, each cluster appearing for 225 ms with 600 ms between onsets (<xref rid="pone.0141125.g001" ref-type="fig">Fig 1A</xref>). These timing parameters were determined during experiment piloting and set to mitigate the perception of apparent motion between the dot clusters. Each trial started with one cluster (left or right, chosen randomly) positioned closer than the 1.3° reference disparity and the other further by the same, randomly chosen distance. Participants were instructed to match the perceived distance of the clusters as they continuously alternated on the display (<xref rid="pone.0141125.g001" ref-type="fig">Fig 1B</xref>). Pressing the left arrow increased the disparity (and perceived distance) of the left cluster while decreasing the disparity of the right cluster by an equal increment (0.02°). A small flash indicator informed the observer that the ‘distance’ had changed. Pressing the right arrow resulted in the opposite disparity change. When the participants were confident that the dot clusters appeared to be the same distance they pressed the down arrow and the difference between the mean cyclopean disparity of the left and right clusters was recorded as their adjustment value. Participants were advised to carefully make the adjustments, viewing the alternating left and right stimuli until they could confidently gauge their relative distance.</p>
                  <p>Task-irrelevant ‘click’ sounds were presented with an asynchrony relative to the visual onsets of each cluster, ranging between 0 ms and 100 ms in increments of 20 ms. For half the trials (randomly selected), sound onset was delayed relative to the visual onset of the left cluster and led the onset of the subsequently appearing right cluster. The remaining trials had the opposite left/right arrangement.</p>
                  <p>Three of the participants completed three experimental sessions. In each session the adjustment task was repeated 3 times when the delay was 0 ms and three times for each asynchrony (20, 40, 60, 80, 100 ms). Two subjects completed an additional session, yielding 3 more adjustments when the delay was 0 ms and at each asynchrony.</p>
                </sec>
                <sec id="sec007">
                  <title>Analysis</title>
                  <p>For each participant, we computed the median adjustment value for each sound delay side (left and right) in each asynchrony condition, and used the difference of these left/right results for each sound delay, yielding bias results shown in <xref rid="pone.0141125.g001" ref-type="fig">Fig 1C</xref>. To compute statistical significance for each asynchrony, we drew 10,000 bootstrap samples from each participant and re-computed bias results as described above. Significance was calculated based on the proportion of the samples where the computed bias was less or equal to 0.</p>
                  <p>We used median adjustment values as some participants occasionally had very large adjustment results, thereby accruing a skewed distribution. Repeating the same bootstrap analysis with mean adjustment values (instead of using medians) also yielded significant biases at 40 and 60 ms asynchrony (<italic>p</italic> = 0.0004 and 0.0001, respectively), matching the median analysis reported below. Data for experiment one are available at the following URL: <ext-link ext-link-type="uri" xlink:href="http://figshare.com/articles/_Jaekl_et_al_2015_Audiovisual_delay_as_a_novel_cue_to_distance_Exp_1_data/1494822">http://figshare.com/articles/_Jaekl_et_al_2015_Audiovisual_delay_as_a_novel_cue_to_distance_Exp_1_data/1494822</ext-link>
</p>
                </sec>
              </sec>
              <sec id="sec008" sec-type="conclusions">
                <title>Results and Discussion</title>
                <p>The task-irrelevant sound asynchrony had significant modulatory effects on median distance adjustments (<xref rid="pone.0141125.g001" ref-type="fig">Fig 1C</xref>): for sound asynchronies of 40 and 60 ms. Dot clusters paired with delayed sounds perceptually matched the sound-leading clusters at physical locations cued by disparity that were physically closer (40 ms delay:<italic>p</italic> = 0.015, 60 ms delay: <italic>p</italic> = 0.048). These results suggest an influential role for audiovisual asynchronies in the computation of visual distance. The present experiment, however, cannot unequivocally determine whether this result was carried by sound delays or sound leads as both asynchrony types were shown together. Because of the ecological validity of audiovisual delays (for an audiovisual event with concurrent stimulus onsets, sound arrival at the ear is always delayed relative to the visual stimulation of the retina) and our previous findings showing no effect of sound leads on size/distance scaling [<xref rid="pone.0141125.ref012" ref-type="bibr">12</xref>], we hypothesized that perceived distance was specifically affected by sound delays. This hypothesis was tested directly in Experiment 2.</p>
              </sec>
            </sec>
            <sec id="sec009">
              <title>Experiment 2</title>
              <p>Next, we conducted an experiment to rigorously test the effect of sound delays on both the <italic>bias</italic> and <italic>precision</italic> of distance judgments. Specifically, we aimed to replicate the asynchrony bias found in Experiment 1 and, more importantly, determine if ecologically valid sound delays can actually improve the precision of visual distance judgments. For this experiment, we focused on audiovisual delays of 42 ms—a value between two delays with significant results in Experiment 1, but closer to the 40 ms delay that yielded the largest effect. Each trial consisted of two intervals. In the first, the mean stereo disparity of a single dot cluster was held constant (<xref rid="pone.0141125.g002" ref-type="fig">Fig 2A</xref>). In the second interval, by changing the disparity, the dot cluster was displaced either away from or toward the observer (<xref rid="pone.0141125.g002" ref-type="fig">Fig 2A</xref>). Participants discriminated this distance change for a range of dot coherences (0–100%)—an approach analogous to the widely used ‘motion coherence’ task [<xref rid="pone.0141125.ref027" ref-type="bibr">27</xref>]. At 100% coherence, the entire cluster was displaced rigidly (<xref rid="pone.0141125.g002" ref-type="fig">Fig 2A</xref>). At lower coherences, the coherent dots were shifted rigidly while the remaining dots were assigned a random disparity within the same volume. As in the first experiment, these stimuli were paired with task-irrelevant sounds. In one randomly chosen interval audiovisual onsets were synchronous, and in the other sound onset was delayed by 42 ms (<xref rid="pone.0141125.g002" ref-type="fig">Fig 2B</xref>), corresponding to an audiovisual delay associated with a more distant physical event.</p>
              <fig id="pone.0141125.g002" orientation="portrait" position="float">
                <object-id pub-id-type="doi">10.1371/journal.pone.0141125.g002</object-id>
                <label>Fig 2</label>
                <caption>
                  <title>Experiment 2.</title>
                  <p>(A) Experimental design. In the first interval, a dot cluster was presented at the reference distance. In the second interval, the disparity of the dot cluster was either increased or decreased, corresponding to a distance change of ~13 cm. The illustration shows shifts at 100% coherence. (B) Stimulus timeline. Sounds were either synchronous with the onset of the visual stimulus (first period in this example) or delayed by 42 ms (second period). Shown here is a trial in which the sound delay is consistent with a visual distance increase. (C) Bias. Proportion of times that subjects reported the second stimulus as being more distant, plotted as a function of dot coherence. Trials in which sound delays were consistent with distance increase are shown in light green symbols while trials consistent with a distance decrease are shown in dark blue symbols. Curves show psychometric functions fitted to the data. Participants were more likely to perceive a distance increase when sound delays were presented in the second interval (i.e., ecologically consistent with a distance increase). Inset shows changes in percent response to ambiguous (i.e., incoherent) stimuli (shaded area) plotted relative to the results of the audiovisually synchronous control condition. Error bars are SEM. (D) Data for individual participants (shown as averaged in panel C). Bias terms of psychometric functions (i.e., Gaussian mean). This plot contrasts performance in trials when sound delays were consistent with distance increases with those consistent with distance decreases (units are % coherence). Points above the unity line indicate bias towards perceiving a distance increase when the delay occurred in the 2<sup>nd</sup> interval. (E) Precision. Same data as in panel C but divided into the conditions where visual and auditory cues were congruent (light green diamonds) or incongruent (dark blue diamonds). The synchronous control condition is plotted as small black squares. (F) Data for individual participants (shown as averaged in panel E). Slope terms of psychometric functions (i.e., Gaussian standard deviation). This plot contrasts data from trials with ecologically congruent stimulus-delay pairings with data from the synchronous condition (units are % coherence). Points above the unity line indicate higher precision when visual distance changes were congruent with distance changes implied by sound delays.</p>
                </caption>
                <graphic xlink:href="pone.0141125.g002"/>
              </fig>
              <sec id="sec010" sec-type="materials|methods">
                <title>Method</title>
                <sec id="sec011">
                  <title>Participants</title>
                  <p>Seven (ages 21 to 37, four male and three female) new participants completed the second experiment. As in the first experiment, participants were pre-screened to ensure they had acute stereopsis (&lt; 32 arcsec, as measured using a stereo acuity test (VAC Random Dot 2, Vision Assessment Corporation) and were able to fuse stereoscopic images in the display.</p>
                </sec>
                <sec id="sec012">
                  <title>Apparatus</title>
                  <p>Participants were seated at a distance of 1.5 m with the display size 37° wide by 21° high. Other apparatus details were as described for Experiment 1. Importantly, the horizontal and vertical coordinates of the perceived sound location were designed to approximate the visual stimulus position.</p>
                </sec>
                <sec id="sec013">
                  <title>Stimuli and Procedure</title>
                  <p>Two stereoscopically displayed dot clusters were presented in sequence and consisted of 218 non-overlapping white dots within a single cylindrical ‘volume’ (visual diameter = 2.5°) centered on the screen. As in experiment one, the luminance of the dots was 115 cd/m<sup>2</sup> and the background was 35 cd/m<sup>2</sup>. The left and right eye dots for each cluster ranged over 0.08° cyclopean disparity, corresponding to a cluster length in depth of about 6 cm. A fixation crosshair at the center of the display (diameter 0.5°) was used to initiate each trial of the experiment. The fixation crosshair was displayed stereoscopically (0.6° cyclopean disparity; corresponding to a simulated distance of about 1.8 m), always positioned at the distance corresponding to the mean disparity of the first dot cluster (i.e. at 1.8 m). The same 5 ms ‘click’ sounds used in experiment one were used in experiment two.</p>
                  <p>Before the experimental sessions, participants completed two practice sessions. For the first, they adjusted the mean disparity between two sequentially viewed dot clusters with unlimited duration, to familiarize themselves with the stimuli. Here, the displacement (100% coherent) occurred when the participant pressed the spacebar of the keyboard. The second practice session involved the same task as the main experiment but we used only high coherence levels (60, 80 and 100%) and only synchronous audio-visual onsets (i.e., key experimental manipulations were not used). If a participant could not meet a criterion of a mean of 80% correct performance for these highly coherent stimuli, they did not participate in the main experiment. Six new potential participants were excluded by this criterion and did not participate in the main experiment. Note that this exclusion procedure was not based on participants’ performance in key experimental conditions (as described below). Rather, it was implemented after our pilot work that revealed that a proportion of potential participants exhibited difficulty with perceiving stereoscopic stimuli when even a small amount of noise was added to the displays.</p>
                  <p>A two-interval forced choice task (2IFC) was used, with each dot cluster accompanied by an uninformative sound. Each trial began with the fixation crosshair displayed for 33 ms. After this duration a blank screen was displayed for a random duration of between 750 ms and 1 s, followed by the onset of the first dot cluster (duration 125 ms). The second cluster (duration 125 ms) was presented after a random inter-stimulus interval of between 830 ms and 1.25 s. As in the first experiment, this large inter-stimulus interval mitigated apparent motion cues. The first cluster was always centered in depth on the fixation distance (1.8 m), effectively constituting a standard stimulus. The second cluster was presented with a shift of about ±13cm in depth (0.15° disparity).</p>
                  <p>In one interval, sound was presented synchronously with the onset of the cluster. In the other interval, the sound was delayed by 42 ms (<xref rid="pone.0141125.g002" ref-type="fig">Fig 2B</xref>). This delay was presented an equal number of times in both the first and second interval. A baseline control condition with synchronous audiovisual onsets in both intervals was also run for comparison purposes. The results from this condition were used for computing the data shown in <xref rid="pone.0141125.g002" ref-type="fig">Fig 2C</xref> (inset) and as a baseline for the congruency analysis (<xref rid="pone.0141125.g002" ref-type="fig">Fig 2E and 2F</xref>).</p>
                  <p>Task difficulty was modulated by varying the dot coherence, defined as the percentage of dots that were displaced rigidly in depth (by ±13cm) in the second interval, maintaining their relative positions. The remaining dots were displaced randomly by up to ±26cm. Coherence was varied between 0% and 100%, in steps of 20%. Participants were instructed to judge the direction of displacement of the dot cluster—i.e., whether the second cluster appeared <italic>further</italic> or <italic>closer</italic> than the first cluster. They indicated their choice by pressing one of two keys: left arrow for further and right arrow for closer. Each coherence level was repeated 20 times for both displacement directions and each sound delay condition (first interval, second interval and synchronous). In total there were 660 trials per participant (3 sound arrangements*(2 displacement directions*5 coherence levels + 1 zero coherence level)*20 repeats).</p>
                  <p>During the debriefing session at the end of the experiment, we found that participants were unaware that sound delays occurred during the experiment—sounds were perceived as simultaneously paired with visual stimuli. This is consistent with previous findings showing a relatively wide range of asynchronies where visual and auditory stimuli are perceived as simultaneous [<xref rid="pone.0141125.ref002" ref-type="bibr">2</xref>,<xref rid="pone.0141125.ref003" ref-type="bibr">3</xref>,<xref rid="pone.0141125.ref011" ref-type="bibr">11</xref>]. To empirically confirm that the stimulus delays used in this experiment were not consciously discriminable, we ran a simple control experiment using stimuli identical to those used in main experiment. Participants (<italic>n</italic> = 7) were explicitly instructed to choose the interval containing the delay while simply “observing” the shifts in the distance of the dot clusters. Coherence levels and shift direction (closer or further) were chosen randomly for 200 trials for each participant. The results revealed near chance level performance (mean = 53.6% correct, range = 48–58.5% correct). When analyzed individually, only one participant exhibited performance that was significantly different than chance (p = 0.02, Binomial test, not corrected for multiple comparisons; all other participants p &gt; 0.14). We note that this control experiment provides a conservative test of the discriminability of sound delays in the main experiment. There, participants’ attention was directed to the distance discrimination task they were performing and they were not informed about the presence of the sound delays. In contrast, in the control experiment, participants’ attention was explicitly directed to detecting the sound delays.</p>
                </sec>
                <sec id="sec014">
                  <title>Analysis</title>
                  <p>The percentage of times each displacement was perceived as ‘further’ was plotted as a function of coherence level (negative coherence values represent percentage of dots shifting coherently <italic>toward</italic> the observer), and was fitted with a cumulative Gaussian function that also included one lapse parameter. The mean of the fitted cumulative Gaussian represents equal likelihood of perceiving the displacement in either direction, representing the point of subjective equality (PSE). To estimate the precision associated with correctly discriminating distance changes in the dot clusters (<xref rid="pone.0141125.g002" ref-type="fig">Fig 2E and 2F</xref>), the data were grouped by the (in)congruency of the visual displacements and sound delays. Specifically, a delayed sound in the second interval is congruent with a displacement of the visual stimulus away from the observer and vice versa. For this analysis, the 0% coherence points were omitted, as this arrangement provides no information about visual distance change. The slope of the fitted cumulative Gaussian was taken as the measure of the precision of visual distance judgments.</p>
                  <p>To compute statistical significance, we drew 10,000 pairs of bootstrap samples from a pooled data set that included data from all participants and fit each sample with a cumulative Gaussian function. To account for the repeated measures experimental design, each pair of bootstrap samples had identical sampling of the two conditions that were compared (<xref rid="pone.0141125.g002" ref-type="fig">Fig 2C and 2D</xref>: sound delay in interval one vs. sound delay in interval two; <xref rid="pone.0141125.g002" ref-type="fig">Fig 2E and 2F</xref>: congruent vs. incongruent trials). For the bias analysis (<xref rid="pone.0141125.g002" ref-type="fig">Fig 2C</xref>), significance was defined as the proportion of the samples where the mean of the fitted Gaussian distribution was larger for sound delays consistent with distance decrease. For the precision analyses (<xref rid="pone.0141125.g002" ref-type="fig">Fig 2E</xref>), significance was defined as the proportion of the samples where the slope of the fitted Gaussian distribution was steeper (i.e., more precise) for stimuli that had sound delays congruent with visual distance change than slopes for incongruent and synchronous stimuli. To test for the robustness of the reported results, we repeated the above-described analysis with the following changes: First, data were fitted with a cumulative Gaussian with no lapse parameters. Second, data were fitted with a cumulative Gaussian with two lapse parameters (i.e., separate floor and ceiling parameters). This did not affect the reported results (bias analysis, p = 0.02 and 0.03; slope analysis, p = 0.01 and 0.006, respectively). Data for experiment two are available at the following URL: <ext-link ext-link-type="uri" xlink:href="http://figshare.com/articles/_Jaekl_et_al_2015_Audiovisual_delay_as_a_novel_cue_to_distance_Exp_2_data/1494821">http://figshare.com/articles/_Jaekl_et_al_2015_Audiovisual_delay_as_a_novel_cue_to_distance_Exp_2_data/1494821</ext-link>
</p>
                </sec>
              </sec>
              <sec id="sec015" sec-type="conclusions">
                <title>Results and Discussion</title>
                <p>To test for a role of sound delays in biasing distance judgments, we fit the results with cumulative Gaussians and compared the distribution means between the two sound delay conditions. Individual participant means of these Gaussian fits are shown in <xref rid="pone.0141125.g002" ref-type="fig">Fig 2D</xref>. Subjects were more likely to perceive the second-interval dot cluster as displaced ‘further’ when the sound delay was presented in the second interval, consistent with the results of the first experiment (<xref rid="pone.0141125.g002" ref-type="fig">Fig 2C</xref>; p = 0.04). This effect of task irrelevant sound delays was most strongly pronounced at the ambiguous, 0% coherence level (<xref rid="pone.0141125.g002" ref-type="fig">Fig 2C</xref> inset in left panel, t<sub>6</sub> = 4.89, p = 0.003).</p>
                <p>If sound delays aid distance perception, changes in distance that provide <italic>congruent</italic> asynchrony cues (i.e. both modalities signal the same direction of distance change) should be judged more precisely than events for which changes in sound delay are incongruent with changes in visual distance information—in line with other studies yielding enhancements for congruent audiovisual stimuli [<xref rid="pone.0141125.ref028" ref-type="bibr">28</xref>–<xref rid="pone.0141125.ref030" ref-type="bibr">30</xref>]. To test this hypothesis, we compared congruent audiovisual trials (e.g., an increase in visual distance paired with a sound delay in the second interval) with trials containing audiovisual incongruency (e.g., a decrease in visual distance paired with a sound delay in the second interval). The cumulative Gaussian fits to the data revealed significantly higher distance estimation precision for congruent trials (<xref rid="pone.0141125.g002" ref-type="fig">Fig 2D</xref>), as evidenced by a significantly steeper slope in the congruent condition (p = 0.003). A comparison with the synchronous control condition (dotted line in <xref rid="pone.0141125.g002" ref-type="fig">Fig 2E</xref>), revealed that this effect is due to ecological congruence between delay interval and displacement direction (p = 0.004) with no corresponding differences between the incongruent and simultaneous conditions (p = 0.76). The incongruent vs. synchronous condition slope values for individual participants are shown in <xref rid="pone.0141125.g002" ref-type="fig">Fig 2F</xref>.</p>
                <p>This pattern of results argues against the simple explanation that participants’ decisions/responses were simply inclined toward the interval with the sound delay. Specifically, as sound delays were not predictive of visual distance, such decisional tendency predicts both an enhancement for congruent audiovisual shifts and a <italic>decrement</italic> in performance for incongruent stimuli across the range of coherence levels. Instead, we only found that distance judgments were more precise when more distant events were accompanied with delayed sounds, a finding indicative of sensory-level changes in distance processing due to the presence of congruent sound delays. This conclusion is further supported by the results of the control experiment which confirmed that the sound delays used the main experiment (<xref rid="pone.0141125.g002" ref-type="fig">Fig 2B</xref>) were not consciously discriminable (see <xref rid="sec010" ref-type="sec">Method</xref>).</p>
              </sec>
            </sec>
            <sec id="sec016">
              <title>General Discussion</title>
              <p>We found that audiovisual delays can both bias visual distance judgments and improve their precision when sound delays are paired with visual stimuli in an ecologically valid manner. These perceptual changes occurred even though audiovisual delays were not overtly informative about object distance and were too brief to be consciously discriminated [<xref rid="pone.0141125.ref009" ref-type="bibr">9</xref>], indicating that small audio-visual time-of-arrival differences can contribute significantly to the perceived distance of events.</p>
              <p>Our finding of strongest effects for audiovisual delays between 40 and 60 ms (Experiment 1) is consistent with neurophysiological data showing sensitivity to sound-delayed audiovisual stimuli in the superior colliculus, a midbrain region that functions imperatively for integrating auditory and visual signals for attending to and localizing audiovisual stimuli, where auditory response latencies can precede visual latencies by 40 to 70 ms (Wallace, Wilkinson &amp; Stein, 1996; Bell, Meredith, Van Opstal, &amp; Munoz, 2006, also see Boehnke &amp; Munoz, 2008). Importantly, there can be considerable multisensory response enhancement within this asynchrony range (Meredith, Nemitz an Stein, 1987)—a response property that could have contributed to the present findings. Speculatively, one explanation is that for audiovisual delays to override other distance cues present in our displays (i.e., stereopsis), sound delays needed to be sufficiently long to exploit this neurophysiological sensitivity [<xref rid="pone.0141125.ref009" ref-type="bibr">9</xref>,<xref rid="pone.0141125.ref010" ref-type="bibr">10</xref>]. This account may also explain why sound delays seem to work as an ordinal rather than a metric depth cue, where a delay needs to be sufficiently long enough before it can affect perceived distance.</p>
              <p>The sound delays found to influence distance judgments were longer than what would be expected if such asynchronies were a metric cue to distance (e.g. a 40 ms audiovisual delay would signal an event at approximately 13.6 m distance). Rather, the present data indicate that sound delays modulate perceived distance such that events paired with delayed sounds are <italic>more likely</italic> to be perceived as more distant, in an ordinal manner. By a way of comparison, this is similar to the well-established depth cue of aerial perspective where lower contrast objects are <italic>more likely</italic> to be perceived as further [<xref rid="pone.0141125.ref001" ref-type="bibr">1</xref>]. Our results are also in line with other studies showing that ordinal depth cues can have modulatory influence on perceived distances cued primarily by disparity [<xref rid="pone.0141125.ref013" ref-type="bibr">13</xref>,<xref rid="pone.0141125.ref031" ref-type="bibr">31</xref>–<xref rid="pone.0141125.ref033" ref-type="bibr">33</xref>]. Our speculation is that perceptual experience makes these ordinal cues effective in aiding distance perception; we learn that objects paired with sound delays are more likely to be distant as we learn that objects that appear obstructed are more likely to be further away. It is possible that sound delays might tend towards being a metric cue that can be used more accurately at substantially longer distances. At greater distances, other depth cues, such as stereopsis and vergence angle, become less effective while audiovisual delays become longer and thus, more discriminable. An important question for future research is whether under such conditions sound delays start to play a more dominant role in distance perception. Another important characteristic of our results is the implicit nature of depth information conveyed by audiovisual delays—subjects were unaware of the sound delays in Experiment 2. While depth information is usually explicit, there are instances where subjects exhibit unconscious processing of depth cues [<xref rid="pone.0141125.ref034" ref-type="bibr">34</xref>].</p>
              <p>Here, we make an important assumption that auditory and visual stimuli in our experiments are bound and perceived as unified events. While we did not explicitly test crossmodal binding, prior research supports this assumption. Humans generally bind crossmodal stimuli that are in close spatial and temporal register [<xref rid="pone.0141125.ref035" ref-type="bibr">35</xref>], into unified percepts [<xref rid="pone.0141125.ref036" ref-type="bibr">36</xref>]. Aside from the incongruence in depth implied by the sound delays, our stimuli generally obeyed the spatial and temporal principles of sensory integration commonly thought to support crossmodal binding [<xref rid="pone.0141125.ref037" ref-type="bibr">37</xref>]. Spatially, the stereo position of the sound approximated the position of the visual stimuli both laterally and vertically. Temporally, the asynchronies at which perceived distances were influenced were well within the putative temporal window for audiovisual integration [<xref rid="pone.0141125.ref038" ref-type="bibr">38</xref>]—an assertion further supported by the control experiment data showing that participants could not consciously discriminate sound delays in Experiment 2. Taken together, this argues that auditory and visual component stimuli were perceptually bound, and thus causally inferred to the same source event. This keeps audiovisual delay duration as the principal source of information that could influence the perceived audiovisual positions.</p>
              <p>The present findings draw comparison to the well-known study of ventriloquism by Alais and Burr [<xref rid="pone.0141125.ref021" ref-type="bibr">21</xref>]. The authors found that for spatially offset audiovisual stimuli, sound can both bias and improve the precision of visual position judgments. Here, we found that for spatially co-localized audiovisual stimuli, perceived as resulting from the same source event, the perceived visual distance can be thought of as ‘ventriloquised’ towards more distant locations implied by sound delays. This significant <italic>modulatory</italic> effect of audiovisual delays on perceived distance reveals an important role of cross-modal interactions in distance computation. Evidently, audiovisual delays can be added to the list of cues humans use to estimate event distances.</p>
            </sec>
          </body>
          <back>
            <ack>
              <p>We thank Randolph Blake for comments on an earlier version of the paper.</p>
            </ack>
            <ref-list>
              <title>References</title>
              <ref id="pone.0141125.ref001">
                <label>1</label>
                <mixed-citation publication-type="book"><name><surname>Howard</surname><given-names>IP</given-names></name>, <name><surname>Rogers</surname><given-names>BJ</given-names></name>. <chapter-title>Seeing in Depth</chapter-title>, Vol. <volume>2</volume>: <source>Depth Perception</source>. <publisher-loc>Toronto</publisher-loc>: <publisher-name>Porteus Press</publisher-name><year>2008</year>.</mixed-citation>
              </ref>
              <ref id="pone.0141125.ref002">
                <label>2</label>
                <mixed-citation publication-type="journal"><name><surname>Coleman</surname><given-names>PD</given-names></name>. <article-title>Failure to localize the source distance of an unfamiliar sound</article-title>. <source>J Acoust Soc Am</source>. <year>1962</year>; <volume>34</volume>: <fpage>345</fpage>–<lpage>346</lpage>.</mixed-citation>
              </ref>
              <ref id="pone.0141125.ref003">
                <label>3</label>
                <mixed-citation publication-type="journal"><name><surname>Fujisaki</surname><given-names>W</given-names></name>, <name><surname>Shimojo</surname><given-names>S</given-names></name>, <name><surname>Kashino</surname><given-names>M</given-names></name>, <name><surname>Nishida</surname><given-names>S</given-names></name>. <article-title>Recalibration of audiovisual simultaneity</article-title>. <source>Nat Neurosci</source>. <year>2006</year>; <volume>7</volume>: <fpage>773</fpage>–<lpage>778</lpage>.</mixed-citation>
              </ref>
              <ref id="pone.0141125.ref004">
                <label>4</label>
                <mixed-citation publication-type="journal"><name><surname>Kopinska</surname><given-names>A</given-names></name>, <name><surname>Harris</surname><given-names>LR</given-names></name>. <article-title>Simultaneity constancy</article-title>. <source>Perception</source>. <year>2004</year>; <volume>33</volume>: <fpage>1049</fpage>–<lpage>1060</lpage>.
<?supplied-pmid 15560507?><pub-id pub-id-type="pmid">15560507</pub-id></mixed-citation>
              </ref>
              <ref id="pone.0141125.ref005">
                <label>5</label>
                <mixed-citation publication-type="journal"><name><surname>Sugita</surname><given-names>Y</given-names></name>, <name><surname>Suzuki</surname><given-names>Y</given-names></name>. <article-title>Audiovisual perception: Implicit estimation of sound-arrival time</article-title>. <source>Nature</source>. <year>2003</year>; <volume>421</volume>: <fpage>911</fpage><?supplied-pmid 12606990?><pub-id pub-id-type="pmid">12606990</pub-id></mixed-citation>
              </ref>
              <ref id="pone.0141125.ref006">
                <label>6</label>
                <mixed-citation publication-type="journal"><name><surname>Stone</surname><given-names>JV</given-names></name>, <name><surname>Hunkin</surname><given-names>NM</given-names></name>, <name><surname>Porrill</surname><given-names>J</given-names></name>, <name><surname>Wood</surname><given-names>R</given-names></name>, <name><surname>Keeler</surname><given-names>V</given-names></name>, <name><surname>Beanland</surname><given-names>M</given-names></name>, <etal>et al</etal><article-title>When is now?</article-title><source>Perception of simultaneity</source>. <year>2001</year>; <fpage>31</fpage>–<lpage>38</lpage>.</mixed-citation>
              </ref>
              <ref id="pone.0141125.ref007">
                <label>7</label>
                <mixed-citation publication-type="journal"><name><surname>Arnold</surname><given-names>DH</given-names></name>, <name><surname>Johnston</surname><given-names>A</given-names></name>, <name><surname>Nishida</surname><given-names>S</given-names></name>. <article-title>Timing sight and sound</article-title>. <source>Vision Res</source>. <year>2005</year>; <volume>45</volume>: <fpage>1275</fpage>–<lpage>1284</lpage>.
<?supplied-pmid 15733960?><pub-id pub-id-type="pmid">15733960</pub-id></mixed-citation>
              </ref>
              <ref id="pone.0141125.ref008">
                <label>8</label>
                <mixed-citation publication-type="journal"><name><surname>Lewald</surname><given-names>J</given-names></name>, <name><surname>Guski</surname><given-names>R</given-names></name>. <article-title>Auditory-visual temporal integration as a function of distance: no compensation for sound-transmission time in human perception</article-title>. <source>Neurosci Lett</source>. <year>2004</year>; <volume>357</volume>: <fpage>119</fpage>–<lpage>122</lpage>.
<?supplied-pmid 15036589?><pub-id pub-id-type="pmid">15036589</pub-id></mixed-citation>
              </ref>
              <ref id="pone.0141125.ref009">
                <label>9</label>
                <mixed-citation publication-type="journal"><name><surname>Bushara</surname><given-names>KO</given-names></name>, <name><surname>Grafman</surname><given-names>J</given-names></name>, <name><surname>Hallett</surname><given-names>M</given-names></name>. <article-title>Neural correlates of auditory-visual stimulus onset asynchrony detection</article-title>. <source>J Neurosci</source>. <year>2001</year>; <volume>21</volume>: <fpage>300</fpage>–<lpage>304</lpage>.
<?supplied-pmid 11150347?><pub-id pub-id-type="pmid">11150347</pub-id></mixed-citation>
              </ref>
              <ref id="pone.0141125.ref010">
                <label>10</label>
                <mixed-citation publication-type="journal"><name><surname>Meredith</surname><given-names>MA</given-names></name>, <name><surname>Nemitz</surname><given-names>JW</given-names></name>, <name><surname>Stein</surname><given-names>BE</given-names></name>. <article-title>Determinants of multisensory integration in superior colliculus neurons. I. Temporal factors</article-title>. <source>J Neurosci</source>. <year>1987</year>; <volume>7</volume>: <fpage>3215</fpage>–<lpage>3229</lpage>.
<?supplied-pmid 3668625?><pub-id pub-id-type="pmid">3668625</pub-id></mixed-citation>
              </ref>
              <ref id="pone.0141125.ref011">
                <label>11</label>
                <mixed-citation publication-type="journal"><name><surname>Alais</surname><given-names>D</given-names></name>, <name><surname>Carlile</surname><given-names>S</given-names></name>. <article-title>Synchronizing to real events: subjective audiovisual alignment scales with perceived auditory depth and speed of sound</article-title>. <source>Proc Natl Acad Sci U S A</source>. <year>2005</year>; <volume>102</volume>: <fpage>2244</fpage>–<lpage>2247</lpage>.
<?supplied-pmid 15668388?><pub-id pub-id-type="pmid">15668388</pub-id></mixed-citation>
              </ref>
              <ref id="pone.0141125.ref012">
                <label>12</label>
                <mixed-citation publication-type="journal"><name><surname>Jaekl</surname><given-names>P</given-names></name>, <name><surname>Soto-Faraco</surname><given-names>S</given-names></name>, <name><surname>Harris</surname><given-names>LR</given-names></name>. <article-title>Perceived size change induced by audiovisual temporal delays</article-title>. <source>Exp Brain Res</source>. <year>2012</year>; <volume>216</volume>: <fpage>457</fpage>–<lpage>462</lpage>. <pub-id pub-id-type="doi">10.1007/s00221-011-2948-9</pub-id>
<?supplied-pmid 22105336?><pub-id pub-id-type="pmid">22105336</pub-id></mixed-citation>
              </ref>
              <ref id="pone.0141125.ref013">
                <label>13</label>
                <mixed-citation publication-type="journal"><name><surname>Burge</surname><given-names>J</given-names></name>, <name><surname>Peterson</surname><given-names>MA</given-names></name>, <name><surname>Palmer</surname><given-names>SE</given-names></name>. <article-title>Ordinal configural cues combine with metric disparity in depth perception</article-title>. <source>J Vis</source>. <year>2005</year>; <volume>5</volume>: <fpage>534</fpage>–<lpage>542</lpage>.
<?supplied-pmid 16097866?><pub-id pub-id-type="pmid">16097866</pub-id></mixed-citation>
              </ref>
              <ref id="pone.0141125.ref014">
                <label>14</label>
                <mixed-citation publication-type="book"><name><surname>Epstein</surname><given-names>W</given-names></name>, <name><surname>Rogers</surname><given-names>S</given-names></name>. <source>Perception of Space and Motion</source>. <publisher-loc>San Diego</publisher-loc>: <publisher-name>Academic Press</publisher-name><year>1995</year>.</mixed-citation>
              </ref>
              <ref id="pone.0141125.ref015">
                <label>15</label>
                <mixed-citation publication-type="journal"><name><surname>Thaler</surname><given-names>L</given-names></name>, <name><surname>Arnott</surname><given-names>SR</given-names></name>, <name><surname>Goodale</surname><given-names>MA</given-names></name>. <article-title>Neural correlates of natural human echolocation in early and late blind echolocation experts</article-title>. <source>PLoS One</source>. <year>2011</year>; <volume>6</volume>: <fpage>e20162</fpage><pub-id pub-id-type="doi">10.1371/journal.pone.0020162</pub-id><?supplied-pmid 21633496?><pub-id pub-id-type="pmid">21633496</pub-id></mixed-citation>
              </ref>
              <ref id="pone.0141125.ref016">
                <label>16</label>
                <mixed-citation publication-type="journal"><name><surname>Schenkman</surname><given-names>BN</given-names></name>, <name><surname>Nilsson</surname><given-names>ME</given-names></name>. <article-title>Human echolocation: Blind and sighted persons' ability to detect sounds recorded in the presence of a reflecting object</article-title>. <source>Perception</source>. <year>2010</year>; <volume>39</volume>: <fpage>483</fpage>–<lpage>501</lpage>.
<?supplied-pmid 20514997?><pub-id pub-id-type="pmid">20514997</pub-id></mixed-citation>
              </ref>
              <ref id="pone.0141125.ref017">
                <label>17</label>
                <mixed-citation publication-type="journal"><name><surname>Teng</surname><given-names>S</given-names></name>, <name><surname>Puri</surname><given-names>A</given-names></name>, <name><surname>Whitney</surname><given-names>D</given-names></name>. <article-title>Ultrafine spatial acuity of blind expert human echolocators</article-title>. <source>Exp Brain Res</source>. <year>2012</year>; <volume>216</volume>: <fpage>483</fpage>–<lpage>488</lpage>. <pub-id pub-id-type="doi">10.1007/s00221-011-2951-1</pub-id>
<?supplied-pmid 22101568?><pub-id pub-id-type="pmid">22101568</pub-id></mixed-citation>
              </ref>
              <ref id="pone.0141125.ref018">
                <label>18</label>
                <mixed-citation publication-type="book"><name><surname>Hughes</surname><given-names>HC</given-names></name>. <source>Sensory Exotica: A World Beyond Human Experience</source>. <publisher-loc>Cambridge, Mass</publisher-loc>: <publisher-name>MIT Press</publisher-name><year>2001</year>.</mixed-citation>
              </ref>
              <ref id="pone.0141125.ref019">
                <label>19</label>
                <mixed-citation publication-type="book"><name><surname>Zuckerwar</surname><given-names>AJ</given-names></name>. <source>Handbook of the Speed of Sound in Real Gases</source>: <publisher-name>Elsevier Science &amp; Technology Books</publisher-name><year>2002</year>.</mixed-citation>
              </ref>
              <ref id="pone.0141125.ref020">
                <label>20</label>
                <mixed-citation publication-type="journal"><name><surname>Bronkhorst</surname><given-names>AW</given-names></name>, <name><surname>Houtgast</surname><given-names>T</given-names></name>. <article-title>Auditory distance perception in rooms</article-title>. <source>Nature</source>. <year>1999</year>; <volume>397</volume>: <fpage>517</fpage>–<lpage>520</lpage>.
<?supplied-pmid 10028966?><pub-id pub-id-type="pmid">10028966</pub-id></mixed-citation>
              </ref>
              <ref id="pone.0141125.ref021">
                <label>21</label>
                <mixed-citation publication-type="journal"><name><surname>Alais</surname><given-names>D</given-names></name>, <name><surname>Burr</surname><given-names>D</given-names></name>. <article-title>The ventriloquist effect results from near-optimal bimodal integration</article-title>. <source>Curr Biol</source>. <year>2004</year>; <volume>14</volume>: <fpage>257</fpage>–<lpage>262</lpage>.
<?supplied-pmid 14761661?><pub-id pub-id-type="pmid">14761661</pub-id></mixed-citation>
              </ref>
              <ref id="pone.0141125.ref022">
                <label>22</label>
                <mixed-citation publication-type="journal"><name><surname>Ernst</surname><given-names>MO</given-names></name>, <name><surname>Banks</surname><given-names>MS</given-names></name>. <article-title>Humans integrate visual and haptic information in a statistically optimal fashion</article-title>. <source>Nature</source>. <year>2002</year>; <volume>415</volume>: <fpage>429</fpage>–<lpage>433</lpage>.
<?supplied-pmid 11807554?><pub-id pub-id-type="pmid">11807554</pub-id></mixed-citation>
              </ref>
              <ref id="pone.0141125.ref023">
                <label>23</label>
                <mixed-citation publication-type="journal"><name><surname>Freeman</surname><given-names>E</given-names></name>, <name><surname>Driver</surname><given-names>J</given-names></name>. <article-title>Direction of visual apparent motion driven solely by timing of a static sound</article-title>. <source>Curr Biol</source>. <year>2008</year>; <volume>18</volume>: <fpage>1262</fpage>–<lpage>1266</lpage>. <pub-id pub-id-type="doi">10.1016/j.cub.2008.07.066</pub-id>
<?supplied-pmid 18718760?><pub-id pub-id-type="pmid">18718760</pub-id></mixed-citation>
              </ref>
              <ref id="pone.0141125.ref024">
                <label>24</label>
                <mixed-citation publication-type="journal"><name><surname>Brainard</surname><given-names>DH</given-names></name>. <article-title>The Psychophysics Toolbox</article-title>. <source>Spat Vis</source>. <year>1997</year>; <volume>10</volume>: <fpage>433</fpage>–<lpage>436</lpage>.
<?supplied-pmid 9176952?><pub-id pub-id-type="pmid">9176952</pub-id></mixed-citation>
              </ref>
              <ref id="pone.0141125.ref025">
                <label>25</label>
                <mixed-citation publication-type="journal"><name><surname>Pelli</surname><given-names>DG</given-names></name>. <article-title>The VideoToolbox software for visual psychophysics: Transforming numbers into movies</article-title>. <source>Spat Vis</source>. <year>1997</year>; <volume>10</volume>: <fpage>437</fpage>–<lpage>442</lpage>.
<?supplied-pmid 9176953?><pub-id pub-id-type="pmid">9176953</pub-id></mixed-citation>
              </ref>
              <ref id="pone.0141125.ref026">
                <label>26</label>
                <mixed-citation publication-type="journal"><name><surname>Spence</surname><given-names>C</given-names></name>. <article-title>Just how important is spatial coincidence to multisensory integration? Evaluating the spatial rule</article-title>. <source>Ann N Y Acad Sci</source>. <year>2013</year>; <volume>1296</volume>: <fpage>31</fpage>–<lpage>49</lpage>. <pub-id pub-id-type="doi">10.1111/nyas.12121</pub-id>
<?supplied-pmid 23710729?><pub-id pub-id-type="pmid">23710729</pub-id></mixed-citation>
              </ref>
              <ref id="pone.0141125.ref027">
                <label>27</label>
                <mixed-citation publication-type="journal"><name><surname>Newsome</surname><given-names>WT</given-names></name>, <name><surname>Pare</surname><given-names>EB</given-names></name>. <article-title>A Selective Impairment of Motion Perception Following Lesions of the Middle Temporal Visual Area (Mt)</article-title>. <source>J Neurosci</source>. <year>1988</year>; <volume>8</volume>: <fpage>2201</fpage>–<lpage>2211</lpage>.
<?supplied-pmid 3385495?><pub-id pub-id-type="pmid">3385495</pub-id></mixed-citation>
              </ref>
              <ref id="pone.0141125.ref028">
                <label>28</label>
                <mixed-citation publication-type="journal"><name><surname>Kim</surname><given-names>RS</given-names></name>, <name><surname>Seitz</surname><given-names>AR</given-names></name>, <name><surname>Shams</surname><given-names>L</given-names></name>. <article-title>Benefits of stimulus congruency for multisensory facilitation of visual learning</article-title>. <source>PloS One</source>. <year>2008</year>; <volume>3</volume>: <fpage>e1532</fpage><pub-id pub-id-type="doi">10.1371/journal.pone.0001532</pub-id><?supplied-pmid 18231612?><pub-id pub-id-type="pmid">18231612</pub-id></mixed-citation>
              </ref>
              <ref id="pone.0141125.ref029">
                <label>29</label>
                <mixed-citation publication-type="journal"><name><surname>Oruc</surname><given-names>I</given-names></name>, <name><surname>Sinnett</surname><given-names>S</given-names></name>, <name><surname>Bischof</surname><given-names>WF</given-names></name>, <name><surname>Soto-Faraco</surname><given-names>S</given-names></name>, <name><surname>Lock</surname><given-names>K</given-names></name>, <name><surname>Kingstone</surname><given-names>A</given-names></name>. <article-title>The effect of attention on the illusory capture of motion in bimodal stimuli</article-title>. <source>Brain Res</source>. <year>2008</year>; <volume>1242</volume>: <fpage>200</fpage>–<lpage>208</lpage>. <pub-id pub-id-type="doi">10.1016/j.brainres.2008.04.014</pub-id>
<?supplied-pmid 18514172?><pub-id pub-id-type="pmid">18514172</pub-id></mixed-citation>
              </ref>
              <ref id="pone.0141125.ref030">
                <label>30</label>
                <mixed-citation publication-type="journal"><name><surname>Soto-Faraco</surname><given-names>S</given-names></name>, <name><surname>Spence</surname><given-names>C</given-names></name>, <name><surname>Kingstone</surname><given-names>A</given-names></name>. <article-title>Cross-modal dynamic capture: congruency effects in the perception of motion across sensory modalities</article-title>. <source>J Exp Psychol Hum Percept Perform</source>. <year>2004</year>; <volume>30</volume>: <fpage>330</fpage>–<lpage>345</lpage>.
<?supplied-pmid 15053692?><pub-id pub-id-type="pmid">15053692</pub-id></mixed-citation>
              </ref>
              <ref id="pone.0141125.ref031">
                <label>31</label>
                <mixed-citation publication-type="journal"><name><surname>Bertamini</surname><given-names>M</given-names></name>, <name><surname>Martinovic</surname><given-names>J</given-names></name>, <name><surname>Wuerger</surname><given-names>SM</given-names></name>. <article-title>Integration of ordinal and metric cues in depth processing</article-title>. <source>J Vis</source>. <year>2008</year>; <volume>8</volume>: <issue>10</issue><fpage>11</fpage>–<lpage>12</lpage>.</mixed-citation>
              </ref>
              <ref id="pone.0141125.ref032">
                <label>32</label>
                <mixed-citation publication-type="journal"><name><surname>Gillam</surname><given-names>BJ</given-names></name>, <name><surname>Cook</surname><given-names>ML</given-names></name>. <article-title>Perspective based on stereopsis and occlusion</article-title>. <source>Psychol Sci</source>. <year>2001</year>; <volume>12</volume>: <fpage>424</fpage>–<lpage>429</lpage>.
<?supplied-pmid 11554678?><pub-id pub-id-type="pmid">11554678</pub-id></mixed-citation>
              </ref>
              <ref id="pone.0141125.ref033">
                <label>33</label>
                <mixed-citation publication-type="journal"><name><surname>Hakkinen</surname><given-names>J</given-names></name>, <name><surname>Nyman</surname><given-names>G</given-names></name>. <article-title>Occlusion constraints and stereoscopic slant</article-title>. <source>Perception</source>. <year>1997</year>; <volume>26</volume>: <fpage>29</fpage>–<lpage>38</lpage>.
<?supplied-pmid 9196688?><pub-id pub-id-type="pmid">9196688</pub-id></mixed-citation>
              </ref>
              <ref id="pone.0141125.ref034">
                <label>34</label>
                <mixed-citation publication-type="journal"><name><surname>Lin</surname><given-names>JY</given-names></name>, <name><surname>Murray</surname><given-names>SO</given-names></name>, <name><surname>Boynton</surname><given-names>GM</given-names></name>. <article-title>Capture of attention to threatening stimuli without perceptual awareness</article-title>. <source>Curr Biol</source>. <year>2009</year>; <volume>19</volume>: <fpage>1118</fpage>–<lpage>1122</lpage>. <pub-id pub-id-type="doi">10.1016/j.cub.2009.05.021</pub-id>
<?supplied-pmid 19523828?><pub-id pub-id-type="pmid">19523828</pub-id></mixed-citation>
              </ref>
              <ref id="pone.0141125.ref035">
                <label>35</label>
                <mixed-citation publication-type="journal"><name><surname>Stein</surname><given-names>BE</given-names></name>, <name><surname>Stanford</surname><given-names>TR</given-names></name>, <name><surname>Rowland</surname><given-names>BA</given-names></name>. <article-title>Development of multisensory integration from the perspective of the individual neuron</article-title>. <source>Nat Rev Neurosci</source>. <year>2014</year>; <volume>15</volume>: <fpage>520</fpage>–<lpage>535</lpage>.
<?supplied-pmid 25158358?><pub-id pub-id-type="pmid">25158358</pub-id></mixed-citation>
              </ref>
              <ref id="pone.0141125.ref036">
                <label>36</label>
                <mixed-citation publication-type="book"><name><surname>Lewkowicz</surname><given-names>DJ</given-names></name>, <name><surname>Lickliter</surname><given-names>R</given-names></name>. <chapter-title>The development of intersensory perception: comparative perspectives</chapter-title><publisher-loc>Hillsdale, N.J.</publisher-loc>: <publisher-name>Erlbaum</publisher-name><year>1994</year>.</mixed-citation>
              </ref>
              <ref id="pone.0141125.ref037">
                <label>37</label>
                <mixed-citation publication-type="book"><name><surname>Stein</surname><given-names>BE</given-names></name>, <name><surname>Meredith</surname><given-names>MA</given-names></name>. <chapter-title>The merging of the senses</chapter-title><publisher-loc>Cambridge, Mass.</publisher-loc>: <publisher-name>MIT Press</publisher-name><year>1993</year>.</mixed-citation>
              </ref>
              <ref id="pone.0141125.ref038">
                <label>38</label>
                <mixed-citation publication-type="book"><name><surname>Keetels</surname><given-names>M</given-names></name>, <name><surname>Vroomen</surname><given-names>J</given-names></name>. <chapter-title>Perception of Synchrony between the Senses</chapter-title> In: <name><surname>Murray</surname><given-names>MM</given-names></name>, <name><surname>Wallace</surname><given-names>MT</given-names></name>, editors. <source>The Neural Bases of Multisensory Processes</source>. <publisher-loc>Boca Raton (FL)</publisher-loc>: <publisher-name>CRC Press</publisher-name>
<year>2012</year>.</mixed-citation>
              </ref>
            </ref-list>
          </back>
        </article>
      </metadata>
    </record>
  </GetRecord>
</OAI-PMH>
