<?xml version="1.0" encoding="UTF-8"?>
<OAI-PMH xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/ http://www.openarchives.org/OAI/2.0/OAI-PMH.xsd">
  <responseDate>2024-05-03T07:19:03Z</responseDate>
  <request verb="GetRecord" identifier="oai:pubmedcentral.nih.gov:3608650" metadataPrefix="pmc">https:/www.ncbi.nlm.nih.gov/pmc/oai/oai.cgi</request>
  <GetRecord>
    <record>
      <header>
        <identifier>oai:pubmedcentral.nih.gov:3608650</identifier>
        <datestamp>2013-04-03</datestamp>
        <setSpec>plosone</setSpec>
        <setSpec>pmc-open</setSpec>
      </header>
      <metadata>
        <article xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" xsi:schemaLocation="https://jats.nlm.nih.gov/ns/archiving/1.3/ https://jats.nlm.nih.gov/archiving/1.3/xsd/JATS-archivearticle1-3.xsd" article-type="research-article">
          <front>
            <journal-meta>
              <journal-id journal-id-type="nlm-ta">PLoS One</journal-id>
              <journal-id journal-id-type="iso-abbrev">PLoS ONE</journal-id>
              <journal-id journal-id-type="publisher-id">plos</journal-id>
              <journal-id journal-id-type="pmc">plosone</journal-id>
              <journal-title-group>
                <journal-title>PLoS ONE</journal-title>
              </journal-title-group>
              <issn pub-type="epub">1932-6203</issn>
              <publisher>
                <publisher-name>Public Library of Science</publisher-name>
                <publisher-loc>San Francisco, USA</publisher-loc>
              </publisher>
            </journal-meta>
            <article-meta>
              <article-id pub-id-type="accession">PMC3608650</article-id>
              <article-id pub-id-type="pmcid">PMC3608650</article-id>
              <article-id pub-id-type="pmc-uid">3608650</article-id>
              <article-id pub-id-type="pmid">23555588</article-id>
              <article-id pub-id-type="pmid">23555588</article-id>
              <article-id pub-id-type="publisher-id">PONE-D-12-26609</article-id>
              <article-id pub-id-type="doi">10.1371/journal.pone.0058594</article-id>
              <article-categories>
                <subj-group subj-group-type="heading">
                  <subject>Research Article</subject>
                </subj-group>
                <subj-group subj-group-type="Discipline-v2">
                  <subject>Biology</subject>
                  <subj-group>
                    <subject>Neuroscience</subject>
                    <subj-group>
                      <subject>Cognitive Neuroscience</subject>
                      <subj-group>
                        <subject>Cognition</subject>
                      </subj-group>
                    </subj-group>
                    <subj-group>
                      <subject>Neuroimaging</subject>
                      <subj-group>
                        <subject>fMRI</subject>
                      </subj-group>
                    </subj-group>
                    <subj-group>
                      <subject>Sensory Systems</subject>
                      <subj-group>
                        <subject>Visual System</subject>
                      </subj-group>
                    </subj-group>
                  </subj-group>
                </subj-group>
                <subj-group subj-group-type="Discipline-v2">
                  <subject>Social and Behavioral Sciences</subject>
                  <subj-group>
                    <subject>Psychology</subject>
                    <subj-group>
                      <subject>Behavior</subject>
                      <subject>Cognitive Psychology</subject>
                      <subject>Experimental Psychology</subject>
                    </subj-group>
                  </subj-group>
                </subj-group>
              </article-categories>
              <title-group>
                <article-title>Good Exemplars of Natural Scene Categories Elicit Clearer Patterns than Bad Exemplars but Not Greater BOLD Activity</article-title>
                <alt-title alt-title-type="running-head">Decoding Natural Scenes Categories</alt-title>
              </title-group>
              <contrib-group>
                <contrib contrib-type="author">
                  <name>
                    <surname>Torralbo</surname>
                    <given-names>Ana</given-names>
                  </name>
                  <xref ref-type="aff" rid="aff1">
                    <sup>1</sup>
                  </xref>
                  <xref ref-type="corresp" rid="cor1">
                    <sup>*</sup>
                  </xref>
                </contrib>
                <contrib contrib-type="author">
                  <name>
                    <surname>Walther</surname>
                    <given-names>Dirk B.</given-names>
                  </name>
                  <xref ref-type="aff" rid="aff2">
                    <sup>2</sup>
                  </xref>
                </contrib>
                <contrib contrib-type="author">
                  <name>
                    <surname>Chai</surname>
                    <given-names>Barry</given-names>
                  </name>
                  <xref ref-type="aff" rid="aff3">
                    <sup>3</sup>
                  </xref>
                </contrib>
                <contrib contrib-type="author">
                  <name>
                    <surname>Caddigan</surname>
                    <given-names>Eamon</given-names>
                  </name>
                  <xref ref-type="aff" rid="aff4">
                    <sup>4</sup>
                  </xref>
                </contrib>
                <contrib contrib-type="author">
                  <name>
                    <surname>Fei-Fei</surname>
                    <given-names>Li</given-names>
                  </name>
                  <xref ref-type="aff" rid="aff3">
                    <sup>3</sup>
                  </xref>
                </contrib>
                <contrib contrib-type="author">
                  <name>
                    <surname>Beck</surname>
                    <given-names>Diane M.</given-names>
                  </name>
                  <xref ref-type="aff" rid="aff5">
                    <sup>5</sup>
                  </xref>
                </contrib>
              </contrib-group>
              <aff id="aff1">
                <label>1</label>
                <addr-line>Institute of Cognitive Neuroscience, University College London, London, United Kingdom</addr-line>
              </aff>
              <aff id="aff2">
                <label>2</label>
                <addr-line>Department of Psychology, The Ohio State University, Columbus, Ohio, United States of America</addr-line>
              </aff>
              <aff id="aff3">
                <label>3</label>
                <addr-line>Department of Computer Science, Stanford University, Stanford, California, United States of America</addr-line>
              </aff>
              <aff id="aff4">
                <label>4</label>
                <addr-line>Institute for Collaborative Biotechnologies, University of California Santa Barbara, Santa Barbara, California, United States of America</addr-line>
              </aff>
              <aff id="aff5">
                <label>5</label>
                <addr-line>Beckman Institute and Psychology Department, University of Illinois, Urbana-Champaign, Illinois, United States of America</addr-line>
              </aff>
              <contrib-group>
                <contrib contrib-type="editor">
                  <name>
                    <surname>Op de Beeck</surname>
                    <given-names>Hans P.</given-names>
                  </name>
                  <role>Editor</role>
                  <xref ref-type="aff" rid="edit1"/>
                </contrib>
              </contrib-group>
              <aff id="edit1">
                <addr-line>University of Leuven, Belgium</addr-line>
              </aff>
              <author-notes>
                <corresp id="cor1">* E-mail: <email>a.torralbo@ucl.ac.uk</email></corresp>
                <fn fn-type="COI-statement">
                  <p><bold>Competing Interests: </bold>The authors acknowledge a Microsoft Research New Faculty Fellowship. This Fellowship was given as a no-string attached gift, not a sponsored research. Thus this does not alter the authors’ adherence to all the PLOS ONE policies on sharing data and materials.</p>
                </fn>
                <fn fn-type="con">
                  <p>Conceived and designed the experiments: AT DW BC EC LF DB. Performed the experiments: AT DW BC EC. Analyzed the data: AT DW BC EC. Contributed reagents/materials/analysis tools: AT DW BC EC LF DB. Wrote the paper: AT DW BC EC LF DB.</p>
                </fn>
              </author-notes>
              <pub-date pub-type="collection">
                <year>2013</year>
              </pub-date>
              <pub-date pub-type="epub">
                <day>26</day>
                <month>3</month>
                <year>2013</year>
              </pub-date>
              <volume>8</volume>
              <issue>3</issue>
              <elocation-id>e58594</elocation-id>
              <history>
                <date date-type="received">
                  <day>1</day>
                  <month>9</month>
                  <year>2012</year>
                </date>
                <date date-type="accepted">
                  <day>6</day>
                  <month>2</month>
                  <year>2013</year>
                </date>
              </history>
              <permissions>
                <copyright-statement>© 2013 Torralbo et al</copyright-statement>
                <copyright-year>2013</copyright-year>
                <copyright-holder>Torralbo et al</copyright-holder>
                <license xlink:href="http://creativecommons.org/licenses/by/4.0/">
                  <license-p>This is an open-access article distributed under the terms of the Creative Commons Attribution License, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are properly credited.</license-p>
                </license>
              </permissions>
              <abstract>
                <p>Within the range of images that we might categorize as a “beach”, for example, some will be more representative of that category than others. Here we first confirmed that humans could categorize “good” exemplars better than “bad” exemplars of six scene categories and then explored whether brain regions previously implicated in natural scene categorization showed a similar sensitivity to how well an image exemplifies a category. In a behavioral experiment participants were more accurate and faster at categorizing good than bad exemplars of natural scenes. In an fMRI experiment participants passively viewed blocks of good or bad exemplars from the same six categories. A multi-voxel pattern classifier trained to discriminate among category blocks showed higher decoding accuracy for good than bad exemplars in the PPA, RSC and V1. This difference in decoding accuracy cannot be explained by differences in overall BOLD signal, as average BOLD activity was either equivalent or higher for bad than good scenes in these areas. These results provide further evidence that V1, RSC and the PPA not only contain information relevant for natural scene categorization, but their activity patterns mirror the fundamentally graded nature of human categories. Analysis of the image statistics of our good and bad exemplars shows that variability in low-level features and image structure is higher among bad than good exemplars. A simulation of our neuroimaging experiment suggests that such a difference in variance could account for the observed differences in decoding accuracy. These results are consistent with both low-level models of scene categorization and models that build categories around a prototype.</p>
              </abstract>
              <funding-group>
                <funding-statement>This work was funded by NIH grant 1 R01 EY019429 (LFF, DMB, DBW), a Beckman Postdoctoral Fellowship (DBW), a Microsoft Research New Faculty Fellowship (LFF) and the Frank Moss Gift Fund (LFF). The funders had no role in study design, data collection and analysis, decision to publish, or preparation of the manuscript.</funding-statement>
              </funding-group>
              <counts>
                <page-count count="12"/>
              </counts>
            </article-meta>
          </front>
          <body>
            <sec id="s1">
              <title>Introduction</title>
              <p>Human observers are able to quickly and efficiently categorize briefly presented images of natural scenes <xref rid="pone.0058594-Potter1" ref-type="bibr">[1]</xref>–<xref rid="pone.0058594-Walther1" ref-type="bibr">[5]</xref>, whether that category is defined by an object within the scene or describes a property of the whole scene. Similarly, brain measures indicate that natural scene images can evoke differential activation very early in processing <xref rid="pone.0058594-Thorpe1" ref-type="bibr">[6]</xref>–<xref rid="pone.0058594-Rousselet2" ref-type="bibr">[8]</xref>. However, not all natural scenes are equivalent in regard to category membership. Some images are better exemplars of their category than others. Here we explore the effects of category membership on both human behavior and fMRI brain activity.</p>
              <p>Pioneering work in fMRI, using univariate statistical techniques, revealed that the parahippocampal place area (PPA) and the retrosplenial cortex (RSC) play a key role in processing scenes as opposed to isolated objects <xref rid="pone.0058594-Epstein1" ref-type="bibr">[9]</xref>–<xref rid="pone.0058594-Aguirre1" ref-type="bibr">[11]</xref>. More recently, this work has been extended to assess the role of these regions in natural scene categorization <xref rid="pone.0058594-Walther1" ref-type="bibr">[5]</xref>, <xref rid="pone.0058594-Chai1" ref-type="bibr">[12]</xref>–<xref rid="pone.0058594-Park1" ref-type="bibr">[18]</xref>. Importantly, this body of work has moved away from standard univariate statistical techniques and instead used multivariate techniques that take advantage of the pattern of activity across an area.</p>
              <p>For example, Walther et al. <xref rid="pone.0058594-Walther1" ref-type="bibr">[5]</xref> used multi-voxel pattern analysis to show that activity patterns in the PPA and RSC, as well as in primary visual cortex (V1) and the lateral occipital complex (LOC), can be used to distinguish between scene categories. More importantly, scene categories could not only be discriminated in PPA, RSC and LOC, but decoding in these regions mirrored behavioral measures in two ways. First, the distribution of decoding errors in these regions, unlike in V1, correlated well with the distribution of behavioral errors made by subjects performing a similar scene categorization task. Second, PPA, and not RSC, LOC or V1, showed a decrease in decoding accuracy when the scenes were presented up-down inverted; a similar drop in accuracy was observed in a related behavioral categorization task.</p>
              <p>Here we ask whether decoding accuracy in these same regions correlates with another aspect of human categorization behavior critical to the concept of a category, that is, the degree to which an image exemplifies its category. If these regions are sensitive to actual scene categories, then they should also be sensitive to the degree to which an image denotes a particular scene category. For example, within the range of images that we might categorize as a “beach,” some are more representative of that category than others. Will the degree to which an image exemplifies the category “beach” influence decoding in visual cortex?</p>
              <p>Such an effect could suggest a connection between the perceived category membership of a scene and its neural representation in these areas. A difference in decoding accuracy could also be due to differences in the strength of the correlation between low-level visual features and scene category. It is possible, after all, that what determines whether a particular image is a good exemplar of a category is the degree to which its features correlate with a category prototype. Thus, regardless of what mediates better decoding accuracy for good than bad exemplars, such a correlation with human judgments would further implicate these regions in the representation of scene category.</p>
              <p>We first verified behaviorally that good exemplars (rated as such by separate observers) were categorized more accurately and quickly than bad exemplars of a category. Then, using a similar approach to Walther et al. <xref rid="pone.0058594-Walther1" ref-type="bibr">[5]</xref>, we asked whether fMRI decoding accuracy in any of the regions previously implicated in natural scene categorization (V1, LOC, RSC and PPA) showed a similar good versus bad exemplar effect.</p>
              <p>We asked a group of observers to rate 4025 images from six natural scene categories (beaches, city streets, forests, highways, mountains and offices) for how representative the images were of their respective category. Images were then grouped into good, medium or bad exemplars of each category based on their average ratings. Another group of participants then took part in two experimental sessions: a behavioral session, in which they categorized these (briefly presented) images, and an fMRI session, in which they passively viewed the images while being scanned. Data from the fMRI session was submitted to two analyses: 1) a univariate linear regression analysis to compare the percent signal change for good and bad exemplars, and 2) multi-voxel pattern analysis to determine whether any of the regions explored contained stronger category signals for good category exemplars than bad. Finally, we analyzed the images used in the experiments in order to explore what properties might make them either good or bad exemplars of their respective natural scene categories.</p>
            </sec>
            <sec sec-type="materials|methods" id="s2">
              <title>Materials and Methods</title>
              <sec id="s2a">
                <title>Participants</title>
                <p>Nine participants from the University of Illinois (5 females, mean age 31), with normal or corrected-to-normal vision, participated in the behavioral and fMRI sessions for monetary reward.</p>
              </sec>
              <sec id="s2b">
                <title>Ethics Statement</title>
                <p>Both experiments were approved by the Institutional Review Board of the University of Illinois and all participants gave written informed consent according to the principles of Declaration of Helsinki.</p>
              </sec>
              <sec id="s2c">
                <title>Stimuli</title>
                <p>4025 color images from 6 different categories (beaches, city streets, forests, highways, mountains and offices) were downloaded from the worldwide web via multiple search engines, using the six category names and their synonyms as search terms. These images were then posted to Amazon Mechanical Turk (AMT) (<ext-link ext-link-type="uri" xlink:href="http://aws.amazon.com/mturk/">http://aws.amazon.com/mturk/</ext-link>) to be rated for how representative they were of their category. Anonymous users of the AMT web service, located all around the world, performed the task on their own computer and received $0.001 per image rated. Images were 400×300 pixels in size, but varied in the visual angle subtended across participants by the monitors and viewing distance they used. Images were shown for approximately 250 ms. Timing was controlled by the Javascript timer in the users’ web browsers, thus the actual presentation time could vary slightly depending on the users’ computer settings. Users were asked to rate each image for how good of an exemplar of a given category the image was (e.g. “How representative is this image of a BEACH?”). Responses were recorded as clicks on a graphical user interface (see <xref ref-type="fig" rid="pone-0058594-g001">Figure 1</xref> for an example of the interface<bold>)</bold> and could range from 1 (“poor” ) to 5 (“good”). In order to ensure that the users of the AMT service were committed to the task, we placed a random “check” trial every 10 trials. Each check trial repeated one of the images randomly chosen among the preceding 10 trials. We computed a “discount” score for each user such that if they responded to the check trial with a different category label they received a discount of 5 and if they chose a different rating than their previous response they received a discount corresponding to the absolute value of the difference between the new and old rating. The discount score was summed over all the check trials for each user. If the total discount score exceeded 10, the data for this user was discarded. This ensured that we only retained the data of consistent and committed users. Besides being rated from 1 to 5, images could also be rated as a member of any of the other categories or as “none of them”. Images that received this response for more than one quarter of the total ratings were not used in subsequent experiments (6.36% of the total). The remaining images each received 20 ratings per image on average.</p>
                <fig id="pone-0058594-g001" orientation="portrait" position="float">
                  <object-id pub-id-type="doi">10.1371/journal.pone.0058594.g001</object-id>
                  <label>Figure 1</label>
                  <caption>
                    <title>Interface used by the AMT workers to rate our images.</title>
                    <p>Users could replay the image once by clicking on a button placed below the image.</p>
                  </caption>
                  <graphic xlink:href="pone.0058594.g001"/>
                </fig>
                <p>For each scene category we first selected the 80 images with the highest ratings as the candidates for “good” exemplars and the 80 images with the lowest ratings as the candidates for “bad” exemplars. In addition, we chose the 80 images closest to the midpoint between the “good” and “bad” average ratings for “medium” exemplars to be used in a preliminary staircasing procedure. We then acquired additional ratings on these candidate images to further refine our good and bad image sets. After one more round of rating, each of the 240 selected images in each category had an average of 137 ratings/image.</p>
                <p>For each category, images from this second round of ratings were sorted in order of descending average rating. The 60 images with the highest average ratings from the 80 “good” candidates were labeled as “good” exemplars. Similarly, we selected 60 “bad” exemplars as the images with the lowest ratings. We selected 60 “medium” exemplars corresponding to the 60 central images in that ranked list. The sets of images in each rating class (good, medium, and bad) were mutually exclusive. Some examples of the images from each of the rating classes can be seen in <xref ref-type="fig" rid="pone-0058594-g002">Figure 2</xref>. Mean ratings at this stage were 4.7, 4 and 2.9 for good, medium and bad exemplars, respectively. The distribution of ratings looked similar for all 6 categories.</p>
                <fig id="pone-0058594-g002" orientation="portrait" position="float">
                  <object-id pub-id-type="doi">10.1371/journal.pone.0058594.g002</object-id>
                  <label>Figure 2</label>
                  <caption>
                    <title>Examples of good, medium, and bad images used in the behavioral and fMRI experiments.</title>
                  </caption>
                  <graphic xlink:href="pone.0058594.g002"/>
                </fig>
                <sec id="s2c1">
                  <title>Behavioral experiment</title>
                  <p>Good, medium and bad images were scaled to 800×600 pixels. For each of the 6 categories, 60 images from each rating class were used, bringing the total number of images to 1080. Each image was presented on a CRT-monitor (resolution 1024×768, display rate 75 Hz) and subtended 22×17 degrees of visual angle. Images were centered on a 50% grey background. Stimulus presentation and response recording were controlled using the open source Vision Egg package for Python <xref rid="pone.0058594-Straw1" ref-type="bibr">[19]</xref>.</p>
                </sec>
                <sec id="s2c2">
                  <title>fMRI experiment</title>
                  <p>Stimuli were the same 360 good and 360 bad images used in the behavioral experiment (60 for each category). For four participants they were projected in a pair of MR-compatible LCD goggles (Resonance Technologies, Northridge, CA) running at a resolution of 800×600, while for the other five they were presented using back-projection set at a resolution of 800×600. In each case the images subtended 22×17 degrees of visual angle. Images were presented using the Psychophysics Toolbox for Matlab <xref rid="pone.0058594-Brainard1" ref-type="bibr">[20]</xref>–<xref rid="pone.0058594-Pelli1" ref-type="bibr">[21]</xref>.</p>
                </sec>
              </sec>
              <sec id="s2d">
                <title>Procedure</title>
                <sec id="s2d1">
                  <title>Behavioral experiment</title>
                  <p>Participants performed six-alternative forced-choice categorization of the images. The session was comprised of: a training phase during which participants learned the response mappings between the six categories and their corresponding keys; a staircasing phase, during which image presentation time varied to reach 65% classification accuracy for each individual participant; and an experimental phase during which we tested the participants’ categorization performance at his or her individual presentation time.</p>
                  <p>During each trial participants viewed a fixation cross for 500 ms prior to a brief image presentation (image duration depended on the experiment phase) that was followed by a perceptual mask (500 ms duration; see <xref rid="pone.0058594-Walther1" ref-type="bibr">[5]</xref> for examples of the masks). Finally, a blank screen was presented for 2000 ms. Participants were asked to press one of six keys on the computer keyboard to indicate the category of the viewed image. If no input was given, the trial timed out after the 2000 ms fixation period and was excluded from the analysis.</p>
                  <p>During the training phase images were presented for 250 ms to ensure adequate learning of response mappings. Once an 80% accuracy rate was achieved in the training phase (after 60 trials on average), the QUEST algorithm <xref rid="pone.0058594-KingSmith1" ref-type="bibr">[22]</xref> was used to staircase the image presentation time to achieve 65% classification accuracy for each individual participant (53 trials on average). Images from our set of medium exemplars were used for training and staircasing. Staircasing was terminated when the standard deviation of the display times over a block was less than the refresh period (13.3 ms) of the monitor. The image presentation duration obtained during the staircasing phase of the experiment was used during the testing phase of the experiment. The average presentation time across subjects was 64 ms (ranging from 26 ms to 133 ms).</p>
                  <p>During the testing phase, good and bad exemplars alternated in separate blocks of 20 images each. Participants completed a total of 36 experimental blocks. The good and bad image sets were only viewed in the testing phase of the experiment and each image was shown exactly once across the whole session. Participants received auditory feedback (800 Hz pure tone, 100 ms) for incorrect responses in both the training and staircasing phases of the experiment, but no feedback was provided in the testing phase.</p>
                </sec>
                <sec id="s2d2">
                  <title>fMRI experiment</title>
                  <p>In order to ensure high signal strength in the scanner, images were presented for longer in the fMRI experiment than in the behavioral experiment. Participants performed the fMRI experiment first. They passively viewed images for 1.6 seconds each, arranged in blocks of 10 images, with no interstimulus interval. Each run contained 6 different blocks corresponding to the 6 categories. Blocks of images were interleaved with blank periods lasting 12 seconds to allow BOLD response to return to baseline. The scanning session was comprised of a total of 12 runs, with 6 runs of good and 6 runs of bad images presented in alternating order with the starting condition (good or bad) counterbalanced across subjects. The category order was randomized but replicated for two consecutive runs (one good and one bad). Each image was shown once in the whole session.</p>
                </sec>
              </sec>
              <sec id="s2e">
                <title>MRI Acquisition and Preprocessing</title>
                <p>Scanning was performed at the Biological Imaging Center at the University of Illinois at Urbana-Champaign. T1-weighted anatomical images and gradient-echo echo planar (EPI) images were acquired in a 3T-head only scanner (Allegra, Siemens) using a standard head coil. EPI images were collected from the entire brain (TR = 2 s, TE = 30 ms, flip angle = 90, matrix 64×64; FOV 22 cm) in interleaved order. 90 volumes of 34 axial slices (3.438×3.438 mm in-plane resolution) were collected in each functional run. Slice thickness was 3 mm and gap size was 1 mm. The first 4 volumes of each run were discarded. A high resolution structural scan (1.25 mm×1.25 mm×1.25 mm; MPRAGE) was collected to assist in registering the images with the retinotopic mapping data. Functional data were motion corrected to the middle image of the 6th run, and normalized to the temporal mean of each run using AFNI <xref rid="pone.0058594-Cox1" ref-type="bibr">[23]</xref>. For the pattern recognition analysis no other image processing steps, such as spatial smoothing, were performed.</p>
                <sec id="s2e1">
                  <title>Multi voxel pattern analysis</title>
                  <p>To address whether the category-specific information in various brain regions differed between good and bad images we constructed a decoder previously shown to be effective for decoding scene category from multi-voxel fMRI activity <xref rid="pone.0058594-Walther1" ref-type="bibr">[5]</xref>. Specifically, after pre-processing, in a given ROI (see below for definition of the ROIs) we extracted the eight time points corresponding to each presentation block, shifted by 4 seconds to approximate the delay in the BOLD response. Multi-voxel pattern analysis was then performed separately on the good and bad runs. We elaborate on the procedure using good exemplars as an example, but the same procedures were used for the bad exemplars. Using five of the six “good” runs, a support vector machine (SVM) classifier with a linear kernel (C = 0.02) was trained to assign the correct category labels to patterns of fMRI activity in the ROI. The classifier was then tested on the fMRI activity from the left-out run. The classifier was trained and tested on each time point separately (as opposed to averaging activity across a block), and disagreements regarding the predicted category label within blocks were resolved by majority voting, i.e., each block was labeled with the category that was most frequently predicted among the eight volumes in the block. Ties were resolved by selecting the category with the largest SVM decision values before thresholding. The procedure was repeated six times with each of the six good runs left out in turn in a leave-one-run-out (LORO) cross validation procedure, thus generating predictions for the blocks in each run. Decoding accuracy was measured as the fraction of blocks with correct category predictions, providing an indication of the strength of the category-specific information in a given ROI for good exemplar images. The same LORO cross-validation procedure was performed for the bad image runs to arrive at the equivalent measure for bad exemplars. Significance of decoding accuracy results was established with a one-tailed t-test, comparing the mean of the accuracies over participants to chance level (1/6). A two-tailed, paired t-test was used to assess whether there was a significant difference in decoding accuracy between good and bad exemplars.</p>
                </sec>
                <sec id="s2e2">
                  <title>Univariate good/bad analysis</title>
                  <p>We performed a univariate linear regression analysis to compare the percent signal change in good and bad images. For this analysis, in addition to the pre-processing mentioned above, fMRI images were spatially smoothed (6 mm FWHM). We defined two regressors of interest: blocks of good images and blocks of bad images were modeled separately and convolved by a gamma function to approximate the hemodynamic response <xref rid="pone.0058594-Cohen1" ref-type="bibr">[24]</xref>. We performed a linear contrast between these two regressors and extracted the percent signal change in each participant’s V1, PPA and RSC ROIs. In each of the ROIs, mean percent signal change values for good and bad images were submitted to a two-tailed t-test to determine whether they differed significantly.</p>
                </sec>
                <sec id="s2e3">
                  <title>Regions of Interest</title>
                  <p>Based on previous work <xref rid="pone.0058594-Walther1" ref-type="bibr">[5]</xref> we identified 5 separate ROIs: V1, the parahippocampal place area (PPA), the retrosplenial cortex (RSC), the lateral occipital complex (LOC) and the fusiform face area (FFA).</p>
                  <p>In a separate scanning session, V1 was delineated using standard retinotopic mapping procedures and analyses described elsewhere: a meridian mapping procedure was used for participant 3 <xref rid="pone.0058594-Kastner1" ref-type="bibr">[25]</xref> and a traveling wave procedure was used for the remaining participants <xref rid="pone.0058594-Schneider1" ref-type="bibr">[26]</xref>. LOC, FFA, PPA and RSC were identified in separate functional localizer scans, where blocks of images of faces, houses, objects, scrambled objects, landscapes and cityscapes were presented. Each block consisted of 20 images of a given category, where each image was presented for 450 ms followed by a 330 ms inter-stimulus interval. In a given run, 4 blocks of each category were shown while a fixation block of 12 seconds was interleaved every two or three blocks. Participants performed a 1-back task, pressing a button each time an image was repeated. Two functional scans (139 volumes each) were recorded in a given session. All subjects were scanned in two sessions while 1 participant required an additional session due to a weak BOLD signal in the previous scans. The 3dAllineate function in AFNI <xref rid="pone.0058594-Cox1" ref-type="bibr">[23]</xref> was used to register the images across localizers and experimental sessions.</p>
                  <p>EPI images from the localizer runs were motion corrected, smoothed (4 mm FWHM) and normalized to the mean of each run. BOLD response in each type of block was modeled separately and convolved with a gamma variate function of the hemodynamic response <xref rid="pone.0058594-Cohen1" ref-type="bibr">[24]</xref>. ROIs were defined from linear contrasts as the sets of contiguous voxels that differentially activated in the following comparisons: PPA and RSC were identified by a (cityscapes, landscapes)&gt;objects contrast, LOC by an objects&gt;scrambled objects contrast and FFA was identified by a faces&gt;(objects, cityscapes, and landscapes) contrast. For all localizer contrasts, a maximum threshold of p&lt;2×10<sup>−3</sup> (uncorrected) was applied. Stricter thresholds were used when necessary to break clusters that spanned multiple ROIs. There was no overlap between any of the ROIs, and all ROI voxels were used for the pattern analysis without any further voxel selection.</p>
                </sec>
              </sec>
              <sec id="s2f">
                <title>Image Analysis</title>
                <p>To explore whether good images are more or less variable than bad images across a given feature space we extracted features describing the form and color of the images. To create a “form” space we created grayscale versions of the images that were downsampled to 600×450 pixels and convolved them with 64 Gabor filters (8 orientation×8 frequencies) with kernel size of 8×8 pixels. As a result we obtained a 64-element vector for each pixel location with each vector entry storing the response of one Gabor filter. We averaged the 64 element vectors from all of the 600×450 pixel locations to obtain a global description of our scene image.</p>
                <p>To capture the distribution of pixel colors over the entire image we created a “color” feature space. For this purpose, we described each image with a two dimensional histogram with the dimensions of hue and saturation discretized uniformly into 8 values, resulting in a 64 element vector (8 hues×8 saturations). The histogram was computed over the entire image, binning the color values from all of the 600×450 pixel locations to obtain a global description of our scene image.</p>
                <p>We then estimated the variance of the images as the distance of each image from the average image in each of these feature spaces. To this end we performed singular value decomposition of the covariance matrix of the feature vectors and summed the eigenvalues of the diagonalized covariance matrix. These eigenvalues represent the amount of variance in the feature vectors along the direction of the eigenvectors of the covariance matrix. Their sum captures the overall variance present in the feature representations of the images. We performed this step separately for the good and the bad exemplars for each scene category and compared variance for each scene category as well as pooled over all categories.</p>
              </sec>
              <sec id="s2g">
                <title>Simulation Analysis</title>
                <p>In order to evaluate if a difference in variance among the exemplars of categories can lead to the observed differences in fMRI decoding accuracy we performed a numerical simulation of our fMRI experiment. We modeled the neural activity elicited by scene categories as multivariate Gaussian distributions with isotropic covariance <inline-formula><inline-graphic xlink:href="pone.0058594.e001.jpg"/></inline-formula>. Activity for an exemplar was modeled as a random draw from this distribution. To account for the different variances between good and bad exemplars, we used smaller values of <inline-formula><inline-graphic xlink:href="pone.0058594.e002.jpg"/></inline-formula>for good <inline-formula><inline-graphic xlink:href="pone.0058594.e003.jpg"/></inline-formula>than bad <inline-formula><inline-graphic xlink:href="pone.0058594.e004.jpg"/></inline-formula> exemplars. In order for the simulation to closely follow the experiments, we estimated the mean of the multivariate Gaussian distribution for a scene category <inline-formula><inline-graphic xlink:href="pone.0058594.e005.jpg"/></inline-formula> from the sample mean of the patterns of voxel activity elicited in PPA by that category for each of our eight human subjects in turn. Note that we did not model the correlations between voxels in this simulation.</p>
                <p>We composed the time course of the simulated experiment to mirror the block design of our fMRI experiment: 12 seconds of fixation (modeled as zero neural activity) were followed by a 16-second block of images, composed of ten image activity patterns of 1.6 seconds each, randomly drawn from the same category distribution. Each category block was followed by 12 seconds of fixation, and for each block the activity was drawn from a different category distribution. We generated data for six runs, with each run containing six blocks, one from each category in a random order. The neural activity was then convolved with a Gamma function to model the hemodynamic response: <inline-formula><inline-graphic xlink:href="pone.0058594.e006.jpg"/></inline-formula>, with <italic>p</italic> = 8.6 and <italic>q</italic> = 0.547 <xref rid="pone.0058594-Pelli1" ref-type="bibr">[21]</xref>. Finally, we added normally distributed measurement noise from <inline-formula><inline-graphic xlink:href="pone.0058594.e007.jpg"/></inline-formula>. We estimated the standard deviation of the measurement noise from the residuals of the univariate regression analysis in the PPA as <inline-formula><inline-graphic xlink:href="pone.0058594.e008.jpg"/></inline-formula>. Once we had computed the simulated fMRI activity, we analyzed it with the same leave-one-run-out cross validation procedure as described for the multivoxel pattern analysis of our experimental data.</p>
                <p>To verify that a difference in variance between good and bad images would result in poorer decoding regardless of whether we ran a blocked or event-related fMRI design, we also simulated a fast event-related experiment. For this simulation we constructed six runs with 60 trials each. Following an initial fixation period of 12 seconds duration we added activity for an image from one of the six categories for 1.6 seconds, followed by 2.4 seconds fixation before the presentation of the next image. We randomly interleaved trials for 60 exemplars (10 from each of the six categories) within a run. Including a final fixation period of 12 seconds, this resulted in a total run length of 264 seconds, compared to 192 seconds for the blocked design. Activity for the event-related design was convolved with the same hemodynamic response function (HRF) as described above, and measurement noise with the same variance was added. We then performed a regression analysis separately on each run with regressors for each of the six categories. Regressors were convolved with the same HRF as the simulated neural activity. The sets of beta-weights for the categories were used as inputs to the leave-one-run-out cross validation analysis.</p>
                <p>We performed the block and the event-related simulations 100 times for each of the eight subjects, both for good and bad exemplars, each time with a new random draw of the exemplar activity and the measurement noise. Significance of the difference between the decoding accuracies for good and bad exemplars was computed with a two-tailed, paired t-test over eight subjects.</p>
              </sec>
            </sec>
            <sec id="s3">
              <title>Results</title>
              <sec id="s3a">
                <title>Behavioral Categorization Task</title>
                <p>Participants were significantly more accurate at categorizing the briefly presented good images than the bad images (92% and 66% respectively; t(8) = 11.57, <italic>p</italic>&lt;0.001; chance was 16.67%), and this effect was significant for all six categories (see <xref ref-type="fig" rid="pone-0058594-g003">Figure 3</xref>; t(8) = 5.86, <italic>p</italic>&lt;0.001 for beaches; t(8) = 5.13, <italic>p</italic>&lt;0.001 for city streets; t(8) = 8.36, <italic>p</italic>&lt;0.001 for forests; t(8) = 7.63, <italic>p</italic>&lt;0.001 for highways; t(8) = 6.03, <italic>p</italic>&lt;0.001 for mountains; and t(8) = 3.84, <italic>p</italic>&lt;0.01 for offices).</p>
                <fig id="pone-0058594-g003" orientation="portrait" position="float">
                  <object-id pub-id-type="doi">10.1371/journal.pone.0058594.g003</object-id>
                  <label>Figure 3</label>
                  <caption>
                    <title>Results of the behavioral categorization task.</title>
                    <p>Graph depicts categorization accuracy of good (green) and bad exemplars (orange) across categories. Error bars show standard error of the mean over nine subjects. The dashed line marks chance level (0.167). Good vs. bad comparisons are significant for all categories; **p&lt;0.01. ***p&lt;0.001.</p>
                  </caption>
                  <graphic xlink:href="pone.0058594.g003"/>
                </fig>
                <p>Response times revealed a similar effect. Accurate responses were significantly faster for good images than bad images (892 ms and 1028 ms respectively; t(8) = 8.82, <italic>p</italic>&lt;0.001), and this effect was significant across all the categories (t(8) = 8.53, <italic>p</italic>&lt;0.001 for beaches; t(8) = 6.27, <italic>p</italic>&lt;0.001 for forests; t(8) = 4.42, <italic>p</italic>&lt;0.01 for highways; t(8) = 5.2, <italic>p</italic>&lt;0.001 for mountains and t(8) = 2.58, <italic>p</italic>&lt;0.05 for offices) except for city streets in which the difference was marginally significant (t(8) = 2.10, <italic>p</italic> = 0.068).</p>
                <p>Moreover, to look for more fine-grained correlations between image rating and categorization accuracy we correlated these two measures separately for good and bad images. We observed a significant positive correlation between ratings and categorization accuracy for the bad images (r = .145, <italic>p</italic>&lt;0.05), indicating that across the bad exemplars, images that are less representative of the category were categorized less accurately. The same correlation was not significant for the good images (r = −0.06, <italic>p</italic> = 0.24). However, it should be noted that the lack of correlation here may be due to substantially smaller variability in the ratings of good images (SD = 0.53) than the bad images (SD = 1.25).</p>
              </sec>
              <sec id="s3b">
                <title>Multivariate fMRI Analysis</title>
                <p>Having established that humans do indeed find good exemplars easier to categorize than bad exemplars of a category, we asked what effect good and bad exemplars would have on fMRI decoding rates.</p>
                <p>Data from one participant was excluded from the fMRI analyses due to excessive movement and a low signal-to-noise ratio, and only 10 runs of fMRI data were included for another participant due to technical problems during data collection in the final 2 runs of the session. In separate functional scans (see ROIs section for details) we identified five ROIs (mean number of voxels and standard deviation in parenthesis): the PPA (93±28 voxels), the RSC (55±13 voxels), the LOC (72±32 voxels), the FFA (66±31 voxels) and V1 (229±161 voxels).</p>
                <p>If a particular ROI is sensitive to scene category, then it should be sensitive as well to the degree to which an image denotes a particular category. Thus, we should find a difference in the decoding accuracy of good images compared to bad images. We tested for the presence of such an effect in the decoding data in the following way. We trained and tested a decoder on good images, using LORO cross validation, and compared the resulting decoding accuracy to that obtained when the decoder was trained and tested on bad images. First, when we trained and tested the decoder on good images, decoding accuracy (rate of correctly predicting the viewed scene category from the voxels’ pattern activity) was significantly above chance (16.7%) in V1 (27%, t(7) = 2.57, <italic>p</italic>&lt;0.05), PPA (32%, t(7) = 4.88, <italic>p</italic>&lt;0.001) and RSC (29%, t(7) = 5.60, <italic>p</italic>&lt;0.001), but not in FFA and LOC (16%, <italic>p</italic> = 0.66 and 18%, <italic>p</italic> = 0.28 respectively). When we trained and tested the decoder on the bad images, decoding accuracy was significantly above chance in PPA (22%, t(7) = 2.31, <italic>p</italic>&lt;0.05) but not in the other ROIs (17%, <italic>p</italic> = 0.53 for V1; 20% for RSC, <italic>p</italic> = 0.11 for RSC; 19%, <italic>p</italic> = 0.09 for FFA and 18%, <italic>p</italic> = 0.21 for LOC).</p>
                <p>We looked for a good/bad effect by comparing the decoding performance for good versus bad images (2-tailed paired t-test). V1, PPA and RSC showed a significant decrement in decoding accuracy for bad exemplars relative to good exemplars: t(7) = 3.00, <italic>p</italic>&lt;0.05 for V1; t(7) = 2.76, <italic>p</italic>&lt;0.05 for PPA and t(7) = 2.45, <italic>p</italic>&lt;0.05 for RSC. These data suggest, as predicted, that these ROIs are sensitive to category information, i.e. that category-related information in these areas is clearer for good than bad exemplars. No such effect was found in LOC and FFA (t&lt;1 for both ROIs; see <xref ref-type="fig" rid="pone-0058594-g004">Figure 4</xref>), but this is not surprising given that decoding did not exceed chance in these regions for either good or bad exemplars. See <xref ref-type="fig" rid="pone-0058594-g005">Figure 5</xref> for confusion matrices for V1, PPA and RSC.</p>
                <fig id="pone-0058594-g004" orientation="portrait" position="float">
                  <object-id pub-id-type="doi">10.1371/journal.pone.0058594.g004</object-id>
                  <label>Figure 4</label>
                  <caption>
                    <title>Accuracy of decoding scene category from V1, FFA, LOC, PPA and RSC.</title>
                    <p>A decoder was trained and tested on fMRI activity evoked by good exemplars (green), and trained and tested on fMRI activity evoked by bad exemplars (orange). Error bars show standard error of the mean across subjects. The dotted line marks chance level (0.167). *p&lt;0.05; **p&lt;0.01.</p>
                  </caption>
                  <graphic xlink:href="pone.0058594.g004"/>
                </fig>
                <fig id="pone-0058594-g005" orientation="portrait" position="float">
                  <object-id pub-id-type="doi">10.1371/journal.pone.0058594.g005</object-id>
                  <label>Figure 5</label>
                  <caption>
                    <title>Confusion matrices for decoding of scenes categories in V1, PPA and RSC.</title>
                    <p>The decoder was trained and tested on good exemplars (left column) and trained and tested on bad exemplars (right column). The rows of each matrix indicate the categories presented (ground truth) and the columns indicate the predictions of the decoder. Diagonal entries are correct decoding rates for the respective categories, and off-diagonal entries indicate decoding errors.</p>
                  </caption>
                  <graphic xlink:href="pone.0058594.g005"/>
                </fig>
                <p>Thus, in keeping with previous research <xref rid="pone.0058594-Walther1" ref-type="bibr">[5]</xref>, <xref rid="pone.0058594-Walther2" ref-type="bibr">[14]</xref>, we once again show a correlation between activity in visual cortex and human behavior: images that humans find easier to categorize are also more accurately categorized by the decoder. However, unlike previous research, this correlation extends to V1, suggesting that differences between good and bad exemplars include features encoded in V1.</p>
              </sec>
              <sec id="s3c">
                <title>Image Analysis</title>
                <p>One possibility for the decoding advantage in both V1 and later visual areas is that good exemplars are all more similar to a particular prototype than bad exemplars. To asses whether this might be the case, we computed a pixel-wise average image of all 60 images from a category, separately for the good and bad exemplars (<xref ref-type="fig" rid="pone-0058594-g006">Figure 6</xref>). Specifically, we simply averaged the RGB values at each pixel in the image. Interestingly, the average image of the good exemplars reveals fairly clear spatial structure that makes it possible to identify the category (e.g., a mountain peak can be made out in the good mountain average). The same is not true of the pixel-wise average of the bad exemplars; little systematic structure is discernible in these images (<xref ref-type="fig" rid="pone-0058594-g006">Figure 6</xref>). This analysis not only suggests that good exemplars are more similar to each other in structure than bad exemplars, it is also suggestive of a potential prototype for each category. For instance, a prototypical mountain scene may contain a single peak in the center; a prototypical city street scene may contain a street that recedes in depth with tall buildings on either side.</p>
                <fig id="pone-0058594-g006" orientation="portrait" position="float">
                  <object-id pub-id-type="doi">10.1371/journal.pone.0058594.g006</object-id>
                  <label>Figure 6</label>
                  <caption>
                    <title>Average images.</title>
                    <p>Pixel-wise RGB average images of good (first row) and bad (second row) exemplars across the categories (columns).</p>
                  </caption>
                  <graphic xlink:href="pone.0058594.g006"/>
                </fig>
                <p>To further explore whether good images are in fact less variable in low-level feature space than bad images we computed how far each image is from the average image in two feature spaces. Because, as our pixel-wise average images illustrate, good exemplars appear to be distinguished from bad exemplars in the consistency of both their spatial layout and color, we chose one feature space that capture the form (or structure) of scenes and one that captured the distribution of colors across an image. <xref ref-type="fig" rid="pone-0058594-g007">Figure 7</xref> shows the variance (mean square distance from the mean image) for each feature space. In the “form” space (using Gabor filters), the variance of the good exemplars is smaller than the variance of the bad exemplars, and this is consistent for all the categories (see the left panel of <xref ref-type="fig" rid="pone-0058594-g007">Figure 7</xref>). A similar pattern can be seen for the color space, although it is less consistent across categories: 4 out of the 6 categories have a smaller variance for good than bad exemplars (see right panel of <xref ref-type="fig" rid="pone-0058594-g007">Figure 7</xref>).</p>
                <fig id="pone-0058594-g007" orientation="portrait" position="float">
                  <object-id pub-id-type="doi">10.1371/journal.pone.0058594.g007</object-id>
                  <label>Figure 7</label>
                  <caption>
                    <title>Variance across feature spaces.</title>
                    <p>Variance of good and bad exemplars across form and color (left and right panels, respectively) spaces.</p>
                  </caption>
                  <graphic xlink:href="pone.0058594.g007"/>
                </fig>
                <p>In short, analysis of the images themselves suggests that good exemplars of a category have more similar low-level image statistics to each other than bad exemplars do. We note that this similarity is itself a novel finding as our raters were asked only to rate the images for how representative they were of their category. At no point did we suggest to the raters that they match the image to a prototype or that representative images should be similar to one another. These more consistent low-level image statistics for good than bad exemplars of a category not only could contribute to the more accurate decoding of good images in the brain but it is also suggestive of the categories being organized around a prototype.</p>
              </sec>
              <sec id="s3d">
                <title>Simulation Analysis</title>
                <p>Is it plausible that the differences in within-category variance between good and bad exemplars gave rise to the differences in decoding accuracy that we found in the fMRI data? To address this question we performed a computer simulation of the fMRI experiment in which we manipulated the variance in the patterns of activity evoked by good and bad exemplars. We simulated the neural activity for exemplars of scene categories as multivariate Gaussian distributions around a prototype mean. So that our prototype approximated the patterns observed in our data we simulated the prototype by taking the mean activity at each voxel for each of the six scene categories in the PPA from each of the eight subjects in turn. Importantly, we used two different settings for the variance of the category distributions, low variance to simulate good exemplars and high variance to simulate bad exemplars. In accordance with the image analysis results we set the variance for bad exemplars to double the value for good exemplars.</p>
                <p>Neural activity patterns were assembled into a sequence of blocks with fixation periods (zero activity) closely mirroring our experimental design. The neural activity was then convolved with a realistic hemodynamic response function (HRF), and normally distributed measurement noise was added. The variance of the measurement noise was estimated from the residuals of the univariate regression analysis of the experimental data. We then performed the same LORO cross validation analysis that we performed on the experimental data. The simulation was repeated 100 times with the PPA voxel activity from each of the eight subjects to estimate the category distribution means. Just as in our fMRI experiment, we found significantly higher decoding accuracy for good (low variance) than bad (high variance) exemplars (t(7) = 20.9, p&lt;0.001, two-tailed, paired t test; <xref ref-type="fig" rid="pone-0058594-g008">Figure 8</xref>).</p>
                <fig id="pone-0058594-g008" orientation="portrait" position="float">
                  <object-id pub-id-type="doi">10.1371/journal.pone.0058594.g008</object-id>
                  <label>Figure 8</label>
                  <caption>
                    <title>Decoding accuracy for simulated fMRI activity for good and bad exemplars.</title>
                    <p>Results simulating a block design are on the left and a fast event-related design on the right. We observe a significant decrease in decoding accuracy from good to bad exemplars in both designs. Error bars are SEM over eight subjects. ***p&lt;0.001.</p>
                  </caption>
                  <graphic xlink:href="pone.0058594.g008"/>
                </fig>
                <p>We chose a block design for our experiment because of its more robust signal compared to event-related designs <xref rid="pone.0058594-Aguirre2" ref-type="bibr">[27]</xref>. Would we expect to see a similar pattern of decoding accuracy if we had used an event-related design? To make the comparison we also simulated an event-related experiment with comparable total scan time and the same number of image exemplars. We found the same effect of higher decoding accuracy for good than bad exemplars as in the block design (t(7) = 10.8, p&lt;0.001 two-tailed, paired t test; <xref ref-type="fig" rid="pone-0058594-g008">fig. 8</xref>), although decoding accuracy overall was lower for the event-related than the block design, validating our design choice.</p>
                <p>Overall, these results suggest that the higher variance among bad compared to good exemplars may account for the difference in decoding that we find. Furthermore, this would be the case regardless of the design we used. Indeed, variance is an issue whenever one is creating a category (training) or assessing membership (testing), because it means that any one exemplar is a less reliable predictor of either the mean (i.e. a prototype) or boundaries of the category. This is certainly true of our classification analysis, but we note that the human brain could also suffer from the same problem. In other words, participants may rate images that share some but not all features of the majority of members of a category as bad, resulting in a set of images whose physical attributes vary more widely than the set of attributes that more clearly and reliably predict the category.</p>
                <p>Finally, we would like to note that although variance may be an issue for the purposes of creating or assessing category membership, the same variability is a benefit in distinguishing between members of a category. For example, memory for a particular beach will be better when the set of possible beaches share fewer attributes <xref rid="pone.0058594-Standing1" ref-type="bibr">[28]</xref>–<xref rid="pone.0058594-Brady1" ref-type="bibr">[29]</xref>. Similarly, we would predict greater fMRI decoding accuracy in distinguishing <italic>between exemplars</italic> when the set of images are drawn from the bad exemplar than the good exemplar sets. However, because we were interested in the category signal we did not design the experiment in such a way that we could separate out the individual exemplars.</p>
              </sec>
              <sec id="s3e">
                <title>Univariate fMRI Analysis</title>
                <p>In our multivariate analysis we have shown that V1, PPA and RSC contain category-related information such that in these ROIs category is decoded more accurately from good than from bad exemplars of natural scenes. How might these results relate to the mean fMRI signal in these areas?</p>
                <p>The univariate analysis revealed that the superior decoding accuracy for good exemplars is not due to a higher BOLD signal for good than bad exemplars. The percent signal change was significantly higher for bad exemplars than good exemplars in PPA (t(7) = 4.34, <italic>p</italic>&lt;0.01; see <xref ref-type="fig" rid="pone-0058594-g009">Figure 9</xref>) but failed to reach significance in RSC (t &lt;1) and V1 (t&lt;1). These data suggest that the higher decoding accuracy for good exemplars is due to clearer activity patterns in these ROIs rather than higher overall BOLD signal.</p>
                <fig id="pone-0058594-g009" orientation="portrait" position="float">
                  <object-id pub-id-type="doi">10.1371/journal.pone.0058594.g009</object-id>
                  <label>Figure 9</label>
                  <caption>
                    <title>BOLD signal.</title>
                    <p>Percent change in BOLD signal in V1, PPA, and RSC for good (green) and bad (orange) exemplar blocks. **p&lt;0.01.</p>
                  </caption>
                  <graphic xlink:href="pone.0058594.g009"/>
                </fig>
                <p>Why might bad exemplars evoke greater BOLD activity in the PPA than good exemplars? One possibility is that the increased variance in the bad exemplars as compared to good exemplars led to this difference. Specifically, because good exemplars are more similar to one another, the good blocks might exhibit stronger repetition suppression (sometimes referred to as “fMRI adaptation”) than the bad <xref rid="pone.0058594-Henson1" ref-type="bibr">[30]</xref>.</p>
                <p>To assess this possibility we performed an additional univariate linear regression where we defined four regressors corresponding to first and second halves (4 time points) of the blocks of good and bad exemplar of natural scenes. These four regressors were modeled separately and convolved by a gamma function to approximate the hemodynamic response <xref rid="pone.0058594-Cohen1" ref-type="bibr">[24]</xref>. We extracted the percent signal change based on the coefficients associated with these regressors in each participant’s V1, PPA and RSC ROIs. In each ROI mean percent signal change obtained for each of these four conditions was submitted to a 2×2 ANOVA with good versus bad exemplars as one factor and first half versus second half of the block as the other factor. Neither the main effects nor the interaction were significant in V1 (F&lt;1 for the main effect good versus bad, p = 0.099 for the main effect first versus second half, and p = 0.214 for the interaction) and RSC (all Fs&lt;1). However, in PPA, the main effect of the good (1.51 percent signal change) versus bad (1.77 percent signal change) factor was significant (F(7) = 18.95, MSE = 0.027, p&lt;0.01), in keeping with the previous univariate analysis. The main effect of first versus second half of the block also reached significance (F(7) = 5.9, MSE = 0.070, p&lt;0.05). Mean percent signal chance for the first half of the block was significantly higher than for the second half of the block (1.75 and 1.53 respectively), reflecting the fact that the signal diminished over the course of the block. However, the interaction between these two factors did not reach significance (F(7) = 3.06, MSE = 0.012, p = 0.123) indicating that the suppression was similar for good and bad exemplars. In other words, although there may have been repetition suppression over the course of the block, this suppression is not sufficient to explain the overall BOLD difference between good and bad exemplars.</p>
                <p>What else might explain the greater activity for bad than good exemplars in the PPA? Our behavioral data show that participants were not only less accurate at categorizing bad exemplars than good exemplars but they were also slower, indicating that they find the bad exemplars harder to categorize than the good exemplars. One possibility then, for the greater BOLD signal in PPA, is that bad exemplars required greater attentional resources than good exemplars of the natural scene categories <xref rid="pone.0058594-Kanwisher1" ref-type="bibr">[31]</xref>.</p>
              </sec>
            </sec>
            <sec id="s4">
              <title>General Discussion</title>
              <sec id="s4a">
                <title>Better Exemplars, Better Categorization and Better Decoding of Brain Signals</title>
                <p>Previous work has shown that natural scene categories are distinguishable in the pattern of activity in V1, PPA, RSC and LOC <xref rid="pone.0058594-Walther1" ref-type="bibr">[5]</xref>. In our current study, we asked whether these regions and human subjects were sensitive to the degree to which an image exemplified its category. In a behavioral study we confirmed that good exemplars of our categories were categorized significantly faster and more accurately than bad exemplars. This benefit for good exemplars was present for all six categories. A similar benefit was seen in decoding category from fMRI patterns in visual cortex, specifically PPA, RSC and V1. The difference between good and bad exemplars was assessed by training and testing a decoder on good and bad exemplars separately, and comparing their accuracies. Decoding accuracy was significantly higher in V1, PPA and RSC for good than bad exemplars. This was true despite the fact that there was either no difference in overall BOLD signal evoked by good and bad scenes (RSC and V1), or the signal was actually stronger for bad scenes (PPA). These data not only implicate all three regions in the representation of scene category, but also show that their activity patterns mirror the fundamental graded nature of human categories. Our decoding results also suggest that the differences between good and bad exemplars range from low-level features (decodable in V1) to more complex properties represented in RSC and PPA, such as scene layout <xref rid="pone.0058594-Epstein1" ref-type="bibr">[9]</xref>–<xref rid="pone.0058594-Epstein2" ref-type="bibr">[10]</xref>.</p>
                <p>Since LOC had been previously implicated in natural scene category processing <xref rid="pone.0058594-Walther1" ref-type="bibr">[5]</xref>, <xref rid="pone.0058594-MacEvoy1" ref-type="bibr">[15]</xref>, we also explored whether LOC was sensitive to the degree to which an image exemplified its category. In contrast to the earlier study, decoding in LOC did not exceed chance. The current study used a different, although not a wholly disjoint, set of images than Walther et al. <xref rid="pone.0058594-Walther1" ref-type="bibr">[5]</xref>, and thus the lack of significant decoding in LOC may indicate that the associations of particular objects with a particular scene category were not as consistent (i.e. less diagnostic of scene category) in this image set as that used by Walther et al. <xref rid="pone.0058594-Walther1" ref-type="bibr">[5]</xref>. In keeping with this hypothesis, LOC failed to produce above chance decoding in a later study <xref rid="pone.0058594-Walther2" ref-type="bibr">[14]</xref> that used a subset of the images used here. We also note that MacEvoy and Epstein <xref rid="pone.0058594-Potter1" ref-type="bibr">[1]</xref>, who also implicated LOC in scene categorization, used man-made scenes that were readily identified by the presence of diagnostic objects (e.g. a bathtub in a bathroom).</p>
              </sec>
              <sec id="s4b">
                <title>Why are Good Exemplars Decoded Better than Bad?</title>
                <p>One possibility for the decoding advantage in both V1 and later visual areas is that good exemplars are all more similar to a particular prototype than bad exemplars <xref rid="pone.0058594-Rosch1" ref-type="bibr">[32]</xref>–<xref rid="pone.0058594-Rosch2" ref-type="bibr">[33]</xref>. Indeed, the pixel-wise average of good and bad exemplars revealed consistency in spatial structure and color among good exemplars of a category. Our analysis of the distribution of good and bad images across the “form” and “color” space further confirmed that the variance in ”form” space among the good examplars was smaller than the variance among the bad examplars, for all six categories. A similar pattern was seen in color space, albeit less consistently across our categories. Of course, such a difference in variance could also explain our decoding results. In fact, our fMRI simulation results show that differential variance among good and bad exemplars of natural scenes leads to similar differences in decoding accuracy as we obtained from our fMRI data. We note, however, that higher variance among bad exemplars could be an intrinsic part of what makes them bad exemplars, contributing to a less clear scene category signal in the brain (in accordance with our fMRI data) and less robust categorization (in accordance with our behavioral data).</p>
                <p>That good exemplars of a category are more similar to each other in terms of low-level images statistics is consistent with computational models suggesting that each scene category has a unique “spatial envelop”, which captures scene structure and layout <xref rid="pone.0058594-Oliva1" ref-type="bibr">[34]</xref>–<xref rid="pone.0058594-Greene1" ref-type="bibr">[35]</xref>. However, we note that these results are also consistent with models of categories in which there exists a prototype of each category and good images are more tightly clustered around the prototype than bad images are <xref rid="pone.0058594-Rosch1" ref-type="bibr">[32]</xref>–<xref rid="pone.0058594-Rosch2" ref-type="bibr">[33]</xref>. Indeed, our average good images are highly suggestive of a prototype.</p>
                <p>Moreover, given the speed with which natural scenes are processed <xref rid="pone.0058594-Potter1" ref-type="bibr">[1]</xref>–<xref rid="pone.0058594-Li1" ref-type="bibr">[2]</xref>, <xref rid="pone.0058594-FeiFei1" ref-type="bibr">[4]</xref>–<xref rid="pone.0058594-Walther1" ref-type="bibr">[5]</xref>, <xref rid="pone.0058594-Rousselet2" ref-type="bibr">[8]</xref> it is reasonable to suppose that V1 would be sensitive to differences between prototypes. In other words, the low-level spatial envelope model and organization of a category around a prototype need not be seen as alternative explanations of scene category but instead different level descriptions of the same phenomena: good exemplars are more similar to a prototype than bad, and those features that distinguish between prototypes are computable by even V1.</p>
              </sec>
              <sec id="s4c">
                <title>Good and Bad Exemplars and Typicality</title>
                <p>We are not the first to use representativeness as a tool to better understand the structure of natural scene categories. In particular, others have used typicality ratings to explore the relationship between natural scene categories and global image properties <xref rid="pone.0058594-Greene1" ref-type="bibr">[35]</xref> and semantic content models <xref rid="pone.0058594-Vogel1" ref-type="bibr">[36]</xref>. We note, however, that although our good and bad exemplars presumably bear a close relationship to typicality measures, we did not have our raters rate the images for typicality per se. Instead, we asked them to rate how representative the exemplar was of its category, and included the labels “good” and “poor” at each end of the scale. To the extent that our good and bad exemplars reflect differences in typicality (i.e. the most typical exemplar is the one that shares the highest number of features with the rest of the members of the category <xref rid="pone.0058594-Rosch3" ref-type="bibr">[37]</xref>), our behavioral results are consistent with previous work on typicality <xref rid="pone.0058594-Ehinger1" ref-type="bibr">[38]</xref>.</p>
              </sec>
              <sec id="s4d">
                <title>Summary</title>
                <p>We have shown that the degree to which an image exemplifies a category has consequences for the way participants categorized those scenes and, importantly, for the neural signals they produced. Our decoding results reveal that good exemplars produced clearer and more discriminable patterns of neural activity than bad exemplars of a category. Importantly, this pattern of results is not due to a higher mean signal for good images as PPA, RSC and V1 showed equivalent or lower BOLD activity for good exemplars than for bad. In other words, a more stable pattern of activity appears to underlie the representation of good exemplars of complex scene categories. Our analysis of the images statistics not only reveals that good images produce a more discernible average image than bad, but also that good images in each category are more similar to each other in structure than bad images<bold>.</bold> These data are consistent both with low-level models of scene categorization and models in which a category is organized around a prototype. Finally, our simulation results suggest that the differences in variance between good and bad images, and thus the activity patterns they evoke, may be the cause of the superior decoding for good compared to bad exemplars.</p>
              </sec>
            </sec>
          </body>
          <back>
            <ack>
              <p>Images used in the study are licensed under Creative Commons. We acknowledge the following people for the images displayed in the figures: Lisa Andres, Love Janine, Stefan Karpiniec, Augapfel, Glennwilliamspdx, Crschmidt, Daquella Manera, Matti Mattila, AwnisALAN, Jdnx, Tostie14, Chad Crowell, Skyseeker, Bbjee, Kretyen, Respres, Strange Ones, Dherrera 96, Compujeramey, Augapfel, ReneS, Koneude, Moriza, Podnox, NighRose, Paul Mannix, Aaron M, Yalaminy, Suzi Rosemberg, Williamedia, Bossco, Polandeze, TheLawleys, Dyobmit.</p>
            </ack>
            <ref-list>
              <title>References</title>
              <ref id="pone.0058594-Potter1">
                <label>1</label>
                <mixed-citation publication-type="journal"><name><surname>Potter</surname><given-names>MC</given-names></name>, <name><surname>Levy</surname><given-names>EI</given-names></name> (<year>1969</year>) <article-title>Recognition memory for a rapid sequence of pictures</article-title>. <source>J Exp Psychol</source>
<volume>81</volume>: <fpage>10</fpage>–<lpage>15</lpage>.<pub-id pub-id-type="pmid">5812164</pub-id></mixed-citation>
              </ref>
              <ref id="pone.0058594-Li1">
                <label>2</label>
                <mixed-citation publication-type="journal"><name><surname>Li</surname><given-names>FF</given-names></name>, <name><surname>VanRullen</surname><given-names>R</given-names></name>, <name><surname>Koch</surname><given-names>C</given-names></name>, <name><surname>Perona</surname><given-names>P</given-names></name> (<year>2002</year>) <article-title>Rapid natural scene categorization in the near absence of attention</article-title>. <source>Proc Natl Acad Sci U S A</source>
<volume>99</volume>: <fpage>8378</fpage>–<lpage>8383</lpage>.<pub-id pub-id-type="pmid">12034865</pub-id></mixed-citation>
              </ref>
              <ref id="pone.0058594-Rousselet1">
                <label>3</label>
                <mixed-citation publication-type="journal"><name><surname>Rousselet</surname><given-names>GA</given-names></name>, <name><surname>Thorpe</surname><given-names>S</given-names></name>, <name><surname>Fabre-Thorpe</surname><given-names>M</given-names></name> (<year>2004</year>) <article-title>How parallel is visual processing in the ventral pathway?</article-title>
<source>Trends Cogn Sci</source>
<volume>8</volume>: <fpage>363</fpage>–<lpage>370</lpage>.<pub-id pub-id-type="pmid">15335463</pub-id></mixed-citation>
              </ref>
              <ref id="pone.0058594-FeiFei1">
                <label>4</label>
                <mixed-citation publication-type="journal"><name><surname>Fei-Fei</surname><given-names>L</given-names></name>, <name><surname>VanRullen</surname><given-names>R</given-names></name>, <name><surname>Koch</surname><given-names>C</given-names></name>, <name><surname>Perona</surname><given-names>P</given-names></name> (<year>2005</year>) <article-title>Why does natural scene categorization require little attention? Exploring attentional requirements for natural and synthetic stimuli</article-title>. <source>Vis Cogn</source>
<volume>12(6)</volume>: <fpage>893</fpage>–<lpage>924</lpage>.</mixed-citation>
              </ref>
              <ref id="pone.0058594-Walther1">
                <label>5</label>
                <mixed-citation publication-type="journal"><name><surname>Walther</surname><given-names>DB</given-names></name>, <name><surname>Caddigan</surname><given-names>E</given-names></name>, <name><surname>Fei-Fei</surname><given-names>L</given-names></name>, <name><surname>Beck</surname><given-names>DM</given-names></name> (<year>2009</year>) <article-title>Natural scene categories revealed in distributed patterns of activity in the human brain</article-title>. <source>J Neurosci</source>
<volume>29</volume>: <fpage>10573</fpage>–<lpage>10581</lpage>.<pub-id pub-id-type="pmid">19710310</pub-id></mixed-citation>
              </ref>
              <ref id="pone.0058594-Thorpe1">
                <label>6</label>
                <mixed-citation publication-type="journal"><name><surname>Thorpe</surname><given-names>S</given-names></name>, <name><surname>Fize</surname><given-names>D</given-names></name>, <name><surname>Marlot</surname><given-names>C</given-names></name> (<year>1996</year>) <article-title>Speed of processing in the human visual system</article-title>. <source>Nature</source>
<volume>381(6582)</volume>: <fpage>520</fpage>–<lpage>522</lpage>.<pub-id pub-id-type="pmid">8632824</pub-id></mixed-citation>
              </ref>
              <ref id="pone.0058594-VanRullen1">
                <label>7</label>
                <mixed-citation publication-type="journal"><name><surname>VanRullen</surname><given-names>R</given-names></name>, <name><surname>Thorpe</surname><given-names>SJ</given-names></name> (<year>2001</year>) <article-title>Is it a bird? Is it a plane? Ultra-rapid visual categorisation of natural and artifactual objects</article-title>. <source>Percept</source>
<volume>30(6)</volume>: <fpage>655</fpage>–<lpage>668</lpage>.</mixed-citation>
              </ref>
              <ref id="pone.0058594-Rousselet2">
                <label>8</label>
                <mixed-citation publication-type="journal"><name><surname>Rousselet</surname><given-names>GA</given-names></name>, <name><surname>Mace</surname><given-names>MJ</given-names></name>, <name><surname>Fabre-Thorpe</surname><given-names>M</given-names></name> (<year>2003</year>) <article-title>Is it an animal? Is it a human face? Fast processing in upright and inverted natural scenes</article-title>. <source>J Vis</source>
<volume>3(6)</volume>: <fpage>440</fpage>–<lpage>55</lpage>.<pub-id pub-id-type="pmid">12901715</pub-id></mixed-citation>
              </ref>
              <ref id="pone.0058594-Epstein1">
                <label>9</label>
                <mixed-citation publication-type="journal"><name><surname>Epstein</surname><given-names>R</given-names></name>, <name><surname>Kanwisher</surname><given-names>N</given-names></name> (<year>1998</year>) <article-title>A cortical representation of the local visual environment</article-title>. <source>Nature</source>
<volume>392</volume>: <fpage>598</fpage>–<lpage>601</lpage>.<pub-id pub-id-type="pmid">9560155</pub-id></mixed-citation>
              </ref>
              <ref id="pone.0058594-Epstein2">
                <label>10</label>
                <mixed-citation publication-type="journal"><name><surname>Epstein</surname><given-names>R</given-names></name>, <name><surname>Graham</surname><given-names>KS</given-names></name>, <name><surname>Downing</surname><given-names>PE</given-names></name> (<year>2003</year>) <article-title>Viewpoint-specific scene representations in human parahippocampal cortex</article-title>. <source>Neuron</source>
<volume>37</volume>: <fpage>865</fpage>–<lpage>876</lpage>.<pub-id pub-id-type="pmid">12628176</pub-id></mixed-citation>
              </ref>
              <ref id="pone.0058594-Aguirre1">
                <label>11</label>
                <mixed-citation publication-type="journal"><name><surname>Aguirre</surname><given-names>GK</given-names></name>, <name><surname>Detre</surname><given-names>JA</given-names></name>, <name><surname>Alsop</surname><given-names>DC</given-names></name>, <name><surname>D’Esposito</surname><given-names>M</given-names></name> (<year>1996</year>) <article-title>“The parahippocampus subserves topographical learning in man”</article-title>. <source>Cereb Cortex</source>
<volume>6(6)</volume>: <fpage>823</fpage>–<lpage>892</lpage>.<pub-id pub-id-type="pmid">8922339</pub-id></mixed-citation>
              </ref>
              <ref id="pone.0058594-Chai1">
                <label>12</label>
                <mixed-citation publication-type="other">Chai B, Walther DB, Beck DM, Fei-Fei L (2009) Exploring functional connectivity of the human brain using multivariate information analysis. Proc Neural Inform Process Sys (NIPS).</mixed-citation>
              </ref>
              <ref id="pone.0058594-Yao1">
                <label>13</label>
                <mixed-citation publication-type="other">Yao B, Walther DB, Beck DM, Fei-Fei L (2009) Hierarchical mixture of classification experts uncovers interactions between brain regions. Proc Neural Inform Process Sys (NIPS).</mixed-citation>
              </ref>
              <ref id="pone.0058594-Walther2">
                <label>14</label>
                <mixed-citation publication-type="other">Walther DB, Chai B, Caddigan E, Beck DM, Fei-Fei L (2011) Simple line drawings suffice for functional MRI decoding of natural scene categories. Proc Nat Acad Sci U S A doi:10.1073/pnas.1015666108.</mixed-citation>
              </ref>
              <ref id="pone.0058594-MacEvoy1">
                <label>15</label>
                <mixed-citation publication-type="journal"><name><surname>MacEvoy</surname><given-names>SP</given-names></name>, <name><surname>Epstein</surname><given-names>RA</given-names></name> (<year>2011</year>) <article-title>Constructing scenes from objects in human occipitotemporal cortex</article-title>. <source>Nat Neurosci</source>
<volume>14</volume>: <fpage>1323</fpage>–<lpage>1329</lpage>.<pub-id pub-id-type="pmid">21892156</pub-id></mixed-citation>
              </ref>
              <ref id="pone.0058594-Kravitz1">
                <label>16</label>
                <mixed-citation publication-type="journal"><name><surname>Kravitz</surname><given-names>DJ</given-names></name>, <name><surname>Peng</surname><given-names>CS</given-names></name>, <name><surname>Baker</surname><given-names>CI</given-names></name> (<year>2011</year>) <article-title>Real-world scene representations in high-level visual cortex: it’s the spaces more than the places</article-title>. <source>J Neurosci</source>
<volume>31</volume>: <fpage>7322</fpage>–<lpage>7333</lpage>.<pub-id pub-id-type="pmid">21593316</pub-id></mixed-citation>
              </ref>
              <ref id="pone.0058594-Kravitz2">
                <label>17</label>
                <mixed-citation publication-type="journal"><name><surname>Kravitz</surname><given-names>DJ</given-names></name>, <name><surname>Saleem</surname><given-names>KS</given-names></name>, <name><surname>Baker</surname><given-names>CI</given-names></name>, <name><surname>Mishkin</surname><given-names>M</given-names></name> (<year>2011</year>) <article-title>A new neural framework for visuospatial processing</article-title>. <source>Nat Rev Neurosci</source>
<volume>12(4)</volume>: <fpage>217</fpage>–<lpage>230</lpage>.<pub-id pub-id-type="pmid">21415848</pub-id></mixed-citation>
              </ref>
              <ref id="pone.0058594-Park1">
                <label>18</label>
                <mixed-citation publication-type="journal"><name><surname>Park</surname><given-names>S</given-names></name>, <name><surname>Brady</surname><given-names>TF</given-names></name>, <name><surname>Greene</surname><given-names>MR</given-names></name>, <name><surname>Oliva</surname><given-names>A</given-names></name> (<year>2011</year>) <article-title>Disentangling scene content from its spatial boundary: Complementary roles for the PPA and LOC in representing real-world scenes</article-title>. <source>J Neurosci</source>
<volume>31(4)</volume>: <fpage>1333</fpage>–<lpage>1340</lpage>.<pub-id pub-id-type="pmid">21273418</pub-id></mixed-citation>
              </ref>
              <ref id="pone.0058594-Straw1">
                <label>19</label>
                <mixed-citation publication-type="other">Straw AD (2008) Vision Egg: an open-source library for realtime visual stimulus generation. Front Neurosci doi:10.3389/neuro.11.004.2008.</mixed-citation>
              </ref>
              <ref id="pone.0058594-Brainard1">
                <label>20</label>
                <mixed-citation publication-type="journal"><name><surname>Brainard</surname><given-names>DH</given-names></name> (<year>1997</year>) <article-title>The Psychophysics Toolbox</article-title>. <source>Spat Vis</source>
<volume>10</volume>: <fpage>433</fpage>–<lpage>436</lpage>.<pub-id pub-id-type="pmid">9176952</pub-id></mixed-citation>
              </ref>
              <ref id="pone.0058594-Pelli1">
                <label>21</label>
                <mixed-citation publication-type="journal"><name><surname>Pelli</surname><given-names>DG</given-names></name> (<year>1997</year>) <article-title>The VideoToolbox software for visual psychophysics: Transforming numbers into movies</article-title>. <source>Spat Vis</source>
<volume>10</volume>: <fpage>437</fpage>–<lpage>442</lpage>.<pub-id pub-id-type="pmid">9176953</pub-id></mixed-citation>
              </ref>
              <ref id="pone.0058594-KingSmith1">
                <label>22</label>
                <mixed-citation publication-type="journal"><name><surname>King-Smith</surname><given-names>PE</given-names></name>, <name><surname>Grigsby</surname><given-names>SS</given-names></name>, <name><surname>Vingrys</surname><given-names>AJ</given-names></name>, <name><surname>Benes</surname><given-names>SC</given-names></name>, <name><surname>Supowit</surname><given-names>A</given-names></name> (<year>1994</year>) <article-title>Efficient and unbiased modifications of the QUEST threshold method: theory, simulations, experimental evaluation and practical implementation</article-title>. <source>Vision Res</source>
<volume>34</volume>: <fpage>885</fpage>–<lpage>912</lpage>.<pub-id pub-id-type="pmid">8160402</pub-id></mixed-citation>
              </ref>
              <ref id="pone.0058594-Cox1">
                <label>23</label>
                <mixed-citation publication-type="journal"><name><surname>Cox</surname><given-names>RW</given-names></name> (<year>1996</year>) <article-title>AFNI: software for analysis and visualization of functional magnetic resonance neuroimages</article-title>. <source>Comput Biomed Res</source>
<volume>29</volume>: <fpage>261</fpage>–<lpage>270</lpage>.</mixed-citation>
              </ref>
              <ref id="pone.0058594-Cohen1">
                <label>24</label>
                <mixed-citation publication-type="journal"><name><surname>Cohen</surname><given-names>M</given-names></name> (<year>1997</year>) <article-title>Parametric analysis of fMRI data using linear systems methods</article-title>. <source>Neuroimage</source>
<volume>6</volume>: <fpage>93</fpage>–<lpage>103</lpage>.<pub-id pub-id-type="pmid">9299383</pub-id></mixed-citation>
              </ref>
              <ref id="pone.0058594-Kastner1">
                <label>25</label>
                <mixed-citation publication-type="journal"><name><surname>Kastner</surname><given-names>S</given-names></name>, <name><surname>De Weerd</surname><given-names>P</given-names></name>, <name><surname>Pinsk</surname><given-names>MA</given-names></name>, <name><surname>Elizondo</surname><given-names>MI</given-names></name>, <name><surname>Desimone</surname><given-names>R</given-names></name>, <etal>et al</etal> (<year>2001</year>) <article-title>Modulation of sensory suppression: Implications for receptive field sizes in the human visual cortex</article-title>. <source>J Neurophysiol</source>
<volume>86</volume>: <fpage>1398</fpage>–<lpage>1411</lpage>.<pub-id pub-id-type="pmid">11535686</pub-id></mixed-citation>
              </ref>
              <ref id="pone.0058594-Schneider1">
                <label>26</label>
                <mixed-citation publication-type="journal"><name><surname>Schneider</surname><given-names>KA</given-names></name>, <name><surname>Richter</surname><given-names>MC</given-names></name>, <name><surname>Kastner</surname><given-names>S</given-names></name> (<year>2004</year>) <article-title>Retinotopic organization and functional subdivisions of the human lateral geniculate nucleus: a high-resolution functional magnetic resonance imaging study</article-title>. <source>J Neurosci</source>
<volume>24</volume>: <fpage>8975</fpage>–<lpage>8985</lpage>.<pub-id pub-id-type="pmid">15483116</pub-id></mixed-citation>
              </ref>
              <ref id="pone.0058594-Aguirre2">
                <label>27</label>
                <mixed-citation publication-type="other">Aguirre G, D’Esposito M (2000) Experimental design for brain fMRI. In Moonen C, Bandettini TW, editors. Functional MRI. Heidelberg: Springer-Verlag Berlin. 369–380.</mixed-citation>
              </ref>
              <ref id="pone.0058594-Standing1">
                <label>28</label>
                <mixed-citation publication-type="journal"><name><surname>Standing</surname><given-names>L</given-names></name>, <name><surname>Conezio</surname><given-names>J</given-names></name>, <name><surname>Haber</surname><given-names>RN</given-names></name> (<year>1970</year>) <article-title>Perception and memory for pictures: single-trial learning of 2500 visual stimuli</article-title>. <source>Psychon Sci</source>
<volume>19(2)</volume>: <fpage>73</fpage>–<lpage>74</lpage>.</mixed-citation>
              </ref>
              <ref id="pone.0058594-Brady1">
                <label>29</label>
                <mixed-citation publication-type="journal"><name><surname>Brady</surname><given-names>TF</given-names></name>, <name><surname>Konkle</surname><given-names>T</given-names></name>, <name><surname>Alvarez</surname><given-names>G</given-names></name>, <name><surname>Oliva</surname><given-names>A</given-names></name> (<year>2008</year>) <article-title>Visual long-term memory has a massive storage capacity for object details</article-title>. <source>Proc Natl Acad Sci U S A</source>
<volume>105(38)</volume>: <fpage>14325</fpage>–<lpage>14329</lpage>.<pub-id pub-id-type="pmid">18787113</pub-id></mixed-citation>
              </ref>
              <ref id="pone.0058594-Henson1">
                <label>30</label>
                <mixed-citation publication-type="journal"><name><surname>Henson</surname><given-names>RNA</given-names></name>, <name><surname>Shallice</surname><given-names>T</given-names></name>, <name><surname>Dolan</surname><given-names>RJ</given-names></name> (<year>2000</year>) <article-title>Neuroimaging evidence for dissociable forms of repetition priming</article-title>. <source>Science</source>
<volume>287</volume>: <fpage>1269</fpage>–<lpage>1272</lpage>.<pub-id pub-id-type="pmid">10678834</pub-id></mixed-citation>
              </ref>
              <ref id="pone.0058594-Kanwisher1">
                <label>31</label>
                <mixed-citation publication-type="journal"><name><surname>Kanwisher</surname><given-names>N</given-names></name>, <name><surname>Wojciulik</surname><given-names>E</given-names></name> (<year>2000</year>) <article-title>Visual attention: Insights from brain imaging, Nat Rev Neurosci</article-title>. <volume>1</volume>: <fpage>91</fpage>–<lpage>100</lpage>.</mixed-citation>
              </ref>
              <ref id="pone.0058594-Rosch1">
                <label>32</label>
                <mixed-citation publication-type="journal"><name><surname>Rosch</surname><given-names>E</given-names></name> (<year>1975</year>) <article-title>Cognitive representations of semantic categories</article-title>. <source>J Exp Psychol</source>
<volume>Gen104</volume>: <fpage>192</fpage>–<lpage>233</lpage>.</mixed-citation>
              </ref>
              <ref id="pone.0058594-Rosch2">
                <label>33</label>
                <mixed-citation publication-type="journal"><name><surname>Rosch</surname><given-names>E</given-names></name>, <name><surname>Simpson</surname><given-names>C</given-names></name>, <name><surname>Miller</surname><given-names>S</given-names></name> (<year>1976</year>) <article-title>Structural bases of typicality effects</article-title>. <source>J Exp Psychol: Hum Percept Perform</source>
<volume>2(4)</volume>: <fpage>491</fpage>–<lpage>502</lpage>.</mixed-citation>
              </ref>
              <ref id="pone.0058594-Oliva1">
                <label>34</label>
                <mixed-citation publication-type="journal"><name><surname>Oliva</surname><given-names>A</given-names></name>, <name><surname>Torralba</surname><given-names>A</given-names></name> (<year>2001</year>) <article-title>Modeling the Shape of the Scene: a Holistic Representation of the Spatial Envelope</article-title>. <source>Int J Comput Vis</source>
<volume>42</volume>: <fpage>145</fpage>–<lpage>175</lpage>.</mixed-citation>
              </ref>
              <ref id="pone.0058594-Greene1">
                <label>35</label>
                <mixed-citation publication-type="journal"><name><surname>Greene</surname><given-names>MR</given-names></name>, <name><surname>Oliva</surname><given-names>A</given-names></name> (<year>2009a</year>) <article-title>Recognition of natural scenes from global properties: seeing the forest without representing the trees</article-title>. <source>Cogn Psychol</source>
<volume>58(2)</volume>: <fpage>137</fpage>–<lpage>179</lpage>.<pub-id pub-id-type="pmid">18762289</pub-id></mixed-citation>
              </ref>
              <ref id="pone.0058594-Vogel1">
                <label>36</label>
                <mixed-citation publication-type="journal"><name><surname>Vogel</surname><given-names>J</given-names></name>, <name><surname>Schiele</surname><given-names>B</given-names></name> (<year>2007</year>) <article-title>Semantic modeling of natural scenes for content-based images retrieval</article-title>. <source>Int J Comput Vis</source>
<volume>72(2)</volume>: <fpage>133</fpage>–<lpage>157</lpage>.</mixed-citation>
              </ref>
              <ref id="pone.0058594-Rosch3">
                <label>37</label>
                <mixed-citation publication-type="other">Rosch E (1978) Principles of Categorization. In Rosch E, Lloyd BB, editors. Cognition and categorization. Hillsdale, NJ: Lawrence Erlbaum. 27–48.</mixed-citation>
              </ref>
              <ref id="pone.0058594-Ehinger1">
                <label>38</label>
                <mixed-citation publication-type="other">Ehinger KA, Xiao J, Torralba A, Oliva A (2011). Estimating scene typicality from human ratings and image features. Proc 33rd Annu Conf Cogni Sci Soc, Boston, MA: Cogni Sci Soc.</mixed-citation>
              </ref>
            </ref-list>
          </back>
        </article>
      </metadata>
    </record>
  </GetRecord>
</OAI-PMH>
