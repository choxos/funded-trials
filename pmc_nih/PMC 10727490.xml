<?xml version="1.0" encoding="UTF-8"?>
<OAI-PMH xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/ http://www.openarchives.org/OAI/2.0/OAI-PMH.xsd">
  <responseDate>2024-05-03T02:03:36Z</responseDate>
  <request verb="GetRecord" identifier="oai:pubmedcentral.nih.gov:10727490" metadataPrefix="pmc">https:/www.ncbi.nlm.nih.gov/pmc/oai/oai.cgi</request>
  <GetRecord>
    <record>
      <header>
        <identifier>oai:pubmedcentral.nih.gov:10727490</identifier>
        <datestamp>2023-12-18</datestamp>
        <setSpec>nihpa</setSpec>
        <setSpec>pmc-open</setSpec>
        <setSpec>manuscript</setSpec>
      </header>
      <metadata>
        <article xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:mml="http://www.w3.org/1998/Math/MathML" xsi:schemaLocation="https://jats.nlm.nih.gov/ns/archiving/1.3/ https://jats.nlm.nih.gov/archiving/1.3/xsd/JATS-archivearticle1-3.xsd" dtd-version="1.3" xml:lang="en" article-type="research-article">
          <processing-meta base-tagset="archiving" mathml-version="3.0" table-model="xhtml" tagset-family="jats">
            <restricted-by>pmc</restricted-by>
          </processing-meta>
          <front>
            <journal-meta>
              <journal-id journal-id-type="nlm-journal-id">8918110</journal-id>
              <journal-id journal-id-type="pubmed-jr-id">2607</journal-id>
              <journal-id journal-id-type="nlm-ta">Eur J Neurosci</journal-id>
              <journal-id journal-id-type="iso-abbrev">Eur J Neurosci</journal-id>
              <journal-title-group>
                <journal-title>The European journal of neuroscience</journal-title>
              </journal-title-group>
              <issn pub-type="ppub">0953-816X</issn>
              <issn pub-type="epub">1460-9568</issn>
            </journal-meta>
            <article-meta>
              <article-id pub-id-type="accession">PMC10727490</article-id>
              <article-id pub-id-type="pmcid">PMC10727490</article-id>
              <article-id pub-id-type="pmc-uid">10727490</article-id>
              <article-id pub-id-type="pmid">37170067</article-id>
              <article-id pub-id-type="pmid">37170067</article-id>
              <article-id pub-id-type="doi">10.1111/ejn.16045</article-id>
              <article-id pub-id-type="manuscript">nihpa1944108</article-id>
              <article-categories>
                <subj-group subj-group-type="heading">
                  <subject>Article</subject>
                </subj-group>
              </article-categories>
              <title-group>
                <article-title>A machine learning approach towards the differentiation between interoceptive and exteroceptive attention</article-title>
              </title-group>
              <contrib-group>
                <contrib contrib-type="author">
                  <contrib-id contrib-id-type="orcid" authenticated="false">http://orcid.org/0000-0001-5620-1045</contrib-id>
                  <name>
                    <surname>Zuo</surname>
                    <given-names>Zoey X.</given-names>
                  </name>
                  <xref rid="A1" ref-type="aff">1</xref>
                </contrib>
                <contrib contrib-type="author">
                  <contrib-id contrib-id-type="orcid" authenticated="false">http://orcid.org/0000-0003-1999-2401</contrib-id>
                  <name>
                    <surname>Price</surname>
                    <given-names>Cynthia J.</given-names>
                  </name>
                  <xref rid="A2" ref-type="aff">2</xref>
                </contrib>
                <contrib contrib-type="author">
                  <contrib-id contrib-id-type="orcid" authenticated="false">http://orcid.org/0000-0002-8407-2938</contrib-id>
                  <name>
                    <surname>Farb</surname>
                    <given-names>Norman A. S.</given-names>
                  </name>
                  <xref rid="A1" ref-type="aff">1</xref>
                  <xref rid="A3" ref-type="aff">3</xref>
                </contrib>
              </contrib-group>
              <aff id="A1"><label>1</label>Department of Psychological Clinical Sciences, University of Toronto Scarborough, Scarborough, Ontario, Canada</aff>
              <aff id="A2"><label>2</label>Department of Biobehavioral Nursing and Health Informatics, University of Washington, Seattle, Washington, USA</aff>
              <aff id="A3"><label>3</label>Department of Psychology, University of Toronto Mississauga, Mississauga, Ontario, Canada</aff>
              <author-notes>
                <fn fn-type="con" id="FN1">
                  <p id="P1">AUTHOR CONTRIBUTIONS</p>
                  <p id="P2">Norman A. S. Farb and Cynthia J. Price designed the study and collected data. Zoey X. Zuo and Norman A. S. Farb conducted data analysis. Zoey X. Zuo, Norman A. S. Farb, and Cynthia J. Price wrote the manuscript.</p>
                </fn>
                <corresp id="CR1"><bold>Correspondence:</bold> Zoey X. Zuo, Graduate Department of Psychological Clinical Science, University of Toronto Scarborough, 1265 Military Trail, Scarborough, Ontario M1C 1A4, Canada. <email>zoey.zuo@mail.utoronto.ca</email></corresp>
              </author-notes>
              <pub-date pub-type="nihms-submitted">
                <day>10</day>
                <month>12</month>
                <year>2023</year>
              </pub-date>
              <pub-date pub-type="ppub">
                <month>7</month>
                <year>2023</year>
              </pub-date>
              <pub-date pub-type="epub">
                <day>24</day>
                <month>5</month>
                <year>2023</year>
              </pub-date>
              <pub-date pub-type="pmc-release">
                <day>18</day>
                <month>12</month>
                <year>2023</year>
              </pub-date>
              <volume>58</volume>
              <issue>2</issue>
              <fpage>2523</fpage>
              <lpage>2546</lpage>
              <permissions>
                <license>
                  <ali:license_ref specific-use="textmining" content-type="ccbynclicense">https://creativecommons.org/licenses/by-nc/4.0/</ali:license_ref>
                  <license-p>This is an open access article under the terms of the <ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by-nc/4.0/">Creative Commons Attribution-NonCommercial</ext-link> License, which permits use, distribution and reproduction in any medium, provided the original work is properly cited and is not used for commercial purposes.</license-p>
                </license>
              </permissions>
              <abstract id="ABS1">
                <p id="P3">Interoception, the representation of the body’s internal state, plays a central role in emotion, motivation and wellbeing. Interoceptive sensibility, the ability to engage in sustained interoceptive awareness, is particularly relevant for mental health but is exclusively measured via self-report, without methods for objective measurement. We used machine learning to classify interoceptive sensibility by contrasting using data from a randomized control trial of interoceptive training, with functional magnetic resonance imaging assessment before and after an 8-week intervention (<italic toggle="yes">N</italic> = 44 scans). The neuroimaging paradigm manipulated attention targets (breath vs. visual stimuli) and reporting demands (active reporting vs. passive monitoring). Machine learning achieved high accuracy in distinguishing between interoceptive and exteroceptive attention, both for within-session classification (~80% accuracy) and out-of-sample classification (~70% accuracy), revealing the reliability of the predictions. We then explored the classifier potential for ‘reading out’ mental states in a 3-min sustained interoceptive attention task. Participants were classified as actively engaged about half of the time, during which interoceptive training enhanced their ability to sustain interoceptive attention. These findings demonstrate that interoceptive and exteroceptive attention is distinguishable at the neural level; these classifiers may help to demarcate periods of interoceptive focus, with implications for developing an objective marker for interoceptive sensibility in mental health research.</p>
              </abstract>
              <kwd-group>
                <kwd>fMRI</kwd>
                <kwd>Interoception</kwd>
                <kwd>machine learning</kwd>
                <kwd>mindful awareness in body-oriented therapy (MABT)</kwd>
                <kwd>randomized controlled trial</kwd>
              </kwd-group>
            </article-meta>
          </front>
          <body>
            <sec id="S1">
              <label>1 |</label>
              <title>INTRODUCTION</title>
              <p id="P4">Interoception, the sense of the body’s internal state, is widely regarded as a foundation for emotion (<xref rid="R78" ref-type="bibr">Wiens, 2005</xref>), motivation (<xref rid="R11" ref-type="bibr">Craig, 2003</xref>), intuition (<xref rid="R20" ref-type="bibr">Dunn et al., 2010</xref>) and wellbeing (<xref rid="R74" ref-type="bibr">Tsakiris &amp; Critchley, 2016</xref>). While interoceptive signals such as respiration, heart rate, temperature or hunger are processed automatically to promote homeostasis in the body (<xref rid="R10" ref-type="bibr">Craig, 2002</xref>; <xref rid="R33" ref-type="bibr">Gu &amp; FitzGerald, 2014</xref>), these signals also serve as a foundation for feeling states that guide consciously coordinated behaviour (<xref rid="R70" ref-type="bibr">Strigo &amp; Craig, 2016</xref>). As such, interoceptive function has recently become the target of psychological theory casting the interoceptive sense at the heart of health and disease, characterizing interoception as affectively privileged and distinct from the exteroceptive senses of vision, hearing, taste, smell and touch (<xref rid="R25" ref-type="bibr">Farb et al., 2015</xref>; <xref rid="R37" ref-type="bibr">Khalsa et al., 2018</xref>; <xref rid="R60" ref-type="bibr">Quadt et al., 2018</xref>).</p>
              <p id="P5">Dysregulation of interoceptive processing has been linked to a variety of pathological conditions such as anxiety (<xref rid="R18" ref-type="bibr">Domschke et al., 2010</xref>), depression (<xref rid="R19" ref-type="bibr">Dunn et al., 2007</xref>; <xref rid="R30" ref-type="bibr">Furman et al., 2013</xref>; <xref rid="R72" ref-type="bibr">Terhaar et al., 2012</xref>), posttraumatic stress disorder (PTSD) (<xref rid="R31" ref-type="bibr">Glenn et al., 2016</xref>), somatic symptom disorders (<xref rid="R2" ref-type="bibr">Barsky et al., 2001</xref>), addiction (<xref rid="R51" ref-type="bibr">Naqvi &amp; Bechara, 2010</xref>), obesity (<xref rid="R35" ref-type="bibr">Herbert &amp; Pollatos, 2014</xref>) and chronic pain (<xref rid="R56" ref-type="bibr">Pollatos et al., 2012</xref>). Such conditions are strongly associated with abnormal interoceptive <italic toggle="yes">sensibility</italic>, the self-reported ability to sustain attention towards interoceptive signals (<xref rid="R6" ref-type="bibr">Calì et al., 2015</xref>; <xref rid="R48" ref-type="bibr">Mehling, 2016</xref>; <xref rid="R73" ref-type="bibr">Trevisan et al., 2019</xref>). Furthermore, changes in interoceptive ability mediate reductions in depression symptoms following evidence-based treatments (<xref rid="R16" ref-type="bibr">de Jong et al., 2016</xref>; <xref rid="R21" ref-type="bibr">Eggart &amp; Valdés-Stauber, 2021</xref>). Such psychometric approaches converge with qualitative appraisals of treatment response: Women treated for substance use disorder using Mindful Awareness in Body-oriented Therapy (MABT) perceived interoceptive awareness as critical for emotional awareness, regulation and relapse prevention, and such endorsements were linked to symptom reduction (<xref rid="R58" ref-type="bibr">Price &amp; Smith-DiJulio, 2016</xref>).</p>
              <p id="P6">However, interoceptive sensibility suffers from a lack of quantifiable measurement relative to measures of interoceptive <italic toggle="yes">accuracy</italic>, such as counting heartbeats over time (<xref rid="R5" ref-type="bibr">Brener &amp; Ring, 2016</xref>). The promise of objective measurement has researchers to define interoceptive awareness as awareness of accuracy rather than focusing on <italic toggle="yes">sensibility</italic> as a marker of interest (<xref rid="R12" ref-type="bibr">Critchley &amp; Garfinkel, 2017</xref>). Heartbeat detection accuracy appears unrelated to interoceptive sensibility (<xref rid="R28" ref-type="bibr">Ferentzi et al., 2018</xref>), and it is sensibility rather than accuracy that correlates with subjective wellbeing (<xref rid="R28" ref-type="bibr">Ferentzi et al., 2018</xref>; <xref rid="R66" ref-type="bibr">Schuette et al., 2021</xref>). Given the understandable desire to rigorously quantify mechanistic markers of health and vulnerability, there is utility in developing an objective method for assessing interoceptive sensibility.</p>
              <p id="P7">To objectively assess interoceptive sensibility, we must first have confidence in our ability to distinguish interoceptive attention from other mental states. Advances in the classification of brain states provide an emerging possibility for this enterprise. Time-series data from functional neuroimaging, such as functional magnetic resonance imaging (fMRI), are often analysed using machine learning algorithms to identify meaningful brain patterns (e.g., <xref rid="R34" ref-type="bibr">Haxby, 2012</xref>; <xref rid="R52" ref-type="bibr">Norman et al., 2006</xref>) such as predicting which traumatic film scenes became intrusive memories (<xref rid="R7" ref-type="bibr">Clark et al., 2014</xref>). Compared to univariate methods, which seek to localize specific areas of neural response, machine learning approaches aggregate these responses to estimate their predictive utility (<xref rid="R15" ref-type="bibr">Davatzikos, 2019</xref>). Applied to the classification of interoceptive attention, a machine learning approach could quantitatively assess a person’s ability to sustain interoceptive awareness, regardless of that person’s level of insight or confidence.</p>
              <p id="P8">Considerable research suggests that interoceptive processing in the brain is at least partially distinct from engagement with the exteroceptive senses. Interoception is supported by a dedicated neuroanatomical pathway, with signals transmitted via sense-receptor C-fibre afferents along the spinal cord to the brainstem and thalamus before reaching the cerebral cortex (<xref rid="R10" ref-type="bibr">Craig, 2002</xref>; <xref rid="R13" ref-type="bibr">Critchley &amp; Harrison, 2013</xref>). Interoceptive attention appears to engage the ventromedial thalamus and right posterior insula (<xref rid="R23" ref-type="bibr">Farb et al., 2013</xref>), which serves as the primary interoceptive cortex (<xref rid="R10" ref-type="bibr">Craig, 2002</xref>; <xref rid="R29" ref-type="bibr">Flynn, 1999</xref>). Supporting the feasibility a classification approach, <xref rid="R76" ref-type="bibr">Weng et al. (2020)</xref> first documented the utility of machine learning to distinguish between sensory and conceptual targets of internally directed attention (i.e., focus on the breath, the self or mind wandering). The authors used a logistic regression model to show that participants’ voluntary direction of attention may be sufficient to generate reliable classification results.</p>
              <p id="P9">Conversely, interoceptive attention has yet to be classified as distinct from attention towards the exteroceptive senses. While a set of frontoparietal brain regions constitute a well-validated dorsal attention network (DAN), which is broadly involved in the regulation of perceptual attention (<xref rid="R17" ref-type="bibr">Dixon et al., 2018</xref>; <xref rid="R71" ref-type="bibr">Szczepanski et al., 2013</xref>). It is plausible that the DAN also supports attention for interoceptive signals, given that interoceptive experiences are usually multimodal combinations of interoceptive and exteroceptive afferents (<xref rid="R38" ref-type="bibr">Khalsa et al., 2009</xref>; <xref rid="R61" ref-type="bibr">Quigley et al., 2021</xref>). Furthermore, despite evidence that the anterior insula facilitates subjective access to body sensations, this integration may not be modality specific (<xref rid="R9" ref-type="bibr">Craig, 2009</xref>; <xref rid="R14" ref-type="bibr">Critchley et al., 2004</xref>; <xref rid="R33" ref-type="bibr">Gu &amp; FitzGerald, 2014</xref>). Instead, the anterior insula seems to integrate both interoceptive and exteroceptive signals (<xref rid="R47" ref-type="bibr">Medford &amp; Critchley, 2010</xref>; <xref rid="R69" ref-type="bibr">Seth et al., 2012</xref>) and serves as the sensory/afferent hub of the salience network (<xref rid="R67" ref-type="bibr">Seeley, 2019</xref>), which could then provide a common ‘neural code’ to higher level cognitive processes such as the regulation of perceptual attention supported by the DAN.</p>
              <p id="P10">Our primary aim was therefore to determine whether BOLD activity in these attentional networks (and beyond) contains sufficient information to distinguish between interoceptive and exteroceptive attentional states. We applied a machine learning approach to explore classification between interoceptive and exteroceptive attention, using a recently developed fMRI paradigm (<xref rid="R27" ref-type="bibr">Farb et al., 2023</xref>). Data were collected as part of a randomized clinical trial of validated mindfulness-based intervention (MABT), which features unique focus on teaching fundamental skills critical to identifying, accessing, sustaining and appraising signals that arise within the body (<xref rid="R57" ref-type="bibr">Price &amp; Hooven, 2018</xref>). The Interoceptive/Exteroceptive Attention Task (IEAT) used for classification analysis focused on four conditions: active interoception and exteroception, which involved continuous button-press tracking of respiration and a visual stimulus, respectively, and passive interoception and exteroception, which involved passive monitoring of respiration and visual targets in the absence of behavioural tracking. We also employed a well-validated self-report instrument to assess the influence of clinically relevant individual differences in interoceptive sensibility.</p>
              <p id="P11">To evaluate classifier sensitivity to interoceptive training effects, we followed classifier training and validation with an application to estimate attentional states during periods of sustained interoceptive attention. We had three aims in this study:</p>
              <list list-type="order" id="L2">
                <list-item>
                  <p id="P12"><italic toggle="yes">Distinguish</italic> between the neural patterns of interoceptive and exteroceptive attention to understand if interoceptive attention involves distinct processes from exteroceptive attention.</p>
                </list-item>
                <list-item>
                  <p id="P13"><italic toggle="yes">Predict</italic> periods of interoceptive and exteroceptive attention using out-of-sample tests to determine the robustness of the models.</p>
                </list-item>
                <list-item>
                  <p id="P14"><italic toggle="yes">Apply</italic> the classifier to estimate momentary attentional states during sustained attention to explore interoceptive training effects and correspondence with self-reported interoceptive awareness and affective distress based on self- and clinician- reports.</p>
                </list-item>
              </list>
              <p id="P15">The clinical trial was registered with <ext-link xlink:href="https://ClinicalTrials.gov" ext-link-type="uri">ClinicalTrials.gov</ext-link> (<ext-link xlink:href="https://clinicaltrials.gov/ct2/show/NCT03583060" ext-link-type="uri">NCT03583060</ext-link>) and pre-registered with the Open Science Framework (OSF; <ext-link xlink:href="https://osf.io/y34ja" ext-link-type="uri">https://osf.io/y34ja</ext-link>). Under the overarching clinical trial, machine learning was pre-registered as an exploratory analysis to classify different experimental conditions and predict sustained interoceptive attention. All study materials and code are available on the OSF (<ext-link xlink:href="https://osf.io/ctqrh/" ext-link-type="uri">https://osf.io/ctqrh/</ext-link>). Univariate analysis of the IEAT task is described in a separate manuscript that is published as a preprint (<ext-link xlink:href="10.1101/2022.05.27.493743v3" ext-link-type="doi">https://www.biorxiv.org/content/10.1101/2022.05.27.493743v3</ext-link>), including quality control analyses and demonstrations of equivalent difficulty between task conditions. This study was reviewed and approved by the institutional review board at the University of Washington in accord with the World Medical Association Declaration of Helsinki.</p>
            </sec>
            <sec id="S2">
              <label>2 |</label>
              <title>METHODS</title>
              <sec id="S3">
                <label>2.1 |</label>
                <title>Research protocol</title>
                <p id="P16">This study was conducted in the context of a 2 (group: MABT vs. control) × 2 (session: baseline vs. post-intervention) randomized control trial of MABT to investigate training-related neural changes in the brain. Participants were assessed at baseline before being randomized to the MABT or the control condition and reassessed within 4 weeks of completing the 8-week MABT intervention period. At both baseline and post-intervention assessments, all participants completed a 20-min self-report questionnaire that surveyed body awareness and symptoms of distress. Participants then completed a series of fMRI scans, including standard anatomical scans, an IEAT and a Sustained Interoceptive Attention Task (SIAT).</p>
              </sec>
              <sec id="S4">
                <label>2.2 |</label>
                <title>Participants</title>
                <p id="P17">Participants were recruited through postings on the University of Washington (UW) Institute of Translational Health Sciences website, flyers on campus and advertisements in local newspapers. Advertisements described the study as a mind–body investigation into the neural processes of body awareness for people with moderate stress. Participants provided written-informed consent and were compensated for their time. All assessments and data collection took place at the UW Integrated Brain Imaging Center.</p>
                <p id="P18">Primary inclusion criteria for the study included (1) over 18 years of age, (2) score on the Perceived Stress Scale (PSS) meeting screening cut-off for moderate stress, no prior experience with mindfulness-based trainings, agrees to forgo mind–body therapies for the duration of the study, (5) fluent in English, (6) able to attend all study sessions and (7) right-handed. Primary exclusion criteria included (1) current diagnosis of any mental health disorder, (2) unable to participate in all sessions, (3) cognitive impairment, (4) head injury involving loss of consciousness, (5) pregnant or (6) MRI contraindications.</p>
                <p id="P19">Fifty-seven participants were recruited. Twenty-four participants were excluded for reasons listed in the <xref rid="SD1" ref-type="supplementary-material">supporting information</xref>. Twenty-three remaining participants were randomly assigned to receive MABT (<italic toggle="yes">n</italic> = 12) or no-intervention to serve as the control (<italic toggle="yes">n</italic> = 11). Out of these participants, 22 completed the required assessments (11 MABT and 11 controls) and were included in the analyses (11 males and 11 females; age range: 18–62 years, mean age = 36.1 years; 20 self-identified as Caucasian, 1 as African American, 2 as Hispanic; highest education: 5 with high school degrees, 2 with 2 years of college, 8 with Bachelor’s degrees, and 7 with Master’s degrees or higher).</p>
                <sec id="S5">
                  <label>2.2.1 |</label>
                  <title>Power analysis</title>
                  <p id="P20">While machine learning approaches test categorization accuracy at the within-participant level, showing that accuracy is significantly better than chance levels can be assessed at the group level as a one sample <italic toggle="yes">t</italic> test. Furthermore, decoding of sustained attention following classifier training could also be evaluated in terms of Group × Time interactions. Given prior observation of large effects of MABT on subjective interoception (<xref rid="R59" ref-type="bibr">Price et al., 2019</xref>), this study was designed to detect medium to large effects (d &gt; .6, f &gt; .3). Power analysis conducted in G*Power software suggested that this sample size would be sufficient to detect such effects with 80% power. At the <italic toggle="yes">p</italic> &lt; .05 significance level, detecting above-chance accuracy (a one sample <italic toggle="yes">t</italic> test of accuracies scores for all participants) would require <italic toggle="yes">N</italic> ≥ 19, and detecting Group × Time interactions on decoding results would require <italic toggle="yes">N</italic> ≥ 18. These analyses do not model power to achieve accurate classification within a given participant’s data, as power analysis methods for MRI are still under development, and no prior data are available given the use of a novel task. Similarly, the study is not power to detect medium sized or weaker effects, so any non-significant effects must be interpreted with caution rather than as evidence of null results.</p>
                </sec>
              </sec>
              <sec id="S6">
                <label>2.3 |</label>
                <title>MABT</title>
                <p id="P21">MABT uses an incremental approach to help build comfort and skills needed to develop and facilitate interoceptive awareness (<xref rid="R57" ref-type="bibr">Price &amp; Hooven, 2018</xref>). The approach was delivered individually using the manualized eight-session protocol developed for research. This protocol involves three phases: sessions 1 and 2 focus on body literacy, sessions 3 and 4 on interoceptive training and sessions 5–8 on development of sustained mindful interoceptive attention and somatic appraisal processes. A take-home practice is collaboratively developed at the end of each session to facilitate integration of interoceptive awareness in daily life. Eight weekly 75-min individual sessions were delivered at the UW School of Nursing Clinical Studies Unit by one of two licenced massage therapists trained in the MABT protocol. Protocol compliance was monitored through audio recording of sessions, process evaluation forms and ongoing clinical supervision. Participants were given up to 10 weeks to complete all eight sessions to accommodate schedule conflicts. All MABT participants completed at least 75% of the sessions (i.e., at least six sessions): Eight participants completed all eight sessions, two completed seven sessions and one completed six sessions. Therapists monitored participants’ progress with quantitative ratings and qualitative descriptions on the process evaluation forms completed after each MABT session.</p>
              </sec>
              <sec id="S7">
                <label>2.4 |</label>
                <title>fMRI tasks</title>
                <p id="P22">FMRI data were collected during a novel IEAT and a SIAT at both baseline and post-intervention. In each assessment session, there were two fMRI scans (i.e., two runs). Each task was administered once in each functional scan. Altogether, participants performed each task four times in this study, twice at baseline and twice at post-intervention. Throughout the fMRI tasks, participants’ respiration data were recorded using a Philips MRI Respiratory Sensor Air Bellows, model number 452213117812.</p>
                <sec id="S8">
                  <label>2.4.1 |</label>
                  <title>Interoceptive/Exteroceptive Attention Task</title>
                  <p id="P23">The novel IEAT (<xref rid="R27" ref-type="bibr">Farb et al., 2023</xref>) consisted of five conditions: passive exteroception, passive interoception, active interoception, active exteroception, and active matching as shown in <xref rid="F1" ref-type="fig">Figure 1</xref>. These conditions varied in terms of reporting demand (active reporting vs. passive watching) and attentional target (interoceptive vs. exteroceptive attention).</p>
                  <p id="P24">Each condition started with a 10-s instruction screen followed by a 30-s task period. All conditions were order-counterbalanced and repeated twice in each functional run. Altogether, 6.7 min of data were collected in each run and 13.4 min in both runs.</p>
                  <sec id="S9">
                    <title>Passive conditions</title>
                    <p id="P25">During passive exteroception, participants were asked to visually monitor a circle as it expanded and contracted periodically on the MRI-compatible visual display without making any behavioural responses. The circle’s pulse frequency was set to match the participants’ estimated in-scanner breathing frequency (usually around 12 Hz). During passive interoception, participants viewed a stationary circle on the screen while attending to sensations of the breath.</p>
                  </sec>
                  <sec id="S10">
                    <title>Active conditions</title>
                    <p id="P26">During active interoception, participants were asked to report on their inhalations and exhalations by making key presses with their right-hand index and middle fingers, respectively. The circle on the screen also responded to these key presses, approximating the frequency of circle movement during passive exteroception. During active exteroception, participants were asked to report on the expansion and contraction of the circle on the screen, which again was set to pulse at participants’ in-scanner respiratory frequency.</p>
                  </sec>
                  <sec id="S11">
                    <title>Active matching condition</title>
                    <p id="P27">During active matching, participants were asked to report on the expansion and contraction of the circle (as in active exteroception) by making button presses while matching their inhalation to the circle’s expansion and their exhalation to the circle’s contraction. Together, these five experimental tasks were developed to address the limitations of prior interoception paradigms. However, the goal of the present study was to directly classify between factors of attentional target (interoception vs. exteroception) and reporting demands (active vs. passive monitoring). Because the active matching condition required aspects of both interoceptive and exteroceptive attention, it was not used in the classification models that are the focus of this paper but is reported in a forthcoming univariate analysis paper (<ext-link xlink:href="10.1101/2022.05.27.493743v3" ext-link-type="doi">https://www.biorxiv.org/content/10.1101/2022.05.27.493743v3</ext-link>).</p>
                  </sec>
                </sec>
                <sec id="S12">
                  <label>2.4.2 |</label>
                  <title>Sustained interoceptive attention task</title>
                  <p id="P28">Immediately before fMRI data acquisition, participants listened to a 2.5-min audio-guided interoceptive awareness meditation in the scanner, directing them to place a hand on their chest and channel mindful attention to the inner space of the chest underneath their hand. After the guided meditation, participants were instructed to sustain attention on inner body awareness for 3 min with their eyes closed during fMRI data acquisition. This procedure was repeated across two runs at baseline and two runs at post-intervention to yield a total of four scans, that is, 12 min of fMRI data.</p>
                </sec>
              </sec>
              <sec id="S13">
                <label>2.5 |</label>
                <title>Questionnaire measures</title>
                <p id="P29">During the baseline and the post-intervention sessions, participants answered a self-report questionnaire consisting of the Multidimensional Assessment of Interoceptive Awareness (MAIA), the Patient Health Questionnaire–Somatic, Anxiety and Depressive Symptoms (PHQ-SADS) and the PSS. In addition, the MABT therapists rated participants’ capacity for sustained interoceptive attention over the second half of the training period (sessions 5–8). For the descriptive statistics of these self-report measures, see our Open Science Framework page (<ext-link xlink:href="https://osf.io/ctqrh/" ext-link-type="uri">https://osf.io/ctqrh/</ext-link>).</p>
                <sec id="S14">
                  <label>2.5.1 |</label>
                  <title>Multidimensional Assessment of Interoceptive Awareness</title>
                  <p id="P30">The MAIA is a 32-item self-report questionnaire used to assess interoceptive body awareness (<xref rid="R49" ref-type="bibr">Mehling et al., 2012</xref>). It consists of eight scales each measuring an aspect of interoceptive awareness: noticing, not-distracting, not-worrying, attention regulation, emotional awareness, self-regulation, body listening and trust. These scales have good evidence of internal-consistency reliability with alphas ranging from .66 to .82 and good evidence of construct validity as assessed by inter-scale correlations as well as differential scores between individuals who were expected to have higher or lower body awareness.</p>
                </sec>
                <sec id="S15">
                  <label>2.5.2 |</label>
                  <title>Composite affective symptom burden</title>
                  <p id="P31">A composite affective symptom burden score was obtained based on responses on the PHQ-SADS (<xref rid="R39" ref-type="bibr">Kroenke et al., 2010</xref>) and the PSS (<xref rid="R8" ref-type="bibr">Cohen et al., 1983</xref>). The PHQ-SADS is a 37-item self-report questionnaire consisting of the PHQ-9 depression scale, PHQ-15 somatic symptom scale and Generalized Anxiety Disorder (GAD)-7 anxiety scale (<xref rid="R39" ref-type="bibr">Kroenke et al., 2010</xref>). All three scales have good evidence of internal-consistency reliability (α = .80 to .92), test–retest reliability (r = .60 to .84) and good sensitivity and specificity to detect depression, anxiety and somatic symptoms. The PSS is a 10-item self-report questionnaire used to assess how feelings and perceived stress levels are affected by various situations (<xref rid="R8" ref-type="bibr">Cohen et al., 1983</xref>). A review study showed that the PSS has good internal-consistency reliability (α &gt; .70 in all 12 studies evaluated) and test–retest reliability (r &gt; .70 in all four studies evaluated), although criterion and known-groups validity need to be further evaluated (<xref rid="R42" ref-type="bibr">Lee, 2012</xref>). We extracted the first principal component from affective symptom scales PHQ-SADS and PSS, which explained 67.2% of the overall variance. A simulation run using the ‘paran’ library confirmed that one factor was sufficient to explain the variances and was used as a composite affective symptom score for this study.</p>
                </sec>
                <sec id="S16">
                  <label>2.5.3 |</label>
                  <title>Therapist rating: Capacity for sustained interoceptive attention</title>
                  <p id="P32">In MABT sessions 5–8, therapists rated participants’ capacity for sustained interoceptive attention on a scale of 0–5 based on their observation: 0 = <italic toggle="yes">none</italic>, 1 = <italic toggle="yes">momentary</italic>, 2 = <italic toggle="yes">fluctuating in and out</italic> (being in the state for brief time, i.e., less than 3 min), 3 = <italic toggle="yes">steady contact for many minutes</italic>, 4 = <italic toggle="yes">fluctuating in and out</italic> (being in the state for longer periods, i.e., more than 3 min) and 5 = <italic toggle="yes">sustained contact</italic> (10–30 min).</p>
                </sec>
              </sec>
              <sec id="S17">
                <label>2.6 |</label>
                <title>Data analysis</title>
                <sec id="S18">
                  <label>2.6.1 |</label>
                  <title>Imaging data acquisition and preprocessing</title>
                  <p id="P33">Neuroimaging data were collected using a 3 T Philips Achieva scanner (Philips Inc., Amsterdam, Netherlands) at the Diagnostic Imaging Sciences Center, University of Washington. Imaging began with the acquisition of a T1-weighted anatomical scan (MPRAGE) to guide normalization of functional images with repetition time (TR) = 7.60 ms, echo time (TE) = 3.52 ms, inversion time (TI) = 1,100 ms, acquisition matrix = 256 × 256, flip angle = 7°, shot interval = 2530 ms and 1 mm isotropic voxel size. Functional data were acquired using a T2*-weighted echo-planar-imaging (EPI) sequence with TR = 2,000, TE = 25 ms, flip angle α = 79°, field of view = 240 × 240 × 129 mm, 33 slices and a voxel size of 3 × 3 × 3.3 mm with 3.3 mm gap. Button presses were registered using a two-button MR-compatible response pad.</p>
                  <p id="P34">Neuroimaging data preprocessing was performed using the fMRIPrep pipeline 20.0.6 (<xref rid="R22" ref-type="bibr">Esteban et al., 2019</xref>) (see the <xref rid="SD1" ref-type="supplementary-material">supporting information</xref> for full details). Preprocessing consisted of realignment and unwarping of functional images, slice timing correction and motion correction. The functional images were resliced using a voxel size of 2 × 2 × 2 mm and smoothed using a 6-mm FWHM isotropic Gaussian kernel.</p>
                </sec>
                <sec id="S19">
                  <label>2.6.2 |</label>
                  <title>Analysis software</title>
                  <p id="P35">The Python Language (Python Software Foundation, <ext-link xlink:href="https://www.python.org/" ext-link-type="uri">https://www.python.org/</ext-link>) was used primarily for machine learning analysis. In-house code was developed with reference to BrainIAK (the Brain Imaging Analysis Kit, <ext-link xlink:href="http://brainiak.org" ext-link-type="uri">http://brainiak.org</ext-link>; <xref rid="R41" ref-type="bibr">Kumar et al., 2020</xref>, <xref rid="R40" ref-type="bibr">2022</xref>). The scikit-learn package was used for machine learning analysis (<xref rid="R53" ref-type="bibr">Pedregosa et al., 2011</xref>), the nilearn package for brain maps (<xref rid="R1" ref-type="bibr">Abraham et al., 2014</xref>) and the seaborn package for data visualization (<xref rid="R75" ref-type="bibr">Waskom, 2021</xref>). The R Language (<xref rid="R62" ref-type="bibr">R Core Team, 2020</xref>) was also used for statistical analyses. The lme4 package was used for statistical modelling (<xref rid="R3" ref-type="bibr">Bates et al., 2015</xref>) and the ggplot2 package for data visualization (<xref rid="R77" ref-type="bibr">Wickham, 2016</xref>).</p>
                </sec>
                <sec id="S20">
                  <label>2.6.3 |</label>
                  <title>Aim 1: Distinguish—Within-sample classification of interoceptive versus exteroceptive attention</title>
                  <p id="P36">We aimed to classify neural patterns to distinguish between states of interoceptive and exteroceptive attention. To do so, we trained machine learning classifiers on fMRI data when participants engaged in interoceptive and exteroceptive attention, assessing how accurately these states could be separated and which brain regions contributed to the separation.</p>
                  <p id="P37">Specifically, we used fMRI blood-oxygen-level-dependent (BOLD) data collected in four conditions: active interoception, active exteroception, passive interoception and passive exteroception. An active matching condition was not analysed in this study, because it blended interoceptive and exteroceptive attention and was therefore deemed unsuitable for distinguishing between these two processes. We conducted the analyses in three steps. First, we combined active interoception and passive interoception into an interoceptive condition and active exteroception and passive exteroception into an exteroceptive condition to examine the gross differences between interoception and exteroception regardless of the reporting demand. Second, we examined active interoception, active exteroception, passive interoception and passive exteroception as four separate conditions, considering both the attentional target (interoception vs. exteroception) and the reporting demand (active tracking vs. passive monitoring). Third, we focused on differences between active interoception and active exteroception to eliminate the confounds in passive interoception, the only condition in which the circle stimulus remained stationary on the screen. Focusing on the active conditions also allowed for classification in a context of high participant engagement, as participants were required to continuous attend to the circle/breath stimulus to meet the reporting demands of the tasks. All three steps were conducted in a similar classification workflow as demonstrated in <xref rid="F2" ref-type="fig">Figure 2</xref>.</p>
                  <p id="P38">Classification was performed at an individual participant level to maximize classification accuracy and account for individual anatomical and functional differences in the brain. For each participant, the four-dimensional fMRI data were reshaped into voxel-by-timepoint matrices. IEAT task-related timepoints were extracted from the full timecourse of the scans. A whole-brain mask was applied to the data to retain voxels that fell within the brain. We used this data-driven whole-brain approach to identify any brain regions that might drive the separation of interoception and exteroception without making a priori assumptions about which regions might be critical in the process.</p>
                  <p id="P39">We implemented a penalized logistic regression with L2 regularization (i.e., Ridge regression) with reference to methods used by <xref rid="R76" ref-type="bibr">Weng et al. (2020)</xref> to classify internal states of attention during meditation. Regularization in general penalizes the overfitting of data and reduces the likelihood of models over-learning from the training data to the extent that they fail to generalize to out-of-sample data. We selected L2 regularization over other methods such as L1 regularization (i.e., Lasso regression) because L2 regularization retained more important features (i.e., voxels) and would therefore reveal more brain regions involved in interoceptive and exteroceptive processes. Many other machine learning algorithms have been used in fMRI studies (see <xref rid="R65" ref-type="bibr">Rashid et al., 2020</xref>, for a review). In our pilot testing, we compared L2 regularization to other commonly used algorithms such as sparse multinomial logistic regression, Gaussian Naïve Bayes, XGBoost and singular value decomposition linear regression but failed to see any evidence of superior classification. This lack of distinction therefore led us to proceed with our planned use of the L2 regularization algorithm.</p>
                  <p id="P40">A k-fold cross-validation method was used to train and evaluate the performance of the classifier models. Each participant’s voxel-by-timepoint matrix in each study session was split into a training set and a test set. In the training set, a logistic regression model used the brain activation values of each voxel at each timepoint and the true label of the experimental condition. Each voxel was assigned a weight that indicated how much evidence it provided for or against an experimental condition. Then, the classifier used these weights learned from the training set to predict the experimental condition of each timepoint in the novel held-out test set. These predictions were evaluated against the true experimental condition labels to derive a measure of classification accuracy. The classifiers were trained and tested within baseline data and within post-intervention data, respectively. This in-sample within-session classification allowed us to evaluate how well the classifiers differentiated the mental states in the same experimental session. We split each session’s data into fivefolds and ran cross validation by training on four folds and testing on the fifth held-out fold (i.e., iteratively training on 80% and testing on 20% of the data). This process was repeated until all fivefolds had been used as the test fold once. An average accuracy score was obtained across the five iterations.</p>
                  <p id="P41">For each participant at each assessment, we applied the binomial theorem to analyse whether the classification accuracy was significantly greater than chance at the <italic toggle="yes">p</italic> &lt; .05 threshold. At each session, the four conditions were comprised of 60 volumes each over the two functional runs: (1 volume/2 s) × (30 s/block) (2 blocks/run) × 2 runs × 4 conditions = 240 volumes in total. For the two condition models, the chance probability of successfully classifying a given functional volume was 50% (i.e., choosing one out of two options at random). The binomial probability distribution suggested that each participant was required to achieve classification accuracy above 55.4% to be considered significantly above chance (1 – cumulative probability &lt; .05). For the four condition models, the chance probability was 25%, which by the binomial theorem required classification accuracy above 29.6% to be significantly above chance.</p>
                  <p id="P42">Group level classification maps were then created to identify important brain regions. For each participant, voxels that had a major contribution to the classification were identified: Voxels whose weights were above 2 standard deviations of the mean weight were assigned a value of 1, and those whose weights were below 2 standard deviations of the mean weight were assigned a value of −1. All participants’ important maps were overlaid to create a group-level importance map in which higher absolute values indicated more discriminative voxels across participants.</p>
                  <p id="P43">One possible confound of the fMRI BOLD signal classification for interoception and exteroception was participants’ respiration during the task. Therefore, as a control analysis, we submitted participants’ respiration rates for each block as features to the L2 regularization classifier to match the classification based on BOLD signals, comparing (1) interoception versus exteroception (collapsing active and passive conditions); (2) active interoception, active exteroception, passive interoception, versus passive exteroception; and (3) active interoception versus active exteroception. A k-fold cross validation (fivefolds) was conducted for each of these three comparisons, training on 80% of the data and testing on 20% of the data, repeated five times until all data have been used as the test set. An average accuracy score was computed across the five classifications. This process was repeated for both the baseline and post-intervention sessions. The only difference between the respiration rate classification and fMRI BOLD signal classification was that the respiration rate classification was conducted across rather than within participants due to a smaller amount of respiration datapoints available for each participant; for each participant, there were only four respiration rates per experimental condition per session. Despite this difference, this analysis would still allow us to obtain a sufficient estimate of the scale of accuracy differences between classification based on respiration rates versus BOLD signal.</p>
                </sec>
                <sec id="S21">
                  <label>2.6.4 |</label>
                  <title>Aim 2: Predict—Out-of-sample classification of interoceptive versus exteroceptive attention</title>
                  <p id="P44">To move from model generation to validation, we examined whether the trained models would generalize to predict attentional states in independent test datasets. Specifically, classifiers that were trained on each participant’s baseline data were tested on post-intervention data, and vice versa. Out-of-sample testing would help us understand whether the neural distinction between interoceptive and exteroceptive attention was reliable within the same individual at different times of assessments, rather than risking model overfitting by cross-validating within the same training dataset.</p>
                </sec>
                <sec id="S22">
                  <label>2.6.5 |</label>
                  <title>Aim 3: Apply—Decoding attention during a SIAT</title>
                  <p id="P45">We finally aimed to apply interoceptive classification models during periods of sustained interoceptive attention to assess sensitivity to interoceptive training (MABT) and covariation with reports of interoceptive sensibility and affective symptom burden. To do so, we used the classifiers trained on IEAT data to estimate participants’ attentional states during the SIAT. For each participant, L2 regularized regression classifiers were trained on each participant’s IEAT data at baseline and post-intervention separately. The classifiers were then used to decode the participant’s within-session SIAT data. As the SIAT instructed participants to maintain active engagement with interoceptive signals, the reporting demand classification was used as an estimator of task engagement, with periods decoded as ‘active tracking’ hypothesized to indicate periods in which a participant was actively engaged in sustained attention, compared to ‘passive monitoring’ periods that were more likely to indicate fatigue or mind wandering.</p>
                  <p id="P46">The initial analysis used a three-factor, multilevel, fully within-participant design. The factors modelled included the four classifier conditions: attentional target (interoception and exteroception) × task engagement (active tracking vs. passive monitoring) × time within the sustained attention task (~150 volumes). As explained below, the second phase of analysis employed only a two-category classifier (active reporting of interoception and exteroception), removing the reporting demand factor. As this was an exploratory analysis, we focused only on highly significant findings in the main text, that is, <italic toggle="yes">p</italic> &lt; .001, although complete results are available in the <xref rid="SD1" ref-type="supplementary-material">supporting information</xref>.</p>
                  <p id="P47">To summarize participants’ degree of engagement in interoceptive versus exteroceptive attention, several metrics were also developed, including (1) the average duration spent in each mental state, with a state defined as any period of six consecutive seconds (three TRs) or more with the same classification label, and (2) the frequency of each mental state. Together, the duration and frequency of events offer an estimate of the stability of interoceptive and exteroceptive attention throughout the task.</p>
                  <sec id="S23">
                    <title>Training effects</title>
                    <p id="P48">Multilevel mixed models were used to examine whether MABT improved sustained interoceptive attention. Group membership (i.e., MABT vs. control) was the between-subjects independent variable; session (i.e., baseline vs. post-intervention) was the within-subjects independent variable. The frequency or proportion of interoceptive attention as well as metrics for the duration and number of interoceptive events was used as dependent variables. Significant Group × Session interactions would be regarded as evidence for group-specific training effects.</p>
                  </sec>
                  <sec id="S24">
                    <title>Associations with subjective reports</title>
                    <p id="P49">Additional exploratory analyses explored how classifier data related to reports of interoceptive awareness and wellbeing. Multilevel mixed models were used to predict the duration and number of interoceptive and exteroceptive events using self-reported interoceptive awareness (MAIA) or affective symptom burden. In addition, for the MABT group only, we examined the relationship between classifier estimates and therapist-rated interoceptive awareness. Multilevel mixed models were used to examine MAIA, composite symptom scores and therapist ratings as independent variables and the interoceptive attention metrics, that is, average duration of events and number of events, as dependent variables.</p>
                  </sec>
                </sec>
              </sec>
            </sec>
            <sec id="S25">
              <label>3 |</label>
              <title>RESULTS</title>
              <sec id="S26">
                <label>3.1 |</label>
                <title>Classification accuracy based on respiration rates</title>
                <p id="P50">We submitted respiration rate data to the L2 classifier to distinguish between (1) interoception versus exteroception (collapsing active and passive conditions); (2) active interoception, active exteroception, passive interoception, versus passive exteroception; and (3) active interoception versus active exteroception. For interoception versus exteroception, the classifiers achieved accuracies of 55.6% at baseline and 50.0% at post-intervention (chance accuracy = 50%). Among the four conditions, the classifiers achieved accuracies of 30.1% at baseline and 23.9% at post-intervention (chance accuracy = 25%). For active interoception versus active exteroception, the classifiers achieved accuracies of 63.1% at baseline and 50.6% at post-intervention (chance accuracy = 50%).</p>
              </sec>
              <sec id="S27">
                <label>3.2 |</label>
                <title>Aim 1: Distinguish—Within-sample classification of interoceptive and exteroceptive attention</title>
                <p id="P51">The first aim of the study was to evaluate whether machine learning classifiers could distinguish between neural patterns associated with four experimental conditions: [active vs. passive] monitoring of [interoceptive vs. exteroceptive] attention. First, we tested a two-state model contrasting both interoception conditions against both exteroception conditions. Second, we evaluated a four-state model featuring: active interoception, passive interoception, active exteroception, versus passive exteroception. Finally, we applied a two-state model again, focusing on the distinction between active interoception and active exteroception. As a liberal test of discriminability, classification was first performed <italic toggle="yes">within-session</italic>, that is, trained and tested using data from the same assessment session at either baseline or post-intervention.</p>
                <sec id="S28">
                  <label>3.2.1 |</label>
                  <title>Two-category classification: Interoceptive versus exteroceptive attention</title>
                  <p id="P52">The first model classified all interoception trials against all exteroception trials, collapsing together active and passive conditions. The classifier achieved a 73% accuracy at baseline and 72% accuracy at post-intervention (<xref rid="F3" ref-type="fig">Figure 3a</xref>). Participants at both baseline and post-intervention were all classified with an accuracy of &gt;55.4%, the <italic toggle="yes">p</italic> &lt; .05 threshold for chance classification except for one participant at post-intervention who was slightly below chance. However, inspection of individual participant classification revealed considerable heterogeneity between well-classified and poorly classified participants (<xref rid="F4" ref-type="fig">Figure 4</xref>). Inspection of the confusion matrices indicated that model was not biased in its errors; that is, it did not make any systematic errors in predicting one condition than the other (<xref rid="F3" ref-type="fig">Figure 3b</xref>).</p>
                  <p id="P53">We generated a group-level frequency map to examine the voxels that contributed the most to the classification (<xref rid="F5" ref-type="fig">Figure 5</xref>). Since our aim was the classification of attentional states rather than the localization of brain regions, we are not claiming that we identified critical brain regions for interoception or exteroception through this analysis. These group-level brain maps are meant to serve an illustrative purpose to help readers visualize the overlap of sources of information across participants.</p>
                  <p id="P54">Overall, 89% of the important voxels were only important for two or fewer participants (<xref rid="SD1" ref-type="supplementary-material">supporting information</xref>); no voxel was important for more than 14 participants. Voxels in the posterior cingulate and the middle insula both contributed evidence for interoceptive attention, whereas voxels in the ventromedial prefrontal cortex (vmPFC), dorsomedial prefrontal cortex (dmPFC), motor and somatosensory areas, the primary visual cortex (V1) and the middle temporal visual area (V5) contributed evidence for exteroceptive attention.</p>
                </sec>
                <sec id="S29">
                  <label>3.2.2 |</label>
                  <title>Four-category classification: Active interoception, active exteroception, passive interoception, and passive exteroception</title>
                  <p id="P55">We then examined classification performance among a more nuanced model that aimed to distinguish between the four experimental conditions. The whole-brain L2 regularized logistic regression classifier achieved a 71% accuracy at baseline and at post-intervention (<xref rid="F6" ref-type="fig">Figure 6</xref>). In addition, each mental state was differentiated from the other three states with above chance accuracy: Active Interoception = 72%, Active Exteroception = 73%, Passive Interoception = 69%, and PastExtero = 70% (<xref rid="F4" ref-type="fig">Figure 4</xref>). Participants at both baseline and post-intervention were all classified with an accuracy of &gt;29.6%, the <italic toggle="yes">p</italic> &lt; .05 threshold for chance classification. Inspection of the confusion matrices indicated that model was not biased in its errors; that is, it did not make any systematic errors in predicting one condition over the others, although within-passive active and within-passive confusions were numerically greater than confusions between active/passive condition combinations.</p>
                  <p id="P56">Next, we generated group-level frequency maps to examine the voxels that contributed the most to the classification (<xref rid="F7" ref-type="fig">Figure 7</xref>). Overall, 90% of the important voxels were important for only two or fewer participants (<xref rid="SD1" ref-type="supplementary-material">supporting information</xref>); no voxel was important for more than 12 participants in the classification of active interoception, 18 participants in the classification of active exteroception, 19 participants in the classification of passive interoception and 15 participants in the classification of passive exteroception.</p>
                  <p id="P57">In keeping with the prior literature, voxels in the posterior cingulate and middle insula both contributed evidence for active interoception, whereas voxels in the vmPFC, dmPFC and primary somatosensory cortex contributed evidence against active interoception. Voxels in the ventral visual pathway, especially V1 and V5, contributed evidence for active exteroception; voxels in the medial orbital prefrontal cortex (moPFC) contributed evidence against active exteroception. Voxels in the perigenual anterior cingulate cortex (pACC), vmPFC and moPFC contributed evidence for the passive interoception condition; the ventral visual pathway, especially V1 and V5, and the medial premotor cortex all contributed evidence against the passive interoception condition. Lastly, V1, V5, the right motor and somatosensory areas, and some small clusters in the prefrontal cortex contribute evidence for passive exteroception condition; the medial premotor cortex, left motor cortex and left somatosensory cortex contributed evidence against passive exteroception.</p>
                  <p id="P58">In addition to voxels in regions that support representation of interoceptive and exteroceptive content, some classification seemed to capitalize on unequal reporting demands across conditions: for example, motor and somatosensory areas corresponding to button presses with the right hand contributed significantly to active interoception and active exteroception classification. As mentioned above, area V5 contributed to classification of passive interoception, the only condition that did not feature motion. In response, an additional two-state classification analysis was conducted, focused on the most closely matched conditions, active interoception versus active exteroception.</p>
                </sec>
                <sec id="S30">
                  <label>3.2.3 |</label>
                  <title>Within-session classification: Active interoception versus active exteroception</title>
                  <p id="P59">Whole-brain classification between the closely matched active interoception and active exteroception conditions achieved an 85% accuracy at baseline and 82% accuracy at post-intervention across participants (<xref rid="F8" ref-type="fig">Figure 8</xref>). Once again, participants at both baseline and post-intervention were all classified with accuracies of &gt;55.4%, the <italic toggle="yes">p</italic> &lt; .05 threshold for chance classification. Inspection of the confusion matrices indicated that model was not biased in its errors; that is, it did not make any systematic errors in predicting one condition than the other.</p>
                  <p id="P60">We generated a group-level frequency map to examine the voxels that contributed the most to the classification (<xref rid="F9" ref-type="fig">Figure 9</xref>). Overall, 90% of the important voxels were only important for two or fewer participants (<xref rid="SD1" ref-type="supplementary-material">supporting information</xref>); no voxel was important for more than 10 participants. Regions identified in the four-category classification also demonstrated importance in this analysis. Specifically, voxels in the posterior cingulate and middle insula both contributed evidence for active interoception, whereas the vmPFC, dmPFC, motor and somatosensory areas, V1 and V5 all contributed evidence for the Active Exteroception condition. Although these two active tracking conditions were matched for motion and button-press requirements, sensorimotor activity supporting classification of exteroception in the four-category model was still retained in this two-category model.</p>
                </sec>
              </sec>
              <sec id="S31">
                <label>3.3 |</label>
                <title>Aim 2: Predict—Out-of-sample classification of interoceptive and exteroceptive attention</title>
                <p id="P61">The second aim of the study was to test whether machine learning classifiers could <italic toggle="yes">predict</italic> individualized neural patterns associated with different attentional states using out-of-sample data. As all participants attended two assessment sessions (baseline and post-intervention), we tested classification models derived from baseline data and then tested on post-intervention data and vice versa. We applied this approach for both the four conditions (active interoception, active exteroception, passive interoception and passive exteroception) and two conditions (active interoception and active exteroception) models.</p>
                <p id="P62">The four-category classifier achieved 51% accuracy across participants when trained on baseline data and tested on post-intervention data and 50% accuracy when trained on post-intervention data and tested on baseline data (<xref rid="F10" ref-type="fig">Figure 10a</xref>). Although less accurate than within-session classification, the overall classification remained significantly above chance.</p>
                <p id="P63">The binomial probability distribution determined that classification accuracy scores above 29.4% were statistically significant above chance (cumulative probability &lt; .05 for a 25% chance level across all TRs). Of all participants, one failed to surpass chance at baseline (accuracy = 28%), and a different participant failed to surpass chance at post-intervention (accuracy = 29%). Confusion matrices suggested that each state was equally distinguishable from the other three states with above chance accuracy: Active Interoception = 52%, Active Exteroception = 49%, Passive Interoception = 50% and Passive Exteroception = 49% (<xref rid="F10" ref-type="fig">Figure 10b</xref>).</p>
                <p id="P64">In the two-category classification model between active interoception and active exteroception, the whole-brain L2 regularized logistic regression classifier achieved an 51% accuracy at baseline and 50% at post-intervention (<xref rid="F10" ref-type="fig">Figure 10c</xref>). The binomial probability distribution determined that classification accuracy scores above 55.4% were significantly above chance (cumulative probability &lt; .05 for a 50% chance level across all TRs). Three participants failed to beat chance at baseline and four at post-intervention. Similar to the four-category classification, the machine learning models did not make any systematic errors over-estimating one condition than the other: Active Interoception = 69% accuracy and Active Exteroception = 70% accuracy (<xref rid="F10" ref-type="fig">Figure 10d</xref>).</p>
                <p id="P65">Out-of-sample accuracy is almost invariably lower than within-sample classification, leading researchers to argue that only out-of-sample classification should be considered true classifier ‘prediction’ (cf. <xref rid="R55" ref-type="bibr">Poldrack et al., 2020</xref>). However, the reasons for this drop are multifaceted, including both measurement error and true changes in participants’ neural representations over time. Accuracy drop was operationalized as the average of [baseline training to post-intervention decoding] and [post-intervention training to baseline decoding] accuracies for each participant. However, these average drop scores were unrelated to experimental group or change in MAIA scores, suggesting that the accuracy drop was not due to intervention-related change.</p>
              </sec>
              <sec id="S32">
                <label>3.4 |</label>
                <title>Aim 3: Apply—Decoding attention during the SIAT</title>
                <p id="P66">Our third aim was to explore whether the classifiers trained on the IEAT data could decode participants’ attentional state over a 3-min period of sustained interoceptive attention. We were particularly interested in exploring whether interoceptive training influenced these sustained attention metrics, and whether these metrics correlated with self- and clinician-reports of interoceptive engagement and symptom burden.</p>
                <p id="P67">We first applied the four-category IEAT classification model to the sustained attention data, which distinguished between Task Engagement (active vs. passive attention) and attentional target (interoception vs. exteroception). The aim of using the four-category model was to explore where participants most clearly showed active engagement with the sustained attention task, as this part of the timeseries would likely be most sensitive to training effects.</p>
                <p id="P68">Classification results were analysed in a multilevel model to explore the effects and potential interactions between task engagement (active vs. passive), attentional target (interoception vs. exteroception) and time within the sustained attention task (seconds). The analysis showed a significant main effect of engagement, such that participants were more likely to be classified as being active than passive, β = 7.23, 95% CI [5.12; 9.33], <italic toggle="yes">p</italic> &lt; .001, and this effect was qualified by an interaction between Engagement × Time, β = −.07, 95% CI [−.11; −.05], <italic toggle="yes">p</italic> &lt; .001, such that greater active engagement was found earlier in the sustained attention run (<xref rid="F11" ref-type="fig">Figure 11a</xref>). Complete results of the multilevel model are available in the <xref rid="SD1" ref-type="supplementary-material">supporting information</xref>.</p>
                <p id="P69">Given the strong performance of active condition classifier, the discovery of a motion confound in the passive interoception condition, and the expectation of greater participant engagement when active responses were required, we focused our subsequent decoding of interoceptive sensibility on the classifier trained on the two active conditions alone. A multilevel model explored the interactions between group (MABT vs. control), session (baseline vs. post-intervention) and time within the sustained attention task (seconds). A significant Group × Session × Time interaction was observed, β = −.07, <italic toggle="yes">p</italic> &lt; .001. Subsequent visualization of the effects (<xref rid="F11" ref-type="fig">Figure 11b</xref>) indicated that active interoception was significantly more frequent in the MABT post-intervention group than the other Group × Session combinations, and the interaction was driven by differences in the first half of the time series, consistent with the previous finding that active Engagement was highest earlier in the task.</p>
                <p id="P70">Complete results of the multilevel model are available in the <xref rid="SD1" ref-type="supplementary-material">supporting information</xref>. Several additional exploratory analyses are also available in the <xref rid="SD1" ref-type="supplementary-material">supporting information</xref>, including an attempted replication of <xref rid="R76" ref-type="bibr">Weng et al. (2020)</xref>; the time-course data were summarized using two derived scores: (1) the total number of events in the SIAT and (2) the average duration of these events over the course of the SIAT. These metrics were then used to investigate self-reported interoceptive sensibility (MAIA scores), affective symptom burden and clinician ratings of interoceptive engagement within the MABT group. While none of these analyses produced statistically significant findings following corrections for multiple comparisons, the approach provided a proof-of-concept for how classification scores could be validated in a better-powered future sample.</p>
              </sec>
            </sec>
            <sec id="S33">
              <label>4 |</label>
              <title>DISCUSSION</title>
              <p id="P71">The present study demonstrated that machine learning classifiers can differentiate between neural patterns of interoceptive attention and exteroceptive attention, a candidate objective biomarker of the capacity for the clinically relevant construct of interoceptive sensibility. The fMRI BOLD signals carried substantially more information than respiration alone as observed from the comparison between classification accuracies based on respiration versus neural data. Following within-sample neural classification, we demonstrated classifier prediction by using a relatively more stringent out-of-sample test, predicting each participants’ attentional state on test data that were independent from the training set, and acquired more than 8 weeks apart. These findings support qualitative distinctions between interoceptive and exteroceptive attention and suggest that whole-brain fMRI can provide sufficient information to distinguish between these states.</p>
              <p id="P72">We also applied the classification model to decode participants’ attentional dynamics during a sustained interoception task. The classifier suggested that participants were only able to maintain an active attentional state for approximately 90 s of the 3-min attention periods, regardless of training status; however, within those periods, the classifier appeared to be sensitive to the clinical intervention, suggesting that training increased the duration of time that participants were able to maintain an active interoceptive state. Finally, we were able to illustrate how such decoding could also be applied to continuous covariates such as subjective interoceptive awareness and affective symptom burden. While the results of the decoding analysis were largely statistically insignificant and thus would be inappropriate for inference to the broader population, it provides a framework for future research to explore the associations between these important constructs.</p>
              <sec id="S34">
                <label>4.1 |</label>
                <title>Distinguishing between interoceptive and exteroceptive attention</title>
                <p id="P73">A series of machine learning models each achieved high levels of accuracy in distinguishing between interoceptive and exteroceptive attention, in both in-sample and out-of-sample tests. While classification relied on a diffuse and often idiosyncratic set of brain regions for each participant, voxels in the posterior cingulate and middle insula often provided evidence for interoceptive attention, in accordance with the literature on these areas’ involvement in interoceptive processing (<xref rid="R11" ref-type="bibr">Craig, 2003</xref>; <xref rid="R23" ref-type="bibr">Farb et al., 2013</xref>). Conversely, voxels in the cortical midline, consistent with the brain’s default mode network, generally contributed evidence against interoceptive attention, which could suggest a break away from effortful, elaborative cognitive processing during internal body focus (<xref rid="R63" ref-type="bibr">Raichle &amp; Snyder, 2007</xref>). It is important to note that few voxels were consistently important across participants in classifying different mental states, which would be the goal of a typical univariate localization approach. Instead, the classification approach generated reliable but idiosyncratic models distinguishing between interoceptive and exteroceptive attention, suggesting that the direction of attention between these targets may be experience-dependent rather than relying on consistent regions such as primary representation cortices.</p>
                <p id="P74">However, the ability to make more nuanced distinctions between active and passive reporting of the sensory targets was limited by non-equivalent task demands and stimulus presentation between the IEAT conditions. The four-category model revealed the opportunistic nature of the machine learning classifier, which capitalized on two design confounds. First, the active and passive conditions differed in their requirement of button-presses to track sensory targets. Accordingly, the four-category classifier relied heavily on left motor cortex activity to distinguish active from passive monitoring, consistent with the active monitoring requirement to perform right hand finger movements (e.g., <xref rid="R45" ref-type="bibr">Lotze et al., 2000</xref>; <xref rid="R64" ref-type="bibr">Rao et al., 1995</xref>). Second, passive interoception was the only condition in which the visual target (the circle stimulus) remained stationary on the screen rather than following a cycle of expansion and contraction. Accordingly, the classifiers relied on the differential neural representations in voxels in the visual pathway, especially V1 and V5, to distinguish passive interoception from the other three conditions, consistent with the literature on these visual areas’ function in mapping the visual field and coding for motion (see <xref rid="R32" ref-type="bibr">Greenlee &amp; Tse, 2008</xref>, for a review). This reliance of the classifier on task difference highlights the need to develop closely matched experimental conditions if one wishes for machine learning algorithms to base classification on constructs of interest. Future research might revisit the four-category model if the task confounds can be better addressed.</p>
                <p id="P75">Given the fixation of the general two-category and four-category classification models on nuisance covariates, we refocused our analyses on the two active conditions (Active exteroception and active interoception). Encouragingly, these conditions showed no obvious confounds, and participants demonstrated equivalent facility for performing the two active tracking tasks (cf. <ext-link xlink:href="10.1101/2022.05.27.493743v3" ext-link-type="doi">https://www.biorxiv.org/content/10.1101/2022.05.27.493743v3</ext-link> for analyses confirming equivalent tracking performance between the two tasks). As mentioned above, the two active conditions were also well matched for stimulus dynamics, and both required button-press tracking of the appropriate sensory target (visual circle or respiratory cycle).</p>
                <p id="P76">The results supported the feasibility of classifying between carefully controlled periods of interoception and exteroception. Even without features driven by motion and motor confounds, the classification accuracy between active interoception and active exteroception was the highest of the three models tested, surpassing a more general model that collapsed together active and passive conditions, as well as the four-category model. The classification relied on voxels in a similar set of brain regions as the previous four-category analysis, excluding the motor and visual cortex confounds described above. What remained was a consistent account of the neural distinction between interoception and exteroception: voxels in the posterior cingulate and middle insula contributed evidence for active interoception, consistent with previous studies that identified these areas as being implicated in the interoceptive neural network (<xref rid="R43" ref-type="bibr">Li et al., 2017</xref>; <xref rid="R46" ref-type="bibr">Matsumoto et al., 2006</xref>). For active exteroception, areas including the vmPFC, dmPFC, motor and somatosensory areas, as well as the visual cortices, contributed important evidence.</p>
                <p id="P77">Why was exteroceptive attention associated with greater reliance on non-visual brain areas despite having the same visual stimulus as the interoceptive condition? One possible explanation is that greater attentional resources might have been required for active exteroception to track and report the movements of the visual stimulus as part of the task requirement. Conversely, active interoception recruited fewer resources in the cortical midline structures, which are associated with cognitively demanding tasks (<xref rid="R68" ref-type="bibr">Seeley et al., 2007</xref>). Interoceptive attention might have shifted cognitive resources from elaborative cognitive processing to an inward, sensory focus as suggested by the engagement of the cingulate cortex and insula (<xref rid="R26" ref-type="bibr">Farb et al., 2007</xref>, <xref rid="R24" ref-type="bibr">2010</xref>, <xref rid="R23" ref-type="bibr">2013</xref>).</p>
                <p id="P78">Another critical finding was the maintenance of above-chance classification when classifiers were applied out-of-sample. Classifiers trained on a participant at one time point were still able to classify attentional states using data acquired at a second timepoint 8 weeks apart, suggesting that the neural patterns of interoception and exteroception were truly differentiable rather than a result of overfitting classifier models. While accuracy predictably declined from within-sample classification, classification remained significantly above chance: approximately 50% accuracy versus a 25% chance in the four-state classification and 70% accuracy versus 50% chance in the two-state classification, comparable to similar studies that have attempted to classify between mental states. For example, <xref rid="R50" ref-type="bibr">Mitchell et al. (2003)</xref>’s Gaussian Naïve Bayes models achieved approximately 80% accuracy in classifying cognitive states such as reading a word about people versus buildings (50% chance accuracy). <xref rid="R76" ref-type="bibr">Weng et al. (2020)</xref>’s L2 regularization models achieved approximately 41% accuracy versus 20% chance for distinguishing between five mental states, including interoceptive sensations of breath, sensations of the feet, mind wandering, self-referential processing and ambient sound listening.</p>
                <p id="P79">In our study, classification rates decreased by 15% when we classified participants’ data out-of-sample compared to within-sample. However, it is unclear whether this drop in accuracy was a ‘bug or a feature’: if within-sample classification led to overfitting, the lower out-of-sample classification rates would represent a more accurate and generalizable estimate of model prediction. Accuracy could also decline because individuals’ neural dynamics truly changed, both because of natural variation over a 2-month interval and because half of the participants were engaged in interoceptive training over this period. In this case, greater model inaccuracy between time points could be a sign of the model’s sensitivity to training effects: Participants who experienced greater training-related change in the neural dynamics of interoception would presumably show poorer model fit. In our sample, we conducted an analysis to show that the accuracy drop was unrelated to intervention group or change in MAIA score. Therefore, the loss of out-of-sample accuracy was unlikely an effect of intervention condition. Nevertheless, the drop could still occur because of natural variation over a 2-month interval, as well as the common loss of classification accuracy observed when moving from within-sample model fitting to true prediction in an out-of-sample dataset (<xref rid="R55" ref-type="bibr">Poldrack et al., 2020</xref>).</p>
                <p id="P80">Thus, the classification models seemed robust against overfitting and performed well above chance on datasets collected 2 months apart. The primary aim of the study was therefore successful: Whole-brain fMRI of participants toggling between interoceptive and exteroceptive attention produces sufficient information to distinguish between neural modes of interoceptive and exteroceptive attention. While classification accuracy can be improved, the current approach already suggests that active engagement in interoception and exteroception are neurally distinguishable states. Furthermore, while activity in primary representation cortices for interoception and vision was important for the classification, less prefrontal and sensorimotor activity appears was indicative of interoception. Why the interoceptive state seems to involve less cortical activity is a ripe topic for further investigation; however, this finding is consistent with the characterization of interoceptive attention as a reducing unnecessary energy demands and driving hemostatic regulation (<xref rid="R61" ref-type="bibr">Quigley et al., 2021</xref>), supporting the conditions for embodied self-awareness as a ‘minimal phenomenal self’ (<xref rid="R44" ref-type="bibr">Limanowski &amp; Blankenburg, 2013</xref>) or ‘proto-self’ (<xref rid="R4" ref-type="bibr">Bosse et al., 2008</xref>).</p>
              </sec>
              <sec id="S35">
                <label>4.2 |</label>
                <title>Decoding sustained interoceptive attention</title>
                <p id="P81">The machine learning models trained on IEAT data were applied to decode 3-min runs of the SIAT. The general properties of the task were decoded in terms of task engagement (active tracking vs. passive monitoring) and attentional target (interoception vs. exteroception) categories. In general, a greater proportion of the SIAT period was decoded as being in an active tracking than passive monitoring state. Two further interactions with were observed. First, active tracking was most prominent early in the sustained attention period, but this advantage diminished over time. This finding is not surprising, given the difficulty inherent to sustaining attention over time; attention should be most focused immediately following the guided audio meditation that preceded each SIAT run but then deteriorate over time as fatigue and/or distraction sets in. Second, participants tended to be engaged in exteroception more than interoception during these earlier phases. This may be a result of two salient exteroceptive cues—the guided meditation recording and then the onset of the scanner functional recording, both of which may bias attention away from the intended interoceptive target at the start of each run.</p>
                <p id="P82">Together, these results suggest some challenges in examining sustained interoceptive attention in the scanner. Attention may be inherently biased towards the noise of the scanner environment, and attention may also cease to be consistently engaged about halfway through each 3-min fMRI scanner run. These considerations provided an important context for our next analyses, suggesting that future research on sustained interoceptive attention might focus most fruitfully on the first few minutes of attention to maximize assessment during periods of participant engagement.</p>
                <sec id="S36">
                  <label>4.2.1 |</label>
                  <title>MABT training effects</title>
                  <p id="P83">Following general characterization of the SIAT, we then examined the influence of MABT intervention training effects upon the neural decoding data. Distinct patterns were observed between the two group, driven by post-intervention differences. At baseline, participants were generally more often engaged in exteroception than interoception. However, at post-intervention, MABT participants showed greater interoception than exteroception, a shift not shared by the control group. This statistically significant training effect therefore qualified a general finding of greater exteroception at the start of each SIAT run. A combination of audio recordings prior to the run, combined with the noise of the fMRI data acquisition, may have created a general bias towards exteroception, but MABT appeared sufficient to overcome this bias and allow for sustainable interoceptive processing. It should be noted that the initial ability to engage interoception in the MABT post-intervention group was still limited by the decline in active engagement over the 3-min run: as active engagement declined, so too did the distinction between the MABT and Control groups.</p>
                  <p id="P84">As outlined in the <xref rid="SD1" ref-type="supplementary-material">supporting information</xref>, we more fully explored training effects using several other metrics of interoceptive capacity. We found some preliminary (but statistically insignificant) indications that post-intervention MABT participants might have experienced more stable periods of interoceptive and exteroceptive attention, suggesting an enhanced ability to sustain attention despite not meeting statistical significance. Reducing the full SIAT timecourse from hundreds of volumes into a single summary score may have resulted in lower statistical power, particularly if active engagement declined halfway through each task run. Hence, there appeared to be a tradeoff of lower power from this statistical reduction that attempted to simplify accounts at the expense of the power inherent when using complete data in multilevel models. Modelling raw rather than summary decoding data may therefore continue to be a useful technique for more powerfully interrogating participant mental states in future research.</p>
                </sec>
              </sec>
              <sec id="S37">
                <label>4.3 |</label>
                <title>Limitations and constraints on generality</title>
                <p id="P85">There were some limitations in the design of the fMRI tasks that can be improved in future iterations of this study. One limitation was the lack of guarantee of participant compliance during the passive conditions of the IEAT in which participants were not required to produce any behavioural responses, making it difficult to verify whether and how much they paid attention to the task. In our analysis, we focused on the active conditions in which participants’ attention was guaranteed through the high accuracy of button presses. Future studies can increase the usability of passive-condition fMRI data, for example, by incorporating occasional catch trials that require button presses.</p>
                <p id="P86">In this study, we modelled two experimental factors, task engagement (active tracking vs. passive monitoring) and attention target (interoceptive vs. exteroceptive attentional states). Future experiments might consider a greater variety of mental states so that interoception can be further compared to states such as mind-wandering, memory recall and focused attention on different types of exteroceptive stimuli. Post-scan qualitative interviewing can also be built into future studies to better understand what different attentional processes feel like for the participants at a phenomenological level (<xref rid="R54" ref-type="bibr">Petitmengin, 2021</xref>). With more experimental conditions designed, it is critical to keep in mind that these conditions need to be closely matched so that machine learning classifiers can focus on important constructs without leveraging too much on nuisance variables.</p>
                <p id="P87">Another limitation of this study was that no data were collected during the audio-guided interoceptive practice before the self-guided sustained attention. Since we found significant MABT training effect in the beginning of the self-guided sustained attention during which participants were more engaged in the task, it is likely that a high level of task engagement is needed to scaffold the recently developed interoceptive capacity in the training group. Therefore, future research might benefit from extending data collection to include the guided meditation period or from providing attentional prompts during the self-guided sustained attention. Individuals with varying degrees of experience in mindfulness or interoceptive trainings might benefit differently from such scaffolded attention, which can be another interesting topic of research.</p>
                <p id="P88">In terms of methods, we trained machine learning classifiers to discriminate between interoceptive and exteroceptive attentional states within individuals rather than across individuals. This analysis allowed us to make predictions and estimates based on each participant’s unique task-related neural signature and generate group-level brain maps to identify common brain areas associated with interoceptive and exteroception. Yet we have not characterized whether interoceptive attention could be consistently observed across individuals from a single training model or whether models are differentiated across individuals. Future studies can potentially develop across-individual decoders, with the caveat that such decoding is highly complex and can produce superfluous results and that interpretations about the decodability of mental states should be made with caution (<xref rid="R36" ref-type="bibr">Jabakhanji et al., 2022</xref>).</p>
              </sec>
            </sec>
            <sec id="S38">
              <label>5 |</label>
              <title>CONCLUSIONS</title>
              <p id="P89">In this proof-of-concept study, we showed that machine learning can be a promising tool to characterize the uniqueness of a mental process as compared to other related processes, in this case, interoceptive versus exteroceptive attention. Since interoception is theorized to be the foundation of our feeling states, meaning in life, and broader appraisals of wellbeing, understanding the neural underpinnings of interoception has great importance. Machine learning also has the potential for predicting individuals’ mental states, especially when self-report is not preferable for reasons such as interrupting the mental process being studied, for example, during sustained attention. Furthermore, machine learning can allow us to examine training-related changes at a neural level, in interoception or other types of mental health interventions. For example, therapies that have elements of interoception can benefit from understanding what role and how big of a role interoception plays in improving wellbeing.</p>
              <p id="P90">The analyses revealed high accuracy of machine learning models in distinguishing interoceptive from exteroceptive attention. For an illustrative purpose, voxels in the posterior cingulate and the middle insula were associated with interoception, whereas voxels in the somatosensory, motor and cortical midline regions were associated with exteroception. We then estimated individuals’ moment-to-moment attention when they were instructed to sustain focus on body sensations. We observed promising MABT training effects on interoceptive attention immediately following an audio-guided meditation, showing that individuals new to interoceptive training increased their capacity for interoceptive attention. Although our exploratory analyses did not reveal statistically significant associations between classifier-output and clinician--rated interoceptive sensibility, subjective interoceptive awareness and affective symptoms, our study provides a framework for analysing these constructs in more refined studies in the future.</p>
              <p id="P91">While further research is needed to address the limitations of this study, the present findings showed that interoceptive and exteroceptive attention appeared to have distinct neural signatures and that machine learning shows promise for advancing our knowledge in interoceptive processes.</p>
            </sec>
            <sec sec-type="supplementary-material" id="SM1">
              <title>Supplementary Material</title>
              <supplementary-material id="SD1" position="float" content-type="local-data">
                <label>Supplementary Materials</label>
                <media xlink:href="NIHMS1944108-supplement-Supplementary_Materials.pdf" id="d64e911" position="anchor"/>
              </supplementary-material>
            </sec>
          </body>
          <back>
            <ack id="S40">
              <title>ACKNOWLEDGEMENTS</title>
              <p id="P93">This work was supported by a RIFP award from the School of Nursing at the University of Washington and a grant from the National Center for Advancing Translational Sciences of the National Institutes of Health (UL1 TR002319). The content is solely the responsibility of the authors and does not necessarily represent the official views of the National Institutes of Health. Data analysis infrastructure and trainee expenses were supported by a Canadian Natural Sciences and Engineering Council Discovery grant (RGPIN-2015-05901). We thank Natalie Koh and Sophie Xie for their help in coordination and data collection and to Dr. Thomas Grabowski, Dr. Chris Gatenby and Ms. Liza Young at the Integrated Brain Research Center at the University of Washington for their collaboration. We thank Dr. Helen Weng for providing the MATLAB code used for her 2020 paper, which served as a helpful conceptual guide in developing our analysis pipeline. Last but not least, we wish to express our appreciation to the study participants and the MABT therapists, Elizabeth Chaison and Carla Wiechman.</p>
              <sec id="S41">
                <title>Funding information</title>
                <p id="P94">University of Washington, Grant/Award Number: RIFP; National Center for Advancing Translational Sciences, Grant/Award Number: UL1 TR002319; Canadian Natural Sciences and Engineering Council Discovery, Grant/Award Number: RGPIN-2015-05901</p>
              </sec>
            </ack>
            <fn-group>
              <fn fn-type="COI-statement" id="FN2">
                <p id="P95">CONFLICT OF INTEREST STATEMENT</p>
                <p id="P96">The authors declare no conflict of interest.</p>
              </fn>
              <fn id="FN4">
                <p id="P97">PEER REVIEW</p>
                <p id="P98">The peer review history for this article is available at Web of Science (<ext-link xlink:href="10.1111/ejn.16045" ext-link-type="doi">https://www.webofscience.com/api/gateway/wos/peer-review/10.1111/ejn.16045</ext-link>).</p>
              </fn>
              <fn id="FN5">
                <p id="P99">SUPPORTING INFORMATION</p>
                <p id="P100">Additional supporting information can be found online in the <xref rid="SD1" ref-type="supplementary-material">Supporting Information</xref> section at the end of this article.</p>
              </fn>
            </fn-group>
            <sec sec-type="data-availability" id="S39">
              <title>DATA AVAILABILITY STATEMENT</title>
              <p id="P92">All study materials and code are available on the Open Science Framework (<ext-link xlink:href="https://osf.io/ctqrh/" ext-link-type="uri">https://osf.io/ctqrh/</ext-link>).</p>
            </sec>
            <glossary>
              <title>Abbreviations:</title>
              <def-list>
                <def-item>
                  <term>BOLD</term>
                  <def>
                    <p id="P101">blood oxygen level dependent</p>
                  </def>
                </def-item>
                <def-item>
                  <term>BrainIAK</term>
                  <def>
                    <p id="P102">Brain Imaging Analysis Kit</p>
                  </def>
                </def-item>
                <def-item>
                  <term>DAN</term>
                  <def>
                    <p id="P103">dorsal attention network</p>
                  </def>
                </def-item>
                <def-item>
                  <term>dmPFC</term>
                  <def>
                    <p id="P104">dorsomedial prefrontal cortex</p>
                  </def>
                </def-item>
                <def-item>
                  <term>EPI</term>
                  <def>
                    <p id="P105">echo-planar-imaging</p>
                  </def>
                </def-item>
                <def-item>
                  <term>fMRI</term>
                  <def>
                    <p id="P106">functional magnetic resonance imaging</p>
                  </def>
                </def-item>
                <def-item>
                  <term>GAD</term>
                  <def>
                    <p id="P107">generalized anxiety disorder</p>
                  </def>
                </def-item>
                <def-item>
                  <term>IEAT</term>
                  <def>
                    <p id="P108">Interoceptive/Exteroceptive Attention Task</p>
                  </def>
                </def-item>
                <def-item>
                  <term>MABT</term>
                  <def>
                    <p id="P109">Mindful Awareness in Body-oriented Therapy</p>
                  </def>
                </def-item>
                <def-item>
                  <term>MAIA</term>
                  <def>
                    <p id="P110">Multidimensional Assessment of Interoceptive Awareness</p>
                  </def>
                </def-item>
                <def-item>
                  <term>MDD</term>
                  <def>
                    <p id="P111">major depressive disorder</p>
                  </def>
                </def-item>
                <def-item>
                  <term>moPFC</term>
                  <def>
                    <p id="P112">medial orbital prefrontal cortex</p>
                  </def>
                </def-item>
                <def-item>
                  <term>pACC</term>
                  <def>
                    <p id="P113">perigenual anterior cingulate cortex</p>
                  </def>
                </def-item>
                <def-item>
                  <term>PHQ-SADS</term>
                  <def>
                    <p id="P114">Patient Health Questionnaire–Somatic, Anxiety and Depressive Symptoms</p>
                  </def>
                </def-item>
                <def-item>
                  <term>PSS</term>
                  <def>
                    <p id="P115">Perceived Stress Scale</p>
                  </def>
                </def-item>
                <def-item>
                  <term>PTSD</term>
                  <def>
                    <p id="P116">posttraumatic stress disorder</p>
                  </def>
                </def-item>
                <def-item>
                  <term>SIAT</term>
                  <def>
                    <p id="P117">Sustained Interoceptive Attention Task</p>
                  </def>
                </def-item>
                <def-item>
                  <term>TE</term>
                  <def>
                    <p id="P118">echo time</p>
                  </def>
                </def-item>
                <def-item>
                  <term>TI</term>
                  <def>
                    <p id="P119">inversion time</p>
                  </def>
                </def-item>
                <def-item>
                  <term>TR</term>
                  <def>
                    <p id="P120">repetition time</p>
                  </def>
                </def-item>
                <def-item>
                  <term>UW</term>
                  <def>
                    <p id="P121">University of Washington</p>
                  </def>
                </def-item>
                <def-item>
                  <term>V1</term>
                  <def>
                    <p id="P122">primary visual cortex</p>
                  </def>
                </def-item>
                <def-item>
                  <term>V5</term>
                  <def>
                    <p id="P123">middle temporal visual area</p>
                  </def>
                </def-item>
                <def-item>
                  <term>vmPFC</term>
                  <def>
                    <p id="P124">ventromedial prefrontal cortex</p>
                  </def>
                </def-item>
              </def-list>
            </glossary>
            <ref-list>
              <title>REFERENCES</title>
              <ref id="R1">
                <mixed-citation publication-type="journal"><name><surname>Abraham</surname><given-names>A</given-names></name>, <name><surname>Pedregosa</surname><given-names>F</given-names></name>, <name><surname>Eickenberg</surname><given-names>M</given-names></name>, <name><surname>Gervais</surname><given-names>P</given-names></name>, <name><surname>Mueller</surname><given-names>A</given-names></name>, <name><surname>Kossaifi</surname><given-names>J</given-names></name>, <name><surname>Gramfort</surname><given-names>A</given-names></name>, <name><surname>Thirion</surname><given-names>B</given-names></name>, &amp; <name><surname>Varoquaux</surname><given-names>G</given-names></name> (<year>2014</year>). <article-title>Machine learning for neuroimaging with scikit-learn.</article-title>
<source>Frontiers in Neuroinformatics</source>, <volume>8</volume>, <fpage>14</fpage>. <pub-id pub-id-type="doi">10.3389/fninf.2014.00014</pub-id><pub-id pub-id-type="pmid">24600388</pub-id>
</mixed-citation>
              </ref>
              <ref id="R2">
                <mixed-citation publication-type="journal"><name><surname>Barsky</surname><given-names>AJ</given-names></name>, <name><surname>Peekna</surname><given-names>HM</given-names></name>, &amp; <name><surname>Borus</surname><given-names>JF</given-names></name> (<year>2001</year>). <article-title>Somatic symptom reporting in women and men.</article-title>
<source>Journal of General Internal Medicine</source>, <volume>16</volume>(<issue>4</issue>), <fpage>266</fpage>–<lpage>275</lpage>. <pub-id pub-id-type="doi">10.1046/j.1525-1497.2001.00229.x</pub-id><pub-id pub-id-type="pmid">11318929</pub-id>
</mixed-citation>
              </ref>
              <ref id="R3">
                <mixed-citation publication-type="journal"><name><surname>Bates</surname><given-names>D</given-names></name>, <name><surname>Mächler</surname><given-names>M</given-names></name>, <name><surname>Bolker</surname><given-names>B</given-names></name>, &amp; <name><surname>Walker</surname><given-names>S</given-names></name> (<year>2015</year>). <article-title>Fitting linear mixed-effects models using lme4.</article-title>
<source>Journal of Statistical Software</source>, <volume>67</volume>(<issue>1</issue>), <fpage>1</fpage>–<lpage>48</lpage>. <pub-id pub-id-type="doi">10.18637/jss.v067.i01</pub-id></mixed-citation>
              </ref>
              <ref id="R4">
                <mixed-citation publication-type="journal"><name><surname>Bosse</surname><given-names>T</given-names></name>, <name><surname>Jonker</surname><given-names>CM</given-names></name>, &amp; <name><surname>Treur</surname><given-names>J</given-names></name> (<year>2008</year>). <article-title>Formalization of Damasio’s theory of emotion, feeling and core consciousness.</article-title>
<source>Consciousness and Cognition</source>, <volume>17</volume>(<issue>1</issue>), <fpage>94</fpage>–<lpage>113</lpage>. <pub-id pub-id-type="doi">10.1016/j.concog.2007.06.006</pub-id><pub-id pub-id-type="pmid">17689980</pub-id>
</mixed-citation>
              </ref>
              <ref id="R5">
                <mixed-citation publication-type="journal"><name><surname>Brener</surname><given-names>J</given-names></name>, &amp; <name><surname>Ring</surname><given-names>C</given-names></name> (<year>2016</year>). <article-title>Towards a psychophysics of interoceptive processes: The measurement of heartbeat detection.</article-title>
<source>Philosophical Transactions of the Royal Society, B: Biological Sciences</source>, <volume>371</volume>(<issue>1708</issue>), <fpage>20160015</fpage>. <pub-id pub-id-type="doi">10.1098/rstb.2016.0015</pub-id></mixed-citation>
              </ref>
              <ref id="R6">
                <mixed-citation publication-type="journal"><name><surname>Calì</surname><given-names>G</given-names></name>, <name><surname>Ambrosini</surname><given-names>E</given-names></name>, <name><surname>Picconi</surname><given-names>L</given-names></name>, <name><surname>Mehling</surname><given-names>W</given-names></name>, &amp; <name><surname>Committeri</surname><given-names>G</given-names></name> (<year>2015</year>). <article-title>Investigating the relationship between interoceptive accuracy, interoceptive awareness, and emotional susceptibility.</article-title>
<source>Frontiers in Psychology</source>, <volume>6</volume>, <fpage>1202</fpage>. <pub-id pub-id-type="doi">10.3389/fpsyg.2015.01202</pub-id><pub-id pub-id-type="pmid">26379571</pub-id>
</mixed-citation>
              </ref>
              <ref id="R7">
                <mixed-citation publication-type="journal"><name><surname>Clark</surname><given-names>IA</given-names></name>, <name><surname>Niehaus</surname><given-names>KE</given-names></name>, <name><surname>Duff</surname><given-names>EP</given-names></name>, <name><surname>di Simplicio</surname><given-names>MC</given-names></name>, <name><surname>Clifford</surname><given-names>GD</given-names></name>, <name><surname>Smith</surname><given-names>SM</given-names></name>, <name><surname>Mackay</surname><given-names>CE</given-names></name>, <name><surname>Woolrich</surname><given-names>MW</given-names></name>, &amp; <name><surname>Holmes</surname><given-names>EA</given-names></name> (<year>2014</year>). <article-title>First steps in using machine learning on fMRI data to predict intrusive memories of traumatic film footage.</article-title>
<source>Behaviour Research and Therapy</source>, <volume>62</volume>, <fpage>37</fpage>–<lpage>46</lpage>. <pub-id pub-id-type="doi">10.1016/j.brat.2014.07.010</pub-id><pub-id pub-id-type="pmid">25151915</pub-id>
</mixed-citation>
              </ref>
              <ref id="R8">
                <mixed-citation publication-type="journal"><name><surname>Cohen</surname><given-names>S</given-names></name>, <name><surname>Kamarck</surname><given-names>T</given-names></name>, &amp; <name><surname>Mermelstein</surname><given-names>R</given-names></name> (<year>1983</year>). <article-title>A global measure of perceived stress.</article-title>
<source>Journal of Health and Social Behavior</source>, <volume>24</volume>(<issue>4</issue>), <fpage>385</fpage>. <pub-id pub-id-type="doi">10.2307/2136404</pub-id><pub-id pub-id-type="pmid">6668417</pub-id>
</mixed-citation>
              </ref>
              <ref id="R9">
                <mixed-citation publication-type="journal"><name><surname>Craig</surname><given-names>AD</given-names></name> (<year>2009</year>). <article-title>How do you feel — now? The anterior insula and human awareness.</article-title>
<source>Nature Reviews Neuroscience</source>, <volume>10</volume>(<issue>1</issue>), <fpage>59</fpage>–<lpage>70</lpage>. <pub-id pub-id-type="doi">10.1038/nrn2555</pub-id><pub-id pub-id-type="pmid">19096369</pub-id>
</mixed-citation>
              </ref>
              <ref id="R10">
                <mixed-citation publication-type="journal"><name><surname>Craig</surname><given-names>AD</given-names></name> (<year>2002</year>). <article-title>How do you feel? Interoception: The sense of the physiological condition of the body.</article-title>
<source>Nature Reviews Neuroscience</source>, <volume>3</volume>(<issue>8</issue>), <fpage>655</fpage>–<lpage>666</lpage>. <pub-id pub-id-type="doi">10.1038/nrn894</pub-id><pub-id pub-id-type="pmid">12154366</pub-id>
</mixed-citation>
              </ref>
              <ref id="R11">
                <mixed-citation publication-type="journal"><name><surname>Craig</surname><given-names>AD</given-names></name> (<year>2003</year>). <article-title>Interoception: The sense of the physiological condition of the body.</article-title>
<source>Current Opinion in Neurobiology</source>, <volume>13</volume>(<issue>4</issue>), <fpage>500</fpage>–<lpage>505</lpage>. <pub-id pub-id-type="doi">10.1016/S0959-4388(03)00090-4</pub-id><pub-id pub-id-type="pmid">12965300</pub-id>
</mixed-citation>
              </ref>
              <ref id="R12">
                <mixed-citation publication-type="journal"><name><surname>Critchley</surname><given-names>HD</given-names></name>, &amp; <name><surname>Garfinkel</surname><given-names>SN</given-names></name> (<year>2017</year>). <article-title>Interoception and emotion.</article-title>
<source>Current Opinion in Psychology</source>, <volume>17</volume>, <fpage>7</fpage>–<lpage>14</lpage>. <pub-id pub-id-type="doi">10.1016/j.copsyc.2017.04.020</pub-id><pub-id pub-id-type="pmid">28950976</pub-id>
</mixed-citation>
              </ref>
              <ref id="R13">
                <mixed-citation publication-type="journal"><name><surname>Critchley</surname><given-names>HD</given-names></name>, &amp; <name><surname>Harrison</surname><given-names>NA</given-names></name> (<year>2013</year>). <article-title>Visceral influences on brain and behavior.</article-title>
<source>Neuron</source>, <volume>77</volume>(<issue>4</issue>), <fpage>624</fpage>–<lpage>638</lpage>. <pub-id pub-id-type="doi">10.1016/j.neuron.2013.02.008</pub-id><pub-id pub-id-type="pmid">23439117</pub-id>
</mixed-citation>
              </ref>
              <ref id="R14">
                <mixed-citation publication-type="journal"><name><surname>Critchley</surname><given-names>HD</given-names></name>, <name><surname>Wiens</surname><given-names>S</given-names></name>, <name><surname>Rotshtein</surname><given-names>P</given-names></name>, <name><surname>Öhman</surname><given-names>A</given-names></name>, &amp; <name><surname>Dolan</surname><given-names>RJ</given-names></name> (<year>2004</year>). <article-title>Neural systems supporting interoceptive awareness.</article-title>
<source>Nature Neuroscience</source>, <volume>7</volume>(<issue>2</issue>), <fpage>189</fpage>–<lpage>195</lpage>. <pub-id pub-id-type="doi">10.1038/nn1176</pub-id><pub-id pub-id-type="pmid">14730305</pub-id>
</mixed-citation>
              </ref>
              <ref id="R15">
                <mixed-citation publication-type="journal"><name><surname>Davatzikos</surname><given-names>C</given-names></name> (<year>2019</year>). <article-title>Machine learning in neuroimaging: Progress and challenges.</article-title>
<source>NeuroImage</source>, <volume>197</volume>, <fpage>652</fpage>–<lpage>656</lpage>. <pub-id pub-id-type="doi">10.1016/j.neuroimage.2018.10.003</pub-id><pub-id pub-id-type="pmid">30296563</pub-id>
</mixed-citation>
              </ref>
              <ref id="R16">
                <mixed-citation publication-type="journal"><name><surname>de Jong</surname><given-names>M</given-names></name>, <name><surname>Lazar</surname><given-names>SW</given-names></name>, <name><surname>Hug</surname><given-names>K</given-names></name>, <name><surname>Mehling</surname><given-names>WE</given-names></name>, <name><surname>Hölzel</surname><given-names>BK</given-names></name>, <name><surname>Sack</surname><given-names>AT</given-names></name>, <name><surname>Peeters</surname><given-names>F</given-names></name>, <name><surname>Ashih</surname><given-names>H</given-names></name>, <name><surname>Mischoulon</surname><given-names>D</given-names></name>, &amp; <name><surname>Gard</surname><given-names>T</given-names></name> (<year>2016</year>). <article-title>Effects of mindfulness-based cognitive therapy on body awareness in patients with chronic pain and comorbid depression.</article-title>
<source>Frontiers in Psychology</source>, <volume>7</volume>, <fpage>967</fpage>. <pub-id pub-id-type="doi">10.3389/fpsyg.2016.00967</pub-id><pub-id pub-id-type="pmid">27445929</pub-id>
</mixed-citation>
              </ref>
              <ref id="R17">
                <mixed-citation publication-type="journal"><name><surname>Dixon</surname><given-names>ML</given-names></name>, <name><surname>de la Vega</surname><given-names>A</given-names></name>, <name><surname>Mills</surname><given-names>C</given-names></name>, <name><surname>Andrews-Hanna</surname><given-names>J</given-names></name>, <name><surname>Spreng</surname><given-names>RN</given-names></name>, <name><surname>Cole</surname><given-names>MW</given-names></name>, &amp; <name><surname>Christoff</surname><given-names>K</given-names></name> (<year>2018</year>). <article-title>Heterogeneity within the frontoparietal control network and its relationship to the default and dorsal attention networks.</article-title>
<source>Proceedings of the National Academy of Sciences</source>, <volume>115</volume>(<issue>7</issue>), <fpage>E1598</fpage>–<lpage>E1607</lpage>. <pub-id pub-id-type="doi">10.1073/pnas.1715766115</pub-id></mixed-citation>
              </ref>
              <ref id="R18">
                <mixed-citation publication-type="journal"><name><surname>Domschke</surname><given-names>K</given-names></name>, <name><surname>Stevens</surname><given-names>S</given-names></name>, <name><surname>Pfleiderer</surname><given-names>B</given-names></name>, &amp; <name><surname>Gerlach</surname><given-names>AL</given-names></name> (<year>2010</year>). <article-title>Interoceptive sensitivity in anxiety and anxiety disorders: An overview and integration of neurobiological findings.</article-title>
<source>Clinical Psychology Review</source>, <volume>30</volume>(<issue>1</issue>), <fpage>1</fpage>–<lpage>11</lpage>. <pub-id pub-id-type="doi">10.1016/j.cpr.2009.08.008</pub-id><pub-id pub-id-type="pmid">19751958</pub-id>
</mixed-citation>
              </ref>
              <ref id="R19">
                <mixed-citation publication-type="journal"><name><surname>Dunn</surname><given-names>BD</given-names></name>, <name><surname>Dalgleish</surname><given-names>T</given-names></name>, <name><surname>Ogilvie</surname><given-names>AD</given-names></name>, &amp; <name><surname>Lawrence</surname><given-names>AD</given-names></name> (<year>2007</year>). <article-title>Heartbeat perception in depression.</article-title>
<source>Behaviour Research and Therapy</source>, <volume>45</volume>(<issue>8</issue>), <fpage>1921</fpage>–<lpage>1930</lpage>. <pub-id pub-id-type="doi">10.1016/j.brat.2006.09.008</pub-id><pub-id pub-id-type="pmid">17087914</pub-id>
</mixed-citation>
              </ref>
              <ref id="R20">
                <mixed-citation publication-type="journal"><name><surname>Dunn</surname><given-names>BD</given-names></name>, <name><surname>Galton</surname><given-names>HC</given-names></name>, <name><surname>Morgan</surname><given-names>R</given-names></name>, <name><surname>Evans</surname><given-names>D</given-names></name>, <name><surname>Oliver</surname><given-names>C</given-names></name>, <name><surname>Meyer</surname><given-names>M</given-names></name>, <name><surname>Cusack</surname><given-names>R</given-names></name>, <name><surname>Lawrence</surname><given-names>AD</given-names></name>, &amp; <name><surname>Dalgleish</surname><given-names>T</given-names></name> (<year>2010</year>). <article-title>Listening to your heart. How interoception shapes emotion experience and intuitive decision making.</article-title>
<source>Psychological Science</source>, <volume>21</volume>(<issue>12</issue>), <fpage>1835</fpage>–<lpage>1844</lpage>. <pub-id pub-id-type="doi">10.1177/0956797610389191</pub-id><pub-id pub-id-type="pmid">21106893</pub-id>
</mixed-citation>
              </ref>
              <ref id="R21">
                <mixed-citation publication-type="journal"><name><surname>Eggart</surname><given-names>M</given-names></name>, &amp; <name><surname>Valdés-Stauber</surname><given-names>J</given-names></name> (<year>2021</year>). <article-title>Can changes in multidimensional self-reported interoception be considered as outcome predictors in severely depressed patients? A moderation and mediation analysis.</article-title>
<source>Journal of Psychosomatic Research</source>, <volume>141</volume>, <fpage>110331</fpage>. <pub-id pub-id-type="doi">10.1016/j.jpsychores.2020.110331</pub-id><pub-id pub-id-type="pmid">33338695</pub-id>
</mixed-citation>
              </ref>
              <ref id="R22">
                <mixed-citation publication-type="journal"><name><surname>Esteban</surname><given-names>O</given-names></name>, <name><surname>Markiewicz</surname><given-names>CJ</given-names></name>, <name><surname>Blair</surname><given-names>RW</given-names></name>, <name><surname>Moodie</surname><given-names>CA</given-names></name>, <name><surname>Isik</surname><given-names>AI</given-names></name>, <name><surname>Erramuzpe</surname><given-names>A</given-names></name>, <name><surname>Kent</surname><given-names>JD</given-names></name>, <name><surname>Goncalves</surname><given-names>M</given-names></name>, <name><surname>DuPre</surname><given-names>E</given-names></name>, <name><surname>Snyder</surname><given-names>M</given-names></name>, <name><surname>Oya</surname><given-names>H</given-names></name>, <name><surname>Ghosh</surname><given-names>SS</given-names></name>, <name><surname>Wright</surname><given-names>J</given-names></name>, <name><surname>Durnez</surname><given-names>J</given-names></name>, <name><surname>Poldrack</surname><given-names>RA</given-names></name>, &amp; <name><surname>Gorgolewski</surname><given-names>KJ</given-names></name> (<year>2019</year>). <article-title>FMRI-Prep: A robust preprocessing pipeline for functional MRI.</article-title>
<source>Nature Methods</source>, <volume>16</volume>(<issue>1</issue>), <fpage>111</fpage>–<lpage>116</lpage>. <pub-id pub-id-type="doi">10.1038/s41592-018-0235-4</pub-id><pub-id pub-id-type="pmid">30532080</pub-id>
</mixed-citation>
              </ref>
              <ref id="R23">
                <mixed-citation publication-type="journal"><name><surname>Farb</surname><given-names>NA</given-names></name>, <name><surname>Segal</surname><given-names>ZV</given-names></name>, &amp; <name><surname>Anderson</surname><given-names>AK</given-names></name> (<year>2013</year>). <article-title>Attentional modulation of primary interoceptive and exteroceptive cortices.</article-title>
<source>Cerebral Cortex</source>, <volume>23</volume>(<issue>1</issue>), <fpage>114</fpage>–<lpage>126</lpage>. <pub-id pub-id-type="doi">10.1093/cercor/bhr385</pub-id><pub-id pub-id-type="pmid">22267308</pub-id>
</mixed-citation>
              </ref>
              <ref id="R24">
                <mixed-citation publication-type="journal"><name><surname>Farb</surname><given-names>NAS</given-names></name>, <name><surname>Anderson</surname><given-names>AK</given-names></name>, <name><surname>Mayberg</surname><given-names>H</given-names></name>, <name><surname>Bean</surname><given-names>J</given-names></name>, <name><surname>McKeon</surname><given-names>D</given-names></name>, &amp; <name><surname>Segal</surname><given-names>ZV</given-names></name> (<year>2010</year>). <article-title>Minding one’s emotions: Mindfulness training alters the neural expression of sadness.</article-title>
<source>Emotion</source>, <volume>10</volume>(<issue>1</issue>), <fpage>25</fpage>–<lpage>33</lpage>. <pub-id pub-id-type="doi">10.1037/a0017151</pub-id><pub-id pub-id-type="pmid">20141299</pub-id>
</mixed-citation>
              </ref>
              <ref id="R25">
                <mixed-citation publication-type="journal"><name><surname>Farb</surname><given-names>NAS</given-names></name>, <name><surname>Daubenmier</surname><given-names>J</given-names></name>, <name><surname>Price</surname><given-names>CJ</given-names></name>, <name><surname>Gard</surname><given-names>T</given-names></name>, <name><surname>Kerr</surname><given-names>C</given-names></name>, <name><surname>Dunn</surname><given-names>BD</given-names></name>, <name><surname>Klein</surname><given-names>AC</given-names></name>, <name><surname>Paulus</surname><given-names>MP</given-names></name>, &amp; <name><surname>Mehling</surname><given-names>WE</given-names></name> (<year>2015</year>). <article-title>Interoception, contemplative practice, and health.</article-title>
<source>Frontiers in Psychology</source>, <volume>6</volume>, <fpage>763</fpage>. <pub-id pub-id-type="doi">10.3389/fpsyg.2015.00763</pub-id><pub-id pub-id-type="pmid">26106345</pub-id>
</mixed-citation>
              </ref>
              <ref id="R26">
                <mixed-citation publication-type="journal"><name><surname>Farb</surname><given-names>NAS</given-names></name>, <name><surname>Segal</surname><given-names>ZV</given-names></name>, <name><surname>Mayberg</surname><given-names>H</given-names></name>, <name><surname>Bean</surname><given-names>J</given-names></name>, <name><surname>McKeon</surname><given-names>D</given-names></name>, <name><surname>Fatima</surname><given-names>Z</given-names></name>, &amp; <name><surname>Anderson</surname><given-names>AK</given-names></name> (<year>2007</year>). <article-title>Attending to the present: Mindfulness meditation reveals distinct neural modes of self-reference.</article-title>
<source>Social Cognitive and Affective Neuroscience</source>, <volume>2</volume>(<issue>4</issue>), <fpage>313</fpage>–<lpage>322</lpage>. <pub-id pub-id-type="doi">10.1093/scan/nsm030</pub-id><pub-id pub-id-type="pmid">18985137</pub-id>
</mixed-citation>
              </ref>
              <ref id="R27">
                <mixed-citation publication-type="other"><name><surname>Farb</surname><given-names>NAS</given-names></name>, <name><surname>Zuo</surname><given-names>Z</given-names></name>, &amp; <name><surname>Price</surname><given-names>CJ</given-names></name> (<year>2023</year>). <article-title>Interoceptive awareness of the breath preserves dorsal attention network activity amidst widespread cortical deactivation: A within-participant neuroimaging study.</article-title>
<source>BioRxiv</source>, <comment>2022.05.27.493743.</comment>
<pub-id pub-id-type="doi">10.1101/2022.05.27.493743</pub-id></mixed-citation>
              </ref>
              <ref id="R28">
                <mixed-citation publication-type="journal"><name><surname>Ferentzi</surname><given-names>E</given-names></name>, <name><surname>Drew</surname><given-names>R</given-names></name>, <name><surname>Tihanyi</surname><given-names>BT</given-names></name>, &amp; <name><surname>Köteles</surname><given-names>F</given-names></name> (<year>2018</year>). <article-title>Interoceptive accuracy and body awareness—Temporal and longitudinal associations in a non-clinical sample.</article-title>
<source>Physiology &amp; Behavior</source>, <volume>184</volume>, <fpage>100</fpage>–<lpage>107</lpage>. <pub-id pub-id-type="doi">10.1016/j.physbeh.2017.11.015</pub-id><pub-id pub-id-type="pmid">29155249</pub-id>
</mixed-citation>
              </ref>
              <ref id="R29">
                <mixed-citation publication-type="journal"><name><surname>Flynn</surname><given-names>FG</given-names></name> (<year>1999</year>). <article-title>Anatomy of the insula functional and clinical correlates.</article-title>
<source>Aphasiology</source>, <volume>13</volume>(<issue>1</issue>), <fpage>55</fpage>–<lpage>78</lpage>. <pub-id pub-id-type="doi">10.1080/026870399402325</pub-id></mixed-citation>
              </ref>
              <ref id="R30">
                <mixed-citation publication-type="journal"><name><surname>Furman</surname><given-names>DJ</given-names></name>, <name><surname>Waugh</surname><given-names>CE</given-names></name>, <name><surname>Bhattacharjee</surname><given-names>K</given-names></name>, <name><surname>Thompson</surname><given-names>RJ</given-names></name>, &amp; <name><surname>Gotlib</surname><given-names>IH</given-names></name> (<year>2013</year>). <article-title>Interoceptive awareness, positive affect, and decision making in Major Depressive Disorder.</article-title>
<source>Journal of Affective Disorders</source>, <volume>151</volume>(<issue>2</issue>), <fpage>780</fpage>–<lpage>785</lpage>. <pub-id pub-id-type="doi">10.1016/j.jad.2013.06.044</pub-id><pub-id pub-id-type="pmid">23972662</pub-id>
</mixed-citation>
              </ref>
              <ref id="R31">
                <mixed-citation publication-type="journal"><name><surname>Glenn</surname><given-names>DE</given-names></name>, <name><surname>Acheson</surname><given-names>DT</given-names></name>, <name><surname>Geyer</surname><given-names>MA</given-names></name>, <name><surname>Nievergelt</surname><given-names>CM</given-names></name>, <name><surname>Baker</surname><given-names>DG</given-names></name>, &amp; <name><surname>Risbrough</surname><given-names>VB</given-names></name> (<year>2016</year>). <article-title>High and low threshold for startle reactivity associated with PTSD symptoms but not PTSD risk: Evidence from a prospective study of active duty marines.</article-title>
<source>Depression and Anxiety</source>, <volume>33</volume>(<issue>3</issue>), <fpage>192</fpage>–<lpage>202</lpage>. <pub-id pub-id-type="doi">10.1002/da.22475</pub-id><pub-id pub-id-type="pmid">26878585</pub-id>
</mixed-citation>
              </ref>
              <ref id="R32">
                <mixed-citation publication-type="book"><name><surname>Greenlee</surname><given-names>MW</given-names></name>, &amp; <name><surname>Tse</surname><given-names>PU</given-names></name> (<year>2008</year>). <part-title>Functional neuroanatomy of the human visual system: A review of functional MRI studies.</part-title> In <name><surname>Lorenz</surname><given-names>B</given-names></name> &amp; <name><surname>Borruat</surname><given-names>F-X</given-names></name> (Eds.), <source>Pediatric ophthalmology, neuro-ophthalmology, genetics</source> (pp. <fpage>119</fpage>–<lpage>138</lpage>). <publisher-name>Springer</publisher-name>. <pub-id pub-id-type="doi">10.1007/978-3-540-33679-2_8</pub-id></mixed-citation>
              </ref>
              <ref id="R33">
                <mixed-citation publication-type="journal"><name><surname>Gu</surname><given-names>X</given-names></name>, &amp; <name><surname>FitzGerald</surname><given-names>THB</given-names></name> (<year>2014</year>). <article-title>Interoceptive inference: Homeostasis and decision-making.</article-title>
<source>Trends in Cognitive Sciences</source>, <volume>18</volume>(<issue>6</issue>), <fpage>269</fpage>–<lpage>270</lpage>. <pub-id pub-id-type="doi">10.1016/j.tics.2014.02.001</pub-id><pub-id pub-id-type="pmid">24582825</pub-id>
</mixed-citation>
              </ref>
              <ref id="R34">
                <mixed-citation publication-type="journal"><name><surname>Haxby</surname><given-names>JV</given-names></name> (<year>2012</year>). <article-title>Multivariate pattern analysis of fMRI: The early beginnings.</article-title>
<source>NeuroImage</source>, <volume>62</volume>(<issue>2</issue>), <fpage>852</fpage>–<lpage>855</lpage>. <pub-id pub-id-type="doi">10.1016/j.neuroimage.2012.03.016</pub-id><pub-id pub-id-type="pmid">22425670</pub-id>
</mixed-citation>
              </ref>
              <ref id="R35">
                <mixed-citation publication-type="journal"><name><surname>Herbert</surname><given-names>BM</given-names></name>, &amp; <name><surname>Pollatos</surname><given-names>O</given-names></name> (<year>2014</year>). <article-title>Attenuated interoceptive sensitivity in overweight and obese individuals.</article-title>
<source>Eating Behaviors</source>, <volume>15</volume>(<issue>3</issue>), <fpage>445</fpage>–<lpage>448</lpage>. <pub-id pub-id-type="doi">10.1016/j.eatbeh.2014.06.002</pub-id><pub-id pub-id-type="pmid">25064297</pub-id>
</mixed-citation>
              </ref>
              <ref id="R36">
                <mixed-citation publication-type="journal"><name><surname>Jabakhanji</surname><given-names>R</given-names></name>, <name><surname>Vigotsky</surname><given-names>AD</given-names></name>, <name><surname>Bielefeld</surname><given-names>J</given-names></name>, <name><surname>Huang</surname><given-names>L</given-names></name>, <name><surname>Baliki</surname><given-names>MN</given-names></name>, <name><surname>Iannetti</surname><given-names>GD</given-names></name>, &amp; <name><surname>Apkarian</surname><given-names>AV</given-names></name> (<year>2022</year>). <article-title>Limits of decoding mental states with fMRI.</article-title>
<source>Cortex</source>, <volume>149</volume>, <fpage>101</fpage>–<lpage>122</lpage>. <pub-id pub-id-type="doi">10.1016/j.cortex.2021.12.015</pub-id><pub-id pub-id-type="pmid">35219121</pub-id>
</mixed-citation>
              </ref>
              <ref id="R37">
                <mixed-citation publication-type="journal"><name><surname>Khalsa</surname><given-names>SS</given-names></name>, <name><surname>Adolphs</surname><given-names>R</given-names></name>, <name><surname>Cameron</surname><given-names>OG</given-names></name>, <name><surname>Critchley</surname><given-names>HD</given-names></name>, <name><surname>Davenport</surname><given-names>PW</given-names></name>, <name><surname>Feinstein</surname><given-names>JS</given-names></name>, <name><surname>Feusner</surname><given-names>JD</given-names></name>, <name><surname>Garfinkel</surname><given-names>SN</given-names></name>, <name><surname>Lane</surname><given-names>RD</given-names></name>, <name><surname>Mehling</surname><given-names>WE</given-names></name>, <name><surname>Meuret</surname><given-names>AE</given-names></name>, <name><surname>Nemeroff</surname><given-names>CB</given-names></name>, <name><surname>Oppenheimer</surname><given-names>S</given-names></name>, <name><surname>Petzschner</surname><given-names>FH</given-names></name>, <name><surname>Pollatos</surname><given-names>O</given-names></name>, <name><surname>Rhudy</surname><given-names>JL</given-names></name>, <name><surname>Schramm</surname><given-names>LP</given-names></name>, <name><surname>Simmons</surname><given-names>WK</given-names></name>, <name><surname>Stein</surname><given-names>MB</given-names></name>, … <collab>Interoception Summit 2016 participants.</collab> (<year>2018</year>). <article-title>Interoception and mental health: A roadmap.</article-title>
<source>Biological Psychiatry. Cognitive Neuroscience and Neuroimaging</source>, <volume>3</volume>(<issue>6</issue>), <fpage>501</fpage>–<lpage>513</lpage>. <pub-id pub-id-type="doi">10.1016/j.bpsc.2017.12.004</pub-id><pub-id pub-id-type="pmid">29884281</pub-id>
</mixed-citation>
              </ref>
              <ref id="R38">
                <mixed-citation publication-type="journal"><name><surname>Khalsa</surname><given-names>SS</given-names></name>, <name><surname>Rudrauf</surname><given-names>D</given-names></name>, <name><surname>Feinstein</surname><given-names>JS</given-names></name>, &amp; <name><surname>Tranel</surname><given-names>D</given-names></name> (<year>2009</year>). <article-title>The pathways of interoceptive awareness.</article-title>
<source>Nature Neuroscience</source>, <volume>12</volume>(<issue>12</issue>), <fpage>1494</fpage>–<lpage>1496</lpage>. <pub-id pub-id-type="doi">10.1038/nn.2411</pub-id><pub-id pub-id-type="pmid">19881506</pub-id>
</mixed-citation>
              </ref>
              <ref id="R39">
                <mixed-citation publication-type="journal"><name><surname>Kroenke</surname><given-names>K</given-names></name>, <name><surname>Spitzer</surname><given-names>RL</given-names></name>, <name><surname>Williams</surname><given-names>JBW</given-names></name>, &amp; <name><surname>Löwe</surname><given-names>B</given-names></name> (<year>2010</year>). <article-title>The patient health questionnaire somatic, anxiety, and depressive symptom scales: A systematic review.</article-title>
<source>General Hospital Psychiatry</source>, <volume>32</volume>(<issue>4</issue>), <fpage>345</fpage>–<lpage>359</lpage>. <pub-id pub-id-type="doi">10.1016/j.genhosppsych.2010.03.006</pub-id><pub-id pub-id-type="pmid">20633738</pub-id>
</mixed-citation>
              </ref>
              <ref id="R40">
                <mixed-citation publication-type="journal"><name><surname>Kumar</surname><given-names>M</given-names></name>, <name><surname>Anderson</surname><given-names>MJ</given-names></name>, <name><surname>Antony</surname><given-names>JW</given-names></name>, <name><surname>Baldassano</surname><given-names>C</given-names></name>, <name><surname>Brooks</surname><given-names>PP</given-names></name>, <name><surname>Cai</surname><given-names>MB</given-names></name>, <name><surname>Chen</surname><given-names>P-HC</given-names></name>, <name><surname>Ellis</surname><given-names>CT</given-names></name>, <name><surname>Henselman-Petrusek</surname><given-names>G</given-names></name>, <name><surname>Huberdeau</surname><given-names>D</given-names></name>, <name><surname>Hutchinson</surname><given-names>JB</given-names></name>, <name><surname>Li</surname><given-names>YP</given-names></name>, <name><surname>Lu</surname><given-names>Q</given-names></name>, <name><surname>Manning</surname><given-names>JR</given-names></name>, <name><surname>Mennen</surname><given-names>AC</given-names></name>, <name><surname>Nastase</surname><given-names>SA</given-names></name>, <name><surname>Richard</surname><given-names>H</given-names></name>, <name><surname>Schapiro</surname><given-names>AC</given-names></name>, <name><surname>Schuck</surname><given-names>NW</given-names></name>, … <name><surname>Norman</surname><given-names>KA</given-names></name> (<year>2022</year>). <article-title>BrainIAK: The brain imaging analysis kit.</article-title>
<source>Aperture Neuro</source>, <volume>2021</volume>(<issue>4</issue>), <fpage>42</fpage>. <pub-id pub-id-type="doi">10.52294/31bb5b68-2184-411b-8c00-a1dacb61e1da</pub-id></mixed-citation>
              </ref>
              <ref id="R41">
                <mixed-citation publication-type="journal"><name><surname>Kumar</surname><given-names>M</given-names></name>, <name><surname>Ellis</surname><given-names>CT</given-names></name>, <name><surname>Lu</surname><given-names>Q</given-names></name>, <name><surname>Zhang</surname><given-names>H</given-names></name>, <name><surname>Capotă</surname><given-names>M</given-names></name>, <name><surname>Willke</surname><given-names>TL</given-names></name>, <name><surname>Ramadge</surname><given-names>PJ</given-names></name>, <name><surname>Turk-Browne</surname><given-names>NB</given-names></name>, &amp; <name><surname>Norman</surname><given-names>KA</given-names></name> (<year>2020</year>). <article-title>BrainIAK tutorials: User-friendly learning materials for advanced fMRI analysis.</article-title>
<source>PLoS Computational Biology</source>, <volume>16</volume>(<issue>1</issue>), <fpage>e1007549</fpage>. <pub-id pub-id-type="doi">10.1371/journal.pcbi.1007549</pub-id><pub-id pub-id-type="pmid">31940340</pub-id>
</mixed-citation>
              </ref>
              <ref id="R42">
                <mixed-citation publication-type="journal"><name><surname>Lee</surname><given-names>E-H</given-names></name> (<year>2012</year>). <article-title>Review of the psychometric evidence of the perceived stress scale.</article-title>
<source>Asian Nursing Research</source>, <volume>6</volume>(<issue>4</issue>), <fpage>121</fpage>–<lpage>127</lpage>. <pub-id pub-id-type="doi">10.1016/j.anr.2012.08.004</pub-id><pub-id pub-id-type="pmid">25031113</pub-id>
</mixed-citation>
              </ref>
              <ref id="R43">
                <mixed-citation publication-type="journal"><name><surname>Li</surname><given-names>D</given-names></name>, <name><surname>Zucker</surname><given-names>NL</given-names></name>, <name><surname>Kragel</surname><given-names>PA</given-names></name>, <name><surname>Covington</surname><given-names>VE</given-names></name>, &amp; <name><surname>LaBar</surname><given-names>KS</given-names></name> (<year>2017</year>). <article-title>Adolescent development of insula-dependent interoceptive regulation.</article-title>
<source>Developmental Science</source>, <volume>20</volume>(<issue>5</issue>), <fpage>e12438</fpage>. <pub-id pub-id-type="doi">10.1111/desc.12438</pub-id></mixed-citation>
              </ref>
              <ref id="R44">
                <mixed-citation publication-type="journal"><name><surname>Limanowski</surname><given-names>J</given-names></name>, &amp; <name><surname>Blankenburg</surname><given-names>F</given-names></name> (<year>2013</year>). <article-title>Minimal self-models and the free energy principle.</article-title>
<source>Frontiers in Human Neuroscience</source>, <volume>7</volume>, <fpage>547</fpage>. <pub-id pub-id-type="doi">10.3389/fnhum.2013.00547</pub-id><pub-id pub-id-type="pmid">24062658</pub-id>
</mixed-citation>
              </ref>
              <ref id="R45">
                <mixed-citation publication-type="journal"><name><surname>Lotze</surname><given-names>M</given-names></name>, <name><surname>Erb</surname><given-names>M</given-names></name>, <name><surname>Flor</surname><given-names>H</given-names></name>, <name><surname>Huelsmann</surname><given-names>E</given-names></name>, <name><surname>Godde</surname><given-names>B</given-names></name>, &amp; <name><surname>Grodd</surname><given-names>W</given-names></name> (<year>2000</year>). <article-title>FMRI evaluation of somatotopic representation in human primary motor cortex.</article-title>
<source>NeuroImage</source>, <volume>11</volume>(<issue>5 Pt 1</issue>), <fpage>473</fpage>–<lpage>481</lpage>. <pub-id pub-id-type="doi">10.1006/nimg.2000.0556</pub-id><pub-id pub-id-type="pmid">10806033</pub-id>
</mixed-citation>
              </ref>
              <ref id="R46">
                <mixed-citation publication-type="journal"><name><surname>Matsumoto</surname><given-names>R</given-names></name>, <name><surname>Kitabayashi</surname><given-names>Y</given-names></name>, <name><surname>Narumoto</surname><given-names>J</given-names></name>, <name><surname>Wada</surname><given-names>Y</given-names></name>, <name><surname>Okamoto</surname><given-names>A</given-names></name>, <name><surname>Ushijima</surname><given-names>Y</given-names></name>, <name><surname>Yokoyama</surname><given-names>C</given-names></name>, <name><surname>Yamashita</surname><given-names>T</given-names></name>, <name><surname>Takahashi</surname><given-names>H</given-names></name>, <name><surname>Yasuno</surname><given-names>F</given-names></name>, <name><surname>Suhara</surname><given-names>T</given-names></name>, &amp; <name><surname>Fukui</surname><given-names>K</given-names></name> (<year>2006</year>). <article-title>Regional cerebral blood flow changes associated with interoceptive awareness in the recovery process of anorexia nervosa.</article-title>
<source>Progress in Neuro-Psychopharmacology &amp; Biological Psychiatry</source>, <volume>30</volume>(<issue>7</issue>), <fpage>1265</fpage>–<lpage>1270</lpage>. <pub-id pub-id-type="doi">10.1016/j.pnpbp.2006.03.042</pub-id><pub-id pub-id-type="pmid">16777310</pub-id>
</mixed-citation>
              </ref>
              <ref id="R47">
                <mixed-citation publication-type="journal"><name><surname>Medford</surname><given-names>N</given-names></name>, &amp; <name><surname>Critchley</surname><given-names>HD</given-names></name> (<year>2010</year>). <article-title>Conjoint activity of anterior insular and anterior cingulate cortex: Awareness and response.</article-title>
<source>Brain Structure and Function</source>, <volume>214</volume>(<issue>5</issue>), <fpage>535</fpage>–<lpage>549</lpage>. <pub-id pub-id-type="doi">10.1007/s00429-010-0265-x</pub-id><pub-id pub-id-type="pmid">20512367</pub-id>
</mixed-citation>
              </ref>
              <ref id="R48">
                <mixed-citation publication-type="journal"><name><surname>Mehling</surname><given-names>W</given-names></name> (<year>2016</year>). <article-title>Differentiating attention styles and regulatory aspects of self-reported interoceptive sensibility.</article-title>
<source>Philosophical Transactions of the Royal Society, B: Biological Sciences</source>, <volume>371</volume>(<issue>1708</issue>), <fpage>20160013</fpage>. <pub-id pub-id-type="doi">10.1098/rstb.2016.0013</pub-id></mixed-citation>
              </ref>
              <ref id="R49">
                <mixed-citation publication-type="journal"><name><surname>Mehling</surname><given-names>WE</given-names></name>, <name><surname>Price</surname><given-names>C</given-names></name>, <name><surname>Daubenmier</surname><given-names>JJ</given-names></name>, <name><surname>Acree</surname><given-names>M</given-names></name>, <name><surname>Bartmess</surname><given-names>E</given-names></name>, &amp; <name><surname>Stewart</surname><given-names>A</given-names></name> (<year>2012</year>). <article-title>The Multidimensional Assessment of Interoceptive Awareness (MAIA).</article-title>
<source>PLoS ONE</source>, <volume>7</volume>(<issue>11</issue>), <fpage>e48230</fpage>. <pub-id pub-id-type="doi">10.1371/journal.pone.0048230</pub-id><pub-id pub-id-type="pmid">23133619</pub-id>
</mixed-citation>
              </ref>
              <ref id="R50">
                <mixed-citation publication-type="journal"><name><surname>Mitchell</surname><given-names>TM</given-names></name>, <name><surname>Hutchinson</surname><given-names>R</given-names></name>, <name><surname>Just</surname><given-names>MA</given-names></name>, <name><surname>Niculescu</surname><given-names>RS</given-names></name>, <name><surname>Pereira</surname><given-names>F</given-names></name>, &amp; <name><surname>Wang</surname><given-names>X</given-names></name> (<year>2003</year>). <article-title>Classifying instantaneous cognitive states from fMRI data.</article-title>
<source>AMIA Annual Symposium Proceedings</source>, <volume>2003</volume>, <fpage>465</fpage>–<lpage>469</lpage>.<pub-id pub-id-type="pmid">14728216</pub-id>
</mixed-citation>
              </ref>
              <ref id="R51">
                <mixed-citation publication-type="journal"><name><surname>Naqvi</surname><given-names>NH</given-names></name>, &amp; <name><surname>Bechara</surname><given-names>A</given-names></name> (<year>2010</year>). <article-title>The insula and drug addiction: An interoceptive view of pleasure, urges, and decision-making.</article-title>
<source>Brain Structure &amp; Function</source>, <volume>214</volume>, <fpage>435</fpage>–<lpage>450</lpage>. <pub-id pub-id-type="doi">10.1007/s00429-010-0268-7</pub-id><pub-id pub-id-type="pmid">20512364</pub-id>
</mixed-citation>
              </ref>
              <ref id="R52">
                <mixed-citation publication-type="journal"><name><surname>Norman</surname><given-names>KA</given-names></name>, <name><surname>Polyn</surname><given-names>SM</given-names></name>, <name><surname>Detre</surname><given-names>GJ</given-names></name>, &amp; <name><surname>Haxby</surname><given-names>JV</given-names></name> (<year>2006</year>). <article-title>Beyond mind-reading: Multi-voxel pattern analysis of fMRI data.</article-title>
<source>Trends in Cognitive Sciences</source>, <volume>10</volume>(<issue>9</issue>), <fpage>424</fpage>–<lpage>430</lpage>. <pub-id pub-id-type="doi">10.1016/j.tics.2006.07.005</pub-id><pub-id pub-id-type="pmid">16899397</pub-id>
</mixed-citation>
              </ref>
              <ref id="R53">
                <mixed-citation publication-type="journal"><name><surname>Pedregosa</surname><given-names>F</given-names></name>, <name><surname>Varoquaux</surname><given-names>G</given-names></name>, <name><surname>Gramfort</surname><given-names>A</given-names></name>, <name><surname>Michel</surname><given-names>V</given-names></name>, <name><surname>Thirion</surname><given-names>B</given-names></name>, <name><surname>Grisel</surname><given-names>O</given-names></name>, <name><surname>Blondel</surname><given-names>M</given-names></name>, <name><surname>Prettenhofer</surname><given-names>P</given-names></name>, <name><surname>Weiss</surname><given-names>R</given-names></name>, <name><surname>Dubourg</surname><given-names>V</given-names></name>, <name><surname>Vanderplas</surname><given-names>J</given-names></name>, <name><surname>Passos</surname><given-names>A</given-names></name>, <name><surname>Cournapeau</surname><given-names>D</given-names></name>, <name><surname>Brucher</surname><given-names>M</given-names></name>, <name><surname>Perrot</surname><given-names>M</given-names></name>, &amp; <name><surname>Duchesnay</surname><given-names>E</given-names></name>´. (<year>2011</year>). <article-title>Scikit-learn: Machine learning in Python.</article-title>
<source>Journal of Machine Learning Research</source>, <volume>12</volume>(<issue>85</issue>), <fpage>2825</fpage>–<lpage>2830</lpage>.</mixed-citation>
              </ref>
              <ref id="R54">
                <mixed-citation publication-type="journal"><name><surname>Petitmengin</surname><given-names>C</given-names></name> (<year>2021</year>). <article-title>On the veiling and unveiling of experience: A comparison between the micro-phenomenological method and the practice of meditation.</article-title>
<source>Journal of Phenomenological Psychology</source>, <volume>52</volume>(<issue>1</issue>), <fpage>36</fpage>–<lpage>77</lpage>. <pub-id pub-id-type="doi">10.1163/15691624-12341383</pub-id></mixed-citation>
              </ref>
              <ref id="R55">
                <mixed-citation publication-type="journal"><name><surname>Poldrack</surname><given-names>RA</given-names></name>, <name><surname>Huckins</surname><given-names>G</given-names></name>, &amp; <name><surname>Varoquaux</surname><given-names>G</given-names></name> (<year>2020</year>). <article-title>Establishment of best practices for evidence for prediction: A review.</article-title>
<source>JAMA Psychiatry</source>, <volume>77</volume>(<issue>5</issue>), <fpage>534</fpage>–<lpage>540</lpage>. <pub-id pub-id-type="doi">10.1001/jamapsychiatry.2019.3671</pub-id><pub-id pub-id-type="pmid">31774490</pub-id>
</mixed-citation>
              </ref>
              <ref id="R56">
                <mixed-citation publication-type="journal"><name><surname>Pollatos</surname><given-names>O</given-names></name>, <name><surname>Füstös</surname><given-names>J</given-names></name>, &amp; <name><surname>Critchley</surname><given-names>HD</given-names></name> (<year>2012</year>). <article-title>On the generalised embodiment of pain: How interoceptive sensitivity modulates cutaneous pain perception.</article-title>
<source>Pain</source>, <volume>153</volume>(<issue>8</issue>), <fpage>1680</fpage>–<lpage>1686</lpage>. <pub-id pub-id-type="doi">10.1016/j.pain.2012.04.030</pub-id><pub-id pub-id-type="pmid">22658270</pub-id>
</mixed-citation>
              </ref>
              <ref id="R57">
                <mixed-citation publication-type="journal"><name><surname>Price</surname><given-names>CJ</given-names></name>, &amp; <name><surname>Hooven</surname><given-names>C</given-names></name> (<year>2018</year>). <article-title>Interoceptive awareness skills for emotion regulation: Theory and approach of Mindful Awareness in Body-Oriented Therapy (MABT).</article-title>
<source>Frontiers in Psychology</source>, <volume>9</volume>, <fpage>798</fpage>. <pub-id pub-id-type="doi">10.3389/fpsyg.2018.00798</pub-id><pub-id pub-id-type="pmid">29892247</pub-id>
</mixed-citation>
              </ref>
              <ref id="R58">
                <mixed-citation publication-type="journal"><name><surname>Price</surname><given-names>CJ</given-names></name>, &amp; <name><surname>Smith-DiJulio</surname><given-names>K</given-names></name> (<year>2016</year>). <article-title>Interoceptive awareness is important for relapse prevention: Perceptions of women who received mindful body awareness in substance use disorder treatment.</article-title>
<source>Journal of Addictions Nursing</source>, <volume>27</volume>(<issue>1</issue>), <fpage>32</fpage>–<lpage>38</lpage>. <pub-id pub-id-type="doi">10.1097/JAN.0000000000000109</pub-id><pub-id pub-id-type="pmid">26950840</pub-id>
</mixed-citation>
              </ref>
              <ref id="R59">
                <mixed-citation publication-type="journal"><name><surname>Price</surname><given-names>CJ</given-names></name>, <name><surname>Thompson</surname><given-names>EA</given-names></name>, <name><surname>Crowell</surname><given-names>SE</given-names></name>, <name><surname>Pike</surname><given-names>K</given-names></name>, <name><surname>Cheng</surname><given-names>SC</given-names></name>, <name><surname>Parent</surname><given-names>S</given-names></name>, &amp; <name><surname>Hooven</surname><given-names>C</given-names></name> (<year>2019</year>). <article-title>Immediate effects of interoceptive awareness training through Mindful Awareness in Body-Oriented Therapy (MABT) for women in substance use disorder treatment.</article-title>
<source>Substance Abuse</source>, <volume>40</volume>(<issue>1</issue>), <fpage>102</fpage>–<lpage>115</lpage>. <pub-id pub-id-type="doi">10.1080/08897077.2018.1488335</pub-id><pub-id pub-id-type="pmid">29949455</pub-id>
</mixed-citation>
              </ref>
              <ref id="R60">
                <mixed-citation publication-type="journal"><name><surname>Quadt</surname><given-names>L</given-names></name>, <name><surname>Critchley</surname><given-names>HD</given-names></name>, &amp; <name><surname>Garfinkel</surname><given-names>SN</given-names></name> (<year>2018</year>). <article-title>The neurobiology of interoception in health and disease: Neuroscience of interoception.</article-title>
<source>Annals of the New York Academy of Sciences</source>, <volume>1428</volume>(<issue>1</issue>), <fpage>112</fpage>–<lpage>128</lpage>. <pub-id pub-id-type="doi">10.1111/nyas.13915</pub-id><pub-id pub-id-type="pmid">29974959</pub-id>
</mixed-citation>
              </ref>
              <ref id="R61">
                <mixed-citation publication-type="journal"><name><surname>Quigley</surname><given-names>KS</given-names></name>, <name><surname>Kanoski</surname><given-names>S</given-names></name>, <name><surname>Grill</surname><given-names>WM</given-names></name>, <name><surname>Barrett</surname><given-names>LF</given-names></name>, &amp; <name><surname>Tsakiris</surname><given-names>M</given-names></name> (<year>2021</year>). <article-title>Functions of interoception: From energy regulation to experience of the self.</article-title>
<source>Trends in Neurosciences</source>, <volume>44</volume>(<issue>1</issue>), <fpage>29</fpage>–<lpage>38</lpage>. <pub-id pub-id-type="doi">10.1016/j.tins.2020.09.008</pub-id><pub-id pub-id-type="pmid">33378654</pub-id>
</mixed-citation>
              </ref>
              <ref id="R62">
                <mixed-citation publication-type="book"><collab>R Core Team.</collab> (<year>2020</year>). <source>R: A language and environment for statistical computing</source>
<publisher-name>R Foundation for Statistical Computing</publisher-name>, <publisher-loc>Vienna, Austria</publisher-loc>. <comment><ext-link xlink:href="https://www.R-project.org/" ext-link-type="uri">https://www.R-project.org/</ext-link></comment></mixed-citation>
              </ref>
              <ref id="R63">
                <mixed-citation publication-type="journal"><name><surname>Raichle</surname><given-names>ME</given-names></name>, &amp; <name><surname>Snyder</surname><given-names>AZ</given-names></name> (<year>2007</year>). <article-title>A default mode of brain function: A brief history of an evolving idea.</article-title>
<source>NeuroImage</source>, <volume>37</volume>(<issue>4</issue>), <fpage>1083</fpage>–<lpage>1090</lpage>; <comment>discussion 1097–9.</comment>
<pub-id pub-id-type="doi">10.1016/j.neuroimage.2007.02.041</pub-id><pub-id pub-id-type="pmid">17719799</pub-id>
</mixed-citation>
              </ref>
              <ref id="R64">
                <mixed-citation publication-type="journal"><name><surname>Rao</surname><given-names>SM</given-names></name>, <name><surname>Binder</surname><given-names>JR</given-names></name>, <name><surname>Hammeke</surname><given-names>TA</given-names></name>, <name><surname>Bandettini</surname><given-names>PA</given-names></name>, <name><surname>Bobholz</surname><given-names>JA</given-names></name>, <name><surname>Frost</surname><given-names>JA</given-names></name>, <name><surname>Myklebust</surname><given-names>BM</given-names></name>, <name><surname>Jacobson</surname><given-names>RD</given-names></name>, &amp; <name><surname>Hyde</surname><given-names>JS</given-names></name> (<year>1995</year>). <article-title>Somatotopic mapping of the human primary motor cortex with functional magnetic resonance imaging.</article-title>
<source>Neurology</source>, <volume>45</volume>(<issue>5</issue>), <fpage>919</fpage>–<lpage>924</lpage>. <pub-id pub-id-type="doi">10.1212/WNL.45.5.919</pub-id><pub-id pub-id-type="pmid">7746407</pub-id>
</mixed-citation>
              </ref>
              <ref id="R65">
                <mixed-citation publication-type="journal"><name><surname>Rashid</surname><given-names>M</given-names></name>, <name><surname>Singh</surname><given-names>H</given-names></name>, &amp; <name><surname>Goyal</surname><given-names>V</given-names></name> (<year>2020</year>). <article-title>The use of machine learning and deep learning algorithms in functional magnetic resonance imaging—A systematic review.</article-title>
<source>Expert Systems</source>, <volume>37</volume>(<issue>6</issue>), <fpage>e12644</fpage>. <pub-id pub-id-type="doi">10.1111/exsy.12644</pub-id></mixed-citation>
              </ref>
              <ref id="R66">
                <mixed-citation publication-type="journal"><name><surname>Schuette</surname><given-names>SA</given-names></name>, <name><surname>Zucker</surname><given-names>NL</given-names></name>, &amp; <name><surname>Smoski</surname><given-names>MJ</given-names></name> (<year>2021</year>). <article-title>Do interoceptive accuracy and interoceptive sensibility predict emotion regulation?</article-title>
<source>Psychological Research</source>, <volume>85</volume>(<issue>5</issue>), <fpage>1894</fpage>–<lpage>1908</lpage>. <pub-id pub-id-type="doi">10.1007/s00426-020-01369-2</pub-id><pub-id pub-id-type="pmid">32556535</pub-id>
</mixed-citation>
              </ref>
              <ref id="R67">
                <mixed-citation publication-type="journal"><name><surname>Seeley</surname><given-names>WW</given-names></name> (<year>2019</year>). <article-title>The salience network: A neural system for perceiving and responding to homeostatic demands.</article-title>
<source>The Journal of Neuroscience: the Official Journal of the Society for Neuroscience</source>, <volume>39</volume>(<issue>50</issue>), <fpage>9878</fpage>–<lpage>9882</lpage>. <pub-id pub-id-type="doi">10.1523/JNEUROSCI.1138-17.2019</pub-id><pub-id pub-id-type="pmid">31676604</pub-id>
</mixed-citation>
              </ref>
              <ref id="R68">
                <mixed-citation publication-type="journal"><name><surname>Seeley</surname><given-names>WW</given-names></name>, <name><surname>Menon</surname><given-names>V</given-names></name>, <name><surname>Schatzberg</surname><given-names>AF</given-names></name>, <name><surname>Keller</surname><given-names>J</given-names></name>, <name><surname>Glover</surname><given-names>GH</given-names></name>, <name><surname>Kenna</surname><given-names>H</given-names></name>, <name><surname>Reiss</surname><given-names>AL</given-names></name>, &amp; <name><surname>Greicius</surname><given-names>MD</given-names></name> (<year>2007</year>). <article-title>Dissociable intrinsic connectivity networks for salience processing and executive control.</article-title>
<source>Journal of Neuroscience</source>, <volume>27</volume>(<issue>9</issue>), <fpage>2349</fpage>–<lpage>2356</lpage>. <pub-id pub-id-type="doi">10.1523/JNEUROSCI.5587-06.2007</pub-id><pub-id pub-id-type="pmid">17329432</pub-id>
</mixed-citation>
              </ref>
              <ref id="R69">
                <mixed-citation publication-type="journal"><name><surname>Seth</surname><given-names>AK</given-names></name>, <name><surname>Suzuki</surname><given-names>K</given-names></name>, &amp; <name><surname>Critchley</surname><given-names>HD</given-names></name> (<year>2012</year>). <article-title>An interoceptive predictive coding model of conscious presence.</article-title>
<source>Frontiers in Psychology</source>, <volume>2</volume>, <fpage>395</fpage>. <pub-id pub-id-type="doi">10.3389/fpsyg.2011.00395</pub-id><pub-id pub-id-type="pmid">22291673</pub-id>
</mixed-citation>
              </ref>
              <ref id="R70">
                <mixed-citation publication-type="journal"><name><surname>Strigo</surname><given-names>IA</given-names></name>, &amp; <name><surname>Craig</surname><given-names>ADB</given-names></name> (<year>2016</year>). <article-title>Interoception, homeostatic emotions and sympathovagal balance.</article-title>
<source>Philosophical Transactions of the Royal Society of London. Series B, Biological Sciences</source>, <volume>371</volume>(<issue>1708</issue>), <fpage>20160010</fpage>. <pub-id pub-id-type="doi">10.1098/rstb.2016.0010</pub-id><pub-id pub-id-type="pmid">28080968</pub-id>
</mixed-citation>
              </ref>
              <ref id="R71">
                <mixed-citation publication-type="journal"><name><surname>Szczepanski</surname><given-names>SM</given-names></name>, <name><surname>Pinsk</surname><given-names>MA</given-names></name>, <name><surname>Douglas</surname><given-names>MM</given-names></name>, <name><surname>Kastner</surname><given-names>S</given-names></name>, &amp; <name><surname>Saalmann</surname><given-names>YB</given-names></name> (<year>2013</year>). <article-title>Functional and structural architecture of the human dorsal frontoparietal attention network.</article-title>
<source>Proceedings of the National Academy of Sciences</source>, <volume>110</volume>(<issue>39</issue>), <fpage>15806</fpage>–<lpage>15811</lpage>. <pub-id pub-id-type="doi">10.1073/pnas.1313903110</pub-id></mixed-citation>
              </ref>
              <ref id="R72">
                <mixed-citation publication-type="journal"><name><surname>Terhaar</surname><given-names>J</given-names></name>, <name><surname>Viola</surname><given-names>FC</given-names></name>, <name><surname>Bär</surname><given-names>K-J</given-names></name>, &amp; <name><surname>Debener</surname><given-names>S</given-names></name> (<year>2012</year>). <article-title>Heartbeat evoked potentials mirror altered body perception in depressed patients.</article-title>
<source>Clinical Neurophysiology</source>, <volume>123</volume>(<issue>10</issue>), <fpage>1950</fpage>–<lpage>1957</lpage>. <pub-id pub-id-type="doi">10.1016/j.clinph.2012.02.086</pub-id><pub-id pub-id-type="pmid">22541740</pub-id>
</mixed-citation>
              </ref>
              <ref id="R73">
                <mixed-citation publication-type="journal"><name><surname>Trevisan</surname><given-names>DA</given-names></name>, <name><surname>Altschuler</surname><given-names>MR</given-names></name>, <name><surname>Bagdasarov</surname><given-names>A</given-names></name>, <name><surname>Carlos</surname><given-names>C</given-names></name>, <name><surname>Duan</surname><given-names>S</given-names></name>, <name><surname>Hamo</surname><given-names>E</given-names></name>, <name><surname>Kala</surname><given-names>S</given-names></name>, <name><surname>McNair</surname><given-names>ML</given-names></name>, <name><surname>Parker</surname><given-names>T</given-names></name>, <name><surname>Stahl</surname><given-names>D</given-names></name>, <name><surname>Winkelman</surname><given-names>T</given-names></name>, <name><surname>Zhou</surname><given-names>M</given-names></name>, &amp; <name><surname>McPartland</surname><given-names>JC</given-names></name> (<year>2019</year>). <article-title>A meta-analysis on the relationship between interoceptive awareness and alexithymia: Distinguishing interoceptive accuracy and sensibility.</article-title>
<source>Journal of Abnormal Psychology</source>, <volume>128</volume>(<issue>8</issue>), <fpage>765</fpage>–<lpage>776</lpage>. <pub-id pub-id-type="doi">10.1037/abn0000454</pub-id><pub-id pub-id-type="pmid">31380655</pub-id>
</mixed-citation>
              </ref>
              <ref id="R74">
                <mixed-citation publication-type="journal"><name><surname>Tsakiris</surname><given-names>M</given-names></name>, &amp; <name><surname>Critchley</surname><given-names>HD</given-names></name> (<year>2016</year>). <article-title>Interoception beyond homeostasis: Affect, cognition and mental health.</article-title>
<source>Philosophical Transactions of the Royal Society, B: Biological Sciences</source>, <volume>371</volume>(<issue>1708</issue>), <fpage>20160002</fpage>. <pub-id pub-id-type="doi">10.1098/rstb.2016.0002</pub-id></mixed-citation>
              </ref>
              <ref id="R75">
                <mixed-citation publication-type="journal"><name><surname>Waskom</surname><given-names>ML</given-names></name> (<year>2021</year>). <article-title>seaborn: Statistical data visualization.</article-title>
<source>Journal of Open Source Software</source>, <volume>6</volume>(<issue>60</issue>), <fpage>3021</fpage>. <pub-id pub-id-type="doi">10.21105/joss.03021</pub-id></mixed-citation>
              </ref>
              <ref id="R76">
                <mixed-citation publication-type="journal"><name><surname>Weng</surname><given-names>HY</given-names></name>, <name><surname>Lewis-Peacock</surname><given-names>JA</given-names></name>, <name><surname>Hecht</surname><given-names>FM</given-names></name>, <name><surname>Uncapher</surname><given-names>MR</given-names></name>, <name><surname>Ziegler</surname><given-names>DA</given-names></name>, <name><surname>Farb</surname><given-names>NAS</given-names></name>, <name><surname>Goldman</surname><given-names>V</given-names></name>, <name><surname>Skinner</surname><given-names>S</given-names></name>, <name><surname>Duncan</surname><given-names>LG</given-names></name>, <name><surname>Chao</surname><given-names>MT</given-names></name>, &amp; <name><surname>Gazzaley</surname><given-names>A</given-names></name> (<year>2020</year>). <article-title>Focus on the breath: Brain decoding reveals internal states of attention during meditation.</article-title>
<source>Frontiers in Human Neuroscience</source>, <volume>14</volume>, <fpage>336</fpage>. <pub-id pub-id-type="doi">10.3389/fnhum.2020.00336</pub-id><pub-id pub-id-type="pmid">33005138</pub-id>
</mixed-citation>
              </ref>
              <ref id="R77">
                <mixed-citation publication-type="book"><name><surname>Wickham</surname><given-names>H</given-names></name> (<year>2016</year>). <source>ggplot2: Elegant graphics for data analysis</source> (<edition>2nd</edition> ed.). <publisher-name>Springer International Publishing.</publisher-name>
<pub-id pub-id-type="doi">10.1007/978-3-319-24277-4</pub-id></mixed-citation>
              </ref>
              <ref id="R78">
                <mixed-citation publication-type="journal"><name><surname>Wiens</surname><given-names>S</given-names></name> (<year>2005</year>). <article-title>Interoception in emotional experience.</article-title>
<source>Current Opinion in Neurology</source>, <volume>18</volume>(<issue>4</issue>), <fpage>442</fpage>–<lpage>447</lpage>. <pub-id pub-id-type="doi">10.1097/01.wco.0000168079.92106.99</pub-id><pub-id pub-id-type="pmid">16003122</pub-id>
</mixed-citation>
              </ref>
            </ref-list>
          </back>
          <floats-group>
            <fig position="float" id="F1">
              <label>FIGURE 1</label>
              <caption>
                <p id="P125">Schematics for the Interoceptive/Exteroceptive Attention Task (IEAT). In the exteroceptive conditions, participants watched a circle expand and contract; in the interoceptive conditions, participants paid attention to their inhalation and exhalation. In the passive conditions, participants simply observed the circle or their breath; in the active conditions, participants pressed buttons when the circle expanded or contracted and when they inhaled or exhaled. In the matching condition, participants reported on the circle’s movements while intentionally matching their breathing to the circle’s movements.</p>
              </caption>
              <graphic xlink:href="nihms-1944108-f0001" position="float"/>
            </fig>
            <fig position="float" id="F2">
              <label>FIGURE 2</label>
              <caption>
                <p id="P126">Machine learning classification workflow. First, four-dimensional fMRI BOLD data were reshaped into a two-dimensional voxel × timepoints matrix. Then, part of the data was used to train the machine learning model and the other part to test the model. The model used training data to learn weights associated with each voxel and used these weights to predict labels for the test data. Cross validation as performed by assigning different chunks of the dataset as the test set. An accuracy score was calculated by averaging the prediction accuracy across all test sets.</p>
              </caption>
              <graphic xlink:href="nihms-1944108-f0002" position="float"/>
            </fig>
            <fig position="float" id="F3">
              <label>FIGURE 3</label>
              <caption>
                <p id="P127">Within-session IEAT classification accuracy: interoception versus exteroception. (a) Classification accuracy scores between interoceptive and exteroceptive attention at baseline (73% accuracy) were replicated at post-intervention (72% accuracy). The box represents the quartiles of the dataset; the whiskers extend to show the rest of the distribution, except for data points beyond 1.5 times of the interquartile range, which were considered outliers. The data points represent each participant’s classification accuracy score. The red dotted line represents the threshold for individual participants’ overall classification accuracy to be considered statistically above chance. (b) The dark diagonal of this confusion matrix shows that the machine learning models did not make any systematic errors. Refer to the <xref rid="SD1" ref-type="supplementary-material">supporting information</xref> for individual participants’ confusion matrices.</p>
              </caption>
              <graphic xlink:href="nihms-1944108-f0003" position="float"/>
            </fig>
            <fig position="float" id="F4">
              <label>FIGURE 4</label>
              <caption>
                <p id="P128">Sample IEAT classification output. The red circles designate classifier predicted labels. The blue lines designate true condition labels. Participant A was selected as one of the most accurate predictions, while Participant B was selected as one of the least accurate predictions. This figure serves to illustrate the most and least accurate classifications; overall classification largely fell within this range.</p>
              </caption>
              <graphic xlink:href="nihms-1944108-f0004" position="float"/>
            </fig>
            <fig position="float" id="F5">
              <label>FIGURE 5</label>
              <caption>
                <p id="P129">Important voxels in the classification of interoceptive and exteroceptive attention. Voxels in red contributed to the classification of exteroceptive attention; voxels in blue contributed to the classification of interoceptive attention. These maps were aggregated across all participants; the visualization was thresholded to show voxels that were important to more than one participant. Top panels are left and right lateral views; bottom panels are left and right medial views.</p>
              </caption>
              <graphic xlink:href="nihms-1944108-f0005" position="float"/>
            </fig>
            <fig position="float" id="F6">
              <label>FIGURE 6</label>
              <caption>
                <p id="P130">Within-session IEAT classification accuracy: active Interoception, active exteroception, passive interoception, versus passive exteroception. (a) Classification accuracy scores between the four IEAT conditions at baseline (71% accuracy) were replicated at post-intervention (71% accuracy). (b) The dark diagonal of this confusion matrix shows that the machine learning models did not make any systematic errors. Refer to the <xref rid="SD1" ref-type="supplementary-material">supporting information</xref> for individual participants’ confusion matrices.</p>
              </caption>
              <graphic xlink:href="nihms-1944108-f0006" position="float"/>
            </fig>
            <fig position="float" id="F7">
              <label>FIGURE 7</label>
              <caption>
                <p id="P131">Important voxels in the classification of active interoception, active exteroception, passive interoception, versus passive exteroception. Voxels in red contributed to the classification of a specific task (a, active interoception; b, active exteroception; c, passive Interoception; d, passive exteroception). Voxels in blue contributed to the classification against that task. These maps were aggregated across all participants; the visualization was thresholded to show voxels that were important to more than one participant.</p>
              </caption>
              <graphic xlink:href="nihms-1944108-f0007" position="float"/>
            </fig>
            <fig position="float" id="F8">
              <label>FIGURE 8</label>
              <caption>
                <p id="P132">Within-session IEAT classification accuracy: active interoception versus active exteroception. (a) Classification accuracy scores between the active interoception and active exteroception conditions at baseline (85% accuracy) were replicated at post-intervention (82% accuracy). The dotted red line represents the threshold for the accuracy scores to be significantly different than chance based on the binomial probability distribution. (b) The dark diagonal of this confusion matrix shows that the machine learning models did not make any systematic errors. Refer to the <xref rid="SD1" ref-type="supplementary-material">supporting information</xref> for individual participants’ confusion matrices.</p>
              </caption>
              <graphic xlink:href="nihms-1944108-f0008" position="float"/>
            </fig>
            <fig position="float" id="F9">
              <label>FIGURE 9</label>
              <caption>
                <p id="P133">Important voxels in the classification of active interoception versus active exteroception. Regions in red contributed to the classification of exteroceptive attention; regions in blue contributed to the classification of interoceptive attention. These maps were aggregated across all participants; the visualization was thresholded to show voxels that were important to more than one participant.</p>
              </caption>
              <graphic xlink:href="nihms-1944108-f0009" position="float"/>
            </fig>
            <fig position="float" id="F10">
              <label>FIGURE 10</label>
              <caption>
                <p id="P134">Out-of-sample IEAT classification accuracy. (a) Classification accuracy scores between the four IEAT conditions at baseline (51% accuracy) were replicated at post-intervention (50% accuracy). The box shows the quartiles of the dataset; the whiskers extend to show the rest of the distribution, except for data points beyond 1.5 times of the interquartile range, which were considered outliers. The data points represent each participant’s classification accuracy score. The dotted red line represents the threshold for the accuracy scores to be significantly different than chance based on the binomial probability distribution. (c) The active interoception–active exteroception classification accuracy scores were 71% at baseline and 69% at post-intervention. (b and d) Confusion matrices show that the models did not make systematic prediction errors by consistently mistaking one condition for another. Refer to the <xref rid="SD1" ref-type="supplementary-material">supporting information</xref> for individual participants’ confusion matrices.</p>
              </caption>
              <graphic xlink:href="nihms-1944108-f0010" position="float"/>
            </fig>
            <fig position="float" id="F11">
              <label>FIGURE 11</label>
              <caption>
                <p id="P135">Sustained attention over time. (a) The estimated frequency of passive and active states over time during the SIAT. (b) Time-course of estimated percentage of active interoception during the SIAT. The moment-by-moment proportion of participants having active interoceptive attention (ActInt) in MABT and control groups at baseline and post-intervention. See the <xref rid="SD1" ref-type="supplementary-material">supporting information</xref> for the timecourse of estimated frequency of other attentional states during the SIAT.</p>
              </caption>
              <graphic xlink:href="nihms-1944108-f0011" position="float"/>
            </fig>
          </floats-group>
        </article>
      </metadata>
    </record>
  </GetRecord>
</OAI-PMH>
