<?xml version="1.0" encoding="UTF-8"?>
<OAI-PMH xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/ http://www.openarchives.org/OAI/2.0/OAI-PMH.xsd">
  <responseDate>2024-05-03T05:03:18Z</responseDate>
  <request verb="GetRecord" identifier="oai:pubmedcentral.nih.gov:6161903" metadataPrefix="pmc">https:/www.ncbi.nlm.nih.gov/pmc/oai/oai.cgi</request>
  <GetRecord>
    <record>
      <header>
        <identifier>oai:pubmedcentral.nih.gov:6161903</identifier>
        <datestamp>2018-10-19</datestamp>
        <setSpec>plosone</setSpec>
        <setSpec>pmc-open</setSpec>
      </header>
      <metadata>
        <article xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" xsi:schemaLocation="https://jats.nlm.nih.gov/ns/archiving/1.3/ https://jats.nlm.nih.gov/archiving/1.3/xsd/JATS-archivearticle1-3.xsd" article-type="research-article">
          <front>
            <journal-meta>
              <journal-id journal-id-type="nlm-ta">PLoS One</journal-id>
              <journal-id journal-id-type="iso-abbrev">PLoS ONE</journal-id>
              <journal-id journal-id-type="publisher-id">plos</journal-id>
              <journal-id journal-id-type="pmc">plosone</journal-id>
              <journal-title-group>
                <journal-title>PLoS ONE</journal-title>
              </journal-title-group>
              <issn pub-type="epub">1932-6203</issn>
              <publisher>
                <publisher-name>Public Library of Science</publisher-name>
                <publisher-loc>San Francisco, CA USA</publisher-loc>
              </publisher>
            </journal-meta>
            <article-meta>
              <article-id pub-id-type="accession">PMC6161903</article-id>
              <article-id pub-id-type="pmcid">PMC6161903</article-id>
              <article-id pub-id-type="pmc-uid">6161903</article-id>
              <article-id pub-id-type="pmid">30265719</article-id>
              <article-id pub-id-type="doi">10.1371/journal.pone.0205041</article-id>
              <article-id pub-id-type="publisher-id">PONE-D-18-17892</article-id>
              <article-categories>
                <subj-group subj-group-type="heading">
                  <subject>Research Article</subject>
                </subj-group>
                <subj-group subj-group-type="Discipline-v3">
                  <subject>Biology and Life Sciences</subject>
                  <subj-group>
                    <subject>Neuroscience</subject>
                    <subj-group>
                      <subject>Sensory Perception</subject>
                      <subj-group>
                        <subject>Vision</subject>
                      </subj-group>
                    </subj-group>
                  </subj-group>
                </subj-group>
                <subj-group subj-group-type="Discipline-v3">
                  <subject>Biology and Life Sciences</subject>
                  <subj-group>
                    <subject>Psychology</subject>
                    <subj-group>
                      <subject>Sensory Perception</subject>
                      <subj-group>
                        <subject>Vision</subject>
                      </subj-group>
                    </subj-group>
                  </subj-group>
                </subj-group>
                <subj-group subj-group-type="Discipline-v3">
                  <subject>Social Sciences</subject>
                  <subj-group>
                    <subject>Psychology</subject>
                    <subj-group>
                      <subject>Sensory Perception</subject>
                      <subj-group>
                        <subject>Vision</subject>
                      </subj-group>
                    </subj-group>
                  </subj-group>
                </subj-group>
                <subj-group subj-group-type="Discipline-v3">
                  <subject>Biology and Life Sciences</subject>
                  <subj-group>
                    <subject>Neuroscience</subject>
                    <subj-group>
                      <subject>Cognitive Science</subject>
                      <subj-group>
                        <subject>Cognition</subject>
                        <subj-group>
                          <subject>Memory</subject>
                          <subj-group>
                            <subject>Face Recognition</subject>
                          </subj-group>
                        </subj-group>
                      </subj-group>
                    </subj-group>
                  </subj-group>
                </subj-group>
                <subj-group subj-group-type="Discipline-v3">
                  <subject>Biology and Life Sciences</subject>
                  <subj-group>
                    <subject>Neuroscience</subject>
                    <subj-group>
                      <subject>Learning and Memory</subject>
                      <subj-group>
                        <subject>Memory</subject>
                        <subj-group>
                          <subject>Face Recognition</subject>
                        </subj-group>
                      </subj-group>
                    </subj-group>
                  </subj-group>
                </subj-group>
                <subj-group subj-group-type="Discipline-v3">
                  <subject>Biology and Life Sciences</subject>
                  <subj-group>
                    <subject>Neuroscience</subject>
                    <subj-group>
                      <subject>Cognitive Science</subject>
                      <subj-group>
                        <subject>Cognitive Psychology</subject>
                        <subj-group>
                          <subject>Perception</subject>
                          <subj-group>
                            <subject>Face Recognition</subject>
                          </subj-group>
                        </subj-group>
                      </subj-group>
                    </subj-group>
                  </subj-group>
                </subj-group>
                <subj-group subj-group-type="Discipline-v3">
                  <subject>Biology and Life Sciences</subject>
                  <subj-group>
                    <subject>Psychology</subject>
                    <subj-group>
                      <subject>Cognitive Psychology</subject>
                      <subj-group>
                        <subject>Perception</subject>
                        <subj-group>
                          <subject>Face Recognition</subject>
                        </subj-group>
                      </subj-group>
                    </subj-group>
                  </subj-group>
                </subj-group>
                <subj-group subj-group-type="Discipline-v3">
                  <subject>Social Sciences</subject>
                  <subj-group>
                    <subject>Psychology</subject>
                    <subj-group>
                      <subject>Cognitive Psychology</subject>
                      <subj-group>
                        <subject>Perception</subject>
                        <subj-group>
                          <subject>Face Recognition</subject>
                        </subj-group>
                      </subj-group>
                    </subj-group>
                  </subj-group>
                </subj-group>
                <subj-group subj-group-type="Discipline-v3">
                  <subject>Biology and Life Sciences</subject>
                  <subj-group>
                    <subject>Neuroscience</subject>
                    <subj-group>
                      <subject>Sensory Perception</subject>
                    </subj-group>
                  </subj-group>
                </subj-group>
                <subj-group subj-group-type="Discipline-v3">
                  <subject>Biology and Life Sciences</subject>
                  <subj-group>
                    <subject>Psychology</subject>
                    <subj-group>
                      <subject>Sensory Perception</subject>
                    </subj-group>
                  </subj-group>
                </subj-group>
                <subj-group subj-group-type="Discipline-v3">
                  <subject>Social Sciences</subject>
                  <subj-group>
                    <subject>Psychology</subject>
                    <subj-group>
                      <subject>Sensory Perception</subject>
                    </subj-group>
                  </subj-group>
                </subj-group>
                <subj-group subj-group-type="Discipline-v3">
                  <subject>Biology and Life Sciences</subject>
                  <subj-group>
                    <subject>Neuroscience</subject>
                    <subj-group>
                      <subject>Cognitive Science</subject>
                      <subj-group>
                        <subject>Cognition</subject>
                        <subj-group>
                          <subject>Memory</subject>
                          <subj-group>
                            <subject>Object Recognition</subject>
                          </subj-group>
                        </subj-group>
                      </subj-group>
                    </subj-group>
                  </subj-group>
                </subj-group>
                <subj-group subj-group-type="Discipline-v3">
                  <subject>Biology and Life Sciences</subject>
                  <subj-group>
                    <subject>Neuroscience</subject>
                    <subj-group>
                      <subject>Learning and Memory</subject>
                      <subj-group>
                        <subject>Memory</subject>
                        <subj-group>
                          <subject>Object Recognition</subject>
                        </subj-group>
                      </subj-group>
                    </subj-group>
                  </subj-group>
                </subj-group>
                <subj-group subj-group-type="Discipline-v3">
                  <subject>Biology and Life Sciences</subject>
                  <subj-group>
                    <subject>Neuroscience</subject>
                    <subj-group>
                      <subject>Cognitive Science</subject>
                      <subj-group>
                        <subject>Cognitive Psychology</subject>
                        <subj-group>
                          <subject>Perception</subject>
                          <subj-group>
                            <subject>Object Recognition</subject>
                          </subj-group>
                        </subj-group>
                      </subj-group>
                    </subj-group>
                  </subj-group>
                </subj-group>
                <subj-group subj-group-type="Discipline-v3">
                  <subject>Biology and Life Sciences</subject>
                  <subj-group>
                    <subject>Psychology</subject>
                    <subj-group>
                      <subject>Cognitive Psychology</subject>
                      <subj-group>
                        <subject>Perception</subject>
                        <subj-group>
                          <subject>Object Recognition</subject>
                        </subj-group>
                      </subj-group>
                    </subj-group>
                  </subj-group>
                </subj-group>
                <subj-group subj-group-type="Discipline-v3">
                  <subject>Social Sciences</subject>
                  <subj-group>
                    <subject>Psychology</subject>
                    <subj-group>
                      <subject>Cognitive Psychology</subject>
                      <subj-group>
                        <subject>Perception</subject>
                        <subj-group>
                          <subject>Object Recognition</subject>
                        </subj-group>
                      </subj-group>
                    </subj-group>
                  </subj-group>
                </subj-group>
                <subj-group subj-group-type="Discipline-v3">
                  <subject>Biology and Life Sciences</subject>
                  <subj-group>
                    <subject>Neuroscience</subject>
                    <subj-group>
                      <subject>Brain Mapping</subject>
                      <subj-group>
                        <subject>Functional Magnetic Resonance Imaging</subject>
                      </subj-group>
                    </subj-group>
                  </subj-group>
                </subj-group>
                <subj-group subj-group-type="Discipline-v3">
                  <subject>Medicine and Health Sciences</subject>
                  <subj-group>
                    <subject>Diagnostic Medicine</subject>
                    <subj-group>
                      <subject>Diagnostic Radiology</subject>
                      <subj-group>
                        <subject>Magnetic Resonance Imaging</subject>
                        <subj-group>
                          <subject>Functional Magnetic Resonance Imaging</subject>
                        </subj-group>
                      </subj-group>
                    </subj-group>
                  </subj-group>
                </subj-group>
                <subj-group subj-group-type="Discipline-v3">
                  <subject>Research and Analysis Methods</subject>
                  <subj-group>
                    <subject>Imaging Techniques</subject>
                    <subj-group>
                      <subject>Diagnostic Radiology</subject>
                      <subj-group>
                        <subject>Magnetic Resonance Imaging</subject>
                        <subj-group>
                          <subject>Functional Magnetic Resonance Imaging</subject>
                        </subj-group>
                      </subj-group>
                    </subj-group>
                  </subj-group>
                </subj-group>
                <subj-group subj-group-type="Discipline-v3">
                  <subject>Medicine and Health Sciences</subject>
                  <subj-group>
                    <subject>Radiology and Imaging</subject>
                    <subj-group>
                      <subject>Diagnostic Radiology</subject>
                      <subj-group>
                        <subject>Magnetic Resonance Imaging</subject>
                        <subj-group>
                          <subject>Functional Magnetic Resonance Imaging</subject>
                        </subj-group>
                      </subj-group>
                    </subj-group>
                  </subj-group>
                </subj-group>
                <subj-group subj-group-type="Discipline-v3">
                  <subject>Research and Analysis Methods</subject>
                  <subj-group>
                    <subject>Imaging Techniques</subject>
                    <subj-group>
                      <subject>Neuroimaging</subject>
                      <subj-group>
                        <subject>Functional Magnetic Resonance Imaging</subject>
                      </subj-group>
                    </subj-group>
                  </subj-group>
                </subj-group>
                <subj-group subj-group-type="Discipline-v3">
                  <subject>Biology and Life Sciences</subject>
                  <subj-group>
                    <subject>Neuroscience</subject>
                    <subj-group>
                      <subject>Neuroimaging</subject>
                      <subj-group>
                        <subject>Functional Magnetic Resonance Imaging</subject>
                      </subj-group>
                    </subj-group>
                  </subj-group>
                </subj-group>
                <subj-group subj-group-type="Discipline-v3">
                  <subject>Medicine and Health Sciences</subject>
                  <subj-group>
                    <subject>Diagnostic Medicine</subject>
                    <subj-group>
                      <subject>Diagnostic Radiology</subject>
                      <subj-group>
                        <subject>Magnetic Resonance Imaging</subject>
                      </subj-group>
                    </subj-group>
                  </subj-group>
                </subj-group>
                <subj-group subj-group-type="Discipline-v3">
                  <subject>Research and Analysis Methods</subject>
                  <subj-group>
                    <subject>Imaging Techniques</subject>
                    <subj-group>
                      <subject>Diagnostic Radiology</subject>
                      <subj-group>
                        <subject>Magnetic Resonance Imaging</subject>
                      </subj-group>
                    </subj-group>
                  </subj-group>
                </subj-group>
                <subj-group subj-group-type="Discipline-v3">
                  <subject>Medicine and Health Sciences</subject>
                  <subj-group>
                    <subject>Radiology and Imaging</subject>
                    <subj-group>
                      <subject>Diagnostic Radiology</subject>
                      <subj-group>
                        <subject>Magnetic Resonance Imaging</subject>
                      </subj-group>
                    </subj-group>
                  </subj-group>
                </subj-group>
                <subj-group subj-group-type="Discipline-v3">
                  <subject>Computer and Information Sciences</subject>
                  <subj-group>
                    <subject>Data Acquisition</subject>
                  </subj-group>
                </subj-group>
                <subj-group subj-group-type="Discipline-v3">
                  <subject>Biology and Life Sciences</subject>
                  <subj-group>
                    <subject>Anatomy</subject>
                    <subj-group>
                      <subject>Head</subject>
                      <subj-group>
                        <subject>Eyes</subject>
                      </subj-group>
                    </subj-group>
                  </subj-group>
                </subj-group>
                <subj-group subj-group-type="Discipline-v3">
                  <subject>Medicine and Health Sciences</subject>
                  <subj-group>
                    <subject>Anatomy</subject>
                    <subj-group>
                      <subject>Head</subject>
                      <subj-group>
                        <subject>Eyes</subject>
                      </subj-group>
                    </subj-group>
                  </subj-group>
                </subj-group>
                <subj-group subj-group-type="Discipline-v3">
                  <subject>Biology and Life Sciences</subject>
                  <subj-group>
                    <subject>Anatomy</subject>
                    <subj-group>
                      <subject>Ocular System</subject>
                      <subj-group>
                        <subject>Eyes</subject>
                      </subj-group>
                    </subj-group>
                  </subj-group>
                </subj-group>
                <subj-group subj-group-type="Discipline-v3">
                  <subject>Medicine and Health Sciences</subject>
                  <subj-group>
                    <subject>Anatomy</subject>
                    <subj-group>
                      <subject>Ocular System</subject>
                      <subj-group>
                        <subject>Eyes</subject>
                      </subj-group>
                    </subj-group>
                  </subj-group>
                </subj-group>
              </article-categories>
              <title-group>
                <article-title>Visual imagery of faces and cars in face-selective visual areas</article-title>
                <alt-title alt-title-type="running-head">Mental imagery in face-selective areas</alt-title>
              </title-group>
              <contrib-group>
                <contrib contrib-type="author">
                  <contrib-id authenticated="true" contrib-id-type="orcid">http://orcid.org/0000-0002-6694-0244</contrib-id>
                  <name>
                    <surname>Sunday</surname>
                    <given-names>Mackenzie A.</given-names>
                  </name>
                  <role content-type="http://credit.casrai.org/">Conceptualization</role>
                  <role content-type="http://credit.casrai.org/">Data curation</role>
                  <role content-type="http://credit.casrai.org/">Formal analysis</role>
                  <role content-type="http://credit.casrai.org/">Funding acquisition</role>
                  <role content-type="http://credit.casrai.org/">Investigation</role>
                  <role content-type="http://credit.casrai.org/">Methodology</role>
                  <role content-type="http://credit.casrai.org/">Project administration</role>
                  <role content-type="http://credit.casrai.org/">Resources</role>
                  <role content-type="http://credit.casrai.org/">Software</role>
                  <role content-type="http://credit.casrai.org/">Writing – original draft</role>
                  <role content-type="http://credit.casrai.org/">Writing – review &amp; editing</role>
                  <xref ref-type="aff" rid="aff001">
                    <sup>1</sup>
                  </xref>
                  <xref ref-type="corresp" rid="cor001">*</xref>
                </contrib>
                <contrib contrib-type="author">
                  <name>
                    <surname>McGugin</surname>
                    <given-names>Rankin W.</given-names>
                  </name>
                  <role content-type="http://credit.casrai.org/">Conceptualization</role>
                  <role content-type="http://credit.casrai.org/">Data curation</role>
                  <role content-type="http://credit.casrai.org/">Formal analysis</role>
                  <role content-type="http://credit.casrai.org/">Investigation</role>
                  <role content-type="http://credit.casrai.org/">Methodology</role>
                  <role content-type="http://credit.casrai.org/">Project administration</role>
                  <role content-type="http://credit.casrai.org/">Resources</role>
                  <role content-type="http://credit.casrai.org/">Software</role>
                  <role content-type="http://credit.casrai.org/">Supervision</role>
                  <role content-type="http://credit.casrai.org/">Validation</role>
                  <role content-type="http://credit.casrai.org/">Writing – review &amp; editing</role>
                  <xref ref-type="aff" rid="aff001">
                    <sup>1</sup>
                  </xref>
                </contrib>
                <contrib contrib-type="author">
                  <name>
                    <surname>Tamber-Rosenau</surname>
                    <given-names>Benjamin J.</given-names>
                  </name>
                  <role content-type="http://credit.casrai.org/">Conceptualization</role>
                  <role content-type="http://credit.casrai.org/">Data curation</role>
                  <role content-type="http://credit.casrai.org/">Formal analysis</role>
                  <role content-type="http://credit.casrai.org/">Methodology</role>
                  <role content-type="http://credit.casrai.org/">Writing – review &amp; editing</role>
                  <xref ref-type="aff" rid="aff001">
                    <sup>1</sup>
                  </xref>
                  <xref ref-type="aff" rid="aff002">
                    <sup>2</sup>
                  </xref>
                </contrib>
                <contrib contrib-type="author">
                  <name>
                    <surname>Gauthier</surname>
                    <given-names>Isabel</given-names>
                  </name>
                  <role content-type="http://credit.casrai.org/">Conceptualization</role>
                  <role content-type="http://credit.casrai.org/">Data curation</role>
                  <role content-type="http://credit.casrai.org/">Formal analysis</role>
                  <role content-type="http://credit.casrai.org/">Funding acquisition</role>
                  <role content-type="http://credit.casrai.org/">Investigation</role>
                  <role content-type="http://credit.casrai.org/">Methodology</role>
                  <role content-type="http://credit.casrai.org/">Project administration</role>
                  <role content-type="http://credit.casrai.org/">Resources</role>
                  <role content-type="http://credit.casrai.org/">Software</role>
                  <role content-type="http://credit.casrai.org/">Supervision</role>
                  <role content-type="http://credit.casrai.org/">Writing – original draft</role>
                  <role content-type="http://credit.casrai.org/">Writing – review &amp; editing</role>
                  <xref ref-type="aff" rid="aff001">
                    <sup>1</sup>
                  </xref>
                </contrib>
              </contrib-group>
              <aff id="aff001">
                <label>1</label>
                <addr-line>Vanderbilt University, Nashville, TN, United States of America</addr-line>
              </aff>
              <aff id="aff002">
                <label>2</label>
                <addr-line>University of Houston, Houston, TX, United States of America</addr-line>
              </aff>
              <contrib-group>
                <contrib contrib-type="editor">
                  <name>
                    <surname>Keil</surname>
                    <given-names>Andreas</given-names>
                  </name>
                  <role>Editor</role>
                  <xref ref-type="aff" rid="edit1"/>
                </contrib>
              </contrib-group>
              <aff id="edit1">
                <addr-line>University of Florida, UNITED STATES</addr-line>
              </aff>
              <author-notes>
                <fn fn-type="COI-statement" id="coi001">
                  <p><bold>Competing Interests: </bold>The authors have declared that no competing interests exist.</p>
                </fn>
                <corresp id="cor001">* E-mail: <email>mackenzie.a.sunday@vanderbilt.edu</email></corresp>
              </author-notes>
              <pub-date pub-type="epub">
                <day>28</day>
                <month>9</month>
                <year>2018</year>
              </pub-date>
              <pub-date pub-type="collection">
                <year>2018</year>
              </pub-date>
              <volume>13</volume>
              <issue>9</issue>
              <elocation-id>e0205041</elocation-id>
              <history>
                <date date-type="received">
                  <day>15</day>
                  <month>6</month>
                  <year>2018</year>
                </date>
                <date date-type="accepted">
                  <day>18</day>
                  <month>9</month>
                  <year>2018</year>
                </date>
              </history>
              <permissions>
                <copyright-statement>© 2018 Sunday et al</copyright-statement>
                <copyright-year>2018</copyright-year>
                <copyright-holder>Sunday et al</copyright-holder>
                <license xlink:href="http://creativecommons.org/licenses/by/4.0/">
                  <license-p>This is an open access article distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution License</ext-link>, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited.</license-p>
                </license>
              </permissions>
              <self-uri content-type="pdf" xlink:href="pone.0205041.pdf"/>
              <abstract>
                <p>Neuroimaging provides a unique tool to investigate otherwise difficult-to-access mental processes like visual imagery. Prior studies support the idea that visual imagery is a top-down reinstatement of visual perception, and it is likely that this extends to object processing. Here we use functional MRI and multi-voxel pattern analysis to ask if mental imagery of cars engages the fusiform face area, similar to what is found during perception. We test only individuals who we assumed could imagine individual car models based on their above-average perceptual abilities with cars. Our results provide evidence that cars are represented differently from common objects in face-selective visual areas, at least in those with above-average car recognition ability. Moreover, pattern classifiers trained on data acquired during imagery can decode the neural response pattern acquired during perception, suggesting that the tested object categories are represented similarly during perception and visual imagery. The results suggest that, even at high-levels of visual processing, visual imagery mirrors perception to some extent, and that face-selective areas may in part support non-face object imagery.</p>
              </abstract>
              <funding-group>
                <award-group id="award001">
                  <funding-source>
                    <institution-wrap>
                      <institution-id institution-id-type="funder-id">http://dx.doi.org/10.13039/100000001</institution-id>
                      <institution>National Science Foundation</institution>
                    </institution-wrap>
                  </funding-source>
                  <award-id>SBE-0542013</award-id>
                  <principal-award-recipient>
                    <name>
                      <surname>Gauthier</surname>
                      <given-names>Isabel</given-names>
                    </name>
                  </principal-award-recipient>
                </award-group>
                <award-group id="award002">
                  <funding-source>
                    <institution-wrap>
                      <institution-id institution-id-type="funder-id">http://dx.doi.org/10.13039/100000001</institution-id>
                      <institution>National Science Foundation</institution>
                    </institution-wrap>
                  </funding-source>
                  <award-id>Graduate Research Fellowship Grant No. 1445197</award-id>
                  <principal-award-recipient>
                    <contrib-id authenticated="true" contrib-id-type="orcid">http://orcid.org/0000-0002-6694-0244</contrib-id>
                    <name>
                      <surname>Sunday</surname>
                      <given-names>Mackenzie A.</given-names>
                    </name>
                  </principal-award-recipient>
                </award-group>
                <award-group id="award003">
                  <funding-source>
                    <institution-wrap>
                      <institution-id institution-id-type="funder-id">http://dx.doi.org/10.13039/100006108</institution-id>
                      <institution>National Center for Advancing Translational Sciences</institution>
                    </institution-wrap>
                  </funding-source>
                  <award-id>CTSA award No. UL1TR000445</award-id>
                  <principal-award-recipient>
                    <contrib-id authenticated="true" contrib-id-type="orcid">http://orcid.org/0000-0002-6694-0244</contrib-id>
                    <name>
                      <surname>Sunday</surname>
                      <given-names>Mackenzie A.</given-names>
                    </name>
                  </principal-award-recipient>
                </award-group>
                <funding-statement>This work was supported by the Temporal Dynamics of Learning Center (National Science Foundation Grant SBE-0542013) and was supported by CTSA award No. UL1TR000445 from the National Center for Advancing Translational Sciences. Its contents are solely the responsibility of the authors and do not necessarily represent official views of the National Center for Advancing Translational Sciences or the National Institutes of Health. This material is based upon work supported by the National Science Foundation Graduate Research Fellowship under Grant No. (1445197). The funders had no role in study design, data collection and analysis, decision to publish, or preparation of the manuscript.</funding-statement>
              </funding-group>
              <counts>
                <fig-count count="5"/>
                <table-count count="4"/>
                <page-count count="21"/>
              </counts>
              <custom-meta-group>
                <custom-meta id="data-availability">
                  <meta-name>Data Availability</meta-name>
                  <meta-value>Data are available at <ext-link ext-link-type="uri" xlink:href="https://figshare.com/s/24fc1314649a6e2b2899">https://figshare.com/s/24fc1314649a6e2b2899</ext-link>.</meta-value>
                </custom-meta>
              </custom-meta-group>
            </article-meta>
            <notes>
              <title>Data Availability</title>
              <p>Data are available at <ext-link ext-link-type="uri" xlink:href="https://figshare.com/s/24fc1314649a6e2b2899">https://figshare.com/s/24fc1314649a6e2b2899</ext-link>.</p>
            </notes>
          </front>
          <body>
            <sec sec-type="intro" id="sec001">
              <title>Introduction</title>
              <p>Visual imagery is usually defined as visual perception in the absence of external stimuli. Colloquially, people refer to imagery as “seeing with the mind’s eye.” Visual imagery mirrors perception both in its neural correlates and its behavioral effects [<xref rid="pone.0205041.ref001" ref-type="bibr">1</xref>–<xref rid="pone.0205041.ref003" ref-type="bibr">3</xref>]. Many studies supporting this conclusion use neuroimaging methods, which are uniquely suited to investigate an intrinsic process like visual imagery. Cortical regions associated with particular perceptual acts are also engaged when we imagine the same content. For example, the fusiform face area (FFA) is engaged by both perceived and imagined faces [<xref rid="pone.0205041.ref004" ref-type="bibr">4</xref>–<xref rid="pone.0205041.ref006" ref-type="bibr">6</xref>], the parahippocampal place area by both perceived and imagined places [<xref rid="pone.0205041.ref004" ref-type="bibr">4</xref>,<xref rid="pone.0205041.ref005" ref-type="bibr">5</xref>,<xref rid="pone.0205041.ref007" ref-type="bibr">7</xref>], and motion-sensitive MT/V5 by both perceived and imagined motion [<xref rid="pone.0205041.ref008" ref-type="bibr">8</xref>–<xref rid="pone.0205041.ref010" ref-type="bibr">10</xref>]. Moreover, multi-voxel pattern analysis (MVPA) demonstrated similar representations of perceptual stimuli and their imagined counterparts [<xref rid="pone.0205041.ref007" ref-type="bibr">7</xref>,<xref rid="pone.0205041.ref010" ref-type="bibr">10</xref>–<xref rid="pone.0205041.ref016" ref-type="bibr">16</xref>].</p>
              <p>Though most of the functional MRI (fMRI) work on visual imagery has focused on regions in early visual cortex ([<xref rid="pone.0205041.ref014" ref-type="bibr">14</xref>,<xref rid="pone.0205041.ref017" ref-type="bibr">17</xref>–<xref rid="pone.0205041.ref022" ref-type="bibr">22</xref>]; for a review, see [<xref rid="pone.0205041.ref022" ref-type="bibr">22</xref>]), a few studies implicate regions further down the visual processing stream [<xref rid="pone.0205041.ref011" ref-type="bibr">11</xref>,<xref rid="pone.0205041.ref012" ref-type="bibr">12</xref>,<xref rid="pone.0205041.ref023" ref-type="bibr">23</xref>]. In particular, several of these studies have examined regions in and around the fusiform gyrus [<xref rid="pone.0205041.ref004" ref-type="bibr">4</xref>–<xref rid="pone.0205041.ref007" ref-type="bibr">7</xref>,<xref rid="pone.0205041.ref024" ref-type="bibr">24</xref>]. The results support the idea that top-down mechanisms responsible for imagery can engage visual areas at any level of the processing hierarchy depending on the task.</p>
              <p>The first study comparing perception and imagery of faces and objects reported considerable overlap between voxels activated during perception and imagery of faces, as well as for perception and imagery of places [<xref rid="pone.0205041.ref004" ref-type="bibr">4</xref>]. A subsequent study found similar overlap between fusiform regions activated during the perception and imagery of faces [<xref rid="pone.0205041.ref006" ref-type="bibr">6</xref>]. However, both of these studies used small samples (eight participants in [<xref rid="pone.0205041.ref004" ref-type="bibr">4</xref>]; nine participants in [<xref rid="pone.0205041.ref006" ref-type="bibr">6</xref>]) and compared face activation with activation for domains specifically chosen because their perception led to different areas being engaged relative to faces (places in [<xref rid="pone.0205041.ref004" ref-type="bibr">4</xref>]; houses and chairs in [<xref rid="pone.0205041.ref006" ref-type="bibr">6</xref>]). Moreover, in both studies, the face-selective regions were not functionally localized which makes it difficult to know if the effects can be assigned to FFA proper. Perhaps the strongest replication of the original study on imagery of faces and scenes by O’Craven and Kanwisher was done by Cichy and coauthors, who compared imagery and perception of bodies, faces, scenes, houses and everyday objects in sixteen participants. They found that extrastriate regions (including the FFA) showed greater activation by their preferred domain during both perception and imagery [<xref rid="pone.0205041.ref005" ref-type="bibr">5</xref>].</p>
              <p>Here we aimed to further this line of work by determining whether visual imagery mirrors perception with regard to both face and object representations within face-selective regions. As was done in prior work, we included domains for which we expected distinct activity patterns during perception (faces and everyday objects). Critically, to address our main question, whether imagery of non-face objects at the subordinate-level recruits face-selective areas, we included cars and recruited a sample of participants with above-average car recognition. Based on prior work, we expected cars to be represented in FFA in these participants, since this area is the most robust predictor of behavioral performance in individuation judgments for cars [<xref rid="pone.0205041.ref025" ref-type="bibr">25</xref>–<xref rid="pone.0205041.ref028" ref-type="bibr">28</xref>]. Another improvement we made upon previous neuroimaging studies of face and object imagery is in our definition of face-selective regions. There are two functionally and anatomically distinct face-selective fusiform sub-regions [<xref rid="pone.0205041.ref029" ref-type="bibr">29</xref>–<xref rid="pone.0205041.ref031" ref-type="bibr">31</xref>] that have in the past often been lumped together, although they do not always respond in the same way. In particular, the anterior FFA has been found to show more robust effects of experience for both faces and cars [<xref rid="pone.0205041.ref031" ref-type="bibr">31</xref>–<xref rid="pone.0205041.ref034" ref-type="bibr">34</xref>]. Here, we defined and analyzed these separate regions of interest, to explore how they engage during visual imagery.</p>
              <p>We asked if imagery of faces and cars engages face-selective fusiform regions in above-average car recognizers, basing this prediction on evidence that perception of faces and cars engage these regions in comparable populations. In recent work measuring individual differences in the vividness of mental imagery, we found that the vividness of domain-specific imagery (for cars) relates to that of domain-general imagery, but not to perceptual or semantic knowledge ability levels with cars [<xref rid="pone.0205041.ref035" ref-type="bibr">35</xref>]. Therefore, it is unclear whether during car imagery, those with above-average car recognition ability would recruit face-selective areas that are engaged during car perception (where the selectivity has been found to predict car recognition ability), or would recruit only object-selective regions. Note that while studies of perceptual expertise reveal that good car recognizers represent cars in the FFA [<xref rid="pone.0205041.ref025" ref-type="bibr">25</xref>–<xref rid="pone.0205041.ref028" ref-type="bibr">28</xref>], the present work did not include variability in car recognition ability (because car novices are unlikely to be able to imagine different car models with any precision) and as such cannot address this variability. In other words, our use of individuals with above-average car recognition is similar in logic to the use of face experts in most face recognition studies, especially those that rely on famous faces and verify that participants can actually recognize the specific faces used, or to the logic of psychophysical studies that necessitate trained observers. Because our goals differ from studies that characterize expertise effects or study the learning process itself, we intentionally recruited individuals who have good subordinate-level skills for both faces and cars to evaluate the similarity of neural representations during imagery and perception (as opposed to seeking a range of expertise levels).</p>
              <p>To address this question, we used MVPA to investigate the information that spatial patterns of neural responses in face-selective regions contain about object categories, when above-average car recognizers imagined faces, cars and a selection of familiar objects. Essentially, MVPA quantifies reproducible spatial patterns of activity that can discriminate between different conditions (see [<xref rid="pone.0205041.ref036" ref-type="bibr">36</xref>] for review). We first used MVPA to determine if representations from these different categories could be distinguished in face-selective areas during perception and during imagery. Critically, MVPA may also be used to demonstrate similarity of representations across conditions by training the MVPA classifier on one discrimination, and testing it on another. We used such an approach here to additionally assess the similarity of imagery and perceptual representations. Specifically, we cross-trained the MVPA classifiers to test if the same patterns of activity that distinguished between categories in imagery also distinguished between categories in perception. Because we expected less overall BOLD activation to be evoked by visual imagery relative to visual perception [<xref rid="pone.0205041.ref004" ref-type="bibr">4</xref>,<xref rid="pone.0205041.ref011" ref-type="bibr">11</xref>,<xref rid="pone.0205041.ref012" ref-type="bibr">12</xref>], analyzing patterns rather than univariate responses provided a more sensitive approach for both of these questions. In addition, MVPA allowed measurement of categorical representational content instead of relative activation amplitude between categories.</p>
              <p>MVPA was also useful in overcoming an inherent limitation of our design. Most fMRI work of recognition expertise has been correlational, relating behavioral indices of recognition with the magnitude of region-specific neural activation. However, though individuals with any level of car recognition ability can perceive cars, it is unlikely that individuals with poor car recognition ability can robustly imagine <italic>specific</italic> cars. With MVPA we could investigate the correspondence between perception and imagery in a sample of people capable of imagining cars at a subordinate level. Because we did not compare across levels of car recognition ability, we could not address whether our results would generalize to any individual–indeed, this question may be moot because those with poor car recognition ability could not plausibly perform this task. We could, however, ask whether it is possible to distinguish cars from objects in face-selective areas. To foreshadow our results, we found evidence that imagined cars and objects are represented differently within the face-selective regions in our sample. Moreover, we found evidence that faces, cars and objects are each represented similarly during imagery and perception in these regions, supporting the idea that visual imagery mirrors perception at subordinate levels of object processing.</p>
            </sec>
            <sec sec-type="materials|methods" id="sec002">
              <title>Methods</title>
              <sec id="sec003">
                <title>Participants</title>
                <p>A power analysis (using G*power software; [<xref rid="pone.0205041.ref037" ref-type="bibr">37</xref>]) indicated that a sample size of 16 was needed to detect previously reported effect sizes for decoding imagined faces in the FFA with more than 80% power at the .05 alpha level (one sample t-test, Cohen’s d ≈ .78; [<xref rid="pone.0205041.ref005" ref-type="bibr">5</xref>]). We aimed for 20 participants because we do not always find every functional area in all participants. Participants were recruited using flyers posted throughout the Vanderbilt University campus and using <ext-link ext-link-type="uri" xlink:href="http://ResearchMatch.com">ResearchMatch.com</ext-link>. We only recruited males since car interest is predominantly reported by males [<xref rid="pone.0205041.ref038" ref-type="bibr">38</xref>]. Thirty-two men were behaviorally screened and those who showed above-average car recognition ability and visual imagery vividness were invited to participate in the fMRI portion of the study. Of those participants, twenty-one qualified for the fMRI portion and were scanned. One participant did not complete the fMRI portion because of previously undisclosed hearing problems that made it impossible to perceive the auditory stimuli. The remaining 20 participants were all healthy males (mean age = 24.3 years, SD = 6.7, 18 right-handed) who reported normal or corrected-to-normal vision and no hearing loss. Informed written consent was obtained at the beginning of both sessions in accordance with guidelines of the Vanderbilt University Institutional Review Board and Vanderbilt University Medical Center. This study was approved by the Vanderbilt University Institutional Review Board under IRB 050082. All participants received either monetary compensation ($15 for the behavioral screening, $15 for the online tasks, and $45 for the fMRI scan) or course credit.</p>
              </sec>
              <sec id="sec004">
                <title>MRI Data acquisition</title>
                <p>All participants were scanned on a Philips 7-Tesla (7T) Achieva human magnetic resonance scanner with a 32 channel parallel receive array coil (Nova). High-resolution (HR) T1-weighted anatomical volumes were acquired with a 3D TFE (Turbo Field Echo) acquisition sequence with sensitivity encoding (SENSE) (TR = 4.3 ms, TE = 1.90 ms (minimum), flip angle = 7°, sagittal plane acquisition, FOV = 224 mm x 224 mm, matrix size = 224 x 224, slice gap = 0 mm, for an isometric voxel size of 1 mm<sup>3</sup>). During the first participant’s scan,174 slices were acquired, and for all following participants’ scans, 190 slices were acquired. All functional scans were acquired using standard gradient-echo echoplanar T2*-weighted imaging (TR = 2000 ms, TE = 25 ms, flip angle = 65°, axial plane acquisition, FOV = 240 mm x 240 mm, matrix size = 80 x 80, slice gap = 0 mm, for an isometric voxel size of 3 x 3 x 3 mm). By using a comparable functional voxel size (3 mm on a side) to that typical at lower field strengths, we were able to capitalize on the higher field strength (7T) to achieve better signal-to-ratio than would occur at lower field strengths [<xref rid="pone.0205041.ref010" ref-type="bibr">10</xref>]. Following 10 dummy scans, 35 ascending interleaved slices were acquired.</p>
              </sec>
              <sec id="sec005">
                <title>Stimuli</title>
                <sec id="sec006">
                  <title>Localizer stimuli</title>
                  <p>Stimuli used in the localizer runs were greyscale images of 36 unfamiliar faces, 36 common objects, and 83 scrambled images presented centrally against a white background. These stimuli have been used in several previous studies for localizer runs [<xref rid="pone.0205041.ref028" ref-type="bibr">28</xref>,<xref rid="pone.0205041.ref031" ref-type="bibr">31</xref>,<xref rid="pone.0205041.ref039" ref-type="bibr">39</xref>]. None of the common objects or faces used in the localizer runs were also used in the experimental runs.</p>
                </sec>
                <sec id="sec007">
                  <title>Experimental stimuli</title>
                  <p>During the experimental runs, participants either perceived or imagined cars, faces or objects. We selected images of 20 faces and 20 objects (none handheld), all relatively easy to imagine based on an online pilot study, and 20 popular car models (all sedans) for the study. We used common objects, similar to previous work [<xref rid="pone.0205041.ref005" ref-type="bibr">5</xref>,<xref rid="pone.0205041.ref011" ref-type="bibr">11</xref>]. For each category, 4 unique stimuli were randomly chosen without replacement for each of the 5 runs, totaling 20 exemplars per category (see <xref rid="pone.0205041.t001" ref-type="table">Table 1</xref>).</p>
                  <table-wrap id="pone.0205041.t001" orientation="portrait" position="float">
                    <object-id pub-id-type="doi">10.1371/journal.pone.0205041.t001</object-id>
                    <label>Table 1</label>
                    <caption>
                      <title>Labels for stimuli from each of the three categories, grouped by which run the stimuli were presented in.</title>
                    </caption>
                    <alternatives>
                      <graphic id="pone.0205041.t001g" xlink:href="pone.0205041.t001"/>
                      <table frame="hsides" rules="groups">
                        <colgroup span="1">
                          <col align="left" valign="middle" span="1"/>
                          <col align="left" valign="middle" span="1"/>
                          <col align="left" valign="middle" span="1"/>
                          <col align="left" valign="middle" span="1"/>
                        </colgroup>
                        <thead>
                          <tr>
                            <th align="center" rowspan="1" colspan="1">Run</th>
                            <th align="left" rowspan="1" colspan="1">Faces</th>
                            <th align="left" rowspan="1" colspan="1">Objects</th>
                            <th align="left" rowspan="1" colspan="1">Cars</th>
                          </tr>
                        </thead>
                        <tbody>
                          <tr>
                            <td align="center" rowspan="4" style="border-bottom:dashed" colspan="1">1<break/></td>
                            <td align="left" rowspan="1" colspan="1">Halle Berry</td>
                            <td align="left" rowspan="1" colspan="1">TV</td>
                            <td align="left" rowspan="1" colspan="1">Aston Martin Vanquis</td>
                          </tr>
                          <tr>
                            <td align="left" rowspan="1" colspan="1">Bill Clinton</td>
                            <td align="left" rowspan="1" colspan="1">Tent</td>
                            <td align="left" rowspan="1" colspan="1">Volvo V40</td>
                          </tr>
                          <tr>
                            <td align="left" rowspan="1" colspan="1">Justin Bieber</td>
                            <td align="left" rowspan="1" colspan="1">Bookshelf</td>
                            <td align="left" rowspan="1" colspan="1">Chevy Impala</td>
                          </tr>
                          <tr>
                            <td align="left" style="border-bottom:dashed" rowspan="1" colspan="1">Jennifer Lopez</td>
                            <td align="left" style="border-bottom:dashed" rowspan="1" colspan="1">Lawn Mower</td>
                            <td align="left" style="border-bottom:dashed" rowspan="1" colspan="1">Rolls Royce Ghost</td>
                          </tr>
                          <tr>
                            <td align="center" rowspan="4" style="border-top:dashed;border-bottom:dashed" colspan="1">2<break/></td>
                            <td align="left" style="border-top:dashed" rowspan="1" colspan="1">Michelle Obama</td>
                            <td align="left" style="border-top:dashed" rowspan="1" colspan="1">Park Bench</td>
                            <td align="left" style="border-top:dashed" rowspan="1" colspan="1">Honda Accord</td>
                          </tr>
                          <tr>
                            <td align="left" rowspan="1" colspan="1">Tom Cruise</td>
                            <td align="left" rowspan="1" colspan="1">Desk</td>
                            <td align="left" rowspan="1" colspan="1">Tesla Model S</td>
                          </tr>
                          <tr>
                            <td align="left" rowspan="1" colspan="1">Robin Williams</td>
                            <td align="left" rowspan="1" colspan="1">Chair</td>
                            <td align="left" rowspan="1" colspan="1">Toyota Prius</td>
                          </tr>
                          <tr>
                            <td align="left" style="border-bottom:dashed" rowspan="1" colspan="1">Madonna</td>
                            <td align="left" style="border-bottom:dashed" rowspan="1" colspan="1">Water Fountain</td>
                            <td align="left" style="border-bottom:dashed" rowspan="1" colspan="1">Volkswagen Passat</td>
                          </tr>
                          <tr>
                            <td align="center" rowspan="4" style="border-top:dashed;border-bottom:dashed" colspan="1">3<break/></td>
                            <td align="left" style="border-top:dashed" rowspan="1" colspan="1">George W Bush</td>
                            <td align="left" style="border-top:dashed" rowspan="1" colspan="1">Slide</td>
                            <td align="left" style="border-top:dashed" rowspan="1" colspan="1">Subaru Legacy</td>
                          </tr>
                          <tr>
                            <td align="left" rowspan="1" colspan="1">Taylor Swift</td>
                            <td align="left" rowspan="1" colspan="1">Swing Set</td>
                            <td align="left" rowspan="1" colspan="1">Ford Mustang</td>
                          </tr>
                          <tr>
                            <td align="left" rowspan="1" colspan="1">Brad Pitt</td>
                            <td align="left" rowspan="1" colspan="1">Picnic Table</td>
                            <td align="left" rowspan="1" colspan="1">Nissan Altima</td>
                          </tr>
                          <tr>
                            <td align="left" style="border-bottom:dashed" rowspan="1" colspan="1">Oprah</td>
                            <td align="left" style="border-bottom:dashed" rowspan="1" colspan="1">Dresser</td>
                            <td align="left" style="border-bottom:dashed" rowspan="1" colspan="1">Pontiac Grand Prix</td>
                          </tr>
                          <tr>
                            <td align="center" rowspan="4" style="border-top:dashed;border-bottom:dashed" colspan="1">4<break/></td>
                            <td align="left" style="border-top:dashed" rowspan="1" colspan="1">Angelina Jolie</td>
                            <td align="left" style="border-top:dashed" rowspan="1" colspan="1">Microwave</td>
                            <td align="left" style="border-top:dashed" rowspan="1" colspan="1">Maserati Quattroporte VI</td>
                          </tr>
                          <tr>
                            <td align="left" rowspan="1" colspan="1">Leonardo DiCaprio</td>
                            <td align="left" rowspan="1" colspan="1">Pillow</td>
                            <td align="left" rowspan="1" colspan="1">Ford Fusion</td>
                          </tr>
                          <tr>
                            <td align="left" rowspan="1" colspan="1">Sandra Bullock</td>
                            <td align="left" rowspan="1" colspan="1">Traffic Light</td>
                            <td align="left" rowspan="1" colspan="1">Volkswagen Beetle</td>
                          </tr>
                          <tr>
                            <td align="left" style="border-bottom:dashed" rowspan="1" colspan="1">Jennifer Aniston</td>
                            <td align="left" style="border-bottom:dashed" rowspan="1" colspan="1">Bed</td>
                            <td align="left" style="border-bottom:dashed" rowspan="1" colspan="1">Kia Forte</td>
                          </tr>
                          <tr>
                            <td align="center" rowspan="4" style="border-top:dashed" colspan="1">5<break/></td>
                            <td align="left" style="border-top:dashed" rowspan="1" colspan="1">Michael Jackson</td>
                            <td align="left" style="border-top:dashed" rowspan="1" colspan="1">Broom</td>
                            <td align="left" style="border-top:dashed" rowspan="1" colspan="1">Hyundai Sonata</td>
                          </tr>
                          <tr>
                            <td align="left" rowspan="1" colspan="1">Johnny Depp</td>
                            <td align="left" rowspan="1" colspan="1">Boat</td>
                            <td align="left" rowspan="1" colspan="1">Porsche Panamera</td>
                          </tr>
                          <tr>
                            <td align="left" rowspan="1" colspan="1">George Clooney</td>
                            <td align="left" rowspan="1" colspan="1">Plane</td>
                            <td align="left" rowspan="1" colspan="1">Toyota Corolla</td>
                          </tr>
                          <tr>
                            <td align="left" rowspan="1" colspan="1">Ellen Degeneres</td>
                            <td align="left" rowspan="1" colspan="1">Laptop</td>
                            <td align="left" rowspan="1" colspan="1">Chevrolet Camaro</td>
                          </tr>
                        </tbody>
                      </table>
                    </alternatives>
                  </table-wrap>
                  <p>For the perceptual runs, stimuli were color images of cars, objects, or faces (10 male and 10 female) with the entire background removed using Adobe Photoshop. During the fMRI portion, these stimuli were presented on a black background. For the imagery runs, participants heard recorded auditory labels. For each trial, a .wav file of a male voice saying the label once was played, lasting less than 4 seconds.</p>
                </sec>
              </sec>
              <sec id="sec008">
                <title>Procedure</title>
                <sec id="sec009">
                  <title>Behavioral screening</title>
                  <p>To ensure that participants were able to imagine individual cars, they were screened in a 1-hour preliminary session. During this session, participants completed 4 behavioral tasks (1 measuring visual imagery vividness, 2 measuring performance for cars and other domains, and 1 measure for self-reports of visual expertise). First, participants completed the Vividness of Visual Imagery Questionnaire (VVIQ; [<xref rid="pone.0205041.ref040" ref-type="bibr">40</xref>]) and 2 additional car imagery questions [<xref rid="pone.0205041.ref035" ref-type="bibr">35</xref>]. The VVIQ measures self-reported individual differences in visual imagery and has been used in behavioral (e.g., [<xref rid="pone.0205041.ref041" ref-type="bibr">41</xref>]) and imaging work [<xref rid="pone.0205041.ref018" ref-type="bibr">18</xref>,<xref rid="pone.0205041.ref023" ref-type="bibr">23</xref>]. Next, participants completed a same/different matching task with 3 categories–cars, birds, and houses–followed by the Vanderbilt Expertise Test (VET; [<xref rid="pone.0205041.ref042" ref-type="bibr">42</xref>]) for cars, birds, and butterflies. We chose to use two categories other than cars to better estimate participants’ general object recognition abilities because we were most interested in their car recognition ability controlling for general recognition ability (see [<xref rid="pone.0205041.ref043" ref-type="bibr">43</xref>]).</p>
                  <p>Participants were included only if they met all of the following cutoffs: 50% accuracy on the VET-car (chance is 33%), a d-prime of at least 1.340 on the car matching tasks, and a score of at least 3 on the VVIQ-general questions. These cutoffs were chosen to be above the corresponding averages from previous larger datasets [<xref rid="pone.0205041.ref042" ref-type="bibr">42</xref>,<xref rid="pone.0205041.ref044" ref-type="bibr">44</xref>–<xref rid="pone.0205041.ref046" ref-type="bibr">46</xref>]. As a group, our sample was chosen to be above average in car recognition ability and also in the vividness of visual imagery. This was important both so participants could perform our task and because there is some evidence that individual differences in vividness can manifest in neural differences [<xref rid="pone.0205041.ref015" ref-type="bibr">15</xref>,<xref rid="pone.0205041.ref018" ref-type="bibr">18</xref>,<xref rid="pone.0205041.ref021" ref-type="bibr">21</xref>,<xref rid="pone.0205041.ref023" ref-type="bibr">23</xref>]. Note that while we recruited participants capable of imagining individual cars from the auditory make and model name cues, our MVPA was done at a categorical level (face vs. car, as done in [<xref rid="pone.0205041.ref004" ref-type="bibr">4</xref>]) not at an individual exemplar level (Ford Fusion vs. Kia Forte, similar to [<xref rid="pone.0205041.ref007" ref-type="bibr">7</xref>]).</p>
                  <p>To further characterize participants’ recognition abilities and relevant semantic knowledge, independent of the tests used to select participants, they completed the following tasks online a few days before scanning. Participants first completed the Cambridge Face Memory Test long-form (CFMT; [<xref rid="pone.0205041.ref047" ref-type="bibr">47</xref>,<xref rid="pone.0205041.ref048" ref-type="bibr">48</xref>]), followed by the Vanderbilt Face Matching Test (VFMT; [<xref rid="pone.0205041.ref049" ref-type="bibr">49</xref>]), and the Semantic Vanderbilt Expertise Test (SVET; [<xref rid="pone.0205041.ref038" ref-type="bibr">38</xref>]) for cars, birds and dinosaurs. Lastly, participants completed a task to familiarize themselves with the experimental stimuli and their labels. During the first 60 trials, an image was shown with the corresponding label beneath it. Participants studied the image and label and then clicked to advance. There were six 10-trial blocks of each stimulus category (faces, objects, and cars) and the images of a given category were randomized within a block, but presented in the same order to every participant. During the last 60 trials, participants completed a two alternative forced choice to decide which of two labels corresponded to the image shown. Correct responses were randomized with respect to presentation location. This task ensured that participants were familiar with both the images and their labels.</p>
                </sec>
                <sec id="sec010">
                  <title>Scanning procedure</title>
                  <p>The MRI portion of the experiment began with a structural run, followed by 2 functional localizer runs and 5 experimental runs. Each participant performed the same tasks with the same stimuli in the same sequence. The localizer runs consisted of 15 16-second blocks with an 8-second block of fixation between each 16-second block, beginning and ending with the task. Each block presented stimuli from either the face, object, or scrambled category and each image was displayed for 900 ms with a 100 ms temporal gap between presentations. Participants completed a 1-back task to detect the one image repeated per block, which they indicated with a button press (right hand index finger). The second localizer run was identical to the first except for the order in which the blocks and stimuli were presented. Each localizer lasted 6 minutes.</p>
                  <p>For both imagery and perception runs, 4 stimuli were chosen from each category (faces, objects, cars) to use in a given run (<xref rid="pone.0205041.t001" ref-type="table">Table 1</xref>). These images (12 total/run) were presented to the participants for review before each run began for approximately 2 minutes. Participants were informed that the images presented on the review slides would be the stimuli used during the following run. Following the two localizers, participants completed an imagery run in which they were instructed to create a mental image corresponding the image label that was aurally presented. The run consisted of 24 blocks of 4 4-second trials each, totaling 96 trials and lasting 6 minutes, 24 seconds. A specific face, object, or car was only presented once per block, while all block and stimulus orders were randomized once and then presented in the same randomized order to all participants. During fixation blocks, participants were instructed to rest, while keeping their eyes open. In imagery runs, a grey fixation cross on a black background changed to red (for one complete 16-second block) to instruct participants to stop imagining and also to encourage participants to keep their eyes open and remain alert despite the minimal visual input. Participants were instructed to keep their eyes open during the entire run so that they would be able to see the fixation cross turn red. Because we wanted participants to focus on creating mental images, the visual imagery itself was the only task participants completed during the imagery runs (<xref ref-type="fig" rid="pone.0205041.g001">Fig 1</xref>).</p>
                  <fig id="pone.0205041.g001" orientation="portrait" position="float">
                    <object-id pub-id-type="doi">10.1371/journal.pone.0205041.g001</object-id>
                    <label>Fig 1</label>
                    <caption>
                      <p><bold>Schematic of scanning procedure for a sample (a) perception run and (b) imagery run</bold>. In perception runs, participants indicated via key press whether the image was displaced upward or downward.</p>
                    </caption>
                    <graphic xlink:href="pone.0205041.g001"/>
                  </fig>
                  <p>During perception runs, stimuli were presented with a grey fixation cross superimposed over the stimuli, and participants determined if the image was displaced up or down relative to the screen’s center. On each trial, images were displaced either 75 pixels upwards or downwards from the screen’s center, but were always centered horizontally (<xref ref-type="fig" rid="pone.0205041.g001">Fig 1</xref>). Participants used a button box to make up or down responses with either their index or middle fingers, respectively. We chose this task to encourage attention to the images while avoiding adding a difficult perceptual/decisional task that was not present in the imagery task. Each perception run consisted of 24 blocks of 8 2-second trials for a total of 192 trials per run. Perception runs, like imagery runs, took 6 minutes, 24 seconds to complete. As with the imagery runs, all blocks and stimuli were randomized. Participants completed 3 imagery runs with 2 perception runs interleaved.</p>
                </sec>
              </sec>
              <sec id="sec011">
                <title>Data analysis</title>
                <sec id="sec012">
                  <title>MRI analysis</title>
                  <p>The HR structural scans were normalized to Talairach space [<xref rid="pone.0205041.ref050" ref-type="bibr">50</xref>]. All functional data were analyzed using Brain Voyager software (<ext-link ext-link-type="uri" xlink:href="http://www.brainvoyager.com/">www.brainvoyager.com</ext-link>), in-house Matlab scripts, and LibSVM [<xref rid="pone.0205041.ref051" ref-type="bibr">51</xref>]. Functional scans were preprocessed using slice scan time correction (cubic spline, sinc interpolation), 3D motion correction (sinc interpolation), temporal filtering (high-pass filtering with a criterion of 2 cycles per run and a Gaussian filter at 3 seconds), and no spatial smoothing. Across all runs for all participants, we identified all motion-spikes over .5 mm, and any blocks with a spike in motion of this magnitude were removed from all subsequent linear models. Of the 6 participants’ data where spikes in motion were detected, a total of 6 perception run blocks and 7 imagery blocks were removed from analyses. Functional data were registered to the original (non-transformed) structural scan before regions of interest (ROIs) were defined.</p>
                  <p>Localizer data were submitted to a general linear model (GLM) with regressors for each stimulus category. Face-selective ROIs were defined using the Face&gt;Object contrast from the localizer GLM. We distinguished between two face-selective fusiform areas: a more posterior FFA1 and a more anterior FFA2 [<xref rid="pone.0205041.ref029" ref-type="bibr">29</xref>,<xref rid="pone.0205041.ref030" ref-type="bibr">30</xref>].</p>
                  <p>As is common when defining these functional regions [<xref rid="pone.0205041.ref028" ref-type="bibr">28</xref>,<xref rid="pone.0205041.ref033" ref-type="bibr">33</xref>,<xref rid="pone.0205041.ref034" ref-type="bibr">34</xref>], we could not localize every regions for every participant, and thus only included the ROIs that we could functionally localize. When possible, bilateral FFA1, FFA2, and OFA regions were defined individually for each participant (<xref ref-type="fig" rid="pone.0205041.g002">Fig 2</xref>) by finding peaks of significant BOLD responses to faces in the fusiform gyrus. Object-selective regions in the parahippocampal gyrus (PHG) were defined using the Object&gt;Face contrast from the localizer GLM. We defined two object-selective PHG regions (one posterior and one middle, here called PHG1 and PHG2 respectively) bilaterally. Additionally, two lateral occipital object-selective regions were defined using the Object&gt;Scrambled contrast from the localizer GLM. For each ROI, the face- or object-selective peak voxel was identified before the region was grown to 4 functional voxels (108 mm<sup>3</sup>) for univariate analyses or 27 functional voxels (729 mm<sup>3</sup>) for MVPA using in-house Matlab scripts. This script created ROIs by growing regions to include the next-highest contiguous activated voxel until a given size (in this case 4 or 27) was reached. Functional voxels belonging to more than one ROI were removed from both ROIs.</p>
                  <fig id="pone.0205041.g002" orientation="portrait" position="float">
                    <object-id pub-id-type="doi">10.1371/journal.pone.0205041.g002</object-id>
                    <label>Fig 2</label>
                    <caption>
                      <title>Example 27 functional voxel ROIs shown on 2 inflated right hemispheres of representative participants (anterior and posterior portions of the brain denoted with A and P, medial and lateral with M and L).</title>
                      <p>Note: All ROIs were defined in volumetric space. ROIs transformed to surface space for illustrative purposes only. Any dis-contiguous clusters or ROI overlap that appears in figure did not occur in volumetric space (all ROIs were contiguous and no ROIs contained overlapping voxels).</p>
                    </caption>
                    <graphic xlink:href="pone.0205041.g002"/>
                  </fig>
                </sec>
                <sec id="sec013">
                  <title>Multi-voxel pattern analysis (MVPA)</title>
                  <p>MVPA was conducted using a linear support-vector machine (SVM). Because we used different subordinate-level exemplars for each run and a blocked design, we trained our classifiers at the category level (e.g. cars vs. objects instead of Kia Forte vs. Honda Accord). We used MVPA to answer two different questions. In an initial series of classification analyses, we asked whether distinct image categories led to different representations in each ROI. Specifically, for each classification, a leave-one-run-out approach was used in which one run was used as test data and the remaining runs were used as training data. Classification rates represent the proportion of times the classifier produced a correct prediction. Because we are interested in object representations during visual imagery, we first focus on our imagery condition. To analyze this imagery condition, the classifier was trained on two runs and tested on one, for the three possible iterations. For the overall classification performance report, an average of these three iterations was calculated. MVPA was done for each participant individually, and results were averaged across participants within equivalent ROIs to form group results. If the classifier is able to distinguish one category from another, this provides evidence that the two categories are represented differently during imagery within that ROI. Most importantly, we were interested in whether imagined cars could be decoded from imagined objects in face-selective regions.</p>
                  <p>Successful car vs. object decoding during imagery cannot, however, tell us if cars are imagined in a manner similar to the way they are perceived–decoding could be based on any difference in the representations during the imagery conditions. To address this question directly, in a second set of analyses we trained a classifier for each participant using data acquired during the imagery condition and then tested the classifier on data acquired during the perception condition. Prior work with designs that collected equal amounts of imagery and perceptual data obtained better cross-task classification when training on imagery than when training on perception [<xref rid="pone.0205041.ref005" ref-type="bibr">5</xref>]. These authors suggested that all the features of an imagined representation should overlap with those in perceived representations, but that the converse was not true. Based on these prior results, we collected more imagery data and chose to compare perception and imagery by training on imagery and testing on perception. Above-chance performance in such a cross-trained classifier constitutes evidence that 1) the two categories are dissimilarly represented within the given ROI and 2) this dissimilarity is present during both perception and imagery.</p>
                </sec>
                <sec id="sec014">
                  <title>Univariate analysis</title>
                  <p>Independent RFX GLMs were fit to the imagery and perception runs with each category (faces, cars, objects and fixation), convolved with the canonical hemodynamic response function, included as a separate regressor. Parameter weights were calculated for each voxel, then averaged across all voxels composing an ROI. We examined all three possible pairwise contrasts of our three stimulus categories: face versus car, face versus object, and car versus object.</p>
                </sec>
              </sec>
            </sec>
            <sec sec-type="results" id="sec015">
              <title>Results</title>
              <sec id="sec016">
                <title>Behavioral results outside of the scanner</title>
                <p>Data are available at <ext-link ext-link-type="uri" xlink:href="https://figshare.com/s/24fc1314649a6e2b2899">https://figshare.com/s/24fc1314649a6e2b2899</ext-link>. Average performance on each behavioral test is reported in <xref rid="pone.0205041.t002" ref-type="table">Table 2</xref>. To compare participants’ car recognition abilities to general object recognition abilities, we averaged the two non-car categories for each task and compared this average to car recognition performance.</p>
                <table-wrap id="pone.0205041.t002" orientation="portrait" position="float">
                  <object-id pub-id-type="doi">10.1371/journal.pone.0205041.t002</object-id>
                  <label>Table 2</label>
                  <caption>
                    <title>Average accuracy (or d-prime for matching task and rating average for VVIQ and SR) score across participants for each of the behavioral tests, along with standard deviations.</title>
                    <p>Tests used to qualify participants for MRI scan are bolded (N = 20 scanned participants).</p>
                  </caption>
                  <alternatives>
                    <graphic id="pone.0205041.t002g" xlink:href="pone.0205041.t002"/>
                    <table frame="hsides" rules="groups">
                      <colgroup span="1">
                        <col align="left" valign="middle" span="1"/>
                        <col align="left" valign="middle" span="1"/>
                        <col align="left" valign="middle" span="1"/>
                      </colgroup>
                      <thead>
                        <tr>
                          <th align="left" rowspan="1" colspan="1">Measure</th>
                          <th align="left" rowspan="1" colspan="1">Mean</th>
                          <th align="left" rowspan="1" colspan="1">SD</th>
                        </tr>
                      </thead>
                      <tbody>
                        <tr>
                          <td align="left" rowspan="1" colspan="1">
                            <bold>VVIQ-General</bold>
                          </td>
                          <td align="left" rowspan="1" colspan="1">
                            <bold>4.09</bold>
                          </td>
                          <td align="left" rowspan="1" colspan="1">
                            <bold>0.39</bold>
                          </td>
                        </tr>
                        <tr>
                          <td align="left" style="border-bottom:dashed" rowspan="1" colspan="1">VVIQ-Car</td>
                          <td align="left" style="border-bottom:dashed" rowspan="1" colspan="1">4.45</td>
                          <td align="left" style="border-bottom:dashed" rowspan="1" colspan="1">0.47</td>
                        </tr>
                        <tr>
                          <td align="left" style="border-top:dashed" rowspan="1" colspan="1">SR-Car</td>
                          <td align="left" style="border-top:dashed" rowspan="1" colspan="1">7.55</td>
                          <td align="left" style="border-top:dashed" rowspan="1" colspan="1">1.00</td>
                        </tr>
                        <tr>
                          <td align="left" rowspan="1" colspan="1">SR-Birds</td>
                          <td align="left" rowspan="1" colspan="1">2.65</td>
                          <td align="left" rowspan="1" colspan="1">1.57</td>
                        </tr>
                        <tr>
                          <td align="left" style="border-bottom:dashed" rowspan="1" colspan="1">SR-Butterfly</td>
                          <td align="left" style="border-bottom:dashed" rowspan="1" colspan="1">2.00</td>
                          <td align="left" style="border-bottom:dashed" rowspan="1" colspan="1">1.08</td>
                        </tr>
                        <tr>
                          <td align="left" style="border-top:dashed" rowspan="1" colspan="1">
                            <bold>VET-Car</bold>
                          </td>
                          <td align="left" style="border-top:dashed" rowspan="1" colspan="1">
                            <bold>0.79</bold>
                          </td>
                          <td align="left" style="border-top:dashed" rowspan="1" colspan="1">
                            <bold>0.12</bold>
                          </td>
                        </tr>
                        <tr>
                          <td align="left" rowspan="1" colspan="1">VET-Bird</td>
                          <td align="left" rowspan="1" colspan="1">0.65</td>
                          <td align="left" rowspan="1" colspan="1">0.12</td>
                        </tr>
                        <tr>
                          <td align="left" style="border-bottom:dashed" rowspan="1" colspan="1">VET-Butterfly</td>
                          <td align="left" style="border-bottom:dashed" rowspan="1" colspan="1">0.60</td>
                          <td align="left" style="border-bottom:dashed" rowspan="1" colspan="1">0.11</td>
                        </tr>
                        <tr>
                          <td align="left" style="border-top:dashed" rowspan="1" colspan="1">
                            <bold>Matching-Cars</bold>
                          </td>
                          <td align="left" style="border-top:dashed" rowspan="1" colspan="1">
                            <bold>2.54</bold>
                          </td>
                          <td align="left" style="border-top:dashed" rowspan="1" colspan="1">
                            <bold>0.42</bold>
                          </td>
                        </tr>
                        <tr>
                          <td align="left" rowspan="1" colspan="1">Matching-Birds</td>
                          <td align="left" rowspan="1" colspan="1">1.56</td>
                          <td align="left" rowspan="1" colspan="1">0.37</td>
                        </tr>
                        <tr>
                          <td align="left" style="border-bottom:dashed" rowspan="1" colspan="1">Matching-Houses</td>
                          <td align="left" style="border-bottom:dashed" rowspan="1" colspan="1">1.72</td>
                          <td align="left" style="border-bottom:dashed" rowspan="1" colspan="1">0.39</td>
                        </tr>
                        <tr>
                          <td align="left" style="border-top:dashed" rowspan="1" colspan="1">CFMT</td>
                          <td align="left" style="border-top:dashed" rowspan="1" colspan="1">0.58</td>
                          <td align="left" style="border-top:dashed" rowspan="1" colspan="1">0.16</td>
                        </tr>
                        <tr>
                          <td align="left" style="border-bottom:dashed" rowspan="1" colspan="1">VFMT</td>
                          <td align="left" style="border-bottom:dashed" rowspan="1" colspan="1">0.59</td>
                          <td align="left" style="border-bottom:dashed" rowspan="1" colspan="1">0.09</td>
                        </tr>
                        <tr>
                          <td align="left" style="border-top:dashed" rowspan="1" colspan="1">SVET-Dino</td>
                          <td align="left" style="border-top:dashed" rowspan="1" colspan="1">0.49</td>
                          <td align="left" style="border-top:dashed" rowspan="1" colspan="1">0.11</td>
                        </tr>
                        <tr>
                          <td align="left" rowspan="1" colspan="1">SVET-Bird</td>
                          <td align="left" rowspan="1" colspan="1">0.46</td>
                          <td align="left" rowspan="1" colspan="1">0.08</td>
                        </tr>
                        <tr>
                          <td align="left" rowspan="1" colspan="1">SVET-Car</td>
                          <td align="left" rowspan="1" colspan="1">0.87</td>
                          <td align="left" rowspan="1" colspan="1">0.14</td>
                        </tr>
                      </tbody>
                    </table>
                  </alternatives>
                </table-wrap>
                <p>As expected (given that we screened based on VET-Car and matching task-Car performance), participants performed significantly better with cars than non-car categories (VET: (<italic>t</italic>(19) = 4.98 <italic>p</italic> ≤ .001, d = 2.28; Matching Task: <italic>t</italic>(19) = 8.71; <italic>p</italic> ≤ .001, d = 4.00). While these tasks are not precisely matched in difficulty, in large unscreened samples the VETs have highly similar means (see [<xref rid="pone.0205041.ref042" ref-type="bibr">42</xref>]). Our participants also performed significantly better on the SVET-Car than non-car SVETs (<italic>t</italic>(19) = 12.14; <italic>p</italic> ≤ .001, d = 5.57).</p>
              </sec>
              <sec id="sec017">
                <title>Behavioral results from the scanner</title>
                <p>Performance on behavioral tasks in the scanner indicated that participants were attending to the stimuli. The average performance on the n-back task during the first and second localizer runs (not including scrambled images) was 79% (SD = 19%) and 93% (SD = 8%), respectively. Average performance on the up/down displacement task during the first and second perception runs was 83% (SD = 26%) and 86% (SD = 17%), respectively.</p>
              </sec>
              <sec id="sec018">
                <title>MRI Results</title>
                <sec id="sec019">
                  <title>ROI identification</title>
                  <p>To grow regions suitable for MVPA, we created ROIs composed of 27 functional voxels. Though past work has shown that ROI sizes of around 100 voxels produce optimal MVPA performance [<xref rid="pone.0205041.ref052" ref-type="bibr">52</xref>], some of our functional regions are small and close to one another (e.g., FFA1 and FFA2). Given that typical searchlight MVPA analyses use spheres of approximately 30 voxels [<xref rid="pone.0205041.ref053" ref-type="bibr">53</xref>], we chose to use 27 voxels since this was the largest size at which we could avoid identifying many overlapping voxels across functional region definitions (<xref rid="pone.0205041.t003" ref-type="table">Table 3</xref>). All ROIs were grown to 27 3x3x3 functional, non-overlapping voxels except for the following which were smaller because of dropout due to the ear canal: the lOFAs in two participants (12 functional voxels), the rOFA in one participant (18 functional voxels), and all ROIs in one participant (17 functional voxels). The MVPA results did not qualitatively differ when these smaller ROIs were excluded from analysis. For these reasons, along with the fact that we could not functionally localize each ROI for each participant, we had unequal numbers of ROIs (see <xref rid="pone.0205041.t003" ref-type="table">Table 3</xref>), as is typical in work with these sub-regions (e.g. 33). We used 4-functional-voxel ROIs for our univariate analyses so that we would include only peak activation [<xref rid="pone.0205041.ref034" ref-type="bibr">34</xref>]. However, univariate results with the 27-functional-voxel ROIs produced qualitatively similar results to the smaller ROIs.</p>
                  <table-wrap id="pone.0205041.t003" orientation="portrait" position="float">
                    <object-id pub-id-type="doi">10.1371/journal.pone.0205041.t003</object-id>
                    <label>Table 3</label>
                    <caption>
                      <title>Peak Talairach coordinates of the peak structural voxel for each 27-functional voxel ROIs with 95% confidence intervals reported in parentheses.</title>
                      <p>N column reports the number of participants in whom we localized the respective ROI.</p>
                    </caption>
                    <alternatives>
                      <graphic id="pone.0205041.t003g" xlink:href="pone.0205041.t003"/>
                      <table frame="hsides" rules="groups">
                        <colgroup span="1">
                          <col align="left" valign="middle" span="1"/>
                          <col align="left" valign="middle" span="1"/>
                          <col align="left" valign="middle" span="1"/>
                          <col align="left" valign="middle" span="1"/>
                          <col align="left" valign="middle" span="1"/>
                        </colgroup>
                        <thead>
                          <tr>
                            <th align="left" rowspan="1" colspan="1">ROI</th>
                            <th align="center" rowspan="1" colspan="1">Peak X</th>
                            <th align="center" rowspan="1" colspan="1">Peak Y</th>
                            <th align="center" rowspan="1" colspan="1">Peak Z</th>
                            <th align="center" rowspan="1" colspan="1">N</th>
                          </tr>
                        </thead>
                        <tbody>
                          <tr>
                            <td align="left" rowspan="1" colspan="1">rFFA1</td>
                            <td align="center" rowspan="1" colspan="1">37.00 (34.46, 39.55)</td>
                            <td align="center" rowspan="1" colspan="1">-60.73 (-63.88, -57.59)</td>
                            <td align="center" rowspan="1" colspan="1">-17.40 (-19.64, -15.16)</td>
                            <td align="center" rowspan="1" colspan="1">15</td>
                          </tr>
                          <tr>
                            <td align="left" rowspan="1" colspan="1">rFFA2</td>
                            <td align="center" rowspan="1" colspan="1">36.83 (34.81, 38.86)</td>
                            <td align="center" rowspan="1" colspan="1">-43.94 (-46.77, -41.12)</td>
                            <td align="center" rowspan="1" colspan="1">-20.78 (-22.83, -18.72)</td>
                            <td align="center" rowspan="1" colspan="1">18</td>
                          </tr>
                          <tr>
                            <td align="left" rowspan="1" colspan="1">rOFA</td>
                            <td align="center" rowspan="1" colspan="1">30.15 (26.41, 33.89)</td>
                            <td align="center" rowspan="1" colspan="1">-77.62 (-80.92, -74.31)</td>
                            <td align="center" rowspan="1" colspan="1">-18.00 (-22.10, -13.90)</td>
                            <td align="center" rowspan="1" colspan="1">13</td>
                          </tr>
                          <tr>
                            <td align="left" rowspan="1" colspan="1">lFFA1</td>
                            <td align="center" rowspan="1" colspan="1">-39.46 (-41.85, -37.08)</td>
                            <td align="center" rowspan="1" colspan="1">-60.08 (-63.85, -56.31)</td>
                            <td align="center" rowspan="1" colspan="1">-19.38 (-22.48, -16.29)</td>
                            <td align="center" rowspan="1" colspan="1">13</td>
                          </tr>
                          <tr>
                            <td align="left" rowspan="1" colspan="1">lFFA2</td>
                            <td align="center" rowspan="1" colspan="1">-39.95 (-41.21, -38.69)</td>
                            <td align="center" rowspan="1" colspan="1">-46.00 (-48.22, -43.78)</td>
                            <td align="center" rowspan="1" colspan="1">-20.85 (-23.32, -18.38)</td>
                            <td align="center" rowspan="1" colspan="1">20</td>
                          </tr>
                          <tr>
                            <td align="left" rowspan="1" colspan="1">lOFA</td>
                            <td align="center" rowspan="1" colspan="1">-37.50 (-39.88, -35.12)</td>
                            <td align="center" rowspan="1" colspan="1">-76.67 (-79.85, -73.48)</td>
                            <td align="center" rowspan="1" colspan="1">-17.92 (-21.26, -14.57)</td>
                            <td align="center" rowspan="1" colspan="1">12</td>
                          </tr>
                          <tr>
                            <td align="left" rowspan="1" colspan="1">rPHG1</td>
                            <td align="center" rowspan="1" colspan="1">25.70 (23.80, 27.60)</td>
                            <td align="center" rowspan="1" colspan="1">-46.30 (-48.46, -44.14)</td>
                            <td align="center" rowspan="1" colspan="1">-13.05 (-15.40, -10.70)</td>
                            <td align="center" rowspan="1" colspan="1">20</td>
                          </tr>
                          <tr>
                            <td align="left" rowspan="1" colspan="1">rPHG2</td>
                            <td align="center" rowspan="1" colspan="1">24.60 (22.93, 26.27)</td>
                            <td align="center" rowspan="1" colspan="1">-63.60 (-66.67, -60.53)</td>
                            <td align="center" rowspan="1" colspan="1">-13.80 (-16.11, -11.49)</td>
                            <td align="center" rowspan="1" colspan="1">20</td>
                          </tr>
                          <tr>
                            <td align="left" rowspan="1" colspan="1">lPHG1</td>
                            <td align="center" rowspan="1" colspan="1">-26.74 (-28.25, -25.22)</td>
                            <td align="center" rowspan="1" colspan="1">-45.58 (-47.95, -43.21)</td>
                            <td align="center" rowspan="1" colspan="1">-16.74 (-19.09, -14.39)</td>
                            <td align="center" rowspan="1" colspan="1">19</td>
                          </tr>
                          <tr>
                            <td align="left" rowspan="1" colspan="1">lPHG2</td>
                            <td align="center" rowspan="1" colspan="1">-27.21 (-28.82, -25.60)</td>
                            <td align="center" rowspan="1" colspan="1">-62.21 (-64.95, -59.47)</td>
                            <td align="center" rowspan="1" colspan="1">-15.32 (-17.89, -12.74)</td>
                            <td align="center" rowspan="1" colspan="1">19</td>
                          </tr>
                          <tr>
                            <td align="left" rowspan="1" colspan="1">rLOC</td>
                            <td align="center" rowspan="1" colspan="1">40.85 (38.38, 43.32)</td>
                            <td align="center" rowspan="1" colspan="1">-70.55 (-72.82, -68.28)</td>
                            <td align="center" rowspan="1" colspan="1">-8.85 (-11.46, -6.24)</td>
                            <td align="center" rowspan="1" colspan="1">20</td>
                          </tr>
                          <tr>
                            <td align="left" rowspan="1" colspan="1">lLOC</td>
                            <td align="center" rowspan="1" colspan="1">-44.50 (-46.52, -42.48)</td>
                            <td align="center" rowspan="1" colspan="1">-69.80 (-70.96, -68.64)</td>
                            <td align="center" rowspan="1" colspan="1">-9.75 (-12.23, -7.27)</td>
                            <td align="center" rowspan="1" colspan="1">20</td>
                          </tr>
                        </tbody>
                      </table>
                    </alternatives>
                  </table-wrap>
                </sec>
              </sec>
              <sec id="sec020">
                <title>Multi-voxel pattern analysis</title>
                <sec id="sec021">
                  <title>Within-task classification</title>
                  <p>To test if activity patterns for imagined faces, cars and objects were distinguishable, we performed MVPA on the imagery task (i.e., both training and testing data from imagery runs) separately for each ROI and each participant. This addresses whether it is possible to decode cars versus objects in face-selective areas. The classifier achieved above chance performance in both lFFA2 and lFFA1, as well as rFFA1 (<xref ref-type="fig" rid="pone.0205041.g003">Fig 3</xref>). Decoding imagined faces from imagined objects was only possible in lFFA2, although the effect in lFFA1 was close in magnitude (note that we had reduced power in lFFA1 because we were only able to localize the ROI in 13 participants, <xref rid="pone.0205041.t003" ref-type="table">Table 3</xref>). Imagined faces and cars could be decoded in all four FFA ROIs. Average MVPA performance (for face vs. car, face vs. object and object vs. car) in face-selective ROIs did not correlate with behaviorally measured car recognition performance across subjects (<italic>r</italic>’s = -.17 –.27, <italic>p</italic>’s &gt; .24). This is unsurprising given that we intentionally selected participants with high car recognition ability.</p>
                  <fig id="pone.0205041.g003" orientation="portrait" position="float">
                    <object-id pub-id-type="doi">10.1371/journal.pone.0205041.g003</object-id>
                    <label>Fig 3</label>
                    <caption>
                      <p><bold>Average accuracies of the classifier when trained on imagery data and tested on imagery data for object versus car (left) and face versus object (middle) and face versus car (right) two-way classifications in face-selective ROIs (upper row) and object-selective ROIs (lower row)</bold>. Error-bars represent one-tail 95% confidence interval, accuracy of decoding below chance was not theoretically meaningful.</p>
                    </caption>
                    <graphic xlink:href="pone.0205041.g003"/>
                  </fig>
                  <p>In object-selective regions, the classifier could decode imagined faces versus imagined objects and imagined cars versus imagined objects in bilateral PHG2 and lLOC (<xref ref-type="fig" rid="pone.0205041.g003">Fig 3</xref>). Imagined faces and cars could be decoded in all object-selective ROIs except the lPHG1. Both within-task and across-task analyses of early visual areas provided little evidence that categories could be distinguished in these regions (see supplement for details).</p>
                </sec>
                <sec id="sec022">
                  <title>Across-task classification</title>
                  <p>To ask whether the categorical differences we observed in imagery reflected the representations evoked by these categories during perception, we trained a classifier with data from the imagery runs and tested its ability to decode the perception runs. Central to our question, we were able to distinguish perceived cars from perceived objects based on imagery information in bilateral FFA2 (<xref ref-type="fig" rid="pone.0205041.g004">Fig 4</xref>). Additionally, we could also decode perceived faces from objects based on imagery information in all face-selective ROIs except rOFA (<xref ref-type="fig" rid="pone.0205041.g004">Fig 4</xref>). Based on imagery information, perceived faces could only be distinguished from cars in left face-selective ROIs.</p>
                  <fig id="pone.0205041.g004" orientation="portrait" position="float">
                    <object-id pub-id-type="doi">10.1371/journal.pone.0205041.g004</object-id>
                    <label>Fig 4</label>
                    <caption>
                      <p><bold>Average accuracies of the classifier when trained on imagery data and tested on perception data for object versus car (left) and face versus object (middle) and face versus car (right) two-way classifications in face-selective ROIs (upper row) and object-selective ROIs (lower row)</bold>. Error-bars represent one-tail 95% confidence interval, accuracy of decoding below chance was not theoretically meaningful.</p>
                    </caption>
                    <graphic xlink:href="pone.0205041.g004"/>
                  </fig>
                  <p>In all object-selective regions, perceived faces were decoded from perceived objects based on imagery information, and perceived cars were also distinguished from perceived objects in all but bilateral LOC ROIs (<xref ref-type="fig" rid="pone.0205041.g004">Fig 4</xref>). All object-selective ROIs except the rPHG regions could distinguish perceived faces from cars based on the imagery-trained classifier.</p>
                </sec>
              </sec>
              <sec id="sec023">
                <title>Univariate analyses</title>
                <p>The average parameter weights are reported in <xref rid="pone.0205041.t004" ref-type="table">Table 4</xref>. Face and car parameter weights are reported relative to an object baseline. We use objects as a baseline condition so that both low- and high-level visual processing activation would be subtracted out. In contrast with previous work reporting significant activation during face imagery in face-selective fusiform and OFA regions [<xref rid="pone.0205041.ref004" ref-type="bibr">4</xref>–<xref rid="pone.0205041.ref006" ref-type="bibr">6</xref>], we only found significantly greater activation for imagined faces vs. imagined objects in one face-selective region, lFFA2 (<xref rid="pone.0205041.t004" ref-type="table">Table 4</xref>). In addition, when examining the parameter weights for the perception runs (<xref rid="pone.0205041.t004" ref-type="table">Table 4</xref>), we noted that while face stimuli showed the predicted pattern of eliciting higher BOLD response in face-selective areas (FFAs and OFAs) relative to objects and lower responses in object-selective areas (PHGs and LOCs), this was not always true of the car stimuli. Notably, in face-selective areas the response magnitudes for perceived cars relative to perceived objects was not significantly different from zero (<xref rid="pone.0205041.t004" ref-type="table">Table 4</xref>).</p>
                <table-wrap id="pone.0205041.t004" orientation="portrait" position="float">
                  <object-id pub-id-type="doi">10.1371/journal.pone.0205041.t004</object-id>
                  <label>Table 4</label>
                  <caption>
                    <title>Average GLM parameter weights for both imagery and perception runs.</title>
                    <p>Each weight is the average of the 4 contiguous most face- or object-selective functional voxels (identified via the independent functional localizer) within the ROI. Beta weight is reported along with the <italic>t</italic>-statistic and FDR adjusted <italic>p</italic>-value (<italic>q</italic>-value) of a two-tailed test of the differences being different from 0.</p>
                  </caption>
                  <alternatives>
                    <graphic id="pone.0205041.t004g" xlink:href="pone.0205041.t004"/>
                    <table frame="hsides" rules="groups">
                      <colgroup span="1">
                        <col align="left" valign="middle" span="1"/>
                        <col align="left" valign="middle" span="1"/>
                        <col align="left" valign="middle" span="1"/>
                        <col align="left" valign="middle" span="1"/>
                        <col align="left" valign="middle" span="1"/>
                        <col align="left" valign="middle" span="1"/>
                        <col align="left" valign="middle" span="1"/>
                        <col align="left" valign="middle" span="1"/>
                        <col align="left" valign="middle" span="1"/>
                        <col align="left" valign="middle" span="1"/>
                        <col align="left" valign="middle" span="1"/>
                      </colgroup>
                      <thead>
                        <tr>
                          <th align="left" rowspan="1" colspan="1"/>
                          <th align="center" colspan="5" rowspan="1">Perception</th>
                          <th align="center" colspan="5" rowspan="1">Imagery</th>
                        </tr>
                        <tr>
                          <th align="left" rowspan="1" colspan="1"/>
                          <th align="center" colspan="3" rowspan="1">Face-Obj</th>
                          <th align="center" colspan="2" rowspan="1">Car-Obj</th>
                          <th align="center" colspan="3" rowspan="1">Face-Obj</th>
                          <th align="center" colspan="2" rowspan="1">Car-Obj</th>
                        </tr>
                        <tr>
                          <th align="left" rowspan="1" colspan="1"/>
                          <th align="center" colspan="2" rowspan="1">β</th>
                          <th align="center" rowspan="1" colspan="1">
                            <italic>t</italic>
                          </th>
                          <th align="center" rowspan="1" colspan="1">β</th>
                          <th align="center" rowspan="1" colspan="1">
                            <italic>t</italic>
                          </th>
                          <th align="center" colspan="2" rowspan="1">β</th>
                          <th align="center" rowspan="1" colspan="1">
                            <italic>t</italic>
                          </th>
                          <th align="center" rowspan="1" colspan="1">β</th>
                          <th align="center" rowspan="1" colspan="1">
                            <italic>t</italic>
                          </th>
                        </tr>
                      </thead>
                      <tbody>
                        <tr>
                          <td align="left" rowspan="1" colspan="1">rFFA1</td>
                          <td align="center" colspan="2" rowspan="1">0.84</td>
                          <td align="center" rowspan="1" colspan="1">8.41 (.00)</td>
                          <td align="center" rowspan="1" colspan="1">0.01</td>
                          <td align="center" rowspan="1" colspan="1">0.15 (.90)</td>
                          <td align="center" colspan="2" rowspan="1">0.04</td>
                          <td align="center" rowspan="1" colspan="1">0.27 (.87)</td>
                          <td align="center" rowspan="1" colspan="1">-0.02</td>
                          <td align="center" rowspan="1" colspan="1">-0.23 (.87)</td>
                        </tr>
                        <tr>
                          <td align="left" rowspan="1" colspan="1">rFFA2</td>
                          <td align="center" colspan="2" rowspan="1">0.68</td>
                          <td align="center" rowspan="1" colspan="1">5.91 (.00)</td>
                          <td align="center" rowspan="1" colspan="1">-0.08</td>
                          <td align="center" rowspan="1" colspan="1">-0.79 (.55)</td>
                          <td align="center" colspan="2" rowspan="1">0.16</td>
                          <td align="center" rowspan="1" colspan="1">2.03 (.12)</td>
                          <td align="center" rowspan="1" colspan="1">0.05</td>
                          <td align="center" rowspan="1" colspan="1">0.65 (.74)</td>
                        </tr>
                        <tr>
                          <td align="left" rowspan="1" colspan="1">rOFA</td>
                          <td align="center" colspan="2" rowspan="1">0.52</td>
                          <td align="center" rowspan="1" colspan="1">2.70 (.02)</td>
                          <td align="center" rowspan="1" colspan="1">0.22</td>
                          <td align="center" rowspan="1" colspan="1">1.48 (.27)</td>
                          <td align="center" colspan="2" rowspan="1">0.02</td>
                          <td align="center" rowspan="1" colspan="1">0.22 (.87)</td>
                          <td align="center" rowspan="1" colspan="1">0.02</td>
                          <td align="center" rowspan="1" colspan="1">0.16 (.87)</td>
                        </tr>
                        <tr>
                          <td align="left" rowspan="1" colspan="1">lFFA1</td>
                          <td align="center" colspan="2" rowspan="1">0.80</td>
                          <td align="center" rowspan="1" colspan="1">4.28 (.00)</td>
                          <td align="center" rowspan="1" colspan="1">-0.02</td>
                          <td align="center" rowspan="1" colspan="1">-0.13 (.90)</td>
                          <td align="center" colspan="2" rowspan="1">0.02</td>
                          <td align="center" rowspan="1" colspan="1">0.17 (.87)</td>
                          <td align="center" rowspan="1" colspan="1">-0.03</td>
                          <td align="center" rowspan="1" colspan="1">-0.36 (.87)</td>
                        </tr>
                        <tr>
                          <td align="left" rowspan="1" colspan="1">lFFA2</td>
                          <td align="center" colspan="2" rowspan="1">0.64</td>
                          <td align="center" rowspan="1" colspan="1">6.44 (.00)</td>
                          <td align="center" rowspan="1" colspan="1">-0.10</td>
                          <td align="center" rowspan="1" colspan="1">-0.91 (.54)</td>
                          <td align="center" colspan="2" rowspan="1">0.32</td>
                          <td align="center" rowspan="1" colspan="1">3.51 (.00)</td>
                          <td align="center" rowspan="1" colspan="1">0.06</td>
                          <td align="center" rowspan="1" colspan="1">0.96 (.74)</td>
                        </tr>
                        <tr>
                          <td align="left" rowspan="1" colspan="1">lOFA</td>
                          <td align="center" colspan="2" rowspan="1">0.72</td>
                          <td align="center" rowspan="1" colspan="1">2.63 (.02)</td>
                          <td align="center" rowspan="1" colspan="1">0.34</td>
                          <td align="center" rowspan="1" colspan="1">1.48 (.27)</td>
                          <td align="center" colspan="2" rowspan="1">-0.17</td>
                          <td align="center" rowspan="1" colspan="1">-0.78 (.75)</td>
                          <td align="center" rowspan="1" colspan="1">-0.24</td>
                          <td align="center" rowspan="1" colspan="1">-1.54 (.50)</td>
                        </tr>
                        <tr>
                          <td align="left" rowspan="1" colspan="1">rPHG1</td>
                          <td align="center" colspan="2" rowspan="1">-0.88</td>
                          <td align="center" rowspan="1" colspan="1">-7.08 (.00)</td>
                          <td align="center" rowspan="1" colspan="1">-0.49</td>
                          <td align="center" rowspan="1" colspan="1">-3.18 (.00)</td>
                          <td align="center" colspan="2" rowspan="1">-0.03</td>
                          <td align="center" rowspan="1" colspan="1">-0.45 (.87)</td>
                          <td align="center" rowspan="1" colspan="1">-0.19</td>
                          <td align="center" rowspan="1" colspan="1">-2.54 (.10)</td>
                        </tr>
                        <tr>
                          <td align="left" rowspan="1" colspan="1">rPHG2</td>
                          <td align="center" colspan="2" rowspan="1">-0.93</td>
                          <td align="center" rowspan="1" colspan="1">-10.52.00)</td>
                          <td align="center" rowspan="1" colspan="1">-0.65</td>
                          <td align="center" rowspan="1" colspan="1">-4.58 (.00)</td>
                          <td align="center" colspan="2" rowspan="1">-0.22</td>
                          <td align="center" rowspan="1" colspan="1">-3.17 (.03)</td>
                          <td align="center" rowspan="1" colspan="1">-0.05</td>
                          <td align="center" rowspan="1" colspan="1">-0.73 (.74)</td>
                        </tr>
                        <tr>
                          <td align="left" rowspan="1" colspan="1">lPHG1</td>
                          <td align="center" colspan="2" rowspan="1">-0.87</td>
                          <td align="center" rowspan="1" colspan="1">-8.31 (.00)</td>
                          <td align="center" rowspan="1" colspan="1">-0.56</td>
                          <td align="center" rowspan="1" colspan="1">-4.70 (.00)</td>
                          <td align="center" colspan="2" rowspan="1">-0.23</td>
                          <td align="center" rowspan="1" colspan="1">-3.16 (.03)</td>
                          <td align="center" rowspan="1" colspan="1">-0.19</td>
                          <td align="center" rowspan="1" colspan="1">-3.03 (.10)</td>
                        </tr>
                        <tr>
                          <td align="left" rowspan="1" colspan="1">lPHG2</td>
                          <td align="center" colspan="2" rowspan="1">-0.80</td>
                          <td align="center" rowspan="1" colspan="1">-11.13(.00)</td>
                          <td align="center" rowspan="1" colspan="1">-0.60</td>
                          <td align="center" rowspan="1" colspan="1">-5.47 (.00)</td>
                          <td align="center" colspan="2" rowspan="1">-0.25</td>
                          <td align="center" rowspan="1" colspan="1">-4.68 (.00)</td>
                          <td align="center" rowspan="1" colspan="1">-0.08</td>
                          <td align="center" rowspan="1" colspan="1">-0.89 (.74)</td>
                        </tr>
                        <tr>
                          <td align="left" rowspan="1" colspan="1">rLOC</td>
                          <td align="center" colspan="2" rowspan="1">-0.29</td>
                          <td align="center" rowspan="1" colspan="1">-2.04 (.00)</td>
                          <td align="center" rowspan="1" colspan="1">-0.42</td>
                          <td align="center" rowspan="1" colspan="1">-4.30 (.00)</td>
                          <td align="center" colspan="2" rowspan="1">-0.04</td>
                          <td align="center" rowspan="1" colspan="1">-0.61 (.46)</td>
                          <td align="center" rowspan="1" colspan="1">-0.11</td>
                          <td align="center" rowspan="1" colspan="1">-1.40 (.50)</td>
                        </tr>
                        <tr>
                          <td align="left" rowspan="1" colspan="1">lLOC</td>
                          <td align="center" colspan="2" rowspan="1">-0.36</td>
                          <td align="center" rowspan="1" colspan="1">-2.87 (.00)</td>
                          <td align="center" rowspan="1" colspan="1">-0.36</td>
                          <td align="center" rowspan="1" colspan="1">-4.66 (.00)</td>
                          <td align="center" colspan="2" rowspan="1">-0.12</td>
                          <td align="center" rowspan="1" colspan="1">-1.65 (.15)</td>
                          <td align="center" rowspan="1" colspan="1">0.00</td>
                          <td align="center" rowspan="1" colspan="1">-0.07 (.84)</td>
                        </tr>
                      </tbody>
                    </table>
                  </alternatives>
                </table-wrap>
                <p>While our main predictions concerned the MVPA results, we explored the possibility that our task led to particularly small univariate responses due to repetition suppression. Repetition suppression has been used as a tool to characterize functional specificity of neuronal populations, since neurons tend to adapt most to their preferred stimuli [<xref rid="pone.0205041.ref054" ref-type="bibr">54</xref>,<xref rid="pone.0205041.ref055" ref-type="bibr">55</xref>]. Previous studies have reported repetition suppression across runs for faces in face-selective areas [<xref rid="pone.0205041.ref056" ref-type="bibr">56</xref>–<xref rid="pone.0205041.ref058" ref-type="bibr">58</xref>].</p>
                <p>The present task required displacement judgments (which are not highly attention-demanding) and a great deal of stimulus repetition (each image repeating 12 times per run). This combination likely fostered repetition suppression, and perhaps to a greater extent within category, for cars and faces, than for objects, since cars are visually (and semantically) more homogeneous. Thus, given both the stimulus characteristics and the fact that face-selective areas of individuals with above-average car recognition ability should be more selective for cars than for objects, we might expect more repetition suppression for cars and faces than objects, resulting in a paradoxically low average activation for these categories.</p>
                <p>We investigated this possibility by calculating parameter weights for each run using fixation as a baseline (see supplemental <xref ref-type="supplementary-material" rid="pone.0205041.s002">S1 Table</xref>). In general, faces and cars had higher parameter weights than objects in the first perception run, and faces and cars showing a larger decrease than objects between run 1 and 2 (see <xref ref-type="fig" rid="pone.0205041.g005">Fig 5</xref>). Of interest was the prediction of more habituation for faces and cars than for objects in face-selective areas specifically. This was supported by an ANOVA using factors of ROI type (face-selective ROIs, PHG ROIs and LOC ROIs), with perception runs (Run1 and Run 2 with weights of 1 and -1), and category (face, car and object with weights of 1, 1 and -2). This led to a significant interaction between ROI type, run and category (F(2,38) = 33.39, <italic>p</italic> &lt; .0001, η<sub>p</sub><sup>2</sup> = .64). We unpacked this interaction with a run x condition ANOVA in each type of ROI. In face-selective ROIs, there was a run x category interaction (F(1,19 = 23.39, p = .0001, η<sub>p</sub><sup>2</sup> = .55), with more habituation for faces and cars than for objects. The same interaction was also significant in the PHG ROIs (F(1,19 = 9.75, p = .006, η<sub>p</sub><sup>2</sup> = .34), but in this case it reflected more habituation for objects than for faces and cars. The interaction was not significant in LOC ROIs (F(1,19 = 0.67, p = .42, η<sub>p</sub><sup>2</sup> = .03).</p>
                <fig id="pone.0205041.g005" orientation="portrait" position="float">
                  <object-id pub-id-type="doi">10.1371/journal.pone.0205041.g005</object-id>
                  <label>Fig 5</label>
                  <caption>
                    <p><bold>Average parameter weights for faces (dashed), objects (dotted) and cars (solid) plotted for perception runs 1 and 2 in combined face-selective regions (right, significant run*category interaction) and object-selective regions (left)</bold>. Fixation is used as a baseline for the face, object and car conditions and error bars show SEM values.</p>
                  </caption>
                  <graphic xlink:href="pone.0205041.g005"/>
                </fig>
                <p>Because repetition suppression is an indication of functional specificity (e.g., [<xref rid="pone.0205041.ref054" ref-type="bibr">54</xref>]), these results are consistent with stronger selectivity of face-selective areas for faces and cars than for objects in our sample and they suggest that mean responses across runs in this design are not a good indication of selectivity.</p>
              </sec>
            </sec>
            <sec sec-type="conclusions" id="sec024">
              <title>Discussion</title>
              <p>We set out to determine if the FFA can support imagery of non-face objects at the subordinate-level. To answer this question, we compared how well classifiers could distinguish between imagined faces, cars and objects in a sample of above-average car recognizers. We found that a classifier trained and tested on imagery runs can distinguish between imagined cars and imagined objects in some face-selective regions, suggesting that car representations exist in face-selective areas during visual imagery.</p>
              <p>In addition, a classifier trained on imagery runs can successfully distinguish perceived cars from perceived objects in the anterior FFA (FFA2), an area that has shown the most robust car expertise effects in prior work [<xref rid="pone.0205041.ref031" ref-type="bibr">31</xref>,<xref rid="pone.0205041.ref034" ref-type="bibr">34</xref>] and also seems particularly sensitive to experience for faces [<xref rid="pone.0205041.ref032" ref-type="bibr">32</xref>,<xref rid="pone.0205041.ref033" ref-type="bibr">33</xref>]. These across-task results further reveal that the representations of cars in our participants are at least in some way similar to those evoked during perception. Importantly however, our results refute the idea that face-selective areas are selectively recruited for face imagery [<xref rid="pone.0205041.ref004" ref-type="bibr">4</xref>]. Future work using an event-related design could further this finding by determining if similar results could be found when training and testing at the subordinate level.</p>
              <p>In fact, the results generally suggest that faces and cars can be decoded from objects in several non-face selective areas. Such results are consistent with the general idea that categorical representations are distributed (e.g., [<xref rid="pone.0205041.ref014" ref-type="bibr">14</xref>]). The parahippocampal gyrus, for instance, tends to show a different response to faces than non-face objects (Martin et al. 2013), has been found be engaged for objects of expertise [<xref rid="pone.0205041.ref025" ref-type="bibr">25</xref>], and its response to faces relative to objects predicts face recognition ability [<xref rid="pone.0205041.ref033" ref-type="bibr">33</xref>].</p>
              <p>Though our main analyses focused on distinguishing imagined cars from imagined objects when training and testing on imagery data, we should note that this within-task (imagery to imagery) classifier appeared to show somewhat weaker decoding of faces from objects than cars from objects. That is, imagined faces vs. imagined objects decoding was only above statistical significance in one face-selective region (lFFA2). We do not wish to over-interpret this relatively weak decoding for faces in this condition, because we do not have sufficient statistical power to show that decoding in lFFA2 is larger than that in other face-selective ROIs or that the decoding for cars was stronger than that for faces. Instead, we would emphasize that the cross-trained analyses (training on imagery and testing on perception) demonstrate that, in most face-selective areas, representations of imagined faces and objects are sufficiently similar to representations of their perceived counterparts to allow training of a classifier that can decode representations of perceived faces and objects. Perhaps what these results highlight is that there may be factors at play that we do not presently have a good way to measure. For instance, we know little about the relative difficulty of imagining objects from different categories, such as cars and faces, which could recruit different strategies, thereby facilitating decoding. While we can measure the neural correlates of imagery, there are few methodological approaches that allow us insight into the quality of the images our participants generate, aside from their similarity to perceptual representations.</p>
              <p>Though we made no predictions about the face versus car classification, we note that previous work found a negative correlation between car recognition ability and face versus car classification performance [<xref rid="pone.0205041.ref031" ref-type="bibr">31</xref>]. Here, we find that decoding faces from cars is possible in some face-selective areas for both our within-task classifier (bilateral FFA1s and FFA2s) and our across-task classifier (lFFA1, lFFA2 and lOFA, see Figs <xref ref-type="fig" rid="pone.0205041.g003">3</xref> and <xref ref-type="fig" rid="pone.0205041.g004">4</xref>). It is important to highlight that the present methods can reveal whether decoding is possible, but do not provide information as to the differences between representations, which are suggested by the well-below perfect decoding performance. Based on McGugin et al. [<xref rid="pone.0205041.ref031" ref-type="bibr">31</xref>], we might speculate that faces and cars would be more easily distinguished in car novices than in our current sample. It is certainly interesting that the rFFA2, which has emerged in other studies as the main area sensitive to expertise [<xref rid="pone.0205041.ref028" ref-type="bibr">28</xref>,<xref rid="pone.0205041.ref031" ref-type="bibr">31</xref>,<xref rid="pone.0205041.ref033" ref-type="bibr">33</xref>], can distinguish both perceived faces and perceived cars from perceived objects based on imagery training, but cannot distinguish faces vs. cars in the same across-task comparisons. This region may be where car and face representations are most similar in car experts, in imagery as in perception.</p>
              <p>We did not systematically compare within- and across-task analyses, because the across-task analysis uses more data (all three imagery runs are used for training the classifier instead of two runs, and all the perception runs are used for decoding rather than only one imagery run). However, a qualitative comparison suggests that decoding is not always best when more data is used, so it is interesting to speculate on this qualitative comparison. In the extreme, if the representations used during perception and imagery were identical, then we would expect across-task classification to always be better than within-task classification because more data is available. This is generally the case for the contrast of faces vs. objects, where decoding was better in across- than within-task analyses in almost every ROI (see <xref ref-type="supplementary-material" rid="pone.0205041.s001">S1 Fig</xref> for a qualitative comparison). It was also the case for decoding faces vs. cars in left face-selective ROIs and cars vs. objects in right object-selective ROIs. In some cases (most clearly in decoding faces vs. cars in object-selective areas) the across- and within-task classifiers performed similarly.</p>
              <p>However, another consideration when comparing the across- and within-task analyses is that the across-task analysis trains and decodes on different tasks, whereas the within-task analysis uses a leave-one-out approach in a homogeneous set of runs. In that sense, even though the within-task analysis uses less data, it may provide better decoding than the across-task analysis <italic>to the extent that the representations used during imagery and perception differ</italic>. It is interesting that within-task decoding appears to outperform across-task decoding mainly in the car vs. object comparison. This could mean that some of what our participants do during imagery may not overlap with perception (more for cars than faces and objects), and yet still be category specific (sufficient to distinguish cars from objects). For instance, despite being asked to imagine the specific images used during the perception task, each car expert may have his or her own preferred representation of the car models we used (e.g. instead of imagining our image of a Kia Forte, they imagine a different image of a Kia Forte). This is obviously speculative but it presents a challenge to the suggestion by Cichy et al [<xref rid="pone.0205041.ref005" ref-type="bibr">5</xref>] that all features of an imagined representation should overlap with those in perceived representations.</p>
              <p>In our univariate analysis, we did not entirely replicate previous findings [<xref rid="pone.0205041.ref004" ref-type="bibr">4</xref>–<xref rid="pone.0205041.ref006" ref-type="bibr">6</xref>] of greater activation for faces in face-selective areas during visual imagery, as we only found this effect in lFFA2. This discrepancy could be due to several factors. First, unlike prior work we divided our face-selective regions into subregions (FFA1 and FFA2), rather than picking the more face-selective peak or averaging the peaks together. Second, the contrast we used in our univariate analysis compared activation during face and car imagery to object imagery activation. This contrast differs from previously used contrasts of faces versus places [<xref rid="pone.0205041.ref004" ref-type="bibr">4</xref>], faces versus non-faces (bodies, objects, scenes and houses, [<xref rid="pone.0205041.ref005" ref-type="bibr">5</xref>]) and faces versus houses and chairs [<xref rid="pone.0205041.ref006" ref-type="bibr">6</xref>]. Additionally, O’Craven and Kanwisher [<xref rid="pone.0205041.ref004" ref-type="bibr">4</xref>] reported that in some participants, they did not find overlap between perception and imagery responses, perhaps due to lower visual imagery ability. Because our sample was selected to be above average on the VVIQ, this seems unlikely to explain the present results. Finally, our participants were men recruited to have above-average car recognition ability, which could explain some of these differences, although their average CFMT performance was not different from that in larger unselected samples (<italic>t</italic>(122) = 1.05; <italic>p</italic> = .30, d = 0.23, larger unselected sample of 104 from [<xref rid="pone.0205041.ref059" ref-type="bibr">59</xref>]). Regardless of the reason, our study was designed for multivariate analysis and prior work has shown multivariate analyses can be informative despite weak or absent univariate effects (e.g., [<xref rid="pone.0205041.ref060" ref-type="bibr">60</xref>]). Additionally, given that all our participants were above-average in car recognition (<xref rid="pone.0205041.t002" ref-type="table">Table 2</xref>), the fact that we did not find greater activation for perceived cars relative to perceived objects may seem odd given that many prior studies reported car expertise effects in the face-selective areas of car experts. However, prior work relied on correlations between car recognition ability and selectivity for cars in face-selective areas rather than absolute values, so it is difficult to compare across these designs (e.g., [<xref rid="pone.0205041.ref025" ref-type="bibr">25</xref>,<xref rid="pone.0205041.ref027" ref-type="bibr">27</xref>,<xref rid="pone.0205041.ref028" ref-type="bibr">28</xref>]). Importantly, in the present participants, who were car experts, cars showed habituation that was similar to faces both in face-selective areas (more habituation than for objects) and in the PHG (less habituation than for objects). Future work could compare repetition suppression for cars in car experts and novices to confirm our interpretation of these results as resulting from car expertise. While we did not vary car expertise in this study, we attribute decoding of cars from objects in FFA during imagery to perceptual expertise. However, we acknowledge that if car novices were tested while they imagined various cars, they could also recruit face-selective areas for this task.</p>
              <p>Our results provide new evidence that visual imagery mirrors visual perception, not only in early visual areas [<xref rid="pone.0205041.ref022" ref-type="bibr">22</xref>], but also further down the visual processing stream. This also applies to a category for which detailed representations would have likely been acquired relatively late in life. Therefore, it appears that experience impacts not only how objects are represented in the brain during perception, but also during imagery. The nature of these representations, however, remains unclear. Though the work reported here and previous work shows that visual imagery can be characterized as a top-down re-instantiation of vision, it has yet to be determined whether these mental image representations are necessarily “visual” or if they draw more upon semantic representations associated with visual images.</p>
            </sec>
            <sec sec-type="supplementary-material" id="sec025">
              <title>Supporting information</title>
              <supplementary-material content-type="local-data" id="pone.0205041.s001">
                <label>S1 Fig</label>
                <caption>
                  <p><bold>Average accuracies of the across-task classifier (train imagery-test perception, <xref ref-type="fig" rid="pone.0205041.g004">Fig 4</xref>) overlaid on top of the within-task classifier (train imagery-test imagery, <xref ref-type="fig" rid="pone.0205041.g003">Fig 3</xref>) for object versus car (left) and face versus object (middle) and face versus car (right) two-way classifications in face-selective ROIs (upper row) and object-selective ROIs (lower row)</bold>. Across-task classifier is in greyscale and translucent.</p>
                  <p>(DOCX)</p>
                </caption>
                <media xlink:href="pone.0205041.s001.docx">
                  <caption>
                    <p>Click here for additional data file.</p>
                  </caption>
                </media>
              </supplementary-material>
              <supplementary-material content-type="local-data" id="pone.0205041.s002">
                <label>S1 Table</label>
                <caption>
                  <title>Run 1 and Run 2 parameter weights for the two perception runs.</title>
                  <p>Fixation is used as the baseline. Run 2 parameter weights that are significantly lower than Run 1 parameter weights (one-tailed) are denoted with an asterisk.</p>
                  <p>(DOCX)</p>
                </caption>
                <media xlink:href="pone.0205041.s002.docx">
                  <caption>
                    <p>Click here for additional data file.</p>
                  </caption>
                </media>
              </supplementary-material>
              <supplementary-material content-type="local-data" id="pone.0205041.s003">
                <label>S2 Table</label>
                <caption>
                  <title>Reports average classifier accuracies when the classifier is trained on imagery runs and then tested on perception runs and vice versa.</title>
                  <p>In both Tables <xref rid="pone.0205041.t002" ref-type="table">2</xref> and <xref rid="pone.0205041.t003" ref-type="table">3</xref> we also report classification accuracies for an early visual cortex region (EVC). These regions (one right and one left) were a single set of 27 contiguous function voxels (729 structural voxels each; right EVC—mean X = 26.22 (SD = 2.94), mean Y = -88.22, (SD = 3.01), mean Z = -10.22 (SD = 2.98); left EVC—mean X = -24.67 (SD = 2.98), mean Y = -89.56 (SD = 2.83), mean Z = -9.00 (SD = 3.06)). The same set of voxels was used for each participant. All <italic>t</italic>-tests are one-tailed.</p>
                  <p>(DOCX)</p>
                </caption>
                <media xlink:href="pone.0205041.s003.docx">
                  <caption>
                    <p>Click here for additional data file.</p>
                  </caption>
                </media>
              </supplementary-material>
              <supplementary-material content-type="local-data" id="pone.0205041.s004">
                <label>S3 Table</label>
                <caption>
                  <title>Reports average classifier accuracies when the classifier is trained on perception and then tested on imagery runs.</title>
                  <p>(DOCX)</p>
                </caption>
                <media xlink:href="pone.0205041.s004.docx">
                  <caption>
                    <p>Click here for additional data file.</p>
                  </caption>
                </media>
              </supplementary-material>
            </sec>
          </body>
          <back>
            <ref-list>
              <title>References</title>
              <ref id="pone.0205041.ref001">
                <label>1</label>
                <mixed-citation publication-type="journal"><name><surname>D’Ascenzo</surname><given-names>S</given-names></name>, <name><surname>Tommasi</surname><given-names>L</given-names></name>, <name><surname>Laeng</surname><given-names>B</given-names></name>. <article-title>Imagining sex and adapting to it: Different aftereffects after perceiving versus imagining faces</article-title>. <source>Vision Res</source> [Internet]. <year>2014</year>;<volume>96</volume>(<issue>January</issue>):<fpage>45</fpage>–<lpage>52</lpage>. Available from: <pub-id pub-id-type="doi">10.1016/j.visres.2014.01.002</pub-id><pub-id pub-id-type="pmid">24440811</pub-id></mixed-citation>
              </ref>
              <ref id="pone.0205041.ref002">
                <label>2</label>
                <mixed-citation publication-type="journal"><name><surname>Pearson</surname><given-names>J</given-names></name>, <name><surname>Clifford</surname><given-names>CWG</given-names></name>, <name><surname>Tong</surname><given-names>F</given-names></name>. <article-title>The Functional Impact of Mental Imagery on Conscious Perception</article-title>. <source>Curr Biol</source>. <year>2008</year>;<volume>18</volume>(<issue>13</issue>):<fpage>982</fpage>–<lpage>6</lpage>. <pub-id pub-id-type="doi">10.1016/j.cub.2008.05.048</pub-id>
<?supplied-pmid 18583132?><pub-id pub-id-type="pmid">18583132</pub-id></mixed-citation>
              </ref>
              <ref id="pone.0205041.ref003">
                <label>3</label>
                <mixed-citation publication-type="journal"><name><surname>Tartaglia</surname><given-names>EM</given-names></name>, <name><surname>Bamert</surname><given-names>L</given-names></name>, <name><surname>Mast</surname><given-names>FW</given-names></name>, <name><surname>Herzog</surname><given-names>MH</given-names></name>. <article-title>Human Perceptual Learning by Mental Imagery</article-title>. <source>Curr Biol</source> [Internet]. <year>2009</year>;<volume>19</volume>(<issue>24</issue>):<fpage>2081</fpage>–<lpage>5</lpage>. Available from: <pub-id pub-id-type="doi">10.1016/j.cub.2009.10.060</pub-id>
<?supplied-pmid 19962313?><pub-id pub-id-type="pmid">19962313</pub-id></mixed-citation>
              </ref>
              <ref id="pone.0205041.ref004">
                <label>4</label>
                <mixed-citation publication-type="journal"><name><surname>O&amp;apos;Craven</surname><given-names>KM</given-names></name>, <name><surname>Kanwisher</surname><given-names>N</given-names></name>. <article-title>Mental imagery of faces and places activates corresponding stimulus-specific brain regions</article-title>. <source>J Cogn Neurosci</source>. <year>2000</year>;<volume>12</volume>(<issue>6</issue>):<fpage>1013</fpage>–<lpage>23</lpage>. <?supplied-pmid 11177421?><pub-id pub-id-type="pmid">11177421</pub-id></mixed-citation>
              </ref>
              <ref id="pone.0205041.ref005">
                <label>5</label>
                <mixed-citation publication-type="journal"><name><surname>Cichy</surname><given-names>RM</given-names></name>, <name><surname>Heinzle</surname><given-names>J</given-names></name>, <name><surname>Haynes</surname><given-names>JD</given-names></name>. <article-title>Imagery and perception share cortical representations of content and location</article-title>. <source>Cereb Cortex</source>. <year>2011</year>;<volume>22</volume>(<issue>2</issue>):<fpage>372</fpage>–<lpage>80</lpage>. <pub-id pub-id-type="doi">10.1093/cercor/bhr106</pub-id>
<?supplied-pmid 21666128?><pub-id pub-id-type="pmid">21666128</pub-id></mixed-citation>
              </ref>
              <ref id="pone.0205041.ref006">
                <label>6</label>
                <mixed-citation publication-type="journal"><name><surname>Ishai</surname><given-names>A</given-names></name>, <name><surname>Haxby</surname><given-names>J V</given-names></name>., <name><surname>Ungerleider</surname><given-names>LG</given-names></name>. <article-title>Visual imagery of famous faces: Effects of memory and attention revealed by fMRI</article-title>. <source>Neuroimage</source>. <year>2002</year>;<volume>17</volume>(<issue>4</issue>):<fpage>1729</fpage>–<lpage>41</lpage>. <?supplied-pmid 12498747?><pub-id pub-id-type="pmid">12498747</pub-id></mixed-citation>
              </ref>
              <ref id="pone.0205041.ref007">
                <label>7</label>
                <mixed-citation publication-type="journal"><name><surname>Johnson</surname><given-names>MR</given-names></name>, <name><surname>Johnson</surname><given-names>MK</given-names></name>. <article-title>Decoding individual natural scene representations during perception and imagery</article-title>. <source>Front Hum Neurosci</source> [Internet]. <year>2014</year>;<volume>8</volume>(<issue>February</issue>):<fpage>1</fpage>–<lpage>14</lpage>. Available from: <ext-link ext-link-type="uri" xlink:href="http://journal.frontiersin.org/article/10.3389/fnhum.2014.00059/abstract">http://journal.frontiersin.org/article/10.3389/fnhum.2014.00059/abstract</ext-link><pub-id pub-id-type="pmid">24474914</pub-id></mixed-citation>
              </ref>
              <ref id="pone.0205041.ref008">
                <label>8</label>
                <mixed-citation publication-type="journal"><name><surname>Kaas</surname><given-names>A</given-names></name>, <name><surname>Weigelt</surname><given-names>S</given-names></name>, <name><surname>Roebroeck</surname><given-names>A</given-names></name>, <name><surname>Kohler</surname><given-names>A</given-names></name>, <name><surname>Muckli</surname><given-names>L</given-names></name>. <article-title>Imagery of a moving object: The role of occipital cortex and human MT/V5+</article-title>. <source>Neuroimage</source> [Internet]. <year>2010</year>;<volume>49</volume>(<issue>1</issue>):<fpage>794</fpage>–<lpage>804</lpage>. Available from: <pub-id pub-id-type="doi">10.1016/j.neuroimage.2009.07.055</pub-id>
<?supplied-pmid 19646536?><pub-id pub-id-type="pmid">19646536</pub-id></mixed-citation>
              </ref>
              <ref id="pone.0205041.ref009">
                <label>9</label>
                <mixed-citation publication-type="journal"><name><surname>Goebel</surname><given-names>R</given-names></name>, <name><surname>Khorram-Sefat</surname><given-names>D</given-names></name>, <name><surname>Muckli</surname><given-names>L</given-names></name>, <name><surname>Hacker</surname><given-names>H</given-names></name>, <name><surname>Singer</surname><given-names>W</given-names></name>. <article-title>The constructive nature of vision: Direct evidence from functional magnetic resonance imaging studies of apparent motion and motion imagery</article-title>. <source>Eur J Neurosci</source>. <year>1998</year>;<volume>10</volume>(<issue>5</issue>):<fpage>1563</fpage>–<lpage>73</lpage>. <?supplied-pmid 9751129?><pub-id pub-id-type="pmid">9751129</pub-id></mixed-citation>
              </ref>
              <ref id="pone.0205041.ref010">
                <label>10</label>
                <mixed-citation publication-type="journal"><name><surname>Emmerling</surname><given-names>TC</given-names></name>, <name><surname>Zimmermann</surname><given-names>J</given-names></name>, <name><surname>Sorger</surname><given-names>B</given-names></name>, <name><surname>Frost</surname><given-names>MA</given-names></name>, <name><surname>Goebel</surname><given-names>R</given-names></name>. <article-title>Decoding the direction of imagined visual motion using 7 T ultra-high field fMRI</article-title>. <source>Neuroimage</source> [Internet]. <year>2016</year>;<volume>125</volume>:<fpage>61</fpage>–<lpage>73</lpage>. Available from: <pub-id pub-id-type="doi">10.1016/j.neuroimage.2015.10.022</pub-id>
<?supplied-pmid 26481673?><pub-id pub-id-type="pmid">26481673</pub-id></mixed-citation>
              </ref>
              <ref id="pone.0205041.ref011">
                <label>11</label>
                <mixed-citation publication-type="journal"><name><surname>Lee</surname><given-names>S</given-names></name>, <name><surname>Kravitz</surname><given-names>DJ</given-names></name>, <name><surname>Baker</surname><given-names>CI</given-names></name>. <article-title>Disentangling visual imagery and perception of real-world objects</article-title>. <source>Neuroimage</source>. <year>2012</year>;<volume>59</volume>(<issue>4</issue>):<fpage>4064</fpage>–<lpage>73</lpage>. <pub-id pub-id-type="doi">10.1016/j.neuroimage.2011.10.055</pub-id>
<?supplied-pmid 22040738?><pub-id pub-id-type="pmid">22040738</pub-id></mixed-citation>
              </ref>
              <ref id="pone.0205041.ref012">
                <label>12</label>
                <mixed-citation publication-type="journal"><name><surname>Stokes</surname><given-names>M</given-names></name>, <name><surname>Thompson</surname><given-names>R</given-names></name>, <name><surname>Cusack</surname><given-names>R</given-names></name>, <name><surname>Duncan</surname><given-names>J</given-names></name>. <article-title>Top-Down Activation of Shape-Specific Population Codes in Visual Cortex during Mental Imagery</article-title>. <source>J Neurosci</source> [Internet]. <year>2009</year>;<volume>29</volume>(<issue>5</issue>):<fpage>1565</fpage>–<lpage>72</lpage>. Available from: <ext-link ext-link-type="uri" xlink:href="http://www.jneurosci.org/cgi/doi/10.1523/JNEUROSCI.4657-08.2009">http://www.jneurosci.org/cgi/doi/10.1523/JNEUROSCI.4657-08.2009</ext-link>
<pub-id pub-id-type="doi">10.1523/JNEUROSCI.4657-08.2009</pub-id>
<?supplied-pmid 19193903?><pub-id pub-id-type="pmid">19193903</pub-id></mixed-citation>
              </ref>
              <ref id="pone.0205041.ref013">
                <label>13</label>
                <mixed-citation publication-type="journal"><name><surname>Albers</surname><given-names>AM</given-names></name>, <name><surname>Kok</surname><given-names>P</given-names></name>, <name><surname>Toni</surname><given-names>I</given-names></name>, <name><surname>Dijkerman</surname><given-names>HC</given-names></name>, <name><surname>de Lange</surname><given-names>FP</given-names></name>. <article-title>Report Shared Representations for Working Memory and Mental Imagery in Early Visual Cortex</article-title>. <source>Curr Biol</source> [Internet]. <year>2013</year>;<volume>23</volume>(<issue>15</issue>):<fpage>1427</fpage>–<lpage>31</lpage>. Available from: <pub-id pub-id-type="doi">10.1016/j.cub.2013.05.065</pub-id>
<?supplied-pmid 23871239?><pub-id pub-id-type="pmid">23871239</pub-id></mixed-citation>
              </ref>
              <ref id="pone.0205041.ref014">
                <label>14</label>
                <mixed-citation publication-type="journal"><name><surname>Reddy</surname><given-names>L</given-names></name>, <name><surname>Tsuchiya</surname><given-names>N</given-names></name>, <name><surname>Serre</surname><given-names>T</given-names></name>. <article-title>Reading the mind’s eye: decoding category information during mental imagery</article-title>. <source>Neuroimage</source>. <year>2010</year>;<volume>50</volume>(<issue>2</issue>):<fpage>818</fpage>–<lpage>25</lpage>. <pub-id pub-id-type="doi">10.1016/j.neuroimage.2009.11.084</pub-id>
<?supplied-pmid 20004247?><pub-id pub-id-type="pmid">20004247</pub-id></mixed-citation>
              </ref>
              <ref id="pone.0205041.ref015">
                <label>15</label>
                <mixed-citation publication-type="journal"><name><surname>Mitchell</surname><given-names>DJ</given-names></name>, <name><surname>Cusack</surname><given-names>R</given-names></name>. <article-title>Semantic and emotional content of imagined representations in human occipitotemporal cortex</article-title>. <source>Sci Rep</source> [Internet]. <year>2016</year>;<volume>6</volume>(December 2015):<fpage>1</fpage>–<lpage>14</lpage>. Available from: <pub-id pub-id-type="doi">10.1038/s41598-016-0001-8</pub-id><pub-id pub-id-type="pmid">28442746</pub-id></mixed-citation>
              </ref>
              <ref id="pone.0205041.ref016">
                <label>16</label>
                <mixed-citation publication-type="journal"><name><surname>Horikawa</surname><given-names>T</given-names></name>, <name><surname>Kamitani</surname><given-names>Y</given-names></name>. <article-title>Generic decoding of seen and imagined objects using hierarchical visual features</article-title>. <source>Nat Commun</source> [Internet]. <year>2017</year>;<volume>8</volume>(<issue>May</issue>):<fpage>1</fpage>–<lpage>15</lpage>. Available from: <pub-id pub-id-type="doi">10.1038/ncomms15037</pub-id><pub-id pub-id-type="pmid">28232747</pub-id></mixed-citation>
              </ref>
              <ref id="pone.0205041.ref017">
                <label>17</label>
                <mixed-citation publication-type="journal"><name><surname>Naselaris</surname><given-names>T</given-names></name>, <name><surname>Olman</surname><given-names>CA</given-names></name>, <name><surname>Stansbury</surname><given-names>DE</given-names></name>, <name><surname>Ugurbil</surname><given-names>K</given-names></name>, <name><surname>Gallant</surname><given-names>JL</given-names></name>. <article-title>A voxel-wise encoding model for early visual areas decodes mental images of remembered scenes</article-title>. <source>Neuroimage</source> [Internet]. <year>2015</year>;<volume>105</volume>:<fpage>215</fpage>–<lpage>28</lpage>. Available from: <pub-id pub-id-type="doi">10.1016/j.neuroimage.2014.10.018</pub-id>
<?supplied-pmid 25451480?><pub-id pub-id-type="pmid">25451480</pub-id></mixed-citation>
              </ref>
              <ref id="pone.0205041.ref018">
                <label>18</label>
                <mixed-citation publication-type="journal"><name><surname>Cui</surname><given-names>X</given-names></name>, <name><surname>Jeter</surname><given-names>CB</given-names></name>, <name><surname>Yang</surname><given-names>D</given-names></name>, <name><surname>Montague</surname><given-names>PR</given-names></name>, <name><surname>Eagleman</surname><given-names>DM</given-names></name>. <article-title>Vividness of mental imagery: Individual variability can be measured objectively</article-title>. <source>Vision Res</source>. <year>2007</year>;<volume>47</volume>(<issue>4</issue>):<fpage>474</fpage>–<lpage>8</lpage>. <pub-id pub-id-type="doi">10.1016/j.visres.2006.11.013</pub-id>
<?supplied-pmid 17239915?><pub-id pub-id-type="pmid">17239915</pub-id></mixed-citation>
              </ref>
              <ref id="pone.0205041.ref019">
                <label>19</label>
                <mixed-citation publication-type="journal"><name><surname>Vetter</surname><given-names>P</given-names></name>, <name><surname>Smith</surname><given-names>FW</given-names></name>, <name><surname>Muckli</surname><given-names>L</given-names></name>. <article-title>Decoding sound and imagery content in early visual cortex</article-title>. <source>Curr Biol</source> [Internet]. <year>2014</year>;<volume>24</volume>(<issue>11</issue>):<fpage>1256</fpage>–<lpage>62</lpage>. Available from: <pub-id pub-id-type="doi">10.1016/j.cub.2014.04.020</pub-id>
<?supplied-pmid 24856208?><pub-id pub-id-type="pmid">24856208</pub-id></mixed-citation>
              </ref>
              <ref id="pone.0205041.ref020">
                <label>20</label>
                <mixed-citation publication-type="journal"><name><surname>Klein</surname><given-names>I</given-names></name>, <name><surname>Paradis</surname><given-names>AL</given-names></name>, <name><surname>Poline</surname><given-names>JB</given-names></name>, <name><surname>Kosslyn</surname><given-names>SM</given-names></name>, <name><surname>Le Bihan</surname><given-names>D</given-names></name>. <article-title>Transient activity in the human calcarine cortex during visual-mental imagery: an event-related fMRI study</article-title>. <source>J Cogn Neurosci</source>. <year>2000</year>;<volume>12</volume>.</mixed-citation>
              </ref>
              <ref id="pone.0205041.ref021">
                <label>21</label>
                <mixed-citation publication-type="journal"><name><surname>Bergmann</surname><given-names>J</given-names></name>, <name><surname>Genç</surname><given-names>E</given-names></name>, <name><surname>Kohler</surname><given-names>A</given-names></name>, <name><surname>Singer</surname><given-names>W</given-names></name>, <name><surname>Pearson</surname><given-names>J</given-names></name>. <article-title>Smaller primary visual cortex is associated with stronger, but less precise mental imagery</article-title>. <source>Cereb Cortex</source>. <year>2016</year>;<volume>26</volume>(<issue>9</issue>):<fpage>3838</fpage>–<lpage>50</lpage>. <pub-id pub-id-type="doi">10.1093/cercor/bhv186</pub-id>
<?supplied-pmid 26286919?><pub-id pub-id-type="pmid">26286919</pub-id></mixed-citation>
              </ref>
              <ref id="pone.0205041.ref022">
                <label>22</label>
                <mixed-citation publication-type="journal"><name><surname>Kosslyn</surname><given-names>SM</given-names></name>, <name><surname>Thompson</surname><given-names>WL</given-names></name>. <article-title>When is early visual cortex activated during visual mental imagery?</article-title><source>Psychol Bull</source>. <year>2003</year>;<volume>129</volume>(<issue>5</issue>).</mixed-citation>
              </ref>
              <ref id="pone.0205041.ref023">
                <label>23</label>
                <mixed-citation publication-type="journal"><name><surname>Dijkstra</surname><given-names>N</given-names></name>, <name><surname>Bosch</surname><given-names>SE</given-names></name>, <name><surname>van Gerven</surname><given-names>MAJ</given-names></name>. <article-title>Vividness of Visual Imagery Depends on the Neural Overlap with Perception in Visual Areas</article-title>. <source>J Neurosci</source> [Internet]. <year>2017</year>;<volume>37</volume>(<issue>5</issue>):<fpage>1367</fpage>–<lpage>73</lpage>. Available from: <ext-link ext-link-type="uri" xlink:href="http://www.jneurosci.org/lookup/doi/10.1523/JNEUROSCI.3022-16.2016">http://www.jneurosci.org/lookup/doi/10.1523/JNEUROSCI.3022-16.2016</ext-link>
<pub-id pub-id-type="doi">10.1523/JNEUROSCI.3022-16.2016</pub-id>
<?supplied-pmid 28073940?><pub-id pub-id-type="pmid">28073940</pub-id></mixed-citation>
              </ref>
              <ref id="pone.0205041.ref024">
                <label>24</label>
                <mixed-citation publication-type="journal"><name><surname>Slotnick</surname><given-names>SD</given-names></name>, <name><surname>Thompson</surname><given-names>WL</given-names></name>, <name><surname>Kosslyn</surname><given-names>SM</given-names></name>. <article-title>Visual memory and visual mental imagery recruit common control and sensory regions of the brain</article-title>. <source>Cogn Neurosci</source>. <year>2012</year>;<volume>3</volume>(<issue>1</issue>):<fpage>14</fpage>–<lpage>20</lpage>. <pub-id pub-id-type="doi">10.1080/17588928.2011.578210</pub-id>
<?supplied-pmid 24168646?><pub-id pub-id-type="pmid">24168646</pub-id></mixed-citation>
              </ref>
              <ref id="pone.0205041.ref025">
                <label>25</label>
                <mixed-citation publication-type="journal"><name><surname>Gauthier</surname><given-names>I</given-names></name>, <name><surname>Skudlarski</surname><given-names>P</given-names></name>, <name><surname>Gore</surname><given-names>JC</given-names></name>, <name><surname>Anderson</surname><given-names>AW</given-names></name>. <article-title>Expertise for cars and birds recruits brain areas involved in face recognition</article-title>. <source>Nat Neurosci</source> [Internet]. <year>2000</year>;<volume>3</volume>(<issue>2</issue>):<fpage>191</fpage>–<lpage>7</lpage>. Available from: <ext-link ext-link-type="uri" xlink:href="http://eutils.ncbi.nlm.nih.gov/entrez/eutils/elink.fcgi?dbfrom=pubmed&amp;id=10649576&amp;retmode=ref&amp;cmd=prlinks%5Cnpapers3://publication/doi/10.1038/72140">http://eutils.ncbi.nlm.nih.gov/entrez/eutils/elink.fcgi?dbfrom=pubmed&amp;id=10649576&amp;retmode=ref&amp;cmd=prlinks%5Cnpapers3://publication/doi/10.1038/72140</ext-link>
<pub-id pub-id-type="doi">10.1038/72140</pub-id>
<?supplied-pmid 10649576?><pub-id pub-id-type="pmid">10649576</pub-id></mixed-citation>
              </ref>
              <ref id="pone.0205041.ref026">
                <label>26</label>
                <mixed-citation publication-type="journal"><name><surname>Gauthier</surname><given-names>I</given-names></name>, <name><surname>Tarr</surname><given-names>MJ</given-names></name>, <name><surname>Moylan</surname><given-names>J</given-names></name>, <name><surname>Skudlarski</surname><given-names>P</given-names></name>, <name><surname>Gore</surname><given-names>JC</given-names></name>, <name><surname>Anderson</surname><given-names>a W</given-names></name>. <article-title>The fusiform “face area” is part of a network that processes faces at the individual level</article-title>. <source>J Cogn Neurosci</source>. <year>2000</year>;<volume>12</volume>(<issue>3</issue>):<fpage>495</fpage>–<lpage>504</lpage>. <?supplied-pmid 10931774?><pub-id pub-id-type="pmid">10931774</pub-id></mixed-citation>
              </ref>
              <ref id="pone.0205041.ref027">
                <label>27</label>
                <mixed-citation publication-type="journal"><name><surname>Xu</surname><given-names>Y</given-names></name>. <article-title>Revisiting the role of the fusiform face area in visual expertise</article-title>. <source>Cereb Cortex</source>. <year>2005</year>;<volume>15</volume>(<issue>8</issue>):<fpage>1234</fpage>–<lpage>42</lpage>. <pub-id pub-id-type="doi">10.1093/cercor/bhi006</pub-id>
<?supplied-pmid 15677350?><pub-id pub-id-type="pmid">15677350</pub-id></mixed-citation>
              </ref>
              <ref id="pone.0205041.ref028">
                <label>28</label>
                <mixed-citation publication-type="journal"><name><surname>McGugin</surname><given-names>RW</given-names></name>, <name><surname>Newton</surname><given-names>AT</given-names></name>, <name><surname>Gore</surname><given-names>JC</given-names></name>, <name><surname>Gauthier</surname><given-names>I</given-names></name>. <article-title>Robust expertise effects in right FFA</article-title>. <source>Neuropsychologia</source> [Internet]. <year>2014</year>;<volume>63</volume>:<fpage>135</fpage>–<lpage>44</lpage>. Available from: <pub-id pub-id-type="doi">10.1016/j.neuropsychologia.2014.08.029</pub-id>
<?supplied-pmid 25192631?><pub-id pub-id-type="pmid">25192631</pub-id></mixed-citation>
              </ref>
              <ref id="pone.0205041.ref029">
                <label>29</label>
                <mixed-citation publication-type="journal"><name><surname>Pinsk</surname><given-names>MA</given-names></name>, <name><surname>Arcaro</surname><given-names>M</given-names></name>, <name><surname>Weiner</surname><given-names>KS</given-names></name>, <name><surname>Kalkus</surname><given-names>JF</given-names></name>, <name><surname>Inati</surname><given-names>SJ</given-names></name>, <name><surname>Gross</surname><given-names>CG</given-names></name>, <etal>et al</etal><article-title>Neural Representations of Faces and Body Parts in Macaque and Human Cortex: A Comparative fMRI Study</article-title>. <source>J Neurophysiol</source> [Internet]. <year>2008</year>;<volume>101</volume>(<issue>5</issue>):<fpage>2581</fpage>–<lpage>600</lpage>. Available from: <ext-link ext-link-type="uri" xlink:href="http://jn.physiology.org/cgi/doi/10.1152/jn.91198.2008">http://jn.physiology.org/cgi/doi/10.1152/jn.91198.2008</ext-link></mixed-citation>
              </ref>
              <ref id="pone.0205041.ref030">
                <label>30</label>
                <mixed-citation publication-type="journal"><name><surname>Weiner</surname><given-names>KS</given-names></name>, <name><surname>Sayres</surname><given-names>R</given-names></name>, <name><surname>Vinberg</surname><given-names>J</given-names></name>, <name><surname>Grill-Spector</surname><given-names>K</given-names></name>. <article-title>fMRI-Adaptation and Category Selectivity in Human Ventral Temporal Cortex: Regional Differences Across Time Scales</article-title>. <source>J Neurophysiol</source> [Internet]. <year>2010</year>;<volume>103</volume>(<issue>6</issue>):<fpage>3349</fpage>–<lpage>65</lpage>. Available from: <ext-link ext-link-type="uri" xlink:href="http://jn.physiology.org/cgi/doi/10.1152/jn.01108.2009">http://jn.physiology.org/cgi/doi/10.1152/jn.01108.2009</ext-link>
<pub-id pub-id-type="doi">10.1152/jn.01108.2009</pub-id>
<?supplied-pmid 20375251?><pub-id pub-id-type="pmid">20375251</pub-id></mixed-citation>
              </ref>
              <ref id="pone.0205041.ref031">
                <label>31</label>
                <mixed-citation publication-type="journal"><name><surname>McGugin</surname><given-names>RW</given-names></name>, <name><surname>Van Gulick</surname><given-names>AE</given-names></name>, <name><surname>Tamber-Rosenau</surname><given-names>BJ</given-names></name>, <name><surname>Ross</surname><given-names>DA</given-names></name>, <name><surname>Gauthier</surname><given-names>I</given-names></name>. <article-title>Expertise effects in face-selective areas are robust to clutter and diverted attention, but not to competition</article-title>. <source>Cereb Cortex</source>. <year>2015</year>;<volume>25</volume>(<issue>9</issue>):<fpage>2610</fpage>–<lpage>22</lpage>. <pub-id pub-id-type="doi">10.1093/cercor/bhu060</pub-id>
<?supplied-pmid 24682187?><pub-id pub-id-type="pmid">24682187</pub-id></mixed-citation>
              </ref>
              <ref id="pone.0205041.ref032">
                <label>32</label>
                <mixed-citation publication-type="journal"><name><surname>Golarai</surname><given-names>G</given-names></name>, <name><surname>Liberman</surname><given-names>A</given-names></name>, <name><surname>Grill-Spector</surname><given-names>K</given-names></name>. <article-title>Experience Shapes the Development of Neural Substrates of Face Processing in Human Ventral Temporal Cortex</article-title>. <source>Cereb Cortex</source>. <year>2017</year>;<volume>27</volume>(<issue>2</issue>):<fpage>1229</fpage>–<lpage>44</lpage>. <pub-id pub-id-type="doi">10.1093/cercor/bhv314</pub-id>
<?supplied-pmid 26683171?><pub-id pub-id-type="pmid">26683171</pub-id></mixed-citation>
              </ref>
              <ref id="pone.0205041.ref033">
                <label>33</label>
                <mixed-citation publication-type="journal"><name><surname>McGugin</surname><given-names>RW</given-names></name>, <name><surname>Ryan</surname><given-names>KF</given-names></name>, <name><surname>Tamber-Rosenau</surname><given-names>BJ</given-names></name>, <name><surname>Gauthier</surname><given-names>I</given-names></name>. <article-title>The Role of Experience in the Face-Selective Response in Right FFA</article-title>. <source>Cereb Cortex</source> [Internet]. <year>2017</year>;<fpage>1</fpage>–<lpage>14</lpage>. Available from: <ext-link ext-link-type="uri" xlink:href="https://academic.oup.com/cercor/article-lookup/doi/10.1093/cercor/bhx113">https://academic.oup.com/cercor/article-lookup/doi/10.1093/cercor/bhx113</ext-link></mixed-citation>
              </ref>
              <ref id="pone.0205041.ref034">
                <label>34</label>
                <mixed-citation publication-type="journal"><name><surname>McGugin</surname><given-names>RW</given-names></name>, <name><surname>Gauthier</surname><given-names>I</given-names></name>. <article-title>The reliability of individual differences in face-selective responses in the fusiform gyrus and their relation to face recognition ability</article-title>. <source>Brain Imaging Behav</source> [Internet]. <year>2016</year>;<volume>10</volume>(<issue>3</issue>):<fpage>707</fpage>–<lpage>18</lpage>. Available from: <pub-id pub-id-type="doi">10.1007/s11682-015-9467-4</pub-id>
<?supplied-pmid 26553580?><pub-id pub-id-type="pmid">26553580</pub-id></mixed-citation>
              </ref>
              <ref id="pone.0205041.ref035">
                <label>35</label>
                <mixed-citation publication-type="journal"><name><surname>Sunday</surname><given-names>MA</given-names></name>, <name><surname>McGugin</surname><given-names>RW</given-names></name>, <name><surname>Gauthier</surname><given-names>I</given-names></name>. <article-title>Domain-specific reports of visual imagery vividness are not related to perceptual expertise</article-title>. <source>Behav Res Methods</source> [Internet]. <year>2017</year>;<volume>49</volume>(<issue>2</issue>):<fpage>733</fpage>–<lpage>738</lpage>. Available from: <ext-link ext-link-type="uri" xlink:href="http://link.springer.com/10.3758/s13428-016-0730-4">http://link.springer.com/10.3758/s13428-016-0730-4</ext-link>
<pub-id pub-id-type="doi">10.3758/s13428-016-0730-4</pub-id>
<?supplied-pmid 27059364?><pub-id pub-id-type="pmid">27059364</pub-id></mixed-citation>
              </ref>
              <ref id="pone.0205041.ref036">
                <label>36</label>
                <mixed-citation publication-type="journal"><name><surname>Norman</surname><given-names>KA</given-names></name>, <name><surname>Polyn</surname><given-names>SM</given-names></name>, <name><surname>Detre</surname><given-names>GJ</given-names></name>, <name><surname>Haxby</surname><given-names>J V</given-names></name>. <article-title>Beyond mind-reading: multi-voxel pattern analysis of fMRI data</article-title>. <source>Trends Cogn Sci</source>. <year>2006</year>;<volume>10</volume>(<issue>9</issue>):<fpage>424</fpage>–<lpage>30</lpage>. <pub-id pub-id-type="doi">10.1016/j.tics.2006.07.005</pub-id>
<?supplied-pmid 16899397?><pub-id pub-id-type="pmid">16899397</pub-id></mixed-citation>
              </ref>
              <ref id="pone.0205041.ref037">
                <label>37</label>
                <mixed-citation publication-type="journal"><name><surname>Faul</surname><given-names>F</given-names></name>, <name><surname>Erdfelder</surname><given-names>E</given-names></name>, <name><surname>Lang</surname><given-names>AG</given-names></name>, <name><surname>Buchner</surname><given-names>A</given-names></name>. <article-title>G* Power 3: A flexible statistical power analysis program for the social, behavioral, and biomedical sciences</article-title>. <source>Behav Res Methods</source>. <year>2007</year>;<volume>39</volume>(<issue>2</issue>):<fpage>175</fpage>–<lpage>91</lpage>. <?supplied-pmid 17695343?><pub-id pub-id-type="pmid">17695343</pub-id></mixed-citation>
              </ref>
              <ref id="pone.0205041.ref038">
                <label>38</label>
                <mixed-citation publication-type="journal"><name><surname>Van Gulick</surname><given-names>AE</given-names></name>, <name><surname>McGugin</surname><given-names>RW</given-names></name>, <name><surname>Gauthier</surname><given-names>I</given-names></name>. <article-title>Measuring nonvisual knowledge about object categories: The Semantic Vanderbilt Expertise Test</article-title>. <source>Behav Res Methods</source> [Internet]. <year>2016</year>;<volume>48</volume>(<issue>3</issue>):<fpage>1178</fpage>–<lpage>96</lpage>. Available from: <ext-link ext-link-type="uri" xlink:href="http://link.springer.com/10.3758/s13428-015-0637-5">http://link.springer.com/10.3758/s13428-015-0637-5</ext-link>
<pub-id pub-id-type="doi">10.3758/s13428-015-0637-5</pub-id>
<?supplied-pmid 26276518?><pub-id pub-id-type="pmid">26276518</pub-id></mixed-citation>
              </ref>
              <ref id="pone.0205041.ref039">
                <label>39</label>
                <mixed-citation publication-type="journal"><name><surname>McGugin</surname><given-names>RW</given-names></name>, <name><surname>Gauthier</surname><given-names>I</given-names></name>, <name><surname>Gatenby</surname><given-names>C</given-names></name>, <name><surname>Gore</surname><given-names>JC</given-names></name>. <article-title>High-resolution imaging of expertise reveals reliable object selectivity in the FFA related to perceptual performance</article-title>. <source>Proc Natl Acad Sci</source>. <year>2012</year>;<volume>109</volume>(<issue>42</issue>):<fpage>17063</fpage>–<lpage>8</lpage>. <pub-id pub-id-type="doi">10.1073/pnas.1116333109</pub-id>
<?supplied-pmid 23027970?><pub-id pub-id-type="pmid">23027970</pub-id></mixed-citation>
              </ref>
              <ref id="pone.0205041.ref040">
                <label>40</label>
                <mixed-citation publication-type="journal"><name><surname>DF</surname><given-names>M</given-names></name>. <article-title>Visual imagery differences in the recall of pictures</article-title>. <source>Br J Psychol</source>. <year>1973</year>;<volume>64</volume>(<issue>1</issue>):<fpage>17</fpage>–<lpage>24</lpage>. <?supplied-pmid 4742442?><pub-id pub-id-type="pmid">4742442</pub-id></mixed-citation>
              </ref>
              <ref id="pone.0205041.ref041">
                <label>41</label>
                <mixed-citation publication-type="journal"><name><surname>Rodway</surname><given-names>P</given-names></name>, <name><surname>Gillies</surname><given-names>K</given-names></name>, <name><surname>Schepman</surname><given-names>A</given-names></name>. <article-title>Vivid imagers are better at detecting salient changes</article-title>. <source>J Individ Differ</source>. <year>2006</year>;<volume>27</volume>(<issue>4</issue>):<fpage>218</fpage>–<lpage>28</lpage>.</mixed-citation>
              </ref>
              <ref id="pone.0205041.ref042">
                <label>42</label>
                <mixed-citation publication-type="journal"><name><surname>McGugin</surname><given-names>RW</given-names></name>, <name><surname>Richler</surname><given-names>JJ</given-names></name>, <name><surname>Herzmann</surname><given-names>G</given-names></name>, <name><surname>Speegle</surname><given-names>M</given-names></name>, <name><surname>Gauthier</surname><given-names>I</given-names></name>. <article-title>The Vanderbilt Expertise Test reveals domain-general and domain-specific sex effects in object recognition</article-title>. <source>Vision Res</source> [Internet]. <year>2012</year>;<volume>69</volume>:<fpage>10</fpage>–<lpage>22</lpage>. Available from: <pub-id pub-id-type="doi">10.1016/j.visres.2012.07.014</pub-id>
<?supplied-pmid 22877929?><pub-id pub-id-type="pmid">22877929</pub-id></mixed-citation>
              </ref>
              <ref id="pone.0205041.ref043">
                <label>43</label>
                <mixed-citation publication-type="journal"><name><surname>Richler</surname><given-names>JJ</given-names></name>, <name><surname>Wilmer</surname><given-names>JB</given-names></name>, <name><surname>Gauthier</surname><given-names>I</given-names></name>. <article-title>General object recognition is specific: Evidence from novel and familiar objects</article-title>. <source>Cognition</source> [Internet]. <year>2017</year>;<volume>166</volume>:<fpage>42</fpage>–<lpage>55</lpage>. Available from: <pub-id pub-id-type="doi">10.1016/j.cognition.2017.05.019</pub-id>
<?supplied-pmid 28554084?><pub-id pub-id-type="pmid">28554084</pub-id></mixed-citation>
              </ref>
              <ref id="pone.0205041.ref044">
                <label>44</label>
                <mixed-citation publication-type="journal"><name><surname>McKelvie</surname><given-names>SJ</given-names></name>. <article-title>The VVIQ as a psychometric test of individual differences in visual imagery vividness: A critical quantitative review and plea for direction</article-title>. <source>J Ment Imag</source>. <year>1995</year>;<volume>19</volume>.</mixed-citation>
              </ref>
              <ref id="pone.0205041.ref045">
                <label>45</label>
                <mixed-citation publication-type="journal"><name><surname>Campos</surname><given-names>A</given-names></name>. <article-title>Internal Consistency and Construct Validity of Two Versions of the Revised Vividness of Visual Imagery Questionnaire</article-title>. <source>Percept Mot Skills</source> [Internet]. <year>2011</year>;<volume>113</volume>(<issue>2</issue>):<fpage>454</fpage>–<lpage>60</lpage>. Available from: <ext-link ext-link-type="uri" xlink:href="http://journals.sagepub.com/doi/10.2466/04.22.PMS.113.5.454-460">http://journals.sagepub.com/doi/10.2466/04.22.PMS.113.5.454-460</ext-link>
<pub-id pub-id-type="doi">10.2466/04.22.PMS.113.5.454-460</pub-id>
<?supplied-pmid 22185060?><pub-id pub-id-type="pmid">22185060</pub-id></mixed-citation>
              </ref>
              <ref id="pone.0205041.ref046">
                <label>46</label>
                <mixed-citation publication-type="journal"><name><surname>Wang</surname><given-names>CC</given-names></name>, <name><surname>Ross</surname><given-names>DA</given-names></name>, <name><surname>Gauthier</surname><given-names>I</given-names></name>, <name><surname>Richler</surname><given-names>JJ</given-names></name>. <article-title>Validation of the vanderbilt holistic face processing test</article-title>. <source>Front Psychol</source>. <year>2016</year>;<volume>7</volume>(November):<fpage>1</fpage>–<lpage>10</lpage>. <pub-id pub-id-type="doi">10.3389/fpsyg.2016.00001</pub-id><pub-id pub-id-type="pmid">26858668</pub-id></mixed-citation>
              </ref>
              <ref id="pone.0205041.ref047">
                <label>47</label>
                <mixed-citation publication-type="journal"><name><surname>Duchaine</surname><given-names>B</given-names></name>, <name><surname>Nakayama</surname><given-names>K</given-names></name>. <article-title>The Cambridge Face Memory Test: Results for neurologically intact individuals and an investigation of its validity using inverted face stimuli and prosopagnosic participants</article-title>. <source>Neuropsychologia</source>. <year>2006</year>;<volume>44</volume>(<issue>4</issue>):<fpage>576</fpage>–<lpage>85</lpage>. <pub-id pub-id-type="doi">10.1016/j.neuropsychologia.2005.07.001</pub-id>
<?supplied-pmid 16169565?><pub-id pub-id-type="pmid">16169565</pub-id></mixed-citation>
              </ref>
              <ref id="pone.0205041.ref048">
                <label>48</label>
                <mixed-citation publication-type="journal"><name><surname>Russell</surname><given-names>R</given-names></name>, <name><surname>Duchaine</surname><given-names>B</given-names></name>, <name><surname>Nakayama</surname><given-names>K</given-names></name>. <article-title>Super-recognizers: people with extraordinary face recognition ability</article-title>. <source>Psychon Bull Rev</source>. <year>2009</year>;<volume>16</volume>(<issue>2</issue>):<fpage>252</fpage>–<lpage>7</lpage>. <pub-id pub-id-type="doi">10.3758/PBR.16.2.252</pub-id>
<?supplied-pmid 19293090?><pub-id pub-id-type="pmid">19293090</pub-id></mixed-citation>
              </ref>
              <ref id="pone.0205041.ref049">
                <label>49</label>
                <mixed-citation publication-type="journal"><name><surname>Sunday</surname><given-names>MA</given-names></name>, <name><surname>Lee</surname><given-names>W-Y</given-names></name>, <name><surname>Gauthier</surname><given-names>I</given-names></name>. <article-title>Age-related differential item functioning in tests of face and car recognition ability</article-title>. <source>J Vis</source>. <year>2018</year>;<volume>18</volume>(<issue>1</issue>).</mixed-citation>
              </ref>
              <ref id="pone.0205041.ref050">
                <label>50</label>
                <mixed-citation publication-type="journal"><name><surname>Talairach</surname><given-names>J</given-names></name>, <name><surname>Tournoux</surname><given-names>P</given-names></name>. <article-title>Co-planar stereotaxic atlas of the human brain</article-title>. 3<source>-Dimensional proportional system: an approach to cerebral imaging</source>. <year>1988</year>;</mixed-citation>
              </ref>
              <ref id="pone.0205041.ref051">
                <label>51</label>
                <mixed-citation publication-type="journal"><name><surname>C</surname><given-names>CC</given-names></name>, <name><surname>L</surname><given-names>CJ</given-names></name>. <article-title>LIBSVM: a library for support vector machines</article-title>. <source>ACM Trans Intell Syst Technol</source>. <year>2011</year>;<volume>2</volume>(<issue>3</issue>).</mixed-citation>
              </ref>
              <ref id="pone.0205041.ref052">
                <label>52</label>
                <mixed-citation publication-type="journal"><name><surname>E</surname><given-names>M</given-names></name>, <name><surname>C</surname><given-names>YC</given-names></name>, <name><surname>T-R</surname><given-names>BJ</given-names></name>, <name><surname>Y</surname><given-names>S</given-names></name>. <article-title>Decoding cognitive control in human parietal cortex</article-title>. <source>Proc Natl Acad Sci</source>. <year>2009</year>;<volume>106</volume>(<issue>42</issue>):<fpage>17974</fpage>–<lpage>9</lpage>. <pub-id pub-id-type="doi">10.1073/pnas.0903593106</pub-id>
<?supplied-pmid 19805050?><pub-id pub-id-type="pmid">19805050</pub-id></mixed-citation>
              </ref>
              <ref id="pone.0205041.ref053">
                <label>53</label>
                <mixed-citation publication-type="journal"><name><surname>Kriegeskorte</surname><given-names>N</given-names></name>, <name><surname>Goebel</surname><given-names>R</given-names></name>, <name><surname>Bandettini</surname><given-names>P</given-names></name>. <article-title>Information-based functional brain mapping</article-title>. <source>Proc Natl Acad Sci U S A</source> [Internet]. <year>2006</year>;<volume>103</volume>:<fpage>3863</fpage>–<lpage>8</lpage>. Available from: <ext-link ext-link-type="uri" xlink:href="http://www.pnas.org/content/103/10/3863.abstract">http://www.pnas.org/content/103/10/3863.abstract</ext-link>
<pub-id pub-id-type="doi">10.1073/pnas.0600244103</pub-id>
<?supplied-pmid 16537458?><pub-id pub-id-type="pmid">16537458</pub-id></mixed-citation>
              </ref>
              <ref id="pone.0205041.ref054">
                <label>54</label>
                <mixed-citation publication-type="journal"><name><surname>Grill-Spector</surname><given-names>K</given-names></name>, <name><surname>Malach</surname><given-names>R</given-names></name>. <article-title>fMR-adaptation: a tool for studying the functional properties of human cortical neurons</article-title>. <source>Acta Psychol (Amst)</source> [Internet]. <year>2001</year>;<volume>107</volume>(<issue>1</issue>):<fpage>293</fpage>–<lpage>321</lpage>. Available from: <ext-link ext-link-type="uri" xlink:href="http://www.ncbi.nlm.nih.gov/pubmed/11388140">http://www.ncbi.nlm.nih.gov/pubmed/11388140</ext-link><pub-id pub-id-type="pmid">11388140</pub-id></mixed-citation>
              </ref>
              <ref id="pone.0205041.ref055">
                <label>55</label>
                <mixed-citation publication-type="journal"><name><surname>Krekelberg</surname><given-names>B</given-names></name>, <name><surname>Boynton</surname><given-names>GM</given-names></name>, <name><surname>van Wezel</surname><given-names>RJ</given-names></name>. <article-title>Adaptation: from single cells to BOLD signals</article-title>. <source>Trends Neurosci</source>. <year>2006</year>;<volume>29</volume>(<issue>5</issue>):<fpage>250</fpage>–<lpage>6</lpage>. <pub-id pub-id-type="doi">10.1016/j.tins.2006.02.008</pub-id>
<?supplied-pmid 16529826?><pub-id pub-id-type="pmid">16529826</pub-id></mixed-citation>
              </ref>
              <ref id="pone.0205041.ref056">
                <label>56</label>
                <mixed-citation publication-type="journal"><name><surname>Kleinhans</surname><given-names>NM</given-names></name>, <name><surname>Richards</surname><given-names>T</given-names></name>, <name><surname>Greenson</surname><given-names>J</given-names></name>, <name><surname>Dawson</surname><given-names>G</given-names></name>, <name><surname>Aylward</surname><given-names>E</given-names></name>. <article-title>Altered Dynamics of the fMRI Response to Faces in Individuals with Autism</article-title>. <source>J Autism Dev Disord</source>. <year>2016</year>;<volume>46</volume>(<issue>1</issue>):<fpage>232</fpage>–<lpage>41</lpage>. <pub-id pub-id-type="doi">10.1007/s10803-015-2565-8</pub-id>
<?supplied-pmid 26340957?><pub-id pub-id-type="pmid">26340957</pub-id></mixed-citation>
              </ref>
              <ref id="pone.0205041.ref057">
                <label>57</label>
                <mixed-citation publication-type="journal"><name><surname>Williams</surname><given-names>LE</given-names></name>, <name><surname>Blackford</surname><given-names>JU</given-names></name>, <name><surname>Luksik</surname><given-names>A</given-names></name>, <name><surname>Gauthier</surname><given-names>I</given-names></name>, <name><surname>Heckers</surname><given-names>S</given-names></name>. <article-title>Reduced habituation in patients with schizophrenia</article-title>. <source>Schizophr Res</source>. <year>2013</year>;<volume>151</volume>(<issue>1–3</issue>):<fpage>124</fpage>–<lpage>32</lpage>. <pub-id pub-id-type="doi">10.1016/j.schres.2013.10.017</pub-id>
<?supplied-pmid 24200419?><pub-id pub-id-type="pmid">24200419</pub-id></mixed-citation>
              </ref>
              <ref id="pone.0205041.ref058">
                <label>58</label>
                <mixed-citation publication-type="journal"><name><surname>Natu</surname><given-names>VS</given-names></name>, <name><surname>Barnett</surname><given-names>MA</given-names></name>, <name><surname>Hartley</surname><given-names>J</given-names></name>, <name><surname>Gomez</surname><given-names>J</given-names></name>, <name><surname>Stigliani</surname><given-names>A</given-names></name>, <name><surname>Grill-Spector</surname><given-names>K</given-names></name>. <article-title>Development of Neural Sensitivity to Face Identity Correlates with Perceptual Discriminability</article-title>. <source>J Neurosci</source> [Internet]. <year>2016</year>;<volume>36</volume>(<issue>42</issue>):<fpage>10893</fpage>–<lpage>907</lpage>. Available from: <ext-link ext-link-type="uri" xlink:href="http://www.jneurosci.org/lookup/doi/10.1523/JNEUROSCI.1886-16.2016">http://www.jneurosci.org/lookup/doi/10.1523/JNEUROSCI.1886-16.2016</ext-link>
<pub-id pub-id-type="doi">10.1523/JNEUROSCI.1886-16.2016</pub-id>
<?supplied-pmid 27798143?><pub-id pub-id-type="pmid">27798143</pub-id></mixed-citation>
              </ref>
              <ref id="pone.0205041.ref059">
                <label>59</label>
                <mixed-citation publication-type="journal"><name><surname>Sunday</surname><given-names>MA</given-names></name>, <name><surname>Richler</surname><given-names>JJ</given-names></name>, <name><surname>Gauthier</surname><given-names>I</given-names></name>. <article-title>Limited evidence of individual differences in holistic processing in different versions of the part-whole paradigm</article-title>. <source>Attention, Perception, Psychophys</source>. <year>2017</year>;<volume>79</volume>(<issue>5</issue>).</mixed-citation>
              </ref>
              <ref id="pone.0205041.ref060">
                <label>60</label>
                <mixed-citation publication-type="journal"><name><surname>Polyn</surname><given-names>S</given-names></name>, <name><surname>Natu</surname><given-names>VS</given-names></name>, <name><surname>Cohen</surname><given-names>JD</given-names></name>, <name><surname>Norman</surname><given-names>KA</given-names></name>. <article-title>Category-specific cortical activity precedes recall during memory search</article-title>. <source>Science</source> (80-). <year>2005</year>;<volume>310</volume>:<fpage>1963</fpage>–<lpage>6</lpage>.<pub-id pub-id-type="pmid">16373577</pub-id></mixed-citation>
              </ref>
            </ref-list>
          </back>
        </article>
      </metadata>
    </record>
  </GetRecord>
</OAI-PMH>
