<?xml version="1.0" encoding="UTF-8"?>
<OAI-PMH xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/ http://www.openarchives.org/OAI/2.0/OAI-PMH.xsd">
  <responseDate>2024-05-03T04:53:15Z</responseDate>
  <request verb="GetRecord" identifier="oai:pubmedcentral.nih.gov:6160476" metadataPrefix="pmc">https:/www.ncbi.nlm.nih.gov/pmc/oai/oai.cgi</request>
  <GetRecord>
    <record>
      <header>
        <identifier>oai:pubmedcentral.nih.gov:6160476</identifier>
        <datestamp>2018-09-28</datestamp>
        <setSpec>scirep</setSpec>
        <setSpec>pmc-open</setSpec>
      </header>
      <metadata>
        <article xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:mml="http://www.w3.org/1998/Math/MathML" xsi:schemaLocation="https://jats.nlm.nih.gov/ns/archiving/1.3/ https://jats.nlm.nih.gov/archiving/1.3/xsd/JATS-archivearticle1-3.xsd" article-type="research-article">
          <front>
            <journal-meta>
              <journal-id journal-id-type="nlm-ta">Sci Rep</journal-id>
              <journal-id journal-id-type="iso-abbrev">Sci Rep</journal-id>
              <journal-title-group>
                <journal-title>Scientific Reports</journal-title>
              </journal-title-group>
              <issn pub-type="epub">2045-2322</issn>
              <publisher>
                <publisher-name>Nature Publishing Group UK</publisher-name>
                <publisher-loc>London</publisher-loc>
              </publisher>
            </journal-meta>
            <article-meta>
              <article-id pub-id-type="accession">PMC6160476</article-id>
              <article-id pub-id-type="pmcid">PMC6160476</article-id>
              <article-id pub-id-type="pmc-uid">6160476</article-id>
              <article-id pub-id-type="pmid">30262826</article-id>
              <article-id pub-id-type="publisher-id">32673</article-id>
              <article-id pub-id-type="doi">10.1038/s41598-018-32673-y</article-id>
              <article-categories>
                <subj-group subj-group-type="heading">
                  <subject>Article</subject>
                </subj-group>
              </article-categories>
              <title-group>
                <article-title>Multisensory perception reflects individual differences in processing temporal correlations</article-title>
              </title-group>
              <contrib-group>
                <contrib contrib-type="author" corresp="yes">
                  <contrib-id contrib-id-type="orcid">http://orcid.org/0000-0001-6319-3493</contrib-id>
                  <name>
                    <surname>Nidiffer</surname>
                    <given-names>Aaron R.</given-names>
                  </name>
                  <address>
                    <email>aaron.r.nidiffer@vanderbilt.edu</email>
                  </address>
                  <xref ref-type="aff" rid="Aff1">1</xref>
                </contrib>
                <contrib contrib-type="author">
                  <name>
                    <surname>Diederich</surname>
                    <given-names>Adele</given-names>
                  </name>
                  <xref ref-type="aff" rid="Aff2">2</xref>
                </contrib>
                <contrib contrib-type="author">
                  <name>
                    <surname>Ramachandran</surname>
                    <given-names>Ramnarayan</given-names>
                  </name>
                  <xref ref-type="aff" rid="Aff1">1</xref>
                  <xref ref-type="aff" rid="Aff3">3</xref>
                  <xref ref-type="aff" rid="Aff4">4</xref>
                  <xref ref-type="aff" rid="Aff6">6</xref>
                </contrib>
                <contrib contrib-type="author">
                  <contrib-id contrib-id-type="orcid">http://orcid.org/0000-0002-0166-906X</contrib-id>
                  <name>
                    <surname>Wallace</surname>
                    <given-names>Mark T.</given-names>
                  </name>
                  <xref ref-type="aff" rid="Aff1">1</xref>
                  <xref ref-type="aff" rid="Aff3">3</xref>
                  <xref ref-type="aff" rid="Aff4">4</xref>
                  <xref ref-type="aff" rid="Aff5">5</xref>
                  <xref ref-type="aff" rid="Aff6">6</xref>
                </contrib>
                <aff id="Aff1"><label>1</label><institution-wrap><institution-id institution-id-type="ISNI">0000 0001 2264 7217</institution-id><institution-id institution-id-type="GRID">grid.152326.1</institution-id><institution>Department of Hearing and Speech Sciences, </institution><institution>Vanderbilt University, </institution></institution-wrap>Nashville, TN USA </aff>
                <aff id="Aff2"><label>2</label><institution-wrap><institution-id institution-id-type="ISNI">0000 0000 9397 8745</institution-id><institution-id institution-id-type="GRID">grid.15078.3b</institution-id><institution>Department of Health, </institution><institution>Life Sciences &amp; Chemistry Jacobs University, </institution></institution-wrap>Bremen, Germany </aff>
                <aff id="Aff3"><label>3</label><institution-wrap><institution-id institution-id-type="ISNI">0000 0001 2264 7217</institution-id><institution-id institution-id-type="GRID">grid.152326.1</institution-id><institution>Vanderbilt Brain Institute, </institution><institution>Vanderbilt University, </institution></institution-wrap>Nashville, TN USA </aff>
                <aff id="Aff4"><label>4</label><institution-wrap><institution-id institution-id-type="ISNI">0000 0001 2264 7217</institution-id><institution-id institution-id-type="GRID">grid.152326.1</institution-id><institution>Department of Psychology, </institution><institution>Vanderbilt University, </institution></institution-wrap>Nashville, TN USA </aff>
                <aff id="Aff5"><label>5</label><institution-wrap><institution-id institution-id-type="ISNI">0000 0001 2264 7217</institution-id><institution-id institution-id-type="GRID">grid.152326.1</institution-id><institution>Department of Psychiatry, </institution><institution>Vanderbilt University, </institution></institution-wrap>Nashville, TN USA </aff>
                <aff id="Aff6"><label>6</label><institution-wrap><institution-id institution-id-type="ISNI">0000 0001 2264 7217</institution-id><institution-id institution-id-type="GRID">grid.152326.1</institution-id><institution>Vanderbilt Kennedy Center, </institution><institution>Vanderbilt University, </institution></institution-wrap>Nashville, TN USA </aff>
              </contrib-group>
              <pub-date pub-type="epub">
                <day>27</day>
                <month>9</month>
                <year>2018</year>
              </pub-date>
              <pub-date pub-type="pmc-release">
                <day>27</day>
                <month>9</month>
                <year>2018</year>
              </pub-date>
              <pub-date pub-type="collection">
                <year>2018</year>
              </pub-date>
              <volume>8</volume>
              <elocation-id>14483</elocation-id>
              <history>
                <date date-type="received">
                  <day>27</day>
                  <month>3</month>
                  <year>2018</year>
                </date>
                <date date-type="accepted">
                  <day>4</day>
                  <month>9</month>
                  <year>2018</year>
                </date>
              </history>
              <permissions>
                <copyright-statement>© The Author(s) 2018</copyright-statement>
                <license license-type="OpenAccess">
                  <license-p><bold>Open Access</bold> This article is licensed under a Creative Commons Attribution 4.0 International License, which permits use, sharing, adaptation, distribution and reproduction in any medium or format, as long as you give appropriate credit to the original author(s) and the source, provide a link to the Creative Commons license, and indicate if changes were made. The images or other third party material in this article are included in the article’s Creative Commons license, unless indicated otherwise in a credit line to the material. If material is not included in the article’s Creative Commons license and your intended use is not permitted by statutory regulation or exceeds the permitted use, you will need to obtain permission directly from the copyright holder. To view a copy of this license, visit <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/">http://creativecommons.org/licenses/by/4.0/</ext-link>.</license-p>
                </license>
              </permissions>
              <abstract id="Abs1">
                <p id="Par1">Sensory signals originating from a single event, such as audiovisual speech, are temporally correlated. Correlated signals are known to facilitate multisensory integration and binding. We sought to further elucidate the nature of this relationship, hypothesizing that multisensory perception will vary with the strength of audiovisual correlation. Human participants detected near-threshold amplitude modulations in auditory and/or visual stimuli. During audiovisual trials, the frequency and phase of auditory modulations were varied, producing signals with a range of correlations. After accounting for individual differences which likely reflect relative unisensory temporal characteristics in participants, we found that multisensory perception varied linearly with strength of correlation. Diffusion modelling confirmed this and revealed that stimulus correlation is supplied to the decisional system as sensory evidence. These data implicate correlation as an important cue in audiovisual feature integration and binding and suggest correlational strength as an important factor for flexibility in these processes.</p>
              </abstract>
              <funding-group>
                <award-group>
                  <funding-source>
                    <institution-wrap>
                      <institution-id institution-id-type="FundRef">https://doi.org/10.13039/501100001659</institution-id>
                      <institution>Deutsche Forschungsgemeinschaft (German Research Foundation)</institution>
                    </institution-wrap>
                  </funding-source>
                  <award-id>DI506/15-1</award-id>
                  <principal-award-recipient>
                    <name>
                      <surname>Diederich</surname>
                      <given-names>Adele</given-names>
                    </name>
                  </principal-award-recipient>
                </award-group>
              </funding-group>
              <funding-group>
                <award-group>
                  <funding-source>
                    <institution-wrap>
                      <institution-id institution-id-type="FundRef">https://doi.org/10.13039/100000016</institution-id>
                      <institution>U.S. Department of Health and Human Services (U.S. Department of Health &amp;amp; Human Services)</institution>
                    </institution-wrap>
                  </funding-source>
                  <award-id>HD083211</award-id>
                  <principal-award-recipient>
                    <name>
                      <surname>Wallace</surname>
                      <given-names>Mark T.</given-names>
                    </name>
                  </principal-award-recipient>
                </award-group>
              </funding-group>
              <custom-meta-group>
                <custom-meta>
                  <meta-name>issue-copyright-statement</meta-name>
                  <meta-value>© The Author(s) 2018</meta-value>
                </custom-meta>
              </custom-meta-group>
            </article-meta>
          </front>
          <body>
            <sec id="Sec1" sec-type="introduction">
              <title>Introduction</title>
              <p id="Par2">Our environment provides us with an enormous amount of information that is encoded by multiple sensory modalities. One of the fundamental tasks of the brain is to construct an accurate and unified representation of our environment from this rich array of sensory signals. To accomplish this, the brain must decide which signals arise from a common source. For example, during conversation among a group of individuals, listeners can group appropriate words from the same voice and further associate voices with the appropriate speakers, a process greatly facilitated by the availability of both audible and visible cues<sup><xref ref-type="bibr" rid="CR1">1</xref></sup>. Benefits that are associated with the presence of multisensory signals include increased detection<sup><xref ref-type="bibr" rid="CR2">2</xref></sup> and localization accuracy<sup><xref ref-type="bibr" rid="CR3">3</xref></sup>, improved speech intelligibility<sup><xref ref-type="bibr" rid="CR4">4</xref></sup> and speeding of reaction times<sup><xref ref-type="bibr" rid="CR5">5</xref>,<xref ref-type="bibr" rid="CR6">6</xref></sup>.</p>
              <p id="Par3">A number of principles have been proposed that relate the spatial and temporal proximity of multisensory signals and the manner in which these enhance neural and behavioral responses<sup><xref ref-type="bibr" rid="CR2">2</xref>,<xref ref-type="bibr" rid="CR7">7</xref>–<xref ref-type="bibr" rid="CR9">9</xref></sup>. These factors have also been related to our brain’s determination that multisensory signals come from the same source<sup><xref ref-type="bibr" rid="CR10">10</xref>,<xref ref-type="bibr" rid="CR11">11</xref></sup>. In addition to these principles, it has been demonstrated that the temporal similarity (i.e., correlation) of these signals are also important in shaping our multisensory perception and causal inference<sup><xref ref-type="bibr" rid="CR12">12</xref>–<xref ref-type="bibr" rid="CR15">15</xref></sup>. Indeed, temporal similarity is a hallmark feature of signals originating from the same source, such as the voice and mouth movements of a speaker<sup><xref ref-type="bibr" rid="CR16">16</xref></sup>, and has been shown to be a robust cue for the binding of unisensory<sup><xref ref-type="bibr" rid="CR17">17</xref>,<xref ref-type="bibr" rid="CR18">18</xref></sup> and multisensory<sup><xref ref-type="bibr" rid="CR19">19</xref>–<xref ref-type="bibr" rid="CR22">22</xref></sup> features. Observers can utilize these temporal correlations in multisensory signals to enhance behavioral performance<sup><xref ref-type="bibr" rid="CR22">22</xref>–<xref ref-type="bibr" rid="CR24">24</xref></sup>.</p>
              <p id="Par4">Although we know that temporal correlation between unisensory signals leads to a unified multisensory percept and enhancement of multisensory behaviors, it is not known whether, and if so how, multisensory behavioral performance varies with the strength of the correlation. We hypothesize that audiovisual temporal correlation provides sensory evidence for multisensory decisions that is proportional to the sign and magnitude of the correlation. Further we hypothesize that these graded changes in sensory evidence will result in corresponding changes in multisensory behavior. To test these hypotheses, we presented participants with audiovisual signals with barely detectable (i.e., near threshold) amplitude modulation (AM). While manipulating the temporal correlation between the auditory and visual signals, we measured how observers’ ability to detect these fluctuations changed with changes in stimulus correlation. We propose a mechanism—analogous to a phase shift—that approximates relative differences in unisensory temporal processing and that accounts for individual differences in behavioral results. Finally, we employed drift-diffusion modelling to test whether multisensory behavioral performance is better approximated by absolute stimulus correlation or by the adjusted correlations that account for this phase shift.</p>
            </sec>
            <sec id="Sec2" sec-type="results">
              <title>Results</title>
              <p id="Par5">Participants (n = 12) detected near-threshold amplitude modulated (AM) audiovisual stimuli (Fig. <xref rid="Fig1" ref-type="fig">1a,b</xref>). The temporal correlation of the AM signals was manipulated by systematic changes in the phase and frequency relationship of the auditory and visual pairs (Fig. <xref rid="Fig1" ref-type="fig">1c</xref>). Our central hypothesis was that multisensory behavioral performance would improve commensurate with increasing temporal similarity between the paired audiovisual stimuli (i.e., as correlation become more positive). To examine the potential dependence of behavior on stimulus correlation, a discriminability (<italic>d’</italic>) matrix and a reaction time (RT) matrix for each participant was constructed and related to the stimulus correlation (r<sub>av</sub>) matrix (Δ frequency × Δ phase; Fig. <xref rid="Fig1" ref-type="fig">1d</xref>).<fig id="Fig1"><label>Figure 1</label><caption><p>Amplitude modulation detection task. (<bold>a</bold>) Schematic representation of a single trial. Each trial began with the illumination of a fixation target. After a variable wait period, simultaneously presented auditory and visual stimuli appeared (see <bold>b</bold>). Participants indicated the presence or absence of amplitude modulation with a button press. (<bold>b</bold>) Auditory and visual stimuli were always present, but modulation was presented in auditory stimuli only (A<sub>signal</sub> trial), visual stimuli only (V<sub>signal</sub> trial), audiovisual stimuli (AV<sub>signal</sub> trial), or neither stimulus (no signal, catch trial). (<bold>c</bold>) During audiovisual presentations, the frequency and phase of auditory modulation could be independently manipulated yielding a range of audiovisual correlations (r<sub>av</sub>). Correlations were computed using the time series of the auditory and visual envelopes. Note that the visual envelope is always constant while the auditory envelope is varied. Four conditions out of forty are shown for illustration. (<bold>d</bold>) Stimulus Correlation Matrix (r<sub>av</sub>|φ<sub>0</sub>). All forty AV stimulus conditions are shown organized according to Δ frequency × Δ phase. Colors represent the correlation values of audiovisual stimuli across the different frequencies and phases presented where each color box represents one condition. In the task structure, there were 21 unique audiovisual stimulus correlations. (<bold>e</bold>) In order to account for phase shifts in individual participant data, the values in the stimulus correlation matrix (r<sub>av</sub>|φ<sub>0</sub>) were correlated to each participant’s discriminability matrix (r<sub>d’</sub>). In the top panel, a series of correlation matrices are shown in which a phase lag, φ<sub>i</sub>, was applied to auditory (positive shifts) or visual (negative shifts) before correlations were computed, (r<sub>av</sub>|φ<sub>i</sub>). A total of 360 correlation matrices were correlated with the participant’s discriminability matrix (r<sub>d’</sub>; middle panel, nine examples shown), approximating a cross-correlation. In the bottom panel, each of the 360 correlations (r<sub>d’</sub>) was plotted against phase lag [(r<sub>d’</sub>|φ); black line, examples shown by blue dots]. This function was fit to a sine wave and the phase of that fit was extracted (φ’; red dot and arrow) and was taken to represent a participant’s individual phase shift. The stimulus correlations at that phase shift (r<sub>av</sub>|φ’) was taken to represent a participant’s “internal” correlation matrix.</p></caption><graphic xlink:href="41598_2018_32673_Fig1_HTML" id="d29e473"/></fig></p>
              <p id="Par6">While RTs did not show a robust systematic pattern (likely a result of the near-threshold nature of the stimuli, although see Table <xref rid="Tab1" ref-type="table">1</xref> for RT correlations in some participants), discriminability had a discernible pattern that reflected the nature of the stimulus correlations. In eight of 12 participants, discriminability was significantly correlated with stimulus correlation (Fig. <xref rid="Fig2" ref-type="fig">2a</xref>). However, upon visual inspection, the discriminability matrices of two of the remaining four participants mirrored the stimulus correlation matrix but with an apparent shift along the Δ phase dimension (see Fig. <xref rid="Fig2" ref-type="fig">2a,b</xref>, middle panels for one example). In fact, this phase shift appeared to be present in most participants to varying degrees and seemed to occur evenly across Δ frequency for each participant (i.e., any shift along the phase dimension was present for all auditory frequencies presented). We therefore hypothesized that this phase shift reflects an internal transformation that alters the relationship between stimulus correlation and behavior (and that is likely driven by individual differences in unisensory temporal processing).<table-wrap id="Tab1"><label>Table 1</label><caption><p>Reaction time (RT), hit rate (HR) and discriminability (d’) correlations.</p></caption><table frame="hsides" rules="groups"><thead><tr><th rowspan="2">Ptc.</th><th colspan="2">RT</th><th colspan="2">HR</th><th colspan="2">d’</th></tr><tr><th>R</th><th>p</th><th>R</th><th>p</th><th>R</th><th>p</th></tr></thead><tbody><tr><td>1</td><td><bold>−0.24</bold></td><td><bold>0.14</bold></td><td>0.77</td><td>4.5e-9</td><td>0.76</td><td>1.2e-8</td></tr><tr><td>2</td><td>−0.59</td><td>5.3e-5</td><td>0.91</td><td>4.0e-16</td><td>0.91</td><td>1.1e-15</td></tr><tr><td>3</td><td>−0.34</td><td>0.031</td><td>0.50</td><td>0.001</td><td>0.49</td><td>0.001</td></tr><tr><td>4</td><td><bold>0.05</bold></td><td><bold>0.78</bold></td><td>0.68</td><td>8.7e-7</td><td>0.68</td><td>1.2e-6</td></tr><tr><td><bold>5</bold></td><td><bold>−0.25</bold></td><td><bold>0.14</bold></td><td><bold>0.14</bold></td><td><bold>0.36</bold></td><td><bold>0.12</bold></td><td><bold>0.45</bold></td></tr><tr><td>6</td><td>−0.44</td><td>0.0042</td><td>0.70</td><td>4.3e-7</td><td>0.68</td><td>1.2e-06</td></tr><tr><td>7</td><td><bold>−0.21</bold></td><td><bold>0.19</bold></td><td>0.71</td><td>2.7e-7</td><td>0.70</td><td>4.7e-7</td></tr><tr><td>8</td><td>−0.54</td><td>3.3e-4</td><td>0.89</td><td>1.2e-14</td><td>0.88</td><td>5.8e-14</td></tr><tr><td>9</td><td><bold>0.05</bold></td><td><bold>0.75</bold></td><td>0.87</td><td>3.1e-13</td><td>0.86</td><td>8.0e-13</td></tr><tr><td>10</td><td>−0.39</td><td>0.014</td><td>0.57</td><td>1.3e-4</td><td>0.57</td><td>1.4e-4</td></tr><tr><td>11</td><td>−0.41</td><td>0.01</td><td>0.70</td><td>4.9e-7</td><td>0.62</td><td>2.2e-5</td></tr><tr><td><bold>12</bold></td><td><bold>0.08</bold></td><td><bold>0.65</bold></td><td><bold>0.19</bold></td><td><bold>0.22</bold></td><td><bold>0.20</bold></td><td><bold>0.21</bold></td></tr></tbody></table><table-wrap-foot><p>Nonsignificant correlations are in bold.</p></table-wrap-foot></table-wrap><fig id="Fig2"><label>Figure 2</label><caption><p>Individual participant data examples. (<bold>a</bold>) Behavioral dependence on stimulus correlation (r<sub>d’</sub>|φ<sub>0</sub>) of three example participants. For parts a-d, each row represents a single participant. Participants are represented by the same color across the figures. (<bold>b</bold>) Discriminability matrices from three participants show how changes in phase (Δ phase; y-axis) and frequency (Δ frequency; x-axis) impact the ability to detect amplitude modulation (discriminability). Diagonal dashed lines represent the computed individual phase shifts (φ’ = 0°, −104°, and 21°) corresponding to the approximate middle of the diagonal of positive correlations in (c). Color values have been scaled separately and range from the lowest to highest value (shown in panel a) for each participant. (<bold>c</bold>) Phase shifted (“perceived”) correlation matrices (r<sub>av</sub>|φ’) from each participant shown in (<bold>a</bold>). Note the strong positive (upward) shift in the second example participant and the moderate negative (downward) shift in the third example participant, relative to Fig. <xref rid="Fig1" ref-type="fig">1d</xref>. (<bold>d</bold>) Behavioral dependence (r<sub>d’</sub>|φ’) on perceived stimulus correlation. Each participant shows a strong positive relationship between perceived stimulus correlation and detection behavior (i.e., discriminability). Note that the data in the middle panel was not significantly correlated to physical stimulus correlation (a) but reached significance when accounting for the phase shift. Further, note that the top participant shown did not differ between the two measures due to the lack of observed phase shift. Colors follow the convention described in (<bold>a</bold>). (<bold>e</bold>) Distribution of observed phase shifts from all participants and mean resultant vector. Phase shifts were concentrated around the mean (14.7°, not uniform across phase). Phases were shifted toward positive values (visual leading) but were not significantly different from zero. (<bold>f</bold>,<bold>g</bold>) Accuracy and reaction time effects between stimuli with the strongest negative and positive perceptual correlations. Strong positive correlation improves detection performance but has no impact on reaction times.</p></caption><graphic xlink:href="41598_2018_32673_Fig2_HTML" id="d29e920"/></fig></p>
              <sec id="Sec3">
                <title>Individuals display unique characteristics for auditory and visual temporal processing</title>
                <p id="Par7">We sought to measure and account for these individualized phase shifts. We modeled this by applying a phase shift to every condition in one of the unisensory modalities before recalculating a stimulus correlation matrix. We then measured the correlation between the discriminability matrix and a series of stimulus correlation matrices computed with phase shifts ranging from −180° to +180° (Fig. <xref rid="Fig1" ref-type="fig">1e</xref>; more detail in methods). We then fit this series of correlations to a sine wave. Due to the cyclical nature of the stimulus correlation matrix along the Δ phase dimension, we expected the correlations to be in the shape of a sine wave. As expected, each participant’s phase-shifted correlations were well fit (r<sup>2</sup> = 0.99999 ± 2.9 × 10<sup>−5</sup>). Another expectation is for these functions to have a period of 360° and to be centered about zero. Indeed, we found no evidence that their period was different from 360° (period = 360.06 ± 0.71; <italic>t</italic><sub>11</sub> = 0.2702, <italic>p</italic> = 0.79) or that their center was different from 0 (center = 1.3 × 10<sup>−4</sup> ± 5.4 × 10<sup>−4</sup>; <italic>t</italic><sub>11</sub> = 0.783, p = 0.45). Therefore, we calculated a participant’s phase shift from these fits and then recomputed a unique correlation matrix for each participant using their individual phase shift.</p>
                <p id="Par8">As a test of the validity of phase shift, the pattern of data in the discriminability matrix should mirror the pattern of the phase-shifted stimulus matrix. This would manifest in several ways. First, if the perceived correlation matrix accounts for the data, large changes in the data should be accounted for by changes in the correlations. Therefore, the residual errors between the two measures should be very small relative to the data and centered on zero. Discriminability values (Fig. <xref rid="Fig2" ref-type="fig">2b</xref>) were significantly above zero (<italic>d’</italic> = 1.30 ± 0.66; <italic>z</italic> = 43.579, <italic>p</italic> = 8.75 × 10<sup>−169</sup>). Subtracting the predicted <italic>d’</italic>, which was computed from the perceptual correlation matrices (see Methods; Fig. <xref rid="Fig2" ref-type="fig">2c</xref>), from the observed <italic>d’</italic>, yielded residual errors which were substantially smaller and less variable compared to <italic>d’</italic> (mean error = 0.018 ± 0.33). Indeed, these residual errors did not differ significantly from zero (<italic>z</italic> = 1.210, <italic>p</italic> = 0.23). Second, we might question the validity of these phase shifts if the data do not mirror perceptual correlations equally for each Δ frequency (e.g., if the diagonal of high <italic>d’</italic> values in the discriminability matrix has a slope that doesn’t match the slope of high <italic>d’</italic> values in the predicted discriminability matrix). To quantify this, we examined residual errors across different frequencies for any systematic changes. Residual error magnitude and variability showed no linear relationships across Δ frequency in any participant (magnitude: slopes = 0.047 ± 0.10, all <italic>p</italic> &gt; 0.12; variability: slopes = 0.016 ± 0.07, all <italic>p</italic> &gt; 0.09). Thus, phase shifts appear to be valid and systematic shifts in the phase dimension are independent of frequency. As such, the correlation matrices constructed using each participant’s unique phase shift could be envisioned to represent the internal (“perceived”) correlations of the external stimuli, accounting for differences in latency of sensory processing between the auditory and visual systems.</p>
                <p id="Par9">These perceptual correlations were used when determining the relationship between discriminability and stimulus correlation (r<sub>d’</sub>; Fig. <xref rid="Fig2" ref-type="fig">2d</xref>). The sine wave fits between phase shift and correlation revealed the degree of participant audiovisual phase shift (φ’; Fig. <xref rid="Fig2" ref-type="fig">2e</xref>). Phase shifts were not significantly different from 0 across participants but favored a visual leading shift (mean φ’ = 14.7 ± 39.7°; 95% CI [42.2° −12.9°]). The distribution of shifts was concentrated about the mean as indexed by the mean resultant vector length (Fig. <xref rid="Fig2" ref-type="fig">2e</xref>; MRVL = 0.76; z = 11.998, p = 1.5 × 10<sup>−8</sup>, Rayleigh Test). To further probe the validity of these phase shifts, we tested whether the magnitude of phase shift was correlated to the strength of the relationship between behavior and stimulus correlation. Smaller correlations associated with larger phase shifts might suggest that the repeated phase shift approach returned spurious correlations. We found no evidence of such a relationship (rho = 0.25, p = 0.68).</p>
              </sec>
              <sec id="Sec4">
                <title>Amplitude modulation discriminability varies with perceived stimulus correlation</title>
                <p id="Par10">Previously, it has been shown that strongly correlated multisensory stimuli provided behavioral and perceptual benefits relative to unisensory performance whereas poorly correlated stimuli fail to provide such benefits<sup><xref ref-type="bibr" rid="CR12">12</xref>,<xref ref-type="bibr" rid="CR20">20</xref>,<xref ref-type="bibr" rid="CR22">22</xref></sup>. To examine whether a similar relationship is evident for the current task, we compared the discriminability of stimuli that had the highest and lowest correlation for each participant. We found that discriminability of audiovisual signals with the strongest correlations was better than for audiovisual signals with the strongest anti-correlations (Fig. <xref rid="Fig2" ref-type="fig">2f</xref>; t<sub>11</sub> = 4.312, p = 0.0062, corrected). In contrast, reaction times failed to differ between correlated signals and uncorrelated signals (Fig. <xref rid="Fig2" ref-type="fig">2g</xref>; t<sub>11</sub> = 3.384, p = 0.19, corrected).</p>
                <p id="Par11">Our focus of the current study was to show that multisensory behavior varied proportionally with stimulus correlation. Although we demonstrated above that this relationship was robust in most participants (Fig. <xref rid="Fig2" ref-type="fig">2a</xref>), there was evidence that this effect was weakened—and in some participants absent—due to significant individual variability. Thus, it still remained unclear whether phase shift plays an important role in this relationship. To test this, we measured the association between perceived stimulus correlation and discriminability (r<sub>d’</sub>|φ’; Fig. <xref rid="Fig3" ref-type="fig">3a</xref>). These correlations were significant in ten out of the twelve participants—two participants more than when not accounting for phase shift. This proportion, 10/12, was significantly greater than expected based on random effects (<italic>p</italic> = 0.019, binomial test). The significant correlations revealed effects that were very strong (Fig. <xref rid="Fig3" ref-type="fig">3b</xref>). The correlation values for discriminability and hit rate are presented for each participant in Table <xref rid="Tab1" ref-type="table">1</xref>.<fig id="Fig3"><label>Figure 3</label><caption><p>Behavioral results. (<bold>a</bold>) Behavioral dependence on perceived stimulus correlation across all participants (same as Fig. <xref rid="Fig2" ref-type="fig">2d</xref>). Behavioral performance in 10 of 12 participants was driven by stimulus correlation. Non-significantly correlated data are represented in grey. Significantly correlated data is depicted in color. As in Fig. <xref rid="Fig2" ref-type="fig">2</xref>, each participant retains the same color across the figures. (<bold>b</bold>) Correlation coefficients (r<sub>d’</sub>|φ’) for each participant. The critical value of the correlation coefficient is denoted by a dashed line. (<bold>c</bold>) Slope of linear data fits shown in (<bold>a</bold>) for each participant. (<bold>d</bold>) Criterion for each participant. Each participant but one held a conservative criterion indicating that participants weren’t biased toward responding “yes.” (<bold>e</bold>) Improvement in correlation (Δr<sub>d’</sub>) is associated with phase shift and the effect is larger than expected by chance. Red line and shaded region represent the average fit and 95% confidence bands of random data from the Monte Carlo simulation fit to a sine wave. The black line represents the fit of the observed data to the sine wave. The amplitude of the data fit sine wave was significantly larger than expected by chance. (<bold>f</bold>) Distribution of phase shifts and the corresponding MRVL obtained from the Monte Carlo simulation. In contrast to observed data shown in Fig. <xref rid="Fig2" ref-type="fig">2e</xref>, these phase shifts are not significantly concentrated about the circular mean. Note the scale difference in the radial axis between Fig. <xref rid="Fig2" ref-type="fig">2e</xref> and here.</p></caption><graphic xlink:href="41598_2018_32673_Fig3_HTML" id="d29e1118"/></fig></p>
                <p id="Par12">Because we varied auditory parameters while holding visual parameters stationary, it remained possible that participant performance was driven by cues in the auditory modality rather than by audiovisual correlation. In order to rule out that the effects reported here may be a result of unisensory auditory performance, four participants returned and completed a new experiment where visual modulation depth was set to zero while auditory depth was set at their individual threshold. We correlated auditory performance with AM frequency, AM phase, and perceived stimulus correlation. These data are summarized in Table <xref rid="Tab2" ref-type="table">2</xref>. None of these correlations were significant in any of the four participants, even when computing perceived correlations based on potential phase shifts in auditory or audiovisual performance data. Moreover, phase shifts obtained from the auditory data were very different than those obtained from audiovisual data. As a final check, we subtracted the auditory data from the audiovisual data and measured the phase shift and resultant correlation. All four participants showed a significant correlation and the obtained phase shifts corresponded well to the phase shifts obtained from audiovisual data. These results suggest that audiovisual correlations—rather than auditory modulations—are responsible for the behavioral effects presented here.<table-wrap id="Tab2"><label>Table 2</label><caption><p>Results of auditory only experiments.</p></caption><table frame="hsides" rules="groups"><thead><tr><th rowspan="2">Ptc.</th><th rowspan="2"/><th colspan="4">Stimulus Correlation Effect on</th><th rowspan="2">Frequency Effect on A</th><th rowspan="2">Phase Effect on A</th></tr><tr><th>AV</th><th>A<sup>a</sup></th><th>A<sup>b</sup></th><th>AV-A</th></tr></thead><tbody><tr><td rowspan="3">1</td><td>Shift</td><td>47</td><td><bold>99</bold></td><td>—</td><td>35</td><td>—</td><td>—</td></tr><tr><td>R</td><td>0.76</td><td><bold>0.29</bold></td><td><bold>0.09</bold></td><td>0.55</td><td><bold>−0.01</bold></td><td><bold>0.20</bold></td></tr><tr><td>p</td><td>1.2e-8</td><td><bold>0.067</bold></td><td><bold>0.59</bold></td><td>2.7e-4</td><td><bold>0.93</bold></td><td><bold>0.43</bold></td></tr><tr><td rowspan="3">6</td><td>Shift</td><td>−89</td><td><bold>100</bold></td><td>—</td><td>−89</td><td>—</td><td>—</td></tr><tr><td>R</td><td>0.68</td><td><bold>0.2</bold></td><td><bold>−0.18</bold></td><td>0.6</td><td><bold>−0.04</bold></td><td><bold>0.26</bold></td></tr><tr><td>p</td><td>1.2e-6</td><td><bold>0.21</bold></td><td><bold>0.22</bold></td><td>4.5e-5</td><td><bold>0.83</bold></td><td><bold>0.25</bold></td></tr><tr><td rowspan="3">8</td><td>Shift</td><td>21</td><td><bold>−152</bold></td><td>—</td><td>23</td><td>—</td><td>—</td></tr><tr><td>R</td><td>0.88</td><td><bold>0.27</bold></td><td><bold>−0.27</bold></td><td>0.81</td><td><bold>0.03</bold></td><td><bold>0.31</bold></td></tr><tr><td>p</td><td>5.8e-14</td><td><bold>0.097</bold></td><td><bold>0.091</bold></td><td>1.9e-10</td><td><bold>0.85</bold></td><td><bold>0.14</bold></td></tr><tr><td rowspan="3">10</td><td>Shift</td><td>−19</td><td><bold>−18</bold></td><td>—</td><td>−17</td><td>—</td><td>—</td></tr><tr><td>R</td><td>0.57</td><td><bold>0.05</bold></td><td><bold>0.04</bold></td><td>0.35</td><td><bold>−0.15</bold></td><td><bold>0.11</bold></td></tr><tr><td>p</td><td>1.4e-4</td><td><bold>0.76</bold></td><td><bold>0.82</bold></td><td>0.026</td><td><bold>0.35</bold></td><td><bold>0.76</bold></td></tr></tbody></table><table-wrap-foot><p><sup>a</sup>Correlations were unconstraint and reflect best possible correlations. <sup>b</sup>Correlations were constrained by audiovisual phase shift. Nonsignificant correlations (Fig. <xref rid="Fig3" ref-type="fig">3b</xref>) are in bold.</p></table-wrap-foot></table-wrap></p>
                <p id="Par13">When accounting for phase shift, the strength of these behavioral effects increased in all participants (Δr<sub>d’</sub> = 0.19 ± 0.29) and the increase was more pronounced in participants with larger magnitude phase shifts (Fig. <xref rid="Fig3" ref-type="fig">3e</xref>, a<sub>obs</sub> = 0.706). Due to the nature of the phase-shift fitting process, simulated random data (details can be found in methods) produces correlational improvement that peaks at ± 180° (a<sub>null</sub> = 0.205, 95% CI [0.144 0.271]). Nonetheless, the observed effect was significantly larger than what would be expected by these random effects (z = 15.49, p = 4.3 × 10<sup>−54</sup>). Lastly, in contrast to the concentrated distribution of observed phase shifts (Fig. <xref rid="Fig2" ref-type="fig">2e</xref>), the distribution of simulated phase shifts was not significantly different from uniform (Fig. <xref rid="Fig3" ref-type="fig">3f</xref>; MRVL = 0.04; z = 2.08, p = 0.125, Rayleigh Test). These findings provide strong support for the notion that phase shift reflects an important transformation between stimulus correlation as it occurs in the environment and how it manifests in perceptual performance.</p>
                <p id="Par14">Individuals showed widely varying dependencies on stimulus correlation as measured by the slope of a linear psychometric function fit to discriminability data (Fig. <xref rid="Fig3" ref-type="fig">3c</xref>; sig. slopes = 0.43 ± 0.18). Lastly, despite the stimuli being presented at threshold levels, we were concerned about the possibility of participants adopting a strategy that exploits the low proportion of catch trials (i.e., they could be always reporting the presence of the stimulus modulation). We therefore quantified participant’s willingness to respond with “modulation present”. Figure <xref rid="Fig3" ref-type="fig">3d</xref> confirms that this strategy was not employed (c = 0.61 ± 0.41) with 11 of 12 participants adopting a conservative criterion. Further reinforcing this, 10 out of 12 participants (including the lone participant with a liberal criterion) were within one standard deviation of an unbiased criterion (−1 &lt; c &lt; 1).</p>
              </sec>
              <sec id="Sec5">
                <title>Perceived stimulus correlation predicts audiovisual behavior via changes in evidence accumulation</title>
                <p id="Par15">Next, we sought to describe how audiovisual temporal correlation and phase shift influence behavioral performance in a decisional framework. Typically, changes in choice frequency and reaction time in a decision task are driven by changes in sensory evidence. We hypothesized that, in our task, sensory evidence was conferred by the temporal correlation of the stimuli. Further, we asked whether perceptual correlations rather than physical correlations better account for changes in behavioral performance on a participant-by-participant basis. To answer these questions, we employed two decision models.</p>
                <p id="Par16">The first model assumed that the drift rates, which index sensory evidence, are related to physical stimulus correlations (r<sub>av</sub>|ϕ<sub>0</sub>) across conditions (Fig. <xref rid="Fig4" ref-type="fig">4a</xref>). For the second model we assumed that the drift rates are related to the perceived stimulus correlations (Fig. <xref rid="Fig4" ref-type="fig">4b</xref>), that is, correlations determined after a phase shift was applied (r<sub>av</sub>|ϕ<sub>i</sub>). This design allowed the models not only to predict choice and reaction times with sensory evidence based on stimulus correlation, but also to measure participant phase shifts, providing converging evidence (in conjunction with results provided above) of an internal phase shift of the representation of the physical stimuli.<fig id="Fig4"><label>Figure 4</label><caption><p>Modeling results and comparison to behavioral results. (<bold>a</bold>) Model 1 fit for a single participant. Proportion of correct responses (top panel), reaction times for correct responses (RTc; middle panel), and reaction times for incorrect responses (RTi; bottom panel) are shown (black dots, ±1 S.E.M) for the 21 unique audiovisual correlations. The same data are shown from the model prediction (red lines). (<bold>b</bold>) Model 2 fit for the same participant. Proportion of correct responses (top panel), reaction times for correct responses (RTc; middle panel), and reaction times for incorrect responses (RTi; bottom panel) are shown (black dots, ± 1 S.E.M) for all 40 audiovisual conditions (top arrow). Model predictions and observed data are shown along a single continuous axis for simplicity with non-continuous data points connected by dashed lines (see panel c for key). (<bold>c</bold>) Representation of experimental conditions (frequency and phase) and how they are represented in panel (b). Conditions are organized in matrices (as in Fig. <xref rid="Fig2" ref-type="fig">2b,c</xref>) with columns representing different frequencies and rows representing different phases. In (<bold>b</bold>), data have been reorganized column-wise such that Condition 1 is the first phase in the first frequency and Condition 40 is the last phase in the last frequencies. Colors of the model fit and bottom axis in (<bold>b</bold>) correspond to columns in the matrix with the same color. The top arrow in (<bold>b</bold>) correspond to the arrow in (<bold>c</bold>), unfolded. (<bold>d</bold>) Across all participants, phase shifts measured from discriminability matrices (Fig. <xref rid="Fig2" ref-type="fig">2e</xref>) are strongly correlated with the phase shift parameters output by the diffusion model. Participant data shown in (<bold>a</bold>&amp;<bold>b</bold>) correspond to the marker indicated by the arrow. (<bold>e</bold>) Measures of bias, criteria (from Fig. <xref rid="Fig3" ref-type="fig">3d</xref>) and evidence starting point parameters are correlated across participants. Participant data shown in (<bold>a</bold>&amp;<bold>b</bold>) correspond to the marker indicated by the arrow. (<bold>f</bold>) Measures of dependence on stimulus correlation, psychometric slopes (from Fig. <xref rid="Fig3" ref-type="fig">3c</xref>) and scaling parameters are correlated across participants. Participant data shown in (<bold>a</bold>&amp;<bold>b</bold>) correspond to the marker indicated by the arrow.</p></caption><graphic xlink:href="41598_2018_32673_Fig4_HTML" id="d29e1698"/></fig></p>
                <p id="Par17">Tables <xref rid="Tab3" ref-type="table">3</xref> and <xref rid="Tab4" ref-type="table">4</xref> show the estimated parameters for each model and their goodness of fit. Both models were well fit to the data and model 2 successfully incorporated the extra parameter for phase shift without compensation from other parameters meant to index bias, speed/accuracy trade-off and sensory encoding/preparation. As evidence that the models were not simply adjusting other parameters to adjust between models, we found that these parameters were strongly correlated between models when accounting for phase shift using partial correlations (θ: rho = 0.78, p = 0.0046; β: rho = 0.98, p = 7.67 × 10<sup>−8</sup>; T<sub>r</sub>: rho = 0.87, p = 0.00044). Using Akaike Information Criterion (AIC) as a model selection metric, we found that most (8/12) participants’ behavior was better described by the second model, in which the perceived correlation, included as a phase shift parameter, drives the decision process. Qualitatively, perceptual choice across conditions can be described as a dampening oscillator with dampening increasing with Δ frequency, a pattern which is also apparent in the model prediction of choice. Figure <xref rid="Fig4" ref-type="fig">4b</xref> shows the model fit (colored lines matching conditions shown in Fig. <xref rid="Fig4" ref-type="fig">4c</xref>) to a single participant’s data (filled circles).<table-wrap id="Tab3"><label>Table 3</label><caption><p>Model 1 parameters.</p></caption><table frame="hsides" rules="groups"><thead><tr><th>Ptc.</th><th>θ</th><th>β</th><th>T<sub>r</sub></th><th>w</th><th>Χ<sup>2</sup></th><th>AIC</th></tr></thead><tbody><tr><td>1</td><td>13</td><td>0.2151</td><td>0.5741</td><td>0.0182</td><td>97.719</td><td>−0.281</td></tr><tr><td>2</td><td>7</td><td>0.4954</td><td>0.7915</td><td>0.0762</td><td>87.192</td><td>−10.808</td></tr><tr><td>3</td><td>19</td><td>2.3893</td><td>0.6113</td><td>0.0111</td><td>175.113</td><td>77.113</td></tr><tr><td>4</td><td>9</td><td>3.5568</td><td>0.7605</td><td>0.0001</td><td>70.945</td><td>−27.055</td></tr><tr><td><bold>5</bold></td><td><bold>13</bold></td><td><bold>−4.4554</bold></td><td><bold>0.8</bold></td><td><bold>0.0018</bold></td><td><bold>125.043</bold></td><td><bold>27.043</bold></td></tr><tr><td>6</td><td>9</td><td>6.2835</td><td>0.6364</td><td>0.0001</td><td>95.408</td><td>−2.592</td></tr><tr><td>7</td><td>8</td><td>−0.6953</td><td>0.6018</td><td>0.0334</td><td>142.682</td><td>44.682</td></tr><tr><td>8</td><td>15</td><td>0.1974</td><td>0.788</td><td>0.0292</td><td>108.722</td><td>10.722</td></tr><tr><td>9</td><td>17</td><td>0.1539</td><td>0.7956</td><td>0.025</td><td>131.553</td><td>33.553</td></tr><tr><td>10</td><td>5</td><td>1.3622</td><td>0.7738</td><td>0.0339</td><td>88.424</td><td>−9.576</td></tr><tr><td>11</td><td>16</td><td>−11.924</td><td>0.588</td><td>0.0333</td><td>163.444</td><td>65.444</td></tr><tr><td><bold>12</bold></td><td><bold>9</bold></td><td><bold>3.2324</bold></td><td><bold>0.7994</bold></td><td><bold>0.0089</bold></td><td><bold>142.744</bold></td><td><bold>44.744</bold></td></tr></tbody></table><table-wrap-foot><p>Participants (Ptc.) with nonsignificant correlations (Fig. <xref rid="Fig3" ref-type="fig">3b</xref>) are in bold. θ = boundary separation, β = evidence starting point, T<sub>r</sub> = residual time, w = drift-rate scaling parameter.</p></table-wrap-foot></table-wrap><table-wrap id="Tab4"><label>Table 4</label><caption><p>Model 2 parameters.</p></caption><table frame="hsides" rules="groups"><thead><tr><th>Ptc.</th><th>Φ</th><th>θ</th><th>β</th><th>T<sub>r</sub></th><th>w</th><th>Χ<sup>2</sup></th><th>AIC</th></tr></thead><tbody><tr><td>1</td><td>46</td><td>16</td><td>1.3753</td><td>0.4859</td><td>0.0214</td><td>214.55</td><td>14.55</td></tr><tr><td>2</td><td>0</td><td>7</td><td>−0.2467</td><td>0.7881</td><td>0.078</td><td>158.4</td><td>−41.6</td></tr><tr><td>3</td><td>0</td><td>19</td><td>2.3988</td><td>0.6074</td><td>0.011</td><td>281.68</td><td>81.681</td></tr><tr><td>4</td><td>−104</td><td>6</td><td>2.6569</td><td>0.7915</td><td>0.0348</td><td>136.03</td><td>−63.97</td></tr><tr><td><bold>5</bold></td><td><bold>−2</bold></td><td><bold>13</bold></td><td><bold>−4.4437</bold></td><td><bold>0.8</bold></td><td><bold>0.0012</bold></td><td><bold>217.53</bold></td><td><bold>17.528</bold></td></tr><tr><td>6</td><td>−92</td><td>8</td><td>4.6167</td><td>0.6443</td><td>0.0519</td><td>181.62</td><td>−18.382</td></tr><tr><td>7</td><td>13</td><td>10</td><td>−2.0867</td><td>0.5639</td><td>0.0298</td><td>252.04</td><td>52.041</td></tr><tr><td>8</td><td>19</td><td>15</td><td>0</td><td>0.7863</td><td>0.0317</td><td>168.74</td><td>−31.256</td></tr><tr><td>9</td><td>−46</td><td>17</td><td>−1.3041</td><td>0.8</td><td>0.0374</td><td>208.79</td><td>8.79</td></tr><tr><td>10</td><td>−16</td><td>14</td><td>2.3778</td><td>0.6013</td><td>0.0133</td><td>237.15</td><td>37.146</td></tr><tr><td>11</td><td>−8</td><td>15</td><td>−11.4521</td><td>0.6079</td><td>0.0375</td><td>225.2</td><td>25.201</td></tr><tr><td><bold>12</bold></td><td><bold>2</bold></td><td><bold>9</bold></td><td><bold>2.2282</bold></td><td><bold>0.7951</bold></td><td><bold>0.0086</bold></td><td><bold>242.31</bold></td><td><bold>42.309</bold></td></tr></tbody></table><table-wrap-foot><p>Participants (Ptc.) with nonsignificant correlations (Fig. <xref rid="Fig3" ref-type="fig">3b</xref>) are in bold. Φ = phase shift, θ = boundary separation, β = evidence starting point, T<sub>r</sub> = residual time, w = drift-rate scaling parameter.</p></table-wrap-foot></table-wrap></p>
                <p id="Par18">Model 2 made accurate predictions of behavioral choice and reaction times based on the perceptual correlations and returned parameters that closely matched their signal detection theory counterpart. Each participant’s model-fit phase shift parameter (<italic>ϕ’</italic>) nearly perfectly matched their phase shift obtained from discriminability (<italic>φ’</italic>, Fig. <xref rid="Fig4" ref-type="fig">4d</xref>; rho = 0.98, p = 0.026, slope = 0.98). Additionally, evidence starting point, <italic>β</italic>, which is the parameter that measures the participant’s bias toward one response over another<sup><xref ref-type="bibr" rid="CR25">25</xref>,<xref ref-type="bibr" rid="CR26">26</xref></sup>, was also correlated with the signal detection theory measure of bias, <italic>c</italic> (Fig. <xref rid="Fig4" ref-type="fig">4e</xref>; rho = 0.77, p = 0.0053). The bias reflects the participant’s tendency to respond with “modulation present” or “modulation absent”, which is unrelated to the sensitivity of the participant. Lastly, the drift-rate weighting coefficient was strongly correlated with the slope of their psychometric functions (Fig. <xref rid="Fig4" ref-type="fig">4f</xref>; rho = −0.86, p = 0.00032), with both measures describing the dependence of behavior on changes in correlation.</p>
              </sec>
            </sec>
            <sec id="Sec6" sec-type="discussion">
              <title>Discussion</title>
              <p id="Par19">Temporal factors such as (a)synchrony have long been known to influence multisensory processes in the brain<sup><xref ref-type="bibr" rid="CR7">7</xref>,<xref ref-type="bibr" rid="CR27">27</xref>–<xref ref-type="bibr" rid="CR31">31</xref></sup> and in behavior<sup><xref ref-type="bibr" rid="CR5">5</xref>,<xref ref-type="bibr" rid="CR32">32</xref>–<xref ref-type="bibr" rid="CR37">37</xref></sup>. More recently, Parise and colleagues<sup><xref ref-type="bibr" rid="CR20">20</xref></sup> presented evidence that the fine temporal structure of an audiovisual stimulus <italic>independent of asynchrony</italic> can influence multisensory perception. They further showed that it is possible to explain a number of multisensory phenomena based on a general correlation detection mechanism<sup><xref ref-type="bibr" rid="CR24">24</xref></sup>.The findings presented in the current study provide additional and unique support for the growing evidence implicating temporal correlation as an important cue in multisensory processing.</p>
              <p id="Par20">In the current work we extend this knowledge about multisensory temporal dependencies by showing that audiovisual detection behavior is a monotonic function of stimulus correlation. As the temporal similarity of two unisensory signals increased, detection of amplitude modulation embedded in the audiovisual signal improved in a linear manner (Fig. <xref rid="Fig3" ref-type="fig">3a</xref>). Additionally, we qualify this finding in a way that provides mechanistic insight into how the brain combines dynamic stimuli across sensory modalities. Thus, the temporal correlation of the audiovisual stimuli did not necessarily map directly onto multisensory behavioral performance; conditions in which physical stimulus correlation was highest did not always result in the best behavioral performance. Instead, it appears that a transform occurs in the brain of each individual and that results in a phase shift in behavioral performance relative to physical stimulus correlation (r<sub>av</sub>|<italic>φ</italic><sub>0</sub>). Calculating temporal correlation after applying a phase lag to one of the stimuli (r<sub>av</sub>|<italic>φ’</italic>), which simulates differential processing times for sensory signals in the brain, accounts for this difference. These phase-shifted correlations presumably represent the correlations as they are available to our decisional system.</p>
              <p id="Par21">Although our task did not reveal any measurable effects of temporal correlation on reaction times, we are not surprised. This lack of effect can be explained in terms of RT variability. Our stimuli employed near-threshold signals which are known to produce reaction times that are more variable than those produced by supra-threshold signals<sup><xref ref-type="bibr" rid="CR38">38</xref></sup>. Additionally, the correlations in some stimulus conditions unfolded over time. In contrast, for some conditions the correlation does not change throughout the course of the signals. For example, when the auditory and visual modulations are both at 6 Hz, across the entire stimulus, the relationship is maintained regardless of phase. However, when the frequencies of visual and auditory AM are different (e.g., 6 Hz and 7 Hz, respectively), the starting and ending phase relationships change. In one phase condition (see Fig. <xref rid="Fig1" ref-type="fig">1c</xref>), stimuli start out of phase (strong negative correlation) and end in phase (strong positive correlation). In another they start in phase and end out of phase. However, both of these conditions have an averaged correlation of 0 across the entire stimulus duration. This difference could introduce more reaction-time variability in some conditions than others, which may mask potential RT effects in some participants. To better measure any potential effect on reaction times, future experiments should be designed using supra-threshold signals that generate more reliable reaction times and take into account how correlations unfold over time.</p>
              <p id="Par22">The current study strongly grounds the relationship between stimulus correlation and multisensory processing in a decisional framework. Our model successfully incorporated the relationship between two signals (i.e., temporal correlation) into a dynamic-stochastic approach to account for choice frequency and response time. With only very few parameters (4 for model 1 and 5 for model 2) stimulus correlation was able to account for the observed patterns. Moreover, it was able to account for individual differences within and across participants. Our primary finding is related to the nature of how stimulus correlation influences the accumulation of sensory evidence for a decision. Specifically, we found that perceived (phase-shifted) stimulus correlation serves as a good predictor of behavior when used to constrain drift rate. For perceptual tasks, drift rate is often interpreted as an index for the quality (e.g., strength) of sensory evidence that is available to the decisional system<sup><xref ref-type="bibr" rid="CR39">39</xref>,<xref ref-type="bibr" rid="CR40">40</xref></sup>. Typically, the strength of sensory evidence is provided by the physical attributes of the stimulus, for instance, the degree of motion coherence<sup><xref ref-type="bibr" rid="CR41">41</xref></sup>, intensity<sup><xref ref-type="bibr" rid="CR42">42</xref></sup>, line length<sup><xref ref-type="bibr" rid="CR43">43</xref></sup>, or numerosity<sup><xref ref-type="bibr" rid="CR44">44</xref></sup>. For simple multisensory behaviors (e.g., detection of simple stimuli), the drift rate relates to the combined evidence obtained from integrating the physical stimulus properties across modalities<sup><xref ref-type="bibr" rid="CR42">42</xref>,<xref ref-type="bibr" rid="CR45">45</xref></sup>, especially when these properties are weak or ambiguous (e.g., low intensity, poor motion coherence, etc.) in the unisensory component stimuli<sup><xref ref-type="bibr" rid="CR42">42</xref></sup>.</p>
              <p id="Par23">In the current task, the key physical parameter that would presumably modulate the magnitude of evidence for detection is the depth of the amplitude modulation, with strength of evidence increasing with depth. However, modulation depth, and thus sensory evidence from the unisensory signals, is held constant across conditions. Although we cannot rule out that evidence is supplied by integration of the unisensory stimulus properties, sensory evidence cannot come from these alone but instead is generated via a computation involving both stimuli. Different types of multisensory decisions require different architectures that depend on the structure of the task or stimulus<sup><xref ref-type="bibr" rid="CR46">46</xref></sup>. The results presented here—that the strength of sensory evidence is based on a computation of the unisensory signals rather than the strength of the unisensory signals themselves—suggests that unisensory signals converge and evidence is computed prior to being evaluated by the decisional system. Other multisensory decisions such as simultaneity judgement<sup><xref ref-type="bibr" rid="CR47">47</xref></sup> and temporal order judgement<sup><xref ref-type="bibr" rid="CR48">48</xref>,<xref ref-type="bibr" rid="CR49">49</xref></sup>, which require a similar comparison of the unisensory signals, have also been described in terms of their cross-modal computations.</p>
              <p id="Par24">It has recently been discussed that the presence or absence of audiovisual temporal correlation is a strong determinant of multisensory binding<sup><xref ref-type="bibr" rid="CR21">21</xref></sup> which manifests in a variety of behavioral enhancements<sup><xref ref-type="bibr" rid="CR20">20</xref>,<xref ref-type="bibr" rid="CR22">22</xref>,<xref ref-type="bibr" rid="CR23">23</xref></sup>. Results presented here extend this concept, despite the substantially different nature of the stimuli and task employed. According to our results, multisensory benefits—and likely by extension the propensity to bind two signals—are monotonically related to the strength and sign of the temporal correlation (similarity) between unisensory signals. This notion implies that the process of binding signals is probabilistic. Stochastic binding related to temporal correlation could be an important mechanism in cognitive flexibility. It must be noted that weak, yet often significant, correlations exist in randomly paired stimuli<sup><xref ref-type="bibr" rid="CR16">16</xref></sup>. In a sensory-rich environment, compulsory binding based on temporal similarity could lead to the perceptual unification of unrelated stimuli, creating great ambiguity in deciphering the sensory world. Instead, since the perceptual system has access to the strength of the correlation, the strongest and likely most appropriate signals can be bound. Further, it’s likely that binding and integration are built on several other features such as spatial and temporal proximity. In the natural environment, these features are very often aligned; a single event will produce energies across different modalities that overlap in space and time and that are temporally correlated. Where these features are somewhat discrepant, the brain will appropriately weight (i.e., according to their reliability) proximity and similarity in the construction of a multisensory percept<sup><xref ref-type="bibr" rid="CR50">50</xref>,<xref ref-type="bibr" rid="CR51">51</xref></sup>.</p>
              <p id="Par25">The perceptual benefits of increased stimulus correlation are likely the result of mechanisms involving synchronized or coherent neural activity across brain regions<sup><xref ref-type="bibr" rid="CR52">52</xref></sup>. Neural coherence has been hypothesized to play a role in shaping our conscious experience<sup><xref ref-type="bibr" rid="CR53">53</xref></sup> by underpinning mechanisms of sensory awareness<sup><xref ref-type="bibr" rid="CR54">54</xref></sup>, attentional selection<sup><xref ref-type="bibr" rid="CR55">55</xref></sup>, cognitive flexibility<sup><xref ref-type="bibr" rid="CR56">56</xref></sup>, and perceptual binding<sup><xref ref-type="bibr" rid="CR17">17</xref>,<xref ref-type="bibr" rid="CR57">57</xref>–<xref ref-type="bibr" rid="CR59">59</xref></sup>. Further, temporally correlated audiovisual streams have been shown to improve the representation of the auditory stimulus envelope and features in auditory cortex<sup><xref ref-type="bibr" rid="CR60">60</xref></sup>. This enhanced representation is likely the end result of why seeing a speaker’s face improves speech intelligibility<sup><xref ref-type="bibr" rid="CR4">4</xref>,<xref ref-type="bibr" rid="CR23">23</xref>,<xref ref-type="bibr" rid="CR61">61</xref></sup>. Rhythmic auditory and visual stimuli like the ones used in the current study are known to entrain neural oscillations<sup><xref ref-type="bibr" rid="CR52">52</xref>,<xref ref-type="bibr" rid="CR62">62</xref>,<xref ref-type="bibr" rid="CR63">63</xref></sup> which index patterns of neuronal excitability over time<sup><xref ref-type="bibr" rid="CR64">64</xref></sup>. Since uni- and multisensory stimuli can simultaneously entrain oscillations in multiple frequency bands<sup><xref ref-type="bibr" rid="CR52">52</xref>,<xref ref-type="bibr" rid="CR65">65</xref></sup>, it is likely that our stimuli do the same and thus induce coherent brain activity commensurate with the correlation in the stimuli.</p>
              <p id="Par26">In the current study, participants’ behavioral performance was not necessarily best for the stimuli with highest physical correlation but were instead phase-shifted by differing amounts for each participant. Behavior very closely matched the correlation of the modulations after a phase lag was applied to one of the modulation signals. This phase lag could be adjusting for different processing times and abilities of participants’ auditory and visual systems. It’s known that oscillations entrain to rhythmic auditory stimuli at different phase lags across listeners<sup><xref ref-type="bibr" rid="CR63">63</xref></sup>. It is possible that visual entrainment occurs in a similar manner and that these phase lags differ between the auditory and visual systems, though we are not aware of such data. Interestingly, phase lag of the entrained oscillations can be calibrated to the particular temporal structure of an audiovisual stimulus<sup><xref ref-type="bibr" rid="CR66">66</xref></sup>. Thus, the phase lags reported in the current study are likely a “preferred” or “natural” phase that can be easily manipulated depending on context (e.g., attending an event that is near or far from the body which would result in different temporal relationships between auditory and visual representations in the brain) in a manner similar to the phenomenon of recalibration of the perception of audiovisual simultaneity<sup><xref ref-type="bibr" rid="CR37">37</xref>,<xref ref-type="bibr" rid="CR67">67</xref></sup>.</p>
              <p id="Par27">During multisensory decisions, temporal correlation between the features of the component stimuli modulates behavior. It does so by changing the nature of the sensory evidence that is evaluated by the sensory system. The strength of the sensory evidence is proportional to the strength of the correlation of the signal. Finally, the physical correlations present in stimuli are transformed, via a phase shift, into “perceptual” correlations that are unique to an individual. This process likely occurs through differences in unisensory temporal processing. This was confirmed by a dynamic-stochastic model in which the drift rate was related to physical or to perceived correlations between the auditory and visual signals in the audiovisual presentation. These results motivate several fundamental questions. Is binding truly stochastic? Can cross-modal correlation embedded in one feature (e.g., intensity) have the same proportional effect on behavioral performance reported here in tasks utilizing orthogonal stimulus features (e.g., frequency or timbre)? What are the neural signatures of this proportional change and their relation to behavior? Finally, does the perception of naturalistic audiovisual stimuli such as speech benefit in the same way with changes in audiovisual correlation?</p>
            </sec>
            <sec id="Sec7" sec-type="materials|methods">
              <title>Materials and Methods</title>
              <sec id="Sec8">
                <title>Participants</title>
                <p id="Par28">Twelve individuals (age = 26.4 ± 5.1, seven females) participated in the current study. All participants reported normal or corrected-to-normal vision and normal hearing and were right handed. The study was conducted in accordance with the declaration of Helsinki, and informed written consent was obtained from all participants. All procedures were approved by the Vanderbilt University Institutional Review Board. When applicable, participants were given monetary compensation for participation.</p>
              </sec>
              <sec id="Sec9">
                <title>Apparatus and stimuli</title>
                <p id="Par29">All stimuli were generated in MATLAB (The MathWorks, Inc., Natick, MA) and presented using PsychToolbox version 3<sup><xref ref-type="bibr" rid="CR68">68</xref>,<xref ref-type="bibr" rid="CR69">69</xref></sup>. Auditory stimuli were digitized at 44.1 kHz, and presented through calibrated open-back circumaural headphones (Sennheisser HD480). Visual stimuli were centered about a red fixation dot in the center of a dark (0.15 cd/m<sup>2</sup>) viewing screen (Samsung Sync Master 2233rz, 120 Hz refresh rate).</p>
                <p id="Par30">Auditory stimuli were frozen tokens of white noise (generated by the <italic>randn</italic> function) at moderate baseline level (48 dB SPL, A-weighted). Visual stimuli consisted of a moderately bright ring (24 cd/m<sup>2</sup> at baseline; inner diameter: 1.8°, outer diameter: 3.6° visual angle). Both stimuli were presented simultaneously, lasted 500 ms, and were gated by a linear 10 ms onset and offset ramp. Stimulus timing was confirmed with a Hameg 507 oscilloscope, photodiode, and microphone.</p>
                <p id="Par31">For each stimulus, auditory intensity and visual luminance, <italic>y</italic>, could be modulated around their baseline over time, <italic>t</italic>, such that<disp-formula id="Equ1"><label>1</label><alternatives><tex-math id="M1">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$y\,(t)=[1+m\,(t)]\times c(t)$$\end{document}</tex-math><mml:math id="M2" display="block"><mml:mi>y</mml:mi><mml:mspace width=".25em"/><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mo stretchy="false">[</mml:mo><mml:mn>1</mml:mn><mml:mo>+</mml:mo><mml:mi>m</mml:mi><mml:mspace width=".25em"/><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">]</mml:mo><mml:mo>×</mml:mo><mml:mi>c</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:math><graphic xlink:href="41598_2018_32673_Article_Equ1.gif" position="anchor"/></alternatives></disp-formula>where<disp-formula id="Equ2"><label>2</label><alternatives><tex-math id="M3">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$m\,(t)=M\times sin(2\pi {f}_{m}t+{\phi }_{0,j})$$\end{document}</tex-math><mml:math id="M4" display="block"><mml:mi>m</mml:mi><mml:mspace width=".25em"/><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mi>M</mml:mi><mml:mo>×</mml:mo><mml:mi>s</mml:mi><mml:mi>i</mml:mi><mml:mi>n</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mn>2</mml:mn><mml:mi>π</mml:mi><mml:msub><mml:mrow><mml:mi>f</mml:mi></mml:mrow><mml:mrow><mml:mi>m</mml:mi></mml:mrow></mml:msub><mml:mi>t</mml:mi><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi>ϕ</mml:mi></mml:mrow><mml:mrow><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:math><graphic xlink:href="41598_2018_32673_Article_Equ2.gif" position="anchor"/></alternatives></disp-formula>and <italic>c(t)</italic> is the time series of the carrier stimulus (auditory: noise; visual: ring). The form of the amplitude modulation (AM) signal <italic>m(t)</italic> is defined by a modulation depth <italic>M</italic> which represents the amplitude of the modulation signal as a proportion of the amplitude of the carrier signal and ranged from 0 (no AM) to 1 (full AM), frequency <italic>f</italic><sub><italic>m</italic></sub> in Hz, and starting phase <italic>φ</italic><sub>0<italic>,j</italic></sub> in degrees.</p>
                <p id="Par32">On any given trial, the AM signal could be present in the auditory channel alone, the visual channel alone, both channels (audiovisual trials), or neither (catch trials; Fig. <xref rid="Fig1" ref-type="fig">1b</xref>). If present, modulation depth was set to individual unisensory thresholds (see below for thresholding procedures). Unisensory signals (AM was present in auditory stimulus only or visual stimulus only) were always presented in cosine phase such that the modulation began at the trough (<italic>φ</italic> = 0°) and at the same frequency (<italic>f</italic><sub><italic>m, visual</italic></sub> = 6 Hz). When AM was present in both stimuli, visual modulation was always 6 Hz and cosine starting phase while auditory signals could be presented at various frequencies (<italic>f</italic><sub><italic>m, auditory</italic></sub> = {6, 6.25, 6.5, 6.75, 7 Hz}) and initial phases (<italic>φ</italic><sub><italic>0</italic></sub> = {−135, −90, −45, 0, 45, 90, 135, 180°}, with <italic>φ</italic><sub><italic>0,j</italic></sub>
<italic>ϵ</italic> φ<sub>0</sub>). This structure results in a total of 40 (5 × 8) different audiovisual stimulus conditions.</p>
                <p id="Par33">Because we are interested in the temporal correlation between the two signals, the Pearson correlation between the auditory and visual envelopes (r<sub>av</sub>) was computed for each of the 40 audiovisual conditions (Fig. <xref rid="Fig1" ref-type="fig">1c</xref>). For example, when the auditory and visual envelopes were characterized by the same frequency and phase, correlation was 1. Conversely, stimuli of the same frequency but presented anti-phase resulted in a correlation of −1. The parameters chosen resulted in a representation of correlations between −1 and 1. A stimulus correlation matrix (r<sub>av</sub>|φ<sub>0</sub>) was constructed for all audiovisual conditions by organizing the correlation values according to their frequency and phase relationship between auditory and visual signals (Δ frequency × Δ phase; Fig. <xref rid="Fig1" ref-type="fig">1d</xref>).</p>
              </sec>
              <sec id="Sec10">
                <title>Procedure</title>
                <p id="Par34">Participants were seated comfortably inside an unlit WhisperRoom™ (SE 2000 Series) with their forehead placed against a HeadSpot™ (University of Houston Optometry) with the forehead rest locked in place such that a participant’s primary eye position was centered with respect to the fixation point at the center of the viewing screen. Chinrest height and chair height were adjusted to the comfort of the participant.</p>
                <p id="Par35">Prior to the main experiment, each participant completed two separate 3-down 1-up staircase procedures to obtain 79.4% modulation depth thresholds for auditory and visual AM at 6 Hz. For these staircase procedures, on a given trial (Fig. <xref rid="Fig1" ref-type="fig">1a</xref>), the red fixation dot appeared at the center of the screen. Participants were instructed to fixate the dot for its entire duration. After a variable time, either an auditory or visual stimulus was presented in which the presence of modulation was determined at random for each trial. Participants were instructed to report the presence of amplitude modulation (described as “flutter”) after the stimulus presentation by pressing “1” on the number pad of a computer keyboard if the modulation was present or pressing “0” if the modulation was absent. The modulation depth decreased after three successive correct responses and increased after one incorrect response. At the beginning of each staircase, the step size was set to increase or decrease modulation depth by 0.05. After two reversals (correct to incorrect response or incorrect to correct response), step size was reduced to 0.025. Finally, after eight reversals, step size became 0.01 in order to arrive at an accurate estimate of modulation depth threshold. Each staircase terminated after 20 reversals. Threshold was determined to be the average of the modulation depth at the last 10 reversals. Instructions included an example of a stimulus with AM at the initial starting modulation depth (<italic>M</italic> = <italic>0.5</italic>) and an example of a stimulus with no AM. So that there was no ambiguity in cases where the first trial did not include a modulation signal, participants were informed that the first trial would have the same modulation depth as the example if present. To control for “runs” of trials with no modulation during the staircase (which could result in erroneously low threshold estimates), a sequence of two trials containing no modulation was always followed by a trial with modulation. The auditory staircase was always completed first and served as a period of dark adaptation prior to the visual staircase.</p>
                <p id="Par36">The main experiment consisted of four blocks lasting approximately 30 minutes each. Each block consisted of 10 trials of each stimulus condition (420 signal trials per block). Additionally, there were catch (no signal) trials included to make up 10% of total trials for that block (47 catch trials per block). Therefore, each block was identical in trial composition (467 total trials per block) but with individual trials presented in a predetermined, pseudorandom order. Each participant completed a total of 1868 trials over the four blocks. Breaks were offered frequently (every 100 trials) to prevent fatigue. Participants completed the full experiment in 2–4 sessions, never completing more than 2 blocks during a session. If a participant completed two blocks in a single session, they were given the opportunity to stretch and walk around while the experimenter set up the second block. Before each block and after any break where the participant was exposed to normal light levels, participants were dark adapted for five minutes. Trials during the main experiment were identical to staircase trials with three exceptions. First, in each trial, both auditory and visual stimuli were presented. Modulation signals could be present in the visual channel alone (V<sub>signal</sub>), auditory channel alone (A<sub>signal</sub>), in both (AV<sub>signal</sub>; with frequency and phase configuration discussed above), or neither channel (no signal). Second, modulation depth was set to a participant’s unique auditory and visual modulation depth thresholds. These threshold values are shown in Table <xref rid="Tab5" ref-type="table">5</xref>. Last, participants were told that they should respond as soon as they had made their decision and were instructed to respond as quickly and accurately as possible. In addition to the participant’s choice, response times were recorded for each trial, sampling every 2.2 μs (4.6 kHz). Response window was terminated after 1.5 seconds. Subsequent responses were censored. This ended up being 2% of trials or less for most participants.<table-wrap id="Tab5"><label>Table 5</label><caption><p>Participant modulation depth thresholds.</p></caption><table frame="hsides" rules="groups"><thead><tr><th>Ptc.</th><th>Aud.</th><th>Vis.</th></tr></thead><tbody><tr><td>1</td><td>0.041</td><td>0.047</td></tr><tr><td>2</td><td>0.081</td><td>0.059</td></tr><tr><td>3</td><td>0.028</td><td>0.049</td></tr><tr><td>4</td><td>0.104</td><td>0.076</td></tr><tr><td>5</td><td>0.051</td><td>0.042</td></tr><tr><td>6</td><td>0.087</td><td>0.062</td></tr><tr><td>7</td><td>0.068</td><td>0.043</td></tr><tr><td>8</td><td>0.048</td><td>0.040</td></tr><tr><td>9</td><td>0.060</td><td>0.058</td></tr><tr><td>10</td><td>0.063</td><td>0.043</td></tr><tr><td>11</td><td>0.072</td><td>0.050</td></tr><tr><td>12</td><td>0.072</td><td>0.070</td></tr></tbody></table></table-wrap></p>
              </sec>
              <sec id="Sec11">
                <title>Behavioral Analysis</title>
                <p id="Par37">Discriminability (<italic>d’</italic>; a measure of sensitivity) for each of the 40 audiovisual conditions and two unisensory conditions was computed from the relative frequencies of the respective responses,<disp-formula id="Equ3"><label>3</label><alternatives><tex-math id="M5">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$d^{\prime} =z({H}_{i})-z(F)$$\end{document}</tex-math><mml:math id="M6" display="block"><mml:mrow><mml:mi>d</mml:mi><mml:mo>′</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mi>z</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mrow><mml:mi>H</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>−</mml:mo><mml:mi>z</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>F</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:math><graphic xlink:href="41598_2018_32673_Article_Equ3.gif" position="anchor"/></alternatives></disp-formula>where <italic>H</italic><sub><italic>i</italic></sub> is the proportion of hits (“1”|modulated stimulus) for the i<sup>th</sup> condition, <italic>F</italic> is the proportion of false alarms (“1”|no modulated stimulus), and <italic>z</italic> is the inverse of the normal distribution function (MATLAB’s <italic>norminv</italic> function) and converts the hit rates and false alarm rates into units of standard deviation of a standard normal distribution. <italic>d’</italic> was organized into a matrix in the same manner as the stimulus correlation matrix. Because the proportion of catch trials was held low and errors had no associated cost<sup><xref ref-type="bibr" rid="CR70">70</xref></sup>, participants could potentially adopt a strategy of simply pressing “1” which would result in a correct choice more often than not. To account for this, criterion (<italic>c</italic>; a measure of bias) for each participant was computed in a similar manner such that<disp-formula id="Equ4"><label>4</label><alternatives><tex-math id="M7">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$c=z(H)+z(F)$$\end{document}</tex-math><mml:math id="M8" display="block"><mml:mi>c</mml:mi><mml:mo>=</mml:mo><mml:mi>z</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>H</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>+</mml:mo><mml:mi>z</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>F</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:math><graphic xlink:href="41598_2018_32673_Article_Equ4.gif" position="anchor"/></alternatives></disp-formula>where <italic>H</italic> is the proportion of hits across all conditions. A single criterion was computed for each participant.</p>
                <p id="Par38">To account for individual differences, which became apparent in assessing the phase shift in the <italic>d’</italic> matrices, a series of correlation matrices based on the stimulus correlation matrix (r<sub>av</sub>|φ<sub>0</sub>) were computed after iteratively applying a single degree phase lag to one stimulus (i.e., <italic>φ</italic><sub>1</sub> = {−134, −89, −44, 1, 46, 91, 136, −179°}, <italic>φ</italic><sub>2</sub> = {−133, −88, −43, 2, 47, 92, 137, −178°}, in general <italic>φ</italic><sub><italic>i</italic></sub> = {−135 + <italic>i</italic>, −90 + <italic>i</italic>, −45 + <italic>i</italic>, 0 + <italic>i</italic>, 45 + <italic>i</italic>, 90 + <italic>i</italic>, 135 + <italic>i</italic>, 180 + <italic>i</italic>} with <italic>i</italic> = −180, …, 180, resulting in a total of 360 different matrices). A phase-shifted correlation matrix (r<sub>av</sub>|<italic>φ</italic><sub><italic>i</italic></sub>) could be conceptualized as the “internal” or “perceived” correlation of the signals given a particular phase lag, <italic>i</italic>, of one of the signals. Each of the phase-shifted correlation matrices (Fig. <xref rid="Fig1" ref-type="fig">1e</xref>, nine examples shown) was in turn evaluated for correlation (r<sub>d’</sub>) with the discriminability matrix of each participant. The resulting correlation values (r<sub>d’</sub>|φ) were then fit to a sine wave using the nonlinear least-squares method. The phase shift value of the fitted sine wave was recorded for each participant (φ’). The CircStat toolbox<sup><xref ref-type="bibr" rid="CR71">71</xref></sup> was used to describe the nature of the phase shifts and compute the directional statistics across the sample of participants. The “perceptual” correlation matrix corresponding to each participant’s unique phase shift (r<sub>av</sub>|φ’) was used to measure the dependence of behavior on perceived correlation (r<sub>d’</sub>|φ’).</p>
                <p id="Par39">To show that phase shift is related to a central mechanism (e.g., a relative difference in processing latencies between auditory and visual systems), we tested whether the phase shift occurred systematically across all Δ frequencies within each participant. First, a predicted discriminability matrix was calculated from phase-shifted correlations. Phase-shifted correlation matrices were normalized to each participant’s discriminability range by scaling and shifting each unique correlation matrix such that the correlation values at the maximum and minimum correlation matched the <italic>d’</italic> values at the corresponding locations in the discriminability matrix. Next, the values in the predicted discriminability matrix were subtracted from the actual discriminability matrix, resulting in a matrix of residual errors. Then, a linear model was used to determine the relationship (i.e., slope) between Δ frequency and the magnitude and variability (standard deviation) of errors. To calculate significance of variability slope across Δ frequency, a permutation test was used that shuffled the Δ frequency label of errors before calculating standard deviation within each Δ frequency and then fitting a line to the shuffled standard deviations.</p>
                <p id="Par40">We sought to demonstrate that accounting for phase shift improved the measured correlation between behavior and stimulus correlation. Therefore, we computed this dependence on stimulus correlation (r<sub>d’</sub>|φ<sub>0</sub>) and subtracted it from the dependence on perceived correlation discussed above (r<sub>d’</sub>|φ’) which yielded a score of improvement (Δr). Because of the nature of the phase shift fitting process described above, (r<sub>d’</sub>|φ’) ≥ (r<sub>d’</sub>|φ<sub>0</sub>) with the difference growing to a maximum when φ =  ± 180° even for data with no effect (random numbers). Therefore, we accounted for this statistical effect by running a simulation where we computed the phase shift (same process described in Fig. <xref rid="Fig1" ref-type="fig">1e</xref>) of 1000 matrices of shuffled data from participants chosen at random. For each matrix, we measured (r<sub>d’</sub>|<italic>φ’</italic>) and (r<sub>d’</sub>|φ<sub>0</sub>) and subtracted them as above so that we had 1000 pairs of φ’ and Δr. These data, along with our observed data, were fit to the function<disp-formula id="Equ5"><label>5</label><alternatives><tex-math id="M9">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${\rm{\Delta }}r=a\times sin(\phi ^{\prime} )+a$$\end{document}</tex-math><mml:math id="M10" display="block"><mml:mi mathvariant="normal">Δ</mml:mi><mml:mi>r</mml:mi><mml:mo>=</mml:mo><mml:mi>a</mml:mi><mml:mo>×</mml:mo><mml:mi>s</mml:mi><mml:mi>i</mml:mi><mml:mi>n</mml:mi><mml:mrow><mml:mo stretchy="true">(</mml:mo><mml:mrow><mml:mi>ϕ</mml:mi><mml:mo>′</mml:mo></mml:mrow><mml:mo stretchy="true">)</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:mi>a</mml:mi></mml:math><graphic xlink:href="41598_2018_32673_Article_Equ5.gif" position="anchor"/></alternatives></disp-formula>which returned <italic>a</italic>, the amplitude of the function. We then bootstrapped (10000 samples of 20 randomly drawn pairs of simulated φ’ and Δr chosen with replacement) fits to the simulated data to obtain a distribution of <italic>a</italic> for these null data (<italic>a</italic><sub><italic>null</italic></sub>). From this distribution, we computed a z-score for the observed amplitude parameter as<disp-formula id="Equ6"><label>6</label><alternatives><tex-math id="M11">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$z=\frac{{a}_{obs}-{a}_{null}}{(u-l)/(2\,\times \,1.96)}$$\end{document}</tex-math><mml:math id="M12" display="block"><mml:mi>z</mml:mi><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:msub><mml:mrow><mml:mi>a</mml:mi></mml:mrow><mml:mrow><mml:mi>o</mml:mi><mml:mi>b</mml:mi><mml:mi>s</mml:mi></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mrow><mml:mi>a</mml:mi></mml:mrow><mml:mrow><mml:mi>n</mml:mi><mml:mi>u</mml:mi><mml:mi>l</mml:mi><mml:mi>l</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>u</mml:mi><mml:mo>−</mml:mo><mml:mi>l</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mn>2</mml:mn><mml:mspace width="-0.15em"/><mml:mo>×</mml:mo><mml:mspace width="-0.15em"/><mml:mn>1.96</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mfrac></mml:math><graphic xlink:href="41598_2018_32673_Article_Equ6.gif" position="anchor"/></alternatives></disp-formula>where <italic>a</italic><sub><italic>obs</italic></sub> is the amplitude parameter of the fit to the observed data and <italic>u</italic> and <italic>l</italic> are the upper and lower 95% confidence bounds from the bootstrapped fits to the shuffled data, respectively.</p>
              </sec>
              <sec id="Sec12">
                <title>Diffusion Model Analysis</title>
                <p id="Par41">For binary choices, sequential-sampling models assume that upon presentation of the stimulus, the decision maker sequentially samples information from the stimulus display over time, which provides sensory evidence to a decision process. It also assumes that the decision process accumulates this evidence in a noisy manner for choosing one option over the other, here “modulation present” or “modulation absent.” Sequential-sampling models account simultaneously for choice frequency and choice response times. However, the focus here will be on choice frequencies. Let X(t) denote the random variable representing the numerical value of the accumulated evidence at time <italic>t</italic>. A bias, <italic>β</italic>, (i.e., prior beliefs about the stimulus before it is presented) can influence the initial starting position of the decision process, X(0). This initial state may either favor choice option “modulation present” (X(0) &gt; 0) or choice option “modulation absent” (X(0) &lt; 0). X(0) = 0 reflects an unbiased response. (The initial states can also be given a probability distribution). The participant then samples small increments of evidence at any moment in time, which either favor response “modulation present” (dX(t) &gt; 0) or response “modulation absent” (dX(t) &lt; 0). The evidence is incremented according to a diffusion process. In particular, we apply a Wiener process with drift, lately called drift-diffusion model<sup><xref ref-type="bibr" rid="CR72">72</xref></sup> with<disp-formula id="Equ7"><label>7</label><alternatives><tex-math id="M13">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$dX(t)=\delta +\sigma dW(t)$$\end{document}</tex-math><mml:math id="M14" display="block"><mml:mi>d</mml:mi><mml:mi>X</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mi>δ</mml:mi><mml:mo>+</mml:mo><mml:mi>σ</mml:mi><mml:mi>d</mml:mi><mml:mi>W</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:math><graphic xlink:href="41598_2018_32673_Article_Equ7.gif" position="anchor"/></alternatives></disp-formula>The drift rate, <italic>δ</italic>, describes the expected value of evidence increments per unit time. The diffusion rate, <italic>σ</italic>, in front of the standard Wiener process, <italic>W(t)</italic>, relates to the variance of the increments. Here we set <italic>σ</italic> = 1. The small increments of evidence sampled at any moment in time are such that they either favor response “modulation present” (<italic>dX(t)</italic> &gt; 0) or response “modulation absent” (<italic>dX(t)</italic> &lt; 0). This process continues until the magnitude of the cumulative evidence exceeds a threshold criterion, <italic>θ</italic>. That is, the process stops and response “modulation present” is initiated as soon as the accumulated evidence reaches a criterion value for choosing response “modulation present” (here, <italic>X(t)</italic> = <italic>θ</italic> &gt; 0), or it stops and a “modulation absent” response is initiated as soon as the accumulated evidence reaches a criterion value for choosing response “modulation absent” (here, <italic>X(t)</italic> = <italic>θ</italic> &lt; 0). The probability of choosing the response “modulation present” over “modulation absent” is determined by the accumulation process reaching the threshold for response “modulation present” before reaching the threshold for response “modulation absent”. The criterion is assumed to be set by the decision maker prior to the decision task. The drift rate may be related to the quality of the stimuli (i.e., the better the quality the higher the drift rate). For instance, stimuli that are easier to discriminate are reflected in a higher drift rate. In the following we consider two models. In Model 1 we assume that the physical correlation between the auditory and visual stimuli, (r<sub>av</sub>|φ<sub>0</sub>), weighted by the decision maker drives the evidence accumulation process for initiating a “modulation present” or “modulation absent” response. That is, the drift rate is defined as<disp-formula id="Equ8"><label>8</label><alternatives><tex-math id="M15">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\delta =w\times ({r}_{av}|{\varphi }_{0})$$\end{document}</tex-math><mml:math id="M16" display="block"><mml:mi>δ</mml:mi><mml:mo>=</mml:mo><mml:mi>w</mml:mi><mml:mo>×</mml:mo><mml:mrow><mml:mo stretchy="true">(</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi>r</mml:mi></mml:mrow><mml:mrow><mml:mi>a</mml:mi><mml:mi>v</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">|</mml:mo><mml:msub><mml:mrow><mml:mi>φ</mml:mi></mml:mrow><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub></mml:mrow><mml:mo stretchy="true">)</mml:mo></mml:mrow></mml:math><graphic xlink:href="41598_2018_32673_Article_Equ8.gif" position="anchor"/></alternatives></disp-formula>Of the 40 correlation coefficients several of them were identical (for instance, a 6 Hz auditory stimulus with starting phases of +45° and −45° both resulted in a correlation of 0.7075) resulting in 21 unique correlation coefficients and by that in 21 different drift rates.</p>
                <p id="Par42">In Model 2 we assume that the physical correlation between the auditory and visual stimuli is distorted by a shift in phase as perceived by the decision maker. That is, the drift rate is defined by<disp-formula id="Equ9"><label>9</label><alternatives><tex-math id="M17">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\delta =w\times ({r}_{av}|{\varphi }_{i})$$\end{document}</tex-math><mml:math id="M18" display="block"><mml:mi>δ</mml:mi><mml:mo>=</mml:mo><mml:mi>w</mml:mi><mml:mo>×</mml:mo><mml:mrow><mml:mo stretchy="true">(</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi>r</mml:mi></mml:mrow><mml:mrow><mml:mi>a</mml:mi><mml:mi>v</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">|</mml:mo><mml:msub><mml:mrow><mml:mi>φ</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo stretchy="true">)</mml:mo></mml:mrow></mml:math><graphic xlink:href="41598_2018_32673_Article_Equ9.gif" position="anchor"/></alternatives></disp-formula>where <italic>i</italic> is a free parameter of the model estimated from the data and its returned value corresponds to a phase shift that is unique to each participant (<italic>ϕ’</italic>). The model term <italic>ϕ</italic><sub><italic>i</italic></sub> relates to the initial phase term <italic>φ</italic><sub><italic>i</italic></sub> introduced earlier and follows the same naming conventions. A phase shift unequal to 0, ± 45, ± 90, ± 135, or ± 180 results in 40 different correlation coefficients which in turn results in 40 drift rates.</p>
              </sec>
              <sec id="Sec13">
                <title>Model parameters</title>
                <p id="Par43">We assume for both models that the observed response time is the sum of the decision time, modeled by the diffusion process, and a residual time, <italic>T</italic><sub><italic>r</italic></sub>, which includes the time for processes other than the decision, e.g., sensory encoding and motor components. Here, <italic>T</italic><sub><italic>r</italic></sub>, is a constant for each participant. Because correlation coefficients varied between 1 and −1 but none of the participants showed perfect performances (e.g. 100% of correct responses to either a perfectly positively correlated stimulus pair or a perfectly negatively correlated stimulus pair), we allow an adjustment by including a weight for the correlations 0 ≤ <italic>w</italic> ≤ 1. We also allow for an a priori response bias, <italic>β</italic>, in favor of one response (present/absent). The decision criteria are <italic>θ</italic> = |−<italic>θ</italic>|.</p>
                <p id="Par44">In addition to these parameters, Model 2 returns a parameter <italic>ϕ’</italic> to account for perceived correlations based on individual phase shifts (rather than correlations based on the physical stimuli only) to be estimated from the data. To summarize: For Model 1 four parameters (<italic>w</italic>, <italic>β</italic>, <italic>θ</italic>, <italic>T</italic><sub><italic>r</italic></sub>) are estimated from 63 data points (21 relative frequencies for correct responses, 21 mean response times for correct responses, 21 mean response times for incorrect responses. Trials with identical correlations were collapsed.) For Model 2 five parameters (<italic>ϕ’</italic>, <italic>w</italic>, <italic>β</italic>, <italic>θ</italic>, <italic>T</italic><sub><italic>r</italic></sub>) are estimated from 120 data points (40 relative frequencies for correct responses, 40 mean response times for correct responses, 40 mean response times for incorrect responses).</p>
                <p id="Par45">The model was implemented in terms of the matrix approach<sup><xref ref-type="bibr" rid="CR73">73</xref></sup> and parameters were estimated by minimizing the chi-square function<sup><xref ref-type="bibr" rid="CR74">74</xref></sup>,<disp-formula id="Equ10"><label>10</label><alternatives><tex-math id="M19">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${\chi }^{2}=\sum {(\frac{R{T}_{obs}-R{T}_{pred}}{S{E}_{R{T}_{obs}}})}^{2}+{\sum (\frac{P{r}_{obs}-P{r}_{pred}}{S{E}_{P{r}_{obs}}})}^{2}$$\end{document}</tex-math><mml:math id="M20" display="block"><mml:msup><mml:mrow><mml:mi>χ</mml:mi></mml:mrow><mml:mn>2</mml:mn></mml:msup><mml:mo>=</mml:mo><mml:mo>∑</mml:mo><mml:mrow><mml:msup><mml:mrow><mml:mo stretchy="true">(</mml:mo><mml:mrow><mml:mfrac><mml:mrow><mml:mi>R</mml:mi><mml:msub><mml:mrow><mml:mi>T</mml:mi></mml:mrow><mml:mrow><mml:mi>o</mml:mi><mml:mi>b</mml:mi><mml:mi>s</mml:mi></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:mi>R</mml:mi><mml:msub><mml:mrow><mml:mi>T</mml:mi></mml:mrow><mml:mrow><mml:mi>p</mml:mi><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>d</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:mi>S</mml:mi><mml:msub><mml:mrow><mml:mi>E</mml:mi></mml:mrow><mml:mrow><mml:mi>R</mml:mi><mml:msub><mml:mrow><mml:mi>T</mml:mi></mml:mrow><mml:mrow><mml:mi>o</mml:mi><mml:mi>b</mml:mi><mml:mi>s</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:msub></mml:mrow></mml:mfrac></mml:mrow><mml:mo stretchy="true">)</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:msup></mml:mrow><mml:mo>+</mml:mo><mml:msup><mml:mrow><mml:mo>∑</mml:mo><mml:mrow><mml:mo stretchy="true">(</mml:mo><mml:mrow><mml:mfrac><mml:mrow><mml:mi>P</mml:mi><mml:msub><mml:mrow><mml:mi>r</mml:mi></mml:mrow><mml:mrow><mml:mi>o</mml:mi><mml:mi>b</mml:mi><mml:mi>s</mml:mi></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:mi>P</mml:mi><mml:msub><mml:mrow><mml:mi>r</mml:mi></mml:mrow><mml:mrow><mml:mi>p</mml:mi><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>d</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:mi>S</mml:mi><mml:msub><mml:mrow><mml:mi>E</mml:mi></mml:mrow><mml:mrow><mml:mi>P</mml:mi><mml:msub><mml:mrow><mml:mi>r</mml:mi></mml:mrow><mml:mrow><mml:mi>o</mml:mi><mml:mi>b</mml:mi><mml:mi>s</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:msub></mml:mrow></mml:mfrac></mml:mrow><mml:mo stretchy="true">)</mml:mo></mml:mrow></mml:mrow><mml:mn>2</mml:mn></mml:msup></mml:math><graphic xlink:href="41598_2018_32673_Article_Equ10.gif" position="anchor"/></alternatives></disp-formula>using the optimization routine <italic>fminsearchbnd</italic> in MATLAB. The <italic>fminsearchbnd</italic> routine is similar to the standard <italic>fminsearch</italic> routine except that the range of the parameters of the parameters can be predetermined, for instance, positive real numbers for the residuals, or real numbers between 0 and 1 for the weights. The <italic>fminsearch</italic> uses the Nelder-Mead simplex search method<sup><xref ref-type="bibr" rid="CR75">75</xref></sup>. <inline-formula id="IEq1"><alternatives><tex-math id="M21">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$S{E}_{R{T}_{obs}}$$\end{document}</tex-math><mml:math id="M22"><mml:mi>S</mml:mi><mml:msub><mml:mrow><mml:mi>E</mml:mi></mml:mrow><mml:mrow><mml:mi>R</mml:mi><mml:msub><mml:mrow><mml:mi>T</mml:mi></mml:mrow><mml:mrow><mml:mi>o</mml:mi><mml:mi>b</mml:mi><mml:mi>s</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:msub></mml:math><inline-graphic xlink:href="41598_2018_32673_Article_IEq1.gif"/></alternatives></inline-formula> and <inline-formula id="IEq2"><alternatives><tex-math id="M23">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$S{E}_{P{r}_{obs}}$$\end{document}</tex-math><mml:math id="M24"><mml:mi>S</mml:mi><mml:msub><mml:mrow><mml:mi>E</mml:mi></mml:mrow><mml:mrow><mml:mi>P</mml:mi><mml:msub><mml:mrow><mml:mi>r</mml:mi></mml:mrow><mml:mrow><mml:mi>o</mml:mi><mml:mi>b</mml:mi><mml:mi>s</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:msub></mml:math><inline-graphic xlink:href="41598_2018_32673_Article_IEq2.gif"/></alternatives></inline-formula> refer to the standard error of the observed mean response times and relative choice frequencies, respectively. Note that mean response times and relative choice frequencies are conditioned on the stimulus presented. Here we consider only the trials in which a modulation was present.</p>
                <p id="Par46">For both models, the following procedures/restrictions to parameter values were imposed in the estimation procedure: The decision criteria (absorbing boundaries) were estimated using a search grid. This was done because it quickens the estimation procedure when boundaries are integers (matrix approach). <italic>θ</italic> ranged from 3 to 20 in steps of 1. The residual time, <italic>T</italic><sub><italic>r</italic></sub>, was restricted to 100 ms ≤ <italic>T</italic><sub><italic>r</italic></sub> ≤ 800 ms and the weight to 0.0001 ≤ <italic>w</italic> ≤ 1. For the Model 2 parameter <italic>ϕ</italic><sub><italic>i</italic></sub>, the value of <italic>i</italic> was restricted to integers ranging from −180 to 180 in steps of 1. For each value of <italic>i</italic> in Model 2, a different set of correlations was computed.</p>
              </sec>
            </sec>
          </body>
          <back>
            <fn-group>
              <fn>
                <p><bold>Publisher's note:</bold> Springer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations.</p>
              </fn>
            </fn-group>
            <ack>
              <title>Acknowledgements</title>
              <p>Support for this work was provided by NIH grant HD083211 to MTW and DFG grant DI506/15-1 to AD. We would like to thank Dr. David Simon for numerous conversations that helped guide development and analysis of the experiments and the Cognition and Cognitive Neuroscience modeling group at Vanderbilt University and especially Dr. Jeffery Annis for valuable advice during an early presentation of these data. Finally, we thank two anonymous reviewers for helpful feedback on the manuscript.</p>
            </ack>
            <notes notes-type="author-contribution">
              <title>Author Contributions</title>
              <p>A.N., R.R. and M.W. designed the experiment. A.N. collected data. A.N. and A.D. analyzed the data and wrote the manuscript. A.N., A.D., R.R. and M.W. revised, edited, and approved the final version of the manuscript.</p>
            </notes>
            <notes notes-type="data-availability">
              <title>Data Availability</title>
              <p>The datasets generated and analyzed during the current study are available from the corresponding author on reasonable request.</p>
            </notes>
            <notes notes-type="COI-statement">
              <sec id="FPar1">
                <title>Competing Interests</title>
                <p>The authors declare no competing interests.</p>
              </sec>
            </notes>
            <ref-list id="Bib1">
              <title>References</title>
              <ref id="CR1">
                <label>1.</label>
                <mixed-citation publication-type="other"><italic>The New Handbook of Multisensory Processes</italic>. (MIT Press, 2012).</mixed-citation>
              </ref>
              <ref id="CR2">
                <label>2.</label>
                <element-citation publication-type="journal">
                  <person-group person-group-type="author">
                    <name>
                      <surname>Frassinetti</surname>
                      <given-names>F</given-names>
                    </name>
                    <name>
                      <surname>Bolognini</surname>
                      <given-names>N</given-names>
                    </name>
                    <name>
                      <surname>Làdavas</surname>
                      <given-names>E</given-names>
                    </name>
                  </person-group>
                  <article-title>Enhancement of visual perception by crossmodal visuo-auditory interaction</article-title>
                  <source>Exp. Brain Res.</source>
                  <year>2002</year>
                  <volume>147</volume>
                  <fpage>332</fpage>
                  <lpage>343</lpage>
                  <pub-id pub-id-type="doi">10.1007/s00221-002-1262-y</pub-id>
                  <pub-id pub-id-type="pmid">12428141</pub-id>
                </element-citation>
              </ref>
              <ref id="CR3">
                <label>3.</label>
                <element-citation publication-type="journal">
                  <person-group person-group-type="author">
                    <name>
                      <surname>Odegaard</surname>
                      <given-names>B</given-names>
                    </name>
                    <name>
                      <surname>Wozny</surname>
                      <given-names>DR</given-names>
                    </name>
                    <name>
                      <surname>Shams</surname>
                      <given-names>L</given-names>
                    </name>
                  </person-group>
                  <article-title>Biases in Visual, Auditory, and Audiovisual Perception of Space</article-title>
                  <source>PLOS Comput. Biol.</source>
                  <year>2015</year>
                  <volume>11</volume>
                  <fpage>e1004649</fpage>
                  <pub-id pub-id-type="doi">10.1371/journal.pcbi.1004649</pub-id>
                  <pub-id pub-id-type="pmid">26646312</pub-id>
                </element-citation>
              </ref>
              <ref id="CR4">
                <label>4.</label>
                <element-citation publication-type="journal">
                  <person-group person-group-type="author">
                    <name>
                      <surname>Sumby</surname>
                      <given-names>WH</given-names>
                    </name>
                    <name>
                      <surname>Pollack</surname>
                      <given-names>I</given-names>
                    </name>
                  </person-group>
                  <article-title>Visual Contribution to Speech Intelligibility in Noise</article-title>
                  <source>J. Acoust. Soc. Am.</source>
                  <year>1954</year>
                  <volume>26</volume>
                  <fpage>212</fpage>
                  <lpage>215</lpage>
                  <pub-id pub-id-type="doi">10.1121/1.1907309</pub-id>
                </element-citation>
              </ref>
              <ref id="CR5">
                <label>5.</label>
                <element-citation publication-type="journal">
                  <person-group person-group-type="author">
                    <name>
                      <surname>Hershenson</surname>
                      <given-names>M</given-names>
                    </name>
                  </person-group>
                  <article-title>Reaction time as a measure of intersensory facilitation</article-title>
                  <source>J. Exp. Psychol.</source>
                  <year>1962</year>
                  <volume>63</volume>
                  <fpage>289</fpage>
                  <lpage>293</lpage>
                  <pub-id pub-id-type="doi">10.1037/h0039516</pub-id>
                  <pub-id pub-id-type="pmid">13906889</pub-id>
                </element-citation>
              </ref>
              <ref id="CR6">
                <label>6.</label>
                <element-citation publication-type="journal">
                  <person-group person-group-type="author">
                    <name>
                      <surname>Frens</surname>
                      <given-names>MA</given-names>
                    </name>
                    <name>
                      <surname>Van Opstal</surname>
                      <given-names>AJ</given-names>
                    </name>
                  </person-group>
                  <article-title>A quantitative study of auditory-evoked saccadic eye movements in two dimensions</article-title>
                  <source>Exp. brain Res.</source>
                  <year>1995</year>
                  <volume>107</volume>
                  <fpage>103</fpage>
                  <lpage>17</lpage>
                  <pub-id pub-id-type="doi">10.1007/BF00228022</pub-id>
                  <pub-id pub-id-type="pmid">8751068</pub-id>
                </element-citation>
              </ref>
              <ref id="CR7">
                <label>7.</label>
                <element-citation publication-type="journal">
                  <person-group person-group-type="author">
                    <name>
                      <surname>Meredith</surname>
                      <given-names>MA</given-names>
                    </name>
                    <name>
                      <surname>Nemitz</surname>
                      <given-names>JW</given-names>
                    </name>
                    <name>
                      <surname>Stein</surname>
                      <given-names>BE</given-names>
                    </name>
                  </person-group>
                  <article-title>Determinants of multisensory integration in superior colliculus neurons. I. Temporal factors</article-title>
                  <source>J. Neurosci.</source>
                  <year>1987</year>
                  <volume>7</volume>
                  <fpage>3215</fpage>
                  <lpage>29</lpage>
                  <pub-id pub-id-type="doi">10.1523/JNEUROSCI.07-10-03215.1987</pub-id>
                  <pub-id pub-id-type="pmid">3668625</pub-id>
                </element-citation>
              </ref>
              <ref id="CR8">
                <label>8.</label>
                <element-citation publication-type="journal">
                  <person-group person-group-type="author">
                    <name>
                      <surname>Meredith</surname>
                      <given-names>MA</given-names>
                    </name>
                    <name>
                      <surname>Stein</surname>
                      <given-names>BE</given-names>
                    </name>
                  </person-group>
                  <article-title>Spatial factors determine the activity of multisensory neurons in cat superior colliculus</article-title>
                  <source>Brain Res.</source>
                  <year>1986</year>
                  <volume>365</volume>
                  <fpage>350</fpage>
                  <lpage>354</lpage>
                  <pub-id pub-id-type="doi">10.1016/0006-8993(86)91648-3</pub-id>
                  <pub-id pub-id-type="pmid">3947999</pub-id>
                </element-citation>
              </ref>
              <ref id="CR9">
                <label>9.</label>
                <element-citation publication-type="journal">
                  <person-group person-group-type="author">
                    <name>
                      <surname>Bolognini</surname>
                      <given-names>N</given-names>
                    </name>
                    <name>
                      <surname>Frassinetti</surname>
                      <given-names>F</given-names>
                    </name>
                    <name>
                      <surname>Serino</surname>
                      <given-names>A</given-names>
                    </name>
                    <name>
                      <surname>Làdavas</surname>
                      <given-names>E</given-names>
                    </name>
                  </person-group>
                  <article-title>‘Acoustical vision’ of below threshold stimuli: Interaction among spatially converging audiovisual inputs</article-title>
                  <source>Exp. Brain Res.</source>
                  <year>2005</year>
                  <volume>160</volume>
                  <fpage>273</fpage>
                  <lpage>282</lpage>
                  <pub-id pub-id-type="doi">10.1007/s00221-004-2005-z</pub-id>
                  <pub-id pub-id-type="pmid">15551091</pub-id>
                </element-citation>
              </ref>
              <ref id="CR10">
                <label>10.</label>
                <element-citation publication-type="journal">
                  <person-group person-group-type="author">
                    <name>
                      <surname>Körding</surname>
                      <given-names>KP</given-names>
                    </name>
                    <etal/>
                  </person-group>
                  <article-title>Causal Inference in Multisensory Perception</article-title>
                  <source>PLoS One</source>
                  <year>2007</year>
                  <volume>2</volume>
                  <fpage>e943</fpage>
                  <pub-id pub-id-type="doi">10.1371/journal.pone.0000943</pub-id>
                  <pub-id pub-id-type="pmid">17895984</pub-id>
                </element-citation>
              </ref>
              <ref id="CR11">
                <label>11.</label>
                <element-citation publication-type="journal">
                  <person-group person-group-type="author">
                    <name>
                      <surname>Magnotti</surname>
                      <given-names>JF</given-names>
                    </name>
                    <name>
                      <surname>Ma</surname>
                      <given-names>WJ</given-names>
                    </name>
                    <name>
                      <surname>Beauchamp</surname>
                      <given-names>MS</given-names>
                    </name>
                  </person-group>
                  <article-title>Causal inference of asynchronous audiovisual speech</article-title>
                  <source>Front. Psychol.</source>
                  <year>2013</year>
                  <volume>4</volume>
                  <fpage>798</fpage>
                  <pub-id pub-id-type="doi">10.3389/fpsyg.2013.00798</pub-id>
                  <pub-id pub-id-type="pmid">24294207</pub-id>
                </element-citation>
              </ref>
              <ref id="CR12">
                <label>12.</label>
                <element-citation publication-type="journal">
                  <person-group person-group-type="author">
                    <name>
                      <surname>Parise</surname>
                      <given-names>CV</given-names>
                    </name>
                    <name>
                      <surname>Harrar</surname>
                      <given-names>V</given-names>
                    </name>
                    <name>
                      <surname>Ernst</surname>
                      <given-names>MO</given-names>
                    </name>
                    <name>
                      <surname>Spence</surname>
                      <given-names>C</given-names>
                    </name>
                  </person-group>
                  <article-title>Cross-correlation between Auditory and Visual Signals Promotes MultisensoryIntegration</article-title>
                  <source>Multisens. Res.</source>
                  <year>2013</year>
                  <volume>26</volume>
                  <fpage>1</fpage>
                  <lpage>10</lpage>
                  <pub-id pub-id-type="pmid">23713196</pub-id>
                </element-citation>
              </ref>
              <ref id="CR13">
                <label>13.</label>
                <element-citation publication-type="journal">
                  <person-group person-group-type="author">
                    <name>
                      <surname>Chuen</surname>
                      <given-names>L</given-names>
                    </name>
                    <name>
                      <surname>Schutz</surname>
                      <given-names>M</given-names>
                    </name>
                  </person-group>
                  <article-title>The unity assumption facilitates cross-modal binding of musical, non-speech stimuli: The role of spectral and amplitude envelope cues</article-title>
                  <source>Attention, Perception, Psychophys.</source>
                  <year>2016</year>
                  <volume>78</volume>
                  <fpage>1512</fpage>
                  <lpage>1528</lpage>
                  <pub-id pub-id-type="doi">10.3758/s13414-016-1088-5</pub-id>
                </element-citation>
              </ref>
              <ref id="CR14">
                <label>14.</label>
                <element-citation publication-type="journal">
                  <person-group person-group-type="author">
                    <name>
                      <surname>Vatakis</surname>
                      <given-names>A</given-names>
                    </name>
                    <name>
                      <surname>Spence</surname>
                      <given-names>C</given-names>
                    </name>
                  </person-group>
                  <article-title>Crossmodal binding: evaluating the ‘unity assumption’ using audiovisual speech stimuli</article-title>
                  <source>Percept. Psychophys.</source>
                  <year>2007</year>
                  <volume>69</volume>
                  <fpage>744</fpage>
                  <lpage>756</lpage>
                  <pub-id pub-id-type="doi">10.3758/BF03193776</pub-id>
                  <pub-id pub-id-type="pmid">17929697</pub-id>
                </element-citation>
              </ref>
              <ref id="CR15">
                <label>15.</label>
                <element-citation publication-type="journal">
                  <person-group person-group-type="author">
                    <name>
                      <surname>Jack</surname>
                      <given-names>CE</given-names>
                    </name>
                    <name>
                      <surname>Thurlow</surname>
                      <given-names>WR</given-names>
                    </name>
                  </person-group>
                  <article-title>Effects of degree of visual association and angle of displacement on the ‘ventriloquism’ effect</article-title>
                  <source>Percept. Mot. Skills</source>
                  <year>1973</year>
                  <volume>37</volume>
                  <fpage>967</fpage>
                  <lpage>979</lpage>
                  <?supplied-pmid 4764534?>
                  <pub-id pub-id-type="pmid">4764534</pub-id>
                </element-citation>
              </ref>
              <ref id="CR16">
                <label>16.</label>
                <mixed-citation publication-type="other">Chandrasekaran, C., Trubanova, A., Stillittano, S., Caplier, A. &amp; Ghazanfar, A. A. The natural statistics of audiovisual speech. <italic>PLoS Comput. Biol</italic>. <bold>5</bold> (2009).</mixed-citation>
              </ref>
              <ref id="CR17">
                <label>17.</label>
                <element-citation publication-type="journal">
                  <person-group person-group-type="author">
                    <name>
                      <surname>Elhilali</surname>
                      <given-names>M</given-names>
                    </name>
                    <name>
                      <surname>Ma</surname>
                      <given-names>L</given-names>
                    </name>
                    <name>
                      <surname>Micheyl</surname>
                      <given-names>C</given-names>
                    </name>
                    <name>
                      <surname>Oxenham</surname>
                      <given-names>AJ</given-names>
                    </name>
                    <name>
                      <surname>Shamma</surname>
                      <given-names>SA</given-names>
                    </name>
                  </person-group>
                  <article-title>Temporal Coherence in the Perceptual Organization and Cortical Representation of Auditory Scenes</article-title>
                  <source>Neuron</source>
                  <year>2009</year>
                  <volume>61</volume>
                  <fpage>317</fpage>
                  <lpage>329</lpage>
                  <pub-id pub-id-type="doi">10.1016/j.neuron.2008.12.005</pub-id>
                  <pub-id pub-id-type="pmid">19186172</pub-id>
                </element-citation>
              </ref>
              <ref id="CR18">
                <label>18.</label>
                <element-citation publication-type="journal">
                  <person-group person-group-type="author">
                    <name>
                      <surname>Blake</surname>
                      <given-names>R</given-names>
                    </name>
                    <name>
                      <surname>Lee</surname>
                      <given-names>S-H</given-names>
                    </name>
                  </person-group>
                  <article-title>The role of temporal structure in human vision</article-title>
                  <source>Behav. Cogn. Neurosci. Rev.</source>
                  <year>2005</year>
                  <volume>4</volume>
                  <fpage>21</fpage>
                  <lpage>42</lpage>
                  <pub-id pub-id-type="doi">10.1177/1534582305276839</pub-id>
                  <pub-id pub-id-type="pmid">15886401</pub-id>
                </element-citation>
              </ref>
              <ref id="CR19">
                <label>19.</label>
                <element-citation publication-type="journal">
                  <person-group person-group-type="author">
                    <name>
                      <surname>Munhall</surname>
                      <given-names>KG</given-names>
                    </name>
                    <name>
                      <surname>Gribble</surname>
                      <given-names>P</given-names>
                    </name>
                    <name>
                      <surname>Sacco</surname>
                      <given-names>L</given-names>
                    </name>
                    <name>
                      <surname>Ward</surname>
                      <given-names>M</given-names>
                    </name>
                  </person-group>
                  <article-title>Temporal constraints on the McGurk effect</article-title>
                  <source>Percept. Psychophys.</source>
                  <year>1996</year>
                  <volume>58</volume>
                  <fpage>351</fpage>
                  <lpage>362</lpage>
                  <pub-id pub-id-type="doi">10.3758/BF03206811</pub-id>
                  <pub-id pub-id-type="pmid">8935896</pub-id>
                </element-citation>
              </ref>
              <ref id="CR20">
                <label>20.</label>
                <element-citation publication-type="journal">
                  <person-group person-group-type="author">
                    <name>
                      <surname>Parise</surname>
                      <given-names>CV</given-names>
                    </name>
                    <name>
                      <surname>Spence</surname>
                      <given-names>C</given-names>
                    </name>
                    <name>
                      <surname>Ernst</surname>
                      <given-names>MO</given-names>
                    </name>
                  </person-group>
                  <article-title>When correlation implies causation in multisensory integration</article-title>
                  <source>Curr. Biol.</source>
                  <year>2012</year>
                  <volume>22</volume>
                  <fpage>46</fpage>
                  <lpage>49</lpage>
                  <pub-id pub-id-type="doi">10.1016/j.cub.2011.11.039</pub-id>
                  <pub-id pub-id-type="pmid">22177899</pub-id>
                </element-citation>
              </ref>
              <ref id="CR21">
                <label>21.</label>
                <element-citation publication-type="journal">
                  <person-group person-group-type="author">
                    <name>
                      <surname>Bizley</surname>
                      <given-names>JK</given-names>
                    </name>
                    <name>
                      <surname>Maddox</surname>
                      <given-names>RK</given-names>
                    </name>
                    <name>
                      <surname>Lee</surname>
                      <given-names>AKC</given-names>
                    </name>
                  </person-group>
                  <article-title>Defining Auditory-Visual Objects: Behavioral Tests and Physiological Mechanisms</article-title>
                  <source>Trends in Neurosciences</source>
                  <year>2016</year>
                  <volume>39</volume>
                  <fpage>74</fpage>
                  <lpage>85</lpage>
                  <pub-id pub-id-type="doi">10.1016/j.tins.2015.12.007</pub-id>
                  <pub-id pub-id-type="pmid">26775728</pub-id>
                </element-citation>
              </ref>
              <ref id="CR22">
                <label>22.</label>
                <element-citation publication-type="journal">
                  <person-group person-group-type="author">
                    <name>
                      <surname>Maddox</surname>
                      <given-names>RK</given-names>
                    </name>
                    <name>
                      <surname>Atilgan</surname>
                      <given-names>H</given-names>
                    </name>
                    <name>
                      <surname>Bizley</surname>
                      <given-names>JK</given-names>
                    </name>
                    <name>
                      <surname>Lee</surname>
                      <given-names>AK</given-names>
                    </name>
                  </person-group>
                  <article-title>Auditory selective attention is enhanced by a task-irrelevant temporally coherent visual stimulus in human listeners</article-title>
                  <source>Elife</source>
                  <year>2015</year>
                  <volume>2015</volume>
                  <fpage>1</fpage>
                  <lpage>11</lpage>
                </element-citation>
              </ref>
              <ref id="CR23">
                <label>23.</label>
                <element-citation publication-type="journal">
                  <person-group person-group-type="author">
                    <name>
                      <surname>Grant</surname>
                      <given-names>KW</given-names>
                    </name>
                    <name>
                      <surname>Seitz</surname>
                      <given-names>PFP</given-names>
                    </name>
                  </person-group>
                  <article-title>The use of visible speech cues for improving auditory detection of spoken sentences</article-title>
                  <source>J. Acoust. Soc. Am.</source>
                  <year>2000</year>
                  <volume>108</volume>
                  <fpage>1197</fpage>
                  <lpage>1208</lpage>
                  <pub-id pub-id-type="doi">10.1121/1.1288668</pub-id>
                  <pub-id pub-id-type="pmid">11008820</pub-id>
                </element-citation>
              </ref>
              <ref id="CR24">
                <label>24.</label>
                <element-citation publication-type="journal">
                  <person-group person-group-type="author">
                    <name>
                      <surname>Parise</surname>
                      <given-names>CV</given-names>
                    </name>
                    <name>
                      <surname>Ernst</surname>
                      <given-names>MO</given-names>
                    </name>
                  </person-group>
                  <article-title>Correlation detection as a general mechanism for multisensory integration</article-title>
                  <source>Nat. Commun.</source>
                  <year>2016</year>
                  <volume>7</volume>
                  <fpage>364</fpage>
                  <pub-id pub-id-type="doi">10.1038/ncomms11543</pub-id>
                </element-citation>
              </ref>
              <ref id="CR25">
                <label>25.</label>
                <element-citation publication-type="journal">
                  <person-group person-group-type="author">
                    <name>
                      <surname>Voss</surname>
                      <given-names>A</given-names>
                    </name>
                    <name>
                      <surname>Rothermund</surname>
                      <given-names>K</given-names>
                    </name>
                    <name>
                      <surname>Voss</surname>
                      <given-names>J</given-names>
                    </name>
                  </person-group>
                  <article-title>Interpreting the parameters of the diffusion model: an empirical validation</article-title>
                  <source>Mem. Cognit.</source>
                  <year>2004</year>
                  <volume>32</volume>
                  <fpage>1206</fpage>
                  <lpage>1220</lpage>
                  <pub-id pub-id-type="doi">10.3758/BF03196893</pub-id>
                </element-citation>
              </ref>
              <ref id="CR26">
                <label>26.</label>
                <element-citation publication-type="journal">
                  <person-group person-group-type="author">
                    <name>
                      <surname>Laming</surname>
                      <given-names>DRJ</given-names>
                    </name>
                  </person-group>
                  <article-title>Information theory of choice-reaction times</article-title>
                  <source>Inf. theory choicereaction times</source>
                  <year>1968</year>
                  <volume>14</volume>
                  <fpage>172</fpage>
                </element-citation>
              </ref>
              <ref id="CR27">
                <label>27.</label>
                <element-citation publication-type="journal">
                  <person-group person-group-type="author">
                    <name>
                      <surname>Wallace</surname>
                      <given-names>MT</given-names>
                    </name>
                    <name>
                      <surname>Wilkinson</surname>
                      <given-names>LK</given-names>
                    </name>
                    <name>
                      <surname>Stein</surname>
                      <given-names>BE</given-names>
                    </name>
                  </person-group>
                  <article-title>Representation and integration of multiple sensory inputs in primate superior colliculus</article-title>
                  <source>J. Neurophysiol.</source>
                  <year>1996</year>
                  <volume>76</volume>
                  <fpage>1246</fpage>
                  <lpage>1266</lpage>
                  <pub-id pub-id-type="doi">10.1152/jn.1996.76.2.1246</pub-id>
                  <pub-id pub-id-type="pmid">8871234</pub-id>
                </element-citation>
              </ref>
              <ref id="CR28">
                <label>28.</label>
                <element-citation publication-type="journal">
                  <person-group person-group-type="author">
                    <name>
                      <surname>Bushara</surname>
                      <given-names>KO</given-names>
                    </name>
                    <name>
                      <surname>Grafman</surname>
                      <given-names>J</given-names>
                    </name>
                    <name>
                      <surname>Hallett</surname>
                      <given-names>M</given-names>
                    </name>
                  </person-group>
                  <article-title>Neural correlates of auditory-visual stimulus onset asynchrony detection</article-title>
                  <source>J. Neurosci.</source>
                  <year>2001</year>
                  <volume>21</volume>
                  <fpage>300</fpage>
                  <lpage>4</lpage>
                  <pub-id pub-id-type="doi">10.1523/JNEUROSCI.21-01-00300.2001</pub-id>
                  <pub-id pub-id-type="pmid">11150347</pub-id>
                </element-citation>
              </ref>
              <ref id="CR29">
                <label>29.</label>
                <element-citation publication-type="journal">
                  <person-group person-group-type="author">
                    <name>
                      <surname>Macaluso</surname>
                      <given-names>E</given-names>
                    </name>
                    <name>
                      <surname>Frith</surname>
                      <given-names>CD</given-names>
                    </name>
                    <name>
                      <surname>Driver</surname>
                      <given-names>J</given-names>
                    </name>
                  </person-group>
                  <article-title>Crossmodal spatial influences of touch on extrastriate visual areas take current gaze direction into account</article-title>
                  <source>Neuron</source>
                  <year>2002</year>
                  <volume>34</volume>
                  <fpage>647</fpage>
                  <lpage>658</lpage>
                  <pub-id pub-id-type="doi">10.1016/S0896-6273(02)00678-5</pub-id>
                  <pub-id pub-id-type="pmid">12062047</pub-id>
                </element-citation>
              </ref>
              <ref id="CR30">
                <label>30.</label>
                <element-citation publication-type="journal">
                  <person-group person-group-type="author">
                    <name>
                      <surname>Macaluso</surname>
                      <given-names>E</given-names>
                    </name>
                    <name>
                      <surname>George</surname>
                      <given-names>N</given-names>
                    </name>
                    <name>
                      <surname>Dolan</surname>
                      <given-names>R</given-names>
                    </name>
                    <name>
                      <surname>Spence</surname>
                      <given-names>C</given-names>
                    </name>
                    <name>
                      <surname>Driver</surname>
                      <given-names>J</given-names>
                    </name>
                  </person-group>
                  <article-title>Spatial and temporal factors during processing of audiovisual speech: a PET study</article-title>
                  <source>Neuroimage</source>
                  <year>2004</year>
                  <volume>21</volume>
                  <fpage>725</fpage>
                  <lpage>732</lpage>
                  <pub-id pub-id-type="doi">10.1016/j.neuroimage.2003.09.049</pub-id>
                  <pub-id pub-id-type="pmid">14980575</pub-id>
                </element-citation>
              </ref>
              <ref id="CR31">
                <label>31.</label>
                <element-citation publication-type="journal">
                  <person-group person-group-type="author">
                    <name>
                      <surname>Senkowski</surname>
                      <given-names>D</given-names>
                    </name>
                    <name>
                      <surname>Talsma</surname>
                      <given-names>D</given-names>
                    </name>
                    <name>
                      <surname>Grigutsch</surname>
                      <given-names>M</given-names>
                    </name>
                    <name>
                      <surname>Herrmann</surname>
                      <given-names>CS</given-names>
                    </name>
                    <name>
                      <surname>Woldorff</surname>
                      <given-names>MG</given-names>
                    </name>
                  </person-group>
                  <article-title>Good times for multisensory integration: Effects of the precision of temporal synchrony as revealed by gamma-band oscillations</article-title>
                  <source>Neuropsychologia</source>
                  <year>2007</year>
                  <volume>45</volume>
                  <fpage>561</fpage>
                  <lpage>571</lpage>
                  <pub-id pub-id-type="doi">10.1016/j.neuropsychologia.2006.01.013</pub-id>
                  <pub-id pub-id-type="pmid">16542688</pub-id>
                </element-citation>
              </ref>
              <ref id="CR32">
                <label>32.</label>
                <element-citation publication-type="journal">
                  <person-group person-group-type="author">
                    <name>
                      <surname>Frens</surname>
                      <given-names>Ma</given-names>
                    </name>
                    <name>
                      <surname>Van Opstal</surname>
                      <given-names>AJ</given-names>
                    </name>
                    <name>
                      <surname>Van der Willigen</surname>
                      <given-names>RF</given-names>
                    </name>
                    <name>
                      <surname>Van Opstal</surname>
                      <given-names>AJ</given-names>
                    </name>
                    <name>
                      <surname>Van Der Willigen</surname>
                      <given-names>RF</given-names>
                    </name>
                  </person-group>
                  <article-title>Spatial and temporal factors determine auditory-visual interactions in human saccadic eye movements</article-title>
                  <source>Percept. Psychophys.</source>
                  <year>1995</year>
                  <volume>57</volume>
                  <fpage>802</fpage>
                  <lpage>816</lpage>
                  <pub-id pub-id-type="doi">10.3758/BF03206796</pub-id>
                  <pub-id pub-id-type="pmid">7651805</pub-id>
                </element-citation>
              </ref>
              <ref id="CR33">
                <label>33.</label>
                <element-citation publication-type="journal">
                  <person-group person-group-type="author">
                    <name>
                      <surname>Dixon</surname>
                      <given-names>NF</given-names>
                    </name>
                    <name>
                      <surname>Spitz</surname>
                      <given-names>L</given-names>
                    </name>
                  </person-group>
                  <article-title>The detection of auditory visual desynchrony</article-title>
                  <source>Perception</source>
                  <year>1980</year>
                  <volume>9</volume>
                  <fpage>719</fpage>
                  <lpage>721</lpage>
                  <pub-id pub-id-type="doi">10.1068/p090719</pub-id>
                  <pub-id pub-id-type="pmid">7220244</pub-id>
                </element-citation>
              </ref>
              <ref id="CR34">
                <label>34.</label>
                <element-citation publication-type="journal">
                  <person-group person-group-type="author">
                    <name>
                      <surname>McGrath</surname>
                      <given-names>M</given-names>
                    </name>
                    <name>
                      <surname>Summerfield</surname>
                      <given-names>Q</given-names>
                    </name>
                  </person-group>
                  <article-title>Intermodal timing relations and audio-visual speech recognition by normal-hearing adults</article-title>
                  <source>J. Acoust. Soc. Am.</source>
                  <year>1985</year>
                  <volume>77</volume>
                  <fpage>678</fpage>
                  <lpage>685</lpage>
                  <pub-id pub-id-type="doi">10.1121/1.392336</pub-id>
                  <pub-id pub-id-type="pmid">3973239</pub-id>
                </element-citation>
              </ref>
              <ref id="CR35">
                <label>35.</label>
                <element-citation publication-type="journal">
                  <person-group person-group-type="author">
                    <name>
                      <surname>Stone</surname>
                      <given-names>JV</given-names>
                    </name>
                    <etal/>
                  </person-group>
                  <article-title>When is now? Perception of simultaneity</article-title>
                  <source>Proc. Biol. Sci.</source>
                  <year>2001</year>
                  <volume>268</volume>
                  <fpage>31</fpage>
                  <lpage>38</lpage>
                  <pub-id pub-id-type="doi">10.1098/rspb.2000.1326</pub-id>
                  <pub-id pub-id-type="pmid">12123295</pub-id>
                </element-citation>
              </ref>
              <ref id="CR36">
                <label>36.</label>
                <element-citation publication-type="journal">
                  <person-group person-group-type="author">
                    <name>
                      <surname>Colonius</surname>
                      <given-names>H</given-names>
                    </name>
                    <name>
                      <surname>Diederich</surname>
                      <given-names>A</given-names>
                    </name>
                  </person-group>
                  <article-title>Multisensory Interaction in Saccadic Reaction Time: A Time-Window-of-Integration Model</article-title>
                  <source>J. Cogn. Neurosci.</source>
                  <year>2004</year>
                  <volume>16</volume>
                  <fpage>1000</fpage>
                  <lpage>1009</lpage>
                  <pub-id pub-id-type="doi">10.1162/0898929041502733</pub-id>
                  <pub-id pub-id-type="pmid">15298787</pub-id>
                </element-citation>
              </ref>
              <ref id="CR37">
                <label>37.</label>
                <element-citation publication-type="journal">
                  <person-group person-group-type="author">
                    <name>
                      <surname>Fujisaki</surname>
                      <given-names>W</given-names>
                    </name>
                    <name>
                      <surname>Shimojo</surname>
                      <given-names>S</given-names>
                    </name>
                    <name>
                      <surname>Kashino</surname>
                      <given-names>M</given-names>
                    </name>
                    <name>
                      <surname>Nishida</surname>
                      <given-names>S</given-names>
                    </name>
                  </person-group>
                  <article-title>Recalibration of audiovisual simultaneity</article-title>
                  <source>Nat. Neurosci.</source>
                  <year>2004</year>
                  <volume>7</volume>
                  <fpage>773</fpage>
                  <lpage>778</lpage>
                  <pub-id pub-id-type="doi">10.1038/nn1268</pub-id>
                  <pub-id pub-id-type="pmid">15195098</pub-id>
                </element-citation>
              </ref>
              <ref id="CR38">
                <label>38.</label>
                <element-citation publication-type="journal">
                  <person-group person-group-type="author">
                    <name>
                      <surname>McKendrick</surname>
                      <given-names>AM</given-names>
                    </name>
                    <name>
                      <surname>Denniss</surname>
                      <given-names>J</given-names>
                    </name>
                    <name>
                      <surname>Turpin</surname>
                      <given-names>A</given-names>
                    </name>
                  </person-group>
                  <article-title>Response times across the visual field: Empirical observations and application to threshold determination</article-title>
                  <source>Vision Res.</source>
                  <year>2014</year>
                  <volume>101</volume>
                  <fpage>1</fpage>
                  <lpage>10</lpage>
                  <pub-id pub-id-type="doi">10.1016/j.visres.2014.04.013</pub-id>
                  <pub-id pub-id-type="pmid">24802595</pub-id>
                </element-citation>
              </ref>
              <ref id="CR39">
                <label>39.</label>
                <element-citation publication-type="journal">
                  <person-group person-group-type="author">
                    <name>
                      <surname>Gold</surname>
                      <given-names>J</given-names>
                    </name>
                    <name>
                      <surname>Shadlen</surname>
                      <given-names>M</given-names>
                    </name>
                  </person-group>
                  <article-title>The neural basis of decision making</article-title>
                  <source>Annu. Rev. Neurosci</source>
                  <year>2007</year>
                  <volume>30</volume>
                  <fpage>535</fpage>
                  <lpage>574</lpage>
                  <pub-id pub-id-type="doi">10.1146/annurev.neuro.29.051605.113038</pub-id>
                  <pub-id pub-id-type="pmid">17600525</pub-id>
                </element-citation>
              </ref>
              <ref id="CR40">
                <label>40.</label>
                <element-citation publication-type="journal">
                  <person-group person-group-type="author">
                    <name>
                      <surname>Ratcliff</surname>
                      <given-names>R</given-names>
                    </name>
                    <name>
                      <surname>Smith</surname>
                      <given-names>PL</given-names>
                    </name>
                  </person-group>
                  <article-title>A Comparison of Sequential Sampling Models for Two-Choice Reaction Time</article-title>
                  <source>Psychol. Rev.</source>
                  <year>2004</year>
                  <volume>111</volume>
                  <fpage>333</fpage>
                  <lpage>367</lpage>
                  <pub-id pub-id-type="doi">10.1037/0033-295X.111.2.333</pub-id>
                  <pub-id pub-id-type="pmid">15065913</pub-id>
                </element-citation>
              </ref>
              <ref id="CR41">
                <label>41.</label>
                <element-citation publication-type="journal">
                  <person-group person-group-type="author">
                    <name>
                      <surname>Ratcliff</surname>
                      <given-names>R</given-names>
                    </name>
                    <name>
                      <surname>McKoon</surname>
                      <given-names>G</given-names>
                    </name>
                  </person-group>
                  <article-title>The Diffusion Decision Model: Theory and Data for Two-Choice Decision Tasks</article-title>
                  <source>Neural Comput.</source>
                  <year>2008</year>
                  <volume>20</volume>
                  <fpage>873</fpage>
                  <lpage>922</lpage>
                  <pub-id pub-id-type="doi">10.1162/neco.2008.12-06-420</pub-id>
                  <pub-id pub-id-type="pmid">18085991</pub-id>
                </element-citation>
              </ref>
              <ref id="CR42">
                <label>42.</label>
                <element-citation publication-type="journal">
                  <person-group person-group-type="author">
                    <name>
                      <surname>Rach</surname>
                      <given-names>S</given-names>
                    </name>
                    <name>
                      <surname>Diederich</surname>
                      <given-names>A</given-names>
                    </name>
                    <name>
                      <surname>Colonius</surname>
                      <given-names>H</given-names>
                    </name>
                  </person-group>
                  <article-title>On quantifying multisensory interaction effects in reaction time and detection rate</article-title>
                  <source>Psychol. Res.</source>
                  <year>2011</year>
                  <volume>75</volume>
                  <fpage>77</fpage>
                  <lpage>94</lpage>
                  <pub-id pub-id-type="doi">10.1007/s00426-010-0289-0</pub-id>
                  <pub-id pub-id-type="pmid">20512352</pub-id>
                </element-citation>
              </ref>
              <ref id="CR43">
                <label>43.</label>
                <element-citation publication-type="journal">
                  <person-group person-group-type="author">
                    <name>
                      <surname>Diederich</surname>
                      <given-names>A</given-names>
                    </name>
                    <name>
                      <surname>Busemeyer</surname>
                      <given-names>JR</given-names>
                    </name>
                  </person-group>
                  <article-title>Modeling the effects of payoff on response bias in a perceptual discrimination task: bound-change, drift-rate-change, or two-stage-processing hypothesis</article-title>
                  <source>Percept. Psychophys.</source>
                  <year>2006</year>
                  <volume>68</volume>
                  <fpage>194</fpage>
                  <lpage>207</lpage>
                  <pub-id pub-id-type="doi">10.3758/BF03193669</pub-id>
                  <pub-id pub-id-type="pmid">16773893</pub-id>
                </element-citation>
              </ref>
              <ref id="CR44">
                <label>44.</label>
                <element-citation publication-type="journal">
                  <person-group person-group-type="author">
                    <name>
                      <surname>Leite</surname>
                      <given-names>FP</given-names>
                    </name>
                  </person-group>
                  <article-title>A comparison of two diffusion process models in accounting for payoff and stimulus frequency manipulations</article-title>
                  <source>Attention, Perception, Psychophys.</source>
                  <year>2012</year>
                  <volume>74</volume>
                  <fpage>1366</fpage>
                  <lpage>1382</lpage>
                  <pub-id pub-id-type="doi">10.3758/s13414-012-0321-0</pub-id>
                </element-citation>
              </ref>
              <ref id="CR45">
                <label>45.</label>
                <element-citation publication-type="journal">
                  <person-group person-group-type="author">
                    <name>
                      <surname>Otto</surname>
                      <given-names>TU</given-names>
                    </name>
                    <name>
                      <surname>Mamassian</surname>
                      <given-names>P</given-names>
                    </name>
                  </person-group>
                  <article-title>Noise and correlations in parallel perceptual decision making</article-title>
                  <source>Curr. Biol.</source>
                  <year>2012</year>
                  <volume>22</volume>
                  <fpage>1391</fpage>
                  <lpage>1396</lpage>
                  <pub-id pub-id-type="doi">10.1016/j.cub.2012.05.031</pub-id>
                  <pub-id pub-id-type="pmid">22771043</pub-id>
                </element-citation>
              </ref>
              <ref id="CR46">
                <label>46.</label>
                <element-citation publication-type="journal">
                  <person-group person-group-type="author">
                    <name>
                      <surname>Bizley</surname>
                      <given-names>JK</given-names>
                    </name>
                    <name>
                      <surname>Jones</surname>
                      <given-names>GP</given-names>
                    </name>
                    <name>
                      <surname>Town</surname>
                      <given-names>SM</given-names>
                    </name>
                  </person-group>
                  <article-title>Where are multisensory signals combined for perceptual decision-making?</article-title>
                  <source>Current Opinion in Neurobiology</source>
                  <year>2016</year>
                  <volume>40</volume>
                  <fpage>31</fpage>
                  <lpage>37</lpage>
                  <pub-id pub-id-type="doi">10.1016/j.conb.2016.06.003</pub-id>
                  <pub-id pub-id-type="pmid">27344253</pub-id>
                </element-citation>
              </ref>
              <ref id="CR47">
                <label>47.</label>
                <mixed-citation publication-type="other">Simon, D. M., Nidiffer, A. R. &amp; Wallace, M. T. Rapid Recalibration to Asynchronous Audiovisual Speech Modulates the Rate of Evidence Accumulation. <italic>Sci. Rep</italic>. (accepted) (2018).</mixed-citation>
              </ref>
              <ref id="CR48">
                <label>48.</label>
                <mixed-citation publication-type="other">Mégevand, P., Molholm, S., Nayak, A. &amp; Foxe, J. J. Recalibration of the Multisensory Temporal Window of Integration Results from Changing Task Demands. <italic>PLoS One</italic><bold>8</bold> (2013).</mixed-citation>
              </ref>
              <ref id="CR49">
                <label>49.</label>
                <element-citation publication-type="journal">
                  <person-group person-group-type="author">
                    <name>
                      <surname>Diederich</surname>
                      <given-names>A</given-names>
                    </name>
                    <name>
                      <surname>Colonius</surname>
                      <given-names>H</given-names>
                    </name>
                  </person-group>
                  <article-title>The time window of multisensory integration: relating reaction times and judgments of temporal order</article-title>
                  <source>Psychol. Rev.</source>
                  <year>2015</year>
                  <volume>122</volume>
                  <fpage>232</fpage>
                  <lpage>41</lpage>
                  <pub-id pub-id-type="doi">10.1037/a0038696</pub-id>
                  <pub-id pub-id-type="pmid">25706404</pub-id>
                </element-citation>
              </ref>
              <ref id="CR50">
                <label>50.</label>
                <element-citation publication-type="journal">
                  <person-group person-group-type="author">
                    <name>
                      <surname>Ernst</surname>
                      <given-names>MO</given-names>
                    </name>
                    <name>
                      <surname>Banks</surname>
                      <given-names>MS</given-names>
                    </name>
                  </person-group>
                  <article-title>Humans integrate visual and haptic information in a statistically optimal fashion</article-title>
                  <source>Nature</source>
                  <year>2002</year>
                  <volume>415</volume>
                  <fpage>429</fpage>
                  <lpage>433</lpage>
                  <pub-id pub-id-type="doi">10.1038/415429a</pub-id>
                  <pub-id pub-id-type="pmid">11807554</pub-id>
                </element-citation>
              </ref>
              <ref id="CR51">
                <label>51.</label>
                <element-citation publication-type="journal">
                  <person-group person-group-type="author">
                    <name>
                      <surname>Alais</surname>
                      <given-names>D</given-names>
                    </name>
                    <name>
                      <surname>Burr</surname>
                      <given-names>D</given-names>
                    </name>
                  </person-group>
                  <article-title>Ventriloquist Effect Results from Near-Optimal Bimodal Integration</article-title>
                  <source>Curr. Biol.</source>
                  <year>2004</year>
                  <volume>14</volume>
                  <fpage>257</fpage>
                  <lpage>262</lpage>
                  <pub-id pub-id-type="doi">10.1016/j.cub.2004.01.029</pub-id>
                  <pub-id pub-id-type="pmid">14761661</pub-id>
                </element-citation>
              </ref>
              <ref id="CR52">
                <label>52.</label>
                <element-citation publication-type="journal">
                  <person-group person-group-type="author">
                    <name>
                      <surname>Nozaradan</surname>
                      <given-names>S</given-names>
                    </name>
                    <name>
                      <surname>Peretz</surname>
                      <given-names>I</given-names>
                    </name>
                    <name>
                      <surname>Mouraux</surname>
                      <given-names>A</given-names>
                    </name>
                  </person-group>
                  <article-title>Steady-state evoked potentials as an index of multisensory temporal binding</article-title>
                  <source>Neuroimage</source>
                  <year>2012</year>
                  <volume>60</volume>
                  <fpage>21</fpage>
                  <lpage>28</lpage>
                  <pub-id pub-id-type="doi">10.1016/j.neuroimage.2011.11.065</pub-id>
                  <pub-id pub-id-type="pmid">22155324</pub-id>
                </element-citation>
              </ref>
              <ref id="CR53">
                <label>53.</label>
                <element-citation publication-type="journal">
                  <person-group person-group-type="author">
                    <name>
                      <surname>Tononi</surname>
                      <given-names>G</given-names>
                    </name>
                    <name>
                      <surname>Koch</surname>
                      <given-names>C</given-names>
                    </name>
                  </person-group>
                  <article-title>The neural correlates of consciousness: An update</article-title>
                  <source>Annals of the New York Academy of Sciences</source>
                  <year>2008</year>
                  <volume>1124</volume>
                  <fpage>239</fpage>
                  <lpage>261</lpage>
                  <pub-id pub-id-type="doi">10.1196/annals.1440.004</pub-id>
                  <pub-id pub-id-type="pmid">18400934</pub-id>
                </element-citation>
              </ref>
              <ref id="CR54">
                <label>54.</label>
                <element-citation publication-type="journal">
                  <person-group person-group-type="author">
                    <name>
                      <surname>Engel</surname>
                      <given-names>AK</given-names>
                    </name>
                    <name>
                      <surname>Singer</surname>
                      <given-names>W</given-names>
                    </name>
                  </person-group>
                  <article-title>Temporal binding and the neural correlates of sensory awareness</article-title>
                  <source>Trends in Cognitive Sciences</source>
                  <year>2001</year>
                  <volume>5</volume>
                  <fpage>16</fpage>
                  <lpage>25</lpage>
                  <pub-id pub-id-type="doi">10.1016/S1364-6613(00)01568-0</pub-id>
                  <pub-id pub-id-type="pmid">11164732</pub-id>
                </element-citation>
              </ref>
              <ref id="CR55">
                <label>55.</label>
                <element-citation publication-type="journal">
                  <person-group person-group-type="author">
                    <name>
                      <surname>Schroeder</surname>
                      <given-names>CE</given-names>
                    </name>
                    <name>
                      <surname>Lakatos</surname>
                      <given-names>P</given-names>
                    </name>
                  </person-group>
                  <article-title>Low-frequency neuronal oscillations as instruments of sensory selection</article-title>
                  <source>Trends Neurosci.</source>
                  <year>2008</year>
                  <volume>32</volume>
                  <fpage>9</fpage>
                  <lpage>18</lpage>
                  <pub-id pub-id-type="doi">10.1016/j.tins.2008.09.012</pub-id>
                  <pub-id pub-id-type="pmid">19012975</pub-id>
                </element-citation>
              </ref>
              <ref id="CR56">
                <label>56.</label>
                <element-citation publication-type="journal">
                  <person-group person-group-type="author">
                    <name>
                      <surname>Fries</surname>
                      <given-names>P</given-names>
                    </name>
                  </person-group>
                  <article-title>A mechanism for cognitive dynamics: Neuronal communication through neuronal coherence</article-title>
                  <source>Trends in Cognitive Sciences</source>
                  <year>2005</year>
                  <volume>9</volume>
                  <fpage>474</fpage>
                  <lpage>480</lpage>
                  <pub-id pub-id-type="doi">10.1016/j.tics.2005.08.011</pub-id>
                  <pub-id pub-id-type="pmid">16150631</pub-id>
                </element-citation>
              </ref>
              <ref id="CR57">
                <label>57.</label>
                <element-citation publication-type="journal">
                  <person-group person-group-type="author">
                    <name>
                      <surname>Senkowski</surname>
                      <given-names>D</given-names>
                    </name>
                    <name>
                      <surname>Schneider</surname>
                      <given-names>TR</given-names>
                    </name>
                    <name>
                      <surname>Foxe</surname>
                      <given-names>JJ</given-names>
                    </name>
                    <name>
                      <surname>Engel</surname>
                      <given-names>AK</given-names>
                    </name>
                  </person-group>
                  <article-title>Crossmodal binding through neural coherence: implications for multisensory processing</article-title>
                  <source>Trends in Neurosciences</source>
                  <year>2008</year>
                  <volume>31</volume>
                  <fpage>401</fpage>
                  <lpage>409</lpage>
                  <pub-id pub-id-type="doi">10.1016/j.tins.2008.05.002</pub-id>
                  <pub-id pub-id-type="pmid">18602171</pub-id>
                </element-citation>
              </ref>
              <ref id="CR58">
                <label>58.</label>
                <element-citation publication-type="journal">
                  <person-group person-group-type="author">
                    <name>
                      <surname>Singer</surname>
                      <given-names>W</given-names>
                    </name>
                    <name>
                      <surname>Gray</surname>
                      <given-names>C</given-names>
                    </name>
                  </person-group>
                  <article-title>Visual feature integration and the temporal correlation hypothesis</article-title>
                  <source>Annu. Rev. Neurosci.</source>
                  <year>1995</year>
                  <volume>18</volume>
                  <fpage>555</fpage>
                  <lpage>586</lpage>
                  <pub-id pub-id-type="doi">10.1146/annurev.ne.18.030195.003011</pub-id>
                  <pub-id pub-id-type="pmid">7605074</pub-id>
                </element-citation>
              </ref>
              <ref id="CR59">
                <label>59.</label>
                <element-citation publication-type="journal">
                  <person-group person-group-type="author">
                    <name>
                      <surname>Hipp</surname>
                      <given-names>JF</given-names>
                    </name>
                    <name>
                      <surname>Engel</surname>
                      <given-names>AK</given-names>
                    </name>
                    <name>
                      <surname>Siegel</surname>
                      <given-names>M</given-names>
                    </name>
                  </person-group>
                  <article-title>Oscillatory synchronization in large-scale cortical networks predicts perception</article-title>
                  <source>Neuron</source>
                  <year>2011</year>
                  <volume>69</volume>
                  <fpage>387</fpage>
                  <lpage>396</lpage>
                  <pub-id pub-id-type="doi">10.1016/j.neuron.2010.12.027</pub-id>
                  <pub-id pub-id-type="pmid">21262474</pub-id>
                </element-citation>
              </ref>
              <ref id="CR60">
                <label>60.</label>
                <element-citation publication-type="journal">
                  <person-group person-group-type="author">
                    <name>
                      <surname>Atilgan</surname>
                      <given-names>H</given-names>
                    </name>
                    <etal/>
                  </person-group>
                  <article-title>Integration of Visual Information in Auditory Cortex Promotes Auditory Scene Analysis through Multisensory Binding</article-title>
                  <source>Neuron</source>
                  <year>2018</year>
                  <volume>97</volume>
                  <fpage>640</fpage>
                  <lpage>655.e4</lpage>
                  <pub-id pub-id-type="doi">10.1016/j.neuron.2017.12.034</pub-id>
                  <pub-id pub-id-type="pmid">29395914</pub-id>
                </element-citation>
              </ref>
              <ref id="CR61">
                <label>61.</label>
                <element-citation publication-type="journal">
                  <person-group person-group-type="author">
                    <name>
                      <surname>Erber</surname>
                      <given-names>NP</given-names>
                    </name>
                  </person-group>
                  <article-title>Interaction of audition and vision in the recognition of oral speech stimuli</article-title>
                  <source>J. Speech Lang. Hear. Res.</source>
                  <year>1969</year>
                  <volume>12</volume>
                  <fpage>423</fpage>
                  <pub-id pub-id-type="doi">10.1044/jshr.1202.423</pub-id>
                </element-citation>
              </ref>
              <ref id="CR62">
                <label>62.</label>
                <mixed-citation publication-type="other">Thut, G., Schyns, P. G. &amp; Gross, J. Entrainment of perceptually relevant brain oscillations by non-invasive rhythmic stimulation of the human brain. <italic>Frontiers in Psychology</italic><bold>2</bold> (2011).</mixed-citation>
              </ref>
              <ref id="CR63">
                <label>63.</label>
                <element-citation publication-type="journal">
                  <person-group person-group-type="author">
                    <name>
                      <surname>Henry</surname>
                      <given-names>MJ</given-names>
                    </name>
                    <name>
                      <surname>Obleser</surname>
                      <given-names>J</given-names>
                    </name>
                  </person-group>
                  <article-title>Frequency modulation entrains slow neural oscillations and optimizes human listening behavior</article-title>
                  <source>Proc. Natl. Acad. Sci.</source>
                  <year>2012</year>
                  <volume>109</volume>
                  <fpage>20095</fpage>
                  <lpage>20100</lpage>
                  <pub-id pub-id-type="doi">10.1073/pnas.1213390109</pub-id>
                  <pub-id pub-id-type="pmid">23151506</pub-id>
                </element-citation>
              </ref>
              <ref id="CR64">
                <label>64.</label>
                <element-citation publication-type="journal">
                  <person-group person-group-type="author">
                    <name>
                      <surname>Bishop</surname>
                      <given-names>GH</given-names>
                    </name>
                  </person-group>
                  <article-title>Cyclic changes in excitability of the optic pathway of the rabbit</article-title>
                  <source>Am. J. Physiol. Content</source>
                  <year>1933</year>
                  <volume>103</volume>
                  <fpage>213</fpage>
                  <lpage>224</lpage>
                  <pub-id pub-id-type="doi">10.1152/ajplegacy.1932.103.1.213</pub-id>
                </element-citation>
              </ref>
              <ref id="CR65">
                <label>65.</label>
                <element-citation publication-type="journal">
                  <person-group person-group-type="author">
                    <name>
                      <surname>Henry</surname>
                      <given-names>MJ</given-names>
                    </name>
                    <name>
                      <surname>Herrmann</surname>
                      <given-names>B</given-names>
                    </name>
                    <name>
                      <surname>Obleser</surname>
                      <given-names>J</given-names>
                    </name>
                  </person-group>
                  <article-title>Entrained neural oscillations in multiple frequency bands comodulate behavior</article-title>
                  <source>Proc. Natl. Acad. Sci. USA</source>
                  <year>2014</year>
                  <volume>111</volume>
                  <fpage>1408741111-</fpage>
                  <pub-id pub-id-type="doi">10.1073/pnas.1408741111</pub-id>
                </element-citation>
              </ref>
              <ref id="CR66">
                <label>66.</label>
                <element-citation publication-type="journal">
                  <person-group person-group-type="author">
                    <name>
                      <surname>Kösem</surname>
                      <given-names>A</given-names>
                    </name>
                    <name>
                      <surname>Gramfort</surname>
                      <given-names>A</given-names>
                    </name>
                    <name>
                      <surname>Van Wassenhove</surname>
                      <given-names>V</given-names>
                    </name>
                  </person-group>
                  <article-title>Encoding of event timing in the phase of neural oscillations</article-title>
                  <source>Neuroimage</source>
                  <year>2014</year>
                  <volume>92</volume>
                  <fpage>274</fpage>
                  <lpage>284</lpage>
                  <pub-id pub-id-type="doi">10.1016/j.neuroimage.2014.02.010</pub-id>
                  <pub-id pub-id-type="pmid">24531044</pub-id>
                </element-citation>
              </ref>
              <ref id="CR67">
                <label>67.</label>
                <element-citation publication-type="journal">
                  <person-group person-group-type="author">
                    <name>
                      <surname>Van der Burg</surname>
                      <given-names>E</given-names>
                    </name>
                    <name>
                      <surname>Alais</surname>
                      <given-names>D</given-names>
                    </name>
                    <name>
                      <surname>Cass</surname>
                      <given-names>J</given-names>
                    </name>
                  </person-group>
                  <article-title>Rapid Recalibration to Audiovisual Asynchrony</article-title>
                  <source>J. Neurosci.</source>
                  <year>2013</year>
                  <volume>33</volume>
                  <fpage>14633</fpage>
                  <lpage>7</lpage>
                  <pub-id pub-id-type="doi">10.1523/JNEUROSCI.1182-13.2013</pub-id>
                  <pub-id pub-id-type="pmid">24027264</pub-id>
                </element-citation>
              </ref>
              <ref id="CR68">
                <label>68.</label>
                <element-citation publication-type="journal">
                  <person-group person-group-type="author">
                    <name>
                      <surname>Brainard</surname>
                      <given-names>DH</given-names>
                    </name>
                  </person-group>
                  <article-title>The Psychophysics Toolbox</article-title>
                  <source>Spat. Vis.</source>
                  <year>1997</year>
                  <volume>10</volume>
                  <fpage>433</fpage>
                  <lpage>436</lpage>
                  <pub-id pub-id-type="doi">10.1163/156856897X00357</pub-id>
                  <pub-id pub-id-type="pmid">9176952</pub-id>
                </element-citation>
              </ref>
              <ref id="CR69">
                <label>69.</label>
                <element-citation publication-type="journal">
                  <person-group person-group-type="author">
                    <name>
                      <surname>Kleiner</surname>
                      <given-names>M</given-names>
                    </name>
                    <etal/>
                  </person-group>
                  <article-title>What’s new in Psychtoolbox-3?</article-title>
                  <source>Perception</source>
                  <year>2007</year>
                  <volume>36</volume>
                  <fpage>S14</fpage>
                </element-citation>
              </ref>
              <ref id="CR70">
                <label>70.</label>
                <element-citation publication-type="journal">
                  <person-group person-group-type="author">
                    <name>
                      <surname>Green</surname>
                      <given-names>DM</given-names>
                    </name>
                    <name>
                      <surname>Swets</surname>
                      <given-names>JA</given-names>
                    </name>
                  </person-group>
                  <article-title>Signal detection theory and psychophysics</article-title>
                  <source>Society</source>
                  <year>1966</year>
                  <volume>1</volume>
                  <fpage>521</fpage>
                </element-citation>
              </ref>
              <ref id="CR71">
                <label>71.</label>
                <mixed-citation publication-type="other">Berens, P. CircStat: A <italic>MATLAB</italic> Toolbox for Circular Statistics. <italic>J. Stat. Softw</italic>. <bold>31</bold> (2009).</mixed-citation>
              </ref>
              <ref id="CR72">
                <label>72.</label>
                <element-citation publication-type="journal">
                  <person-group person-group-type="author">
                    <name>
                      <surname>Bogacz</surname>
                      <given-names>R</given-names>
                    </name>
                    <name>
                      <surname>Brown</surname>
                      <given-names>E</given-names>
                    </name>
                    <name>
                      <surname>Moehlis</surname>
                      <given-names>J</given-names>
                    </name>
                    <name>
                      <surname>Holmes</surname>
                      <given-names>P</given-names>
                    </name>
                    <name>
                      <surname>Cohen</surname>
                      <given-names>JD</given-names>
                    </name>
                  </person-group>
                  <article-title>The physics of optimal decision making: A formal analysis of models of performance in two-alternative forced-choice tasks</article-title>
                  <source>Psychol. Rev.</source>
                  <year>2006</year>
                  <volume>113</volume>
                  <fpage>700</fpage>
                  <lpage>765</lpage>
                  <pub-id pub-id-type="doi">10.1037/0033-295X.113.4.700</pub-id>
                  <pub-id pub-id-type="pmid">17014301</pub-id>
                </element-citation>
              </ref>
              <ref id="CR73">
                <label>73.</label>
                <element-citation publication-type="journal">
                  <person-group person-group-type="author">
                    <name>
                      <surname>Diederich</surname>
                      <given-names>A</given-names>
                    </name>
                    <name>
                      <surname>Busemeyer</surname>
                      <given-names>JR</given-names>
                    </name>
                  </person-group>
                  <article-title>Simple matrix methods for analyzing diffusion models of choice probability, choice response time, and simple response time</article-title>
                  <source>J. Math. Psychol.</source>
                  <year>2003</year>
                  <volume>47</volume>
                  <fpage>304</fpage>
                  <lpage>322</lpage>
                  <pub-id pub-id-type="doi">10.1016/S0022-2496(03)00003-8</pub-id>
                </element-citation>
              </ref>
              <ref id="CR74">
                <label>74.</label>
                <element-citation publication-type="journal">
                  <person-group person-group-type="author">
                    <name>
                      <surname>Smith</surname>
                      <given-names>PL</given-names>
                    </name>
                    <name>
                      <surname>Vickers</surname>
                      <given-names>D</given-names>
                    </name>
                  </person-group>
                  <article-title>The accumulator model of two-choice discrimination</article-title>
                  <source>J. Math. Psychol.</source>
                  <year>1988</year>
                  <volume>32</volume>
                  <fpage>135</fpage>
                  <lpage>168</lpage>
                  <pub-id pub-id-type="doi">10.1016/0022-2496(88)90043-0</pub-id>
                </element-citation>
              </ref>
              <ref id="CR75">
                <label>75.</label>
                <element-citation publication-type="journal">
                  <person-group person-group-type="author">
                    <name>
                      <surname>Lagarias</surname>
                      <given-names>JC</given-names>
                    </name>
                    <name>
                      <surname>Reeds</surname>
                      <given-names>JA</given-names>
                    </name>
                    <name>
                      <surname>Wright</surname>
                      <given-names>MH</given-names>
                    </name>
                    <name>
                      <surname>Wright</surname>
                      <given-names>PE</given-names>
                    </name>
                  </person-group>
                  <article-title>Convergence Properties of the Nelder–Mead Simplex Method in Low Dimensions</article-title>
                  <source>SIAM J. Optim.</source>
                  <year>1998</year>
                  <volume>9</volume>
                  <fpage>112</fpage>
                  <lpage>147</lpage>
                  <pub-id pub-id-type="doi">10.1137/S1052623496303470</pub-id>
                </element-citation>
              </ref>
            </ref-list>
          </back>
        </article>
      </metadata>
    </record>
  </GetRecord>
</OAI-PMH>
