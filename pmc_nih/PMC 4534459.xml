<?xml version="1.0" encoding="UTF-8"?>
<OAI-PMH xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/ http://www.openarchives.org/OAI/2.0/OAI-PMH.xsd">
  <responseDate>2024-05-03T05:34:00Z</responseDate>
  <request verb="GetRecord" identifier="oai:pubmedcentral.nih.gov:4534459" metadataPrefix="pmc">https:/www.ncbi.nlm.nih.gov/pmc/oai/oai.cgi</request>
  <GetRecord>
    <record>
      <header>
        <identifier>oai:pubmedcentral.nih.gov:4534459</identifier>
        <datestamp>2015-08-24</datestamp>
        <setSpec>plosone</setSpec>
        <setSpec>pmc-open</setSpec>
      </header>
      <metadata>
        <article xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" xsi:schemaLocation="https://jats.nlm.nih.gov/ns/archiving/1.3/ https://jats.nlm.nih.gov/archiving/1.3/xsd/JATS-archivearticle1-3.xsd" article-type="research-article">
          <front>
            <journal-meta>
              <journal-id journal-id-type="nlm-ta">PLoS One</journal-id>
              <journal-id journal-id-type="iso-abbrev">PLoS ONE</journal-id>
              <journal-id journal-id-type="publisher-id">plos</journal-id>
              <journal-id journal-id-type="pmc">plosone</journal-id>
              <journal-title-group>
                <journal-title>PLoS ONE</journal-title>
              </journal-title-group>
              <issn pub-type="epub">1932-6203</issn>
              <publisher>
                <publisher-name>Public Library of Science</publisher-name>
                <publisher-loc>San Francisco, CA USA</publisher-loc>
              </publisher>
            </journal-meta>
            <article-meta>
              <article-id pub-id-type="accession">PMC4534459</article-id>
              <article-id pub-id-type="pmcid">PMC4534459</article-id>
              <article-id pub-id-type="pmc-uid">4534459</article-id>
              <article-id pub-id-type="pmid">26267865</article-id>
              <article-id pub-id-type="pmid">26267865</article-id>
              <article-id pub-id-type="doi">10.1371/journal.pone.0135539</article-id>
              <article-id pub-id-type="publisher-id">PONE-D-15-16372</article-id>
              <article-categories>
                <subj-group subj-group-type="heading">
                  <subject>Research Article</subject>
                </subj-group>
              </article-categories>
              <title-group>
                <article-title>Coordinates of Human Visual and Inertial Heading Perception</article-title>
                <alt-title alt-title-type="running-head">Human Visual and Inertial Heading</alt-title>
              </title-group>
              <contrib-group>
                <contrib contrib-type="author">
                  <name>
                    <surname>Crane</surname>
                    <given-names>Benjamin Thomas</given-names>
                  </name>
                  <xref ref-type="aff" rid="aff001">
                    <sup>1</sup>
                  </xref>
                  <xref ref-type="aff" rid="aff002">
                    <sup>2</sup>
                  </xref>
                  <xref ref-type="aff" rid="aff003">
                    <sup>3</sup>
                  </xref>
                  <xref rid="cor001" ref-type="corresp">*</xref>
                </contrib>
              </contrib-group>
              <aff id="aff001">
                <label>1</label>
                <addr-line>Department of Otolaryngology, University of Rochester, Rochester, NY, United States of America</addr-line>
              </aff>
              <aff id="aff002">
                <label>2</label>
                <addr-line>Department of Bioengineering, University of Rochester, Rochester, NY, United States of America</addr-line>
              </aff>
              <aff id="aff003">
                <label>3</label>
                <addr-line>Department of Neurobiology and Anatomy, University of Rochester, Rochester, NY, United States of America</addr-line>
              </aff>
              <contrib-group>
                <contrib contrib-type="editor">
                  <name>
                    <surname>Wylie</surname>
                    <given-names>Doug</given-names>
                  </name>
                  <role>Editor</role>
                  <xref ref-type="aff" rid="edit1"/>
                </contrib>
              </contrib-group>
              <aff id="edit1">
                <addr-line>University of Alberta, CANADA</addr-line>
              </aff>
              <author-notes>
                <fn fn-type="COI-statement" id="coi001">
                  <p><bold>Competing Interests: </bold>The authors have declared that no competing interests exist.</p>
                </fn>
                <fn fn-type="con" id="contrib001">
                  <p>Conceived and designed the experiments: BC. Performed the experiments: BC. Analyzed the data: BC. Contributed reagents/materials/analysis tools: BC. Wrote the paper: BC.</p>
                </fn>
                <corresp id="cor001">* E-mail: <email>craneb@gmail.com</email></corresp>
              </author-notes>
              <pub-date pub-type="epub">
                <day>12</day>
                <month>8</month>
                <year>2015</year>
              </pub-date>
              <pub-date pub-type="collection">
                <year>2015</year>
              </pub-date>
              <volume>10</volume>
              <issue>8</issue>
              <elocation-id>e0135539</elocation-id>
              <history>
                <date date-type="received">
                  <day>15</day>
                  <month>4</month>
                  <year>2015</year>
                </date>
                <date date-type="accepted">
                  <day>22</day>
                  <month>7</month>
                  <year>2015</year>
                </date>
              </history>
              <permissions>
                <copyright-statement>© 2015 Benjamin Thomas Crane</copyright-statement>
                <copyright-year>2015</copyright-year>
                <copyright-holder>Benjamin Thomas Crane</copyright-holder>
                <license xlink:href="http://creativecommons.org/licenses/by/4.0/">
                  <license-p>This is an open-access article distributed under the terms of the Creative Commons Attribution License, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are properly credited.</license-p>
                </license>
              </permissions>
              <self-uri content-type="pdf" xlink:type="simple" xlink:href="pone.0135539.pdf"/>
              <abstract>
                <p>Heading estimation involves both inertial and visual cues. Inertial motion is sensed by the labyrinth, somatic sensation by the body, and optic flow by the retina. Because the eye and head are mobile these stimuli are sensed relative to different reference frames and it remains unclear if a perception occurs in a common reference frame. Recent neurophysiologic evidence has suggested the reference frames remain separate even at higher levels of processing but has not addressed the resulting perception. Seven human subjects experienced a 2s, 16 cm/s translation and/or a visual stimulus corresponding with this translation. For each condition 72 stimuli (360° in 5° increments) were delivered in random order. After each stimulus the subject identified the perceived heading using a mechanical dial. Some trial blocks included interleaved conditions in which the influence of ±28° of gaze and/or head position were examined. The observations were fit using a two degree-of-freedom population vector decoder (PVD) model which considered the relative sensitivity to lateral motion and coordinate system offset. For visual stimuli gaze shifts caused shifts in perceived head estimates in the direction opposite the gaze shift in all subjects. These perceptual shifts averaged 13 ± 2° for eye only gaze shifts and 17 ± 2° for eye-head gaze shifts. This finding indicates visual headings are biased towards retina coordinates. Similar gaze and head direction shifts prior to inertial headings had no significant influence on heading direction. Thus inertial headings are perceived in body-centered coordinates. Combined visual and inertial stimuli yielded intermediate results.</p>
              </abstract>
              <funding-group>
                <funding-statement>This work was funded by National Institute on Deafness and Other Communication Disorders (NIDCD) K23 DC011298 and R01 DC0135580. It was also supported by a Triological Society Career Scientist Award. The funders had no role in study design, data collection and analysis, decision to publish, or preparation of the manuscript.</funding-statement>
              </funding-group>
              <counts>
                <fig-count count="4"/>
                <table-count count="0"/>
                <page-count count="14"/>
              </counts>
              <custom-meta-group>
                <custom-meta id="data-availability">
                  <meta-name>Data Availability</meta-name>
                  <meta-value>All relevant data are within the paper.</meta-value>
                </custom-meta>
              </custom-meta-group>
            </article-meta>
            <notes>
              <title>Data Availability</title>
              <p>All relevant data are within the paper.</p>
            </notes>
          </front>
          <body>
            <sec sec-type="intro" id="sec001">
              <title>Introduction</title>
              <p>Human heading estimation is multi-sensory involving visual and inertial cues[<xref rid="pone.0135539.ref001" ref-type="bibr">1</xref>–<xref rid="pone.0135539.ref003" ref-type="bibr">3</xref>]. However these sensory modalities use different reference frames with vision represented relative to the retina[<xref rid="pone.0135539.ref004" ref-type="bibr">4</xref>–<xref rid="pone.0135539.ref006" ref-type="bibr">6</xref>], vestibular relative to the head[<xref rid="pone.0135539.ref006" ref-type="bibr">6</xref>–<xref rid="pone.0135539.ref009" ref-type="bibr">9</xref>], and somatosensation relative to the body[<xref rid="pone.0135539.ref010" ref-type="bibr">10</xref>, <xref rid="pone.0135539.ref011" ref-type="bibr">11</xref>]. It has been proposed that multisensory integration should occur in a common reference frame[<xref rid="pone.0135539.ref012" ref-type="bibr">12</xref>–<xref rid="pone.0135539.ref015" ref-type="bibr">15</xref>]. However, a single reference frame may be implausible based on recent findings for visual-vestibular integration[<xref rid="pone.0135539.ref006" ref-type="bibr">6</xref>, <xref rid="pone.0135539.ref016" ref-type="bibr">16</xref>] as well as visual-proprioceptive[<xref rid="pone.0135539.ref017" ref-type="bibr">17</xref>] and auditory[<xref rid="pone.0135539.ref018" ref-type="bibr">18</xref>] integration which suggests multiple references frames.</p>
              <p>Although the coordinates of perceived heading estimates have not previously been directly measured several studies have looked at the neurophysiology underlying this perception. The ventral intraparietal area (VIP) is a region that is likely to be importation for visual-vestibular integration[<xref rid="pone.0135539.ref001" ref-type="bibr">1</xref>, <xref rid="pone.0135539.ref019" ref-type="bibr">19</xref>–<xref rid="pone.0135539.ref022" ref-type="bibr">22</xref>]. In VIP visual stimuli are represented in eye-centered coordinates[<xref rid="pone.0135539.ref023" ref-type="bibr">23</xref>, <xref rid="pone.0135539.ref024" ref-type="bibr">24</xref>] while vestibular headings in VIP are in body coordinates that do not vary with changes in eye or head position[<xref rid="pone.0135539.ref016" ref-type="bibr">16</xref>]. In contrast, in both the dorsal medial superior temporal area (MSTd) and parietoinsular vestibular cortex vestibular headings are more head centered[<xref rid="pone.0135539.ref016" ref-type="bibr">16</xref>]. Although, the neurophysiology has not eliminated the possibility of a common coordinate system for perception of visual and vestibular headings, no such common coordinate system has been found as visual headings have only been found to be represented in retinal coordinates and vestibular headings have been found in only head and body coordinates.</p>
              <p>It has recently been shown that human heading estimates are systematically biased so that lateral component is overestimated with both visual and vestibular stimuli[<xref rid="pone.0135539.ref025" ref-type="bibr">25</xref>, <xref rid="pone.0135539.ref026" ref-type="bibr">26</xref>]. This behavior can be predicted by a population vector decoder (PVD) model based on a relatively larger number of units with sensitivity to lateral motion in MSTd[<xref rid="pone.0135539.ref027" ref-type="bibr">27</xref>]. However previously studies did not attempt to measure the effect of eye and head position on these biases.</p>
              <p>In the current experiment, human visual and inertial head estimates were measured while systematically varying eye and head position. This experiment was designed to address two current controversies: First, determine the coordinate systems in which visual and vestibular stimuli are perceived. Second, determine if multisensory visual-vestibular integration occurs in a common coordinate system. Perception of visual headings shifted with gaze position demonstrating visual headings were perceived in retina-centered coordinates. Inertial heading estimates were not influenced by either head or eye position indicating a body-centered coordinates. When both visual and vestibular stimuli are present an intermediate coordinate system was used.</p>
            </sec>
            <sec sec-type="materials|methods" id="sec002">
              <title>Methods</title>
              <sec id="sec003">
                <title>Ethics Statement</title>
                <p>The research was conducted according to the principles expressed in the Declaration of Helsinki. Written informed consent was obtained from all participants. The protocol and written consent form were approved by the University of Rochester Research Science Review Board (RSRB).</p>
              </sec>
              <sec id="sec004">
                <title>Equipment</title>
                <p>Motion stimuli were delivered using a 6-degree-of-freedom motion platform (Moog, East Aurora, NY, model 6DOF2000E) similar to that used in other laboratories for human motion perception studies [<xref rid="pone.0135539.ref026" ref-type="bibr">26</xref>, <xref rid="pone.0135539.ref028" ref-type="bibr">28</xref>–<xref rid="pone.0135539.ref030" ref-type="bibr">30</xref>] and previously described in the current laboratory for heading estimation studies[<xref rid="pone.0135539.ref025" ref-type="bibr">25</xref>, <xref rid="pone.0135539.ref031" ref-type="bibr">31</xref>].</p>
                <p>Head and platform movements were monitored in all six-degrees of freedom using a flux-gate magnetometer (trakSTAR, Ascension Technologies, Burlington, VT) using two model 800 position sensors, one on the subject’s head and other on the chair as previously described[<xref rid="pone.0135539.ref032" ref-type="bibr">32</xref>].</p>
                <p>Monocular eye position was monitored and recorded at 60 Hz using an infrared video eye tracking system (LiveTrack, Cambridge Research Systems, Rochester England). Prior to each experiment the position was calibrated using a series of fixation points between ±30° in the horizontal plane and ±15° in the vertical plane. This system was used predominately as test of fixation, although failures of fixation were found to be extremely rare.</p>
                <p>During both visual and vestibular stimuli, an audible white noise was reproduced from two platform-mounted speakers on either side of the subject as previously described [<xref rid="pone.0135539.ref033" ref-type="bibr">33</xref>]. The noise from the platform was similar regardless of motion direction. Tests in the current laboratory previously demonstrated that subjects could not predict the platform motion direction based on sound alone[<xref rid="pone.0135539.ref033" ref-type="bibr">33</xref>].</p>
                <p>Sounds from the platform were further masked using a white noise stimulus reproduced from two platform-mounted speakers on either side of the subject. The intensity of the masking noise used in the current study varied with time as a half-sine wave so that the peak masking noise occurred at the same time the peak velocity was reached. This created a masking noise similar to the noise made by the platform. Sound levels at the location of the subject were measured using a Quest Technologies, model 1900 sound level meter (Quest Technologies, Oconomowoc, WI). Average sound pressure level (SPL) of the ambient sound was 58 dB, with a peak level of 68 dB when no motion was delivered. The masking noise had a peak of 92 dB. The motion platform had a peak noise level of 84 dB for velocities of 30 deg or cm/s for movements in the horizontal plane (yaw, surge, and sway) and 88 dB for heave. The peak noise of the platform was 74 dB. The masking sound intensity was the same for every stimulus independent of the stimulus direction and masking was also used for visual stimuli for consistency even though the visual stimulus made no sound. No masking noise was used between stimuli. We found this type of masking much more effective than a continuous masking noise of constant intensity.</p>
                <p>Responses were collected using a two-button control box with a dial in the middle that could be freely rotated in the horizontal plane without any discontinuity points as previously described in the current laboratory[<xref rid="pone.0135539.ref025" ref-type="bibr">25</xref>].</p>
              </sec>
              <sec id="sec005">
                <title>Stimulus</title>
                <p>There were three types of stimuli: visual only, inertial only, and combined visual-inertial. During the combined stimulus condition the visual and inertial motion were synchronous and represented the same direction and magnitude of motion. The visual and inertial stimuli consisted of a single cycle 2s (0.5 Hz) sine wave in acceleration. This motion profile has previously been used for threshold determination[<xref rid="pone.0135539.ref029" ref-type="bibr">29</xref>, <xref rid="pone.0135539.ref033" ref-type="bibr">33</xref>, <xref rid="pone.0135539.ref034" ref-type="bibr">34</xref>] and for heading estimation[<xref rid="pone.0135539.ref025" ref-type="bibr">25</xref>, <xref rid="pone.0135539.ref031" ref-type="bibr">31</xref>]. Directions tested included the 360° range of headings in 5° increments (72 total), delivered in random order. The displacement of the stimulus was 16 cm with a peak velocity of 16 cm/s and peak velocity of 25 cm/s/s.</p>
                <p>Visual stimuli were presented on a color LCD screen (Samsung model LN52B75OU1FXZA) with a resolution of 1920 x 1080 pixels 50 cm from the subject filling 98° horizontal field of view. A fixation point consisted of a 2x2 cm cross at eye level could be presented centered or ±28° and was visible throughout every trial. The visual stimulus consisted of a star field which simulated movement of the observer through a random-dot cloud with binocular disparity as previously described[<xref rid="pone.0135539.ref025" ref-type="bibr">25</xref>]. Each star consisted of a triangle 0.5 cm in height and width at the place of the screen adjusted appropriately for distance. The star density was 0.01 per cubic cm. The depth of the field was 130 cm (the nearest stars were 20 cm and the furthest 150 cm). Visual coherence was fixed at 100%. Disparity was provided using red-green anaglyph glasses made with Kodak (Rochester, NY) Wratten filters #29 (dark red) and #61 (deep green). The colors were adjusted such that the intensities of the two were similar when viewed through the respective filters and rejection ratio was better than ten fold.</p>
              </sec>
              <sec id="sec006" sec-type="materials|methods">
                <title>Experimental Procedure</title>
                <p>Three stimulus conditions were used: Inertial motion in which the platform moved, visual motion in which the platform remained stationary but star field motion was present, and combined visual and inertial motion.</p>
                <p>Subjects were instructed that each stimulus would be a linear motion in the horizontal plan. Prior to testing subjects were show how to orient a mechanical dial as previously described[<xref rid="pone.0135539.ref025" ref-type="bibr">25</xref>]. A few practice trials were conducted in the light prior to doing the experiment. Although no feedback was given during these trials, if they were making systematic errors such as identifying the direction of the star field motion rather than their direction through the star field such errors were corrected.</p>
                <p>Four types of head/gaze variations were used, each in a separate block of trials: Head centered gaze centered (HCGC) in which the head and gaze remained fixed at the midline. This condition was essentially the same as a previously published study[<xref rid="pone.0135539.ref025" ref-type="bibr">25</xref>], but was repeated using the current subjects. In the remaining 3 types of trial blocks there were interleaved conditions in which head or gaze position was randomly varied between trials. The head remained fixed in the head centered gaze varied (HCGV) condition while gaze was varied between fixation points 28° to the right or left prior to each stimulus presentation. In the head varied, gaze centered (HVGC) condition the visual fixation point remained centered and the head was rotated to 28° to the right or left prior to each stimulus presentation. In the head varied, gaze varied condition (HVGV) both the head and gaze were rotated 28° to the right or left prior to the stimulus presentation such that the eye remained near straight ahead relative to the head.</p>
                <p>In the HVGC and HVGV conditions prior to each trial (i.e. each stimulus presentation) a location representing the ideal head position (28° right or left) was shown on the video display. A box ±1° on a side represented the current head position in real time and the subject was instructed to move their head so that the cross was in the center of the box. Afterwards the head was stabilized against a rubber headrest. A fixation point was displayed to indicate gaze position which in the case of HVGV was the center of the box and in HVGC was in the center of the screen. Once the head and gaze were stabilized at the desired position the subject pressed the start button. After pressing the start button the box and current head location were no longer displayed but were recorded through out the trial. Immediately after the stimulus was delivered subjects heard two 0.125s tones in rapid succession to indicate their response could be reported using the dial. After they were finished they pushed a button to indicate they were done.</p>
              </sec>
              <sec id="sec007">
                <title>Subjects</title>
                <p>A total of 7 subjects (5 female) completed the 11 trial block protocol. Ages ranged from 20 to 67 (39 ± 22, mean ± SD). None of the subjects was familiar with the design of the experiment. The order of trial blocks was randomized between subjects and no feedback was given. To minimize fatigue the trials blocks were completed in multiple sessions on different days with 2–3 blocks completed in each session. All subjects were screened prior to participation and had normal or corrected vision, normal hearing, normal vestibular function on caloric testing, and no dizziness or balance symptoms.</p>
              </sec>
              <sec id="sec008">
                <title>Analysis</title>
                <p>Each dial setting was compared with the actual heading for each trial to calculate a response error. A simple population vector decoder (PVD) model was fit to each participants responses for each test condition. The general form of a PVD model is given in Eq (<xref rid="pone.0135539.e001" ref-type="disp-formula">1</xref>).</p>
                <disp-formula id="pone.0135539.e001">
                  <alternatives>
                    <graphic xlink:href="pone.0135539.e001.jpg" id="pone.0135539.e001g" position="anchor" mimetype="image" orientation="portrait"/>
                    <mml:math id="M1">
                      <mml:mrow>
                        <mml:mover accent="true">
                          <mml:mi>P</mml:mi>
                          <mml:mo stretchy="true">⇀</mml:mo>
                        </mml:mover>
                        <mml:mo>=</mml:mo>
                        <mml:mstyle displaystyle="true">
                          <mml:munderover>
                            <mml:mo>∑</mml:mo>
                            <mml:mrow>
                              <mml:mi>i</mml:mi>
                              <mml:mo>=</mml:mo>
                              <mml:mn>1</mml:mn>
                            </mml:mrow>
                            <mml:mi>n</mml:mi>
                          </mml:munderover>
                          <mml:mrow>
                            <mml:msub>
                              <mml:mi>w</mml:mi>
                              <mml:mi>i</mml:mi>
                            </mml:msub>
                            <mml:mover accent="true">
                              <mml:mrow>
                                <mml:msub>
                                  <mml:mi>C</mml:mi>
                                  <mml:mi>i</mml:mi>
                                </mml:msub>
                              </mml:mrow>
                              <mml:mo stretchy="true">⇀</mml:mo>
                            </mml:mover>
                          </mml:mrow>
                        </mml:mstyle>
                      </mml:mrow>
                    </mml:math>
                  </alternatives>
                  <label>(1)</label>
                </disp-formula>
                <p>Because there is little known about the actual distribution of human sensitivities, the model was simplified to include only two orthogonal vectors representing surge and sway with independent weights. Although including additional vectors in the population would allow more degrees of freedom and a better fit to the data, this was not done due to the risk of over fitting:
<disp-formula id="pone.0135539.e002"><alternatives><graphic xlink:href="pone.0135539.e002.jpg" id="pone.0135539.e002g" position="anchor" mimetype="image" orientation="portrait"/><mml:math id="M2"><mml:mrow><mml:mover accent="true"><mml:mi>P</mml:mi><mml:mo stretchy="true">⇀</mml:mo></mml:mover><mml:mo>=</mml:mo><mml:msub><mml:mi>w</mml:mi><mml:mrow><mml:mi>s</mml:mi><mml:mi>w</mml:mi><mml:mi>a</mml:mi><mml:mi>y</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mrow><mml:mover accent="true"><mml:mi>C</mml:mi><mml:mo stretchy="true">⇀</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>s</mml:mi><mml:mi>w</mml:mi><mml:mi>a</mml:mi><mml:mi>y</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>w</mml:mi><mml:mrow><mml:mi>s</mml:mi><mml:mi>u</mml:mi><mml:mi>r</mml:mi><mml:mi>g</mml:mi><mml:mi>e</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mrow><mml:mover accent="true"><mml:mi>C</mml:mi><mml:mo stretchy="true">⇀</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>s</mml:mi><mml:mi>u</mml:mi><mml:mi>r</mml:mi><mml:mi>g</mml:mi><mml:mi>e</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></alternatives><label>(2)</label></disp-formula>
</p>
                <p>The absolute weights are not important but only their relative sizes, so they can be replaced with a ratio:
<disp-formula id="pone.0135539.e003"><alternatives><graphic xlink:href="pone.0135539.e003.jpg" id="pone.0135539.e003g" position="anchor" mimetype="image" orientation="portrait"/><mml:math id="M3"><mml:mrow><mml:mi>r</mml:mi><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:msub><mml:mi>w</mml:mi><mml:mrow><mml:mi>s</mml:mi><mml:mi>w</mml:mi><mml:mi>a</mml:mi><mml:mi>y</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:msub><mml:mi>w</mml:mi><mml:mrow><mml:mi>s</mml:mi><mml:mi>u</mml:mi><mml:mi>r</mml:mi><mml:mi>g</mml:mi><mml:mi>e</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfrac></mml:mrow></mml:math></alternatives><label>(3)</label></disp-formula>
</p>
                <p>This gives us:
<disp-formula id="pone.0135539.e004"><alternatives><graphic xlink:href="pone.0135539.e004.jpg" id="pone.0135539.e004g" position="anchor" mimetype="image" orientation="portrait"/><mml:math id="M4"><mml:mrow><mml:mover accent="true"><mml:mi>P</mml:mi><mml:mo stretchy="true">⇀</mml:mo></mml:mover><mml:mo>=</mml:mo><mml:msub><mml:mi>w</mml:mi><mml:mrow><mml:mi>s</mml:mi><mml:mi>u</mml:mi><mml:mi>r</mml:mi><mml:mi>g</mml:mi><mml:mi>e</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>r</mml:mi><mml:msub><mml:mrow><mml:mover accent="true"><mml:mi>C</mml:mi><mml:mo stretchy="true">⇀</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>s</mml:mi><mml:mi>w</mml:mi><mml:mi>a</mml:mi><mml:mi>y</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mover accent="true"><mml:mi>C</mml:mi><mml:mo stretchy="true">⇀</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>s</mml:mi><mml:mi>u</mml:mi><mml:mi>r</mml:mi><mml:mi>g</mml:mi><mml:mi>e</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></alternatives><label>(4)</label></disp-formula>
</p>
                <p>Because we are only interested in the direction of the vector and the length is not important the absolute weight of the surge component is not needed. This simplifies the PVD to (<xref rid="pone.0135539.g001" ref-type="fig">Fig 1A</xref>):
<disp-formula id="pone.0135539.e005"><alternatives><graphic xlink:href="pone.0135539.e005.jpg" id="pone.0135539.e005g" position="anchor" mimetype="image" orientation="portrait"/><mml:math id="M5"><mml:mrow><mml:mover accent="true"><mml:mi>P</mml:mi><mml:mo stretchy="true">⇀</mml:mo></mml:mover><mml:mo>=</mml:mo><mml:mi>r</mml:mi><mml:msub><mml:mrow><mml:mover accent="true"><mml:mi>C</mml:mi><mml:mo stretchy="true">⇀</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>s</mml:mi><mml:mi>w</mml:mi><mml:mi>a</mml:mi><mml:mi>y</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mover accent="true"><mml:mi>C</mml:mi><mml:mo stretchy="true">⇀</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>s</mml:mi><mml:mi>u</mml:mi><mml:mi>r</mml:mi><mml:mi>g</mml:mi><mml:mi>e</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></alternatives><label>(5)</label></disp-formula>
</p>
                <fig id="pone.0135539.g001" orientation="portrait" position="float">
                  <object-id pub-id-type="doi">10.1371/journal.pone.0135539.g001</object-id>
                  <label>Fig 1</label>
                  <caption>
                    <title>The population vector decoder (PVD) model used and its predictions.</title>
                    <p>(A) The model in graphical form for a stimulus heading, <italic>θ</italic>
<sub><italic>s</italic></sub>, of 45° to the right. This 45° vector can be represented by orthogonal surge, <inline-formula id="pone.0135539.e006"><alternatives><graphic id="pone.0135539.e006g" xlink:href="pone.0135539.e006"/><mml:math id="M6"><mml:mrow><mml:msub><mml:mrow><mml:mover accent="true"><mml:mi>C</mml:mi><mml:mo stretchy="true">⇀</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>s</mml:mi><mml:mi>u</mml:mi><mml:mi>r</mml:mi><mml:mi>g</mml:mi><mml:mi>e</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></alternatives></inline-formula>, and sway, <inline-formula id="pone.0135539.e007"><alternatives><graphic id="pone.0135539.e007g" xlink:href="pone.0135539.e007"/><mml:math id="M7"><mml:mrow><mml:msub><mml:mrow><mml:mover accent="true"><mml:mi>C</mml:mi><mml:mo stretchy="true">⇀</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>s</mml:mi><mml:mi>w</mml:mi><mml:mi>a</mml:mi><mml:mi>y</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></alternatives></inline-formula>, vectors offset by <italic>φ</italic>. The perceived heading, <italic>θ</italic>
<sub><italic>p</italic></sub>, is the sum of the surge and sway vectors after the sway component is multiplied by <italic>r</italic>. (B) The quantitative predictions of the model when r = 2 (sway component weighted twice that of surge). The solid blue tracing represents a 27° leftward gaze and the dashed red tracing represents a 27° rightward gaze. Ideal performance would be a perceived heading error of 0° for all reference headings. The model predicts a gaze shift changes the coordinate system and influences both the phase and offset of the perceived heading error.</p>
                  </caption>
                  <graphic xlink:href="pone.0135539.g001"/>
                </fig>
                <p>The orientation of the stimulus in space is given by <italic>θ</italic>
<sub><italic>s</italic></sub>. The surge and sway vectors relative to space were allowed to vary by an offset angle or phase (<italic>φ</italic>). Here the perceived direction, <inline-formula id="pone.0135539.e008"><alternatives><graphic xlink:href="pone.0135539.e008.jpg" id="pone.0135539.e008g" position="anchor" mimetype="image" orientation="portrait"/><mml:math id="M8"><mml:mrow><mml:mover accent="true"><mml:mi>P</mml:mi><mml:mo stretchy="true">⇀</mml:mo></mml:mover></mml:mrow></mml:math></alternatives></inline-formula>, is related to the unit forward vection, <inline-formula id="pone.0135539.e009"><alternatives><graphic xlink:href="pone.0135539.e009.jpg" id="pone.0135539.e009g" position="anchor" mimetype="image" orientation="portrait"/><mml:math id="M9"><mml:mrow><mml:mover accent="true"><mml:mi>y</mml:mi><mml:mo stretchy="true">^</mml:mo></mml:mover></mml:mrow></mml:math></alternatives></inline-formula>, and unit sway vector, <inline-formula id="pone.0135539.e010"><alternatives><graphic xlink:href="pone.0135539.e010.jpg" id="pone.0135539.e010g" position="anchor" mimetype="image" orientation="portrait"/><mml:math id="M10"><mml:mrow><mml:mover accent="true"><mml:mi>x</mml:mi><mml:mo stretchy="true">^</mml:mo></mml:mover></mml:mrow></mml:math></alternatives></inline-formula>:
<disp-formula id="pone.0135539.e011"><alternatives><graphic xlink:href="pone.0135539.e011.jpg" id="pone.0135539.e011g" position="anchor" mimetype="image" orientation="portrait"/><mml:math id="M11"><mml:mrow><mml:mover accent="true"><mml:mi>P</mml:mi><mml:mo stretchy="true">⇀</mml:mo></mml:mover><mml:mo>=</mml:mo><mml:mi>r</mml:mi><mml:mi mathvariant="normal">sin</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>θ</mml:mi><mml:mi>s</mml:mi></mml:msub><mml:mo>+</mml:mo><mml:mi>φ</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mover accent="true"><mml:mi>x</mml:mi><mml:mo stretchy="true">^</mml:mo></mml:mover><mml:mo>+</mml:mo><mml:mi mathvariant="normal">cos</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>θ</mml:mi><mml:mi>s</mml:mi></mml:msub><mml:mo>+</mml:mo><mml:mi>φ</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mover accent="true"><mml:mi>y</mml:mi><mml:mo stretchy="true">^</mml:mo></mml:mover></mml:mrow></mml:math></alternatives><label>(6)</label></disp-formula>
</p>
                <p>Thus when <inline-formula id="pone.0135539.e012"><alternatives><graphic xlink:href="pone.0135539.e012.jpg" id="pone.0135539.e012g" position="anchor" mimetype="image" orientation="portrait"/><mml:math id="M12"><mml:mrow><mml:msub><mml:mrow><mml:mover accent="true"><mml:mi>C</mml:mi><mml:mo stretchy="true">⇀</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>s</mml:mi><mml:mi>u</mml:mi><mml:mi>r</mml:mi><mml:mi>g</mml:mi><mml:mi>e</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></alternatives></inline-formula> is forward (i.e. cos(<italic>θ</italic>
<sub><italic>s</italic></sub> + <italic>φ</italic>) &gt; 0) the perceived heading angle, is given by:
<disp-formula id="pone.0135539.e013"><alternatives><graphic xlink:href="pone.0135539.e013.jpg" id="pone.0135539.e013g" position="anchor" mimetype="image" orientation="portrait"/><mml:math id="M13"><mml:mrow><mml:msub><mml:mi>θ</mml:mi><mml:mi>p</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="normal">tan</mml:mi></mml:mrow><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:mrow><mml:mo stretchy="true">(</mml:mo><mml:mrow><mml:mi>r</mml:mi><mml:mi mathvariant="normal">tan</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:msub><mml:mi>θ</mml:mi><mml:mi>s</mml:mi></mml:msub><mml:mo>+</mml:mo><mml:mi>φ</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo stretchy="true">)</mml:mo></mml:mrow></mml:mrow></mml:math></alternatives><label>(7)</label></disp-formula>
</p>
                <p>When <inline-formula id="pone.0135539.e014"><alternatives><graphic xlink:href="pone.0135539.e014.jpg" id="pone.0135539.e014g" position="anchor" mimetype="image" orientation="portrait"/><mml:math id="M14"><mml:mrow><mml:msub><mml:mrow><mml:mover accent="true"><mml:mi>C</mml:mi><mml:mo stretchy="true">⇀</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>s</mml:mi><mml:mi>u</mml:mi><mml:mi>r</mml:mi><mml:mi>g</mml:mi><mml:mi>e</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></alternatives></inline-formula> is backwards and <inline-formula id="pone.0135539.e015"><alternatives><graphic xlink:href="pone.0135539.e015.jpg" id="pone.0135539.e015g" position="anchor" mimetype="image" orientation="portrait"/><mml:math id="M15"><mml:mrow><mml:msub><mml:mrow><mml:mover accent="true"><mml:mi>C</mml:mi><mml:mo stretchy="true">⇀</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>s</mml:mi><mml:mi>w</mml:mi><mml:mi>a</mml:mi><mml:mi>y</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></alternatives></inline-formula> is to the right (i.e. sin(<italic>θ</italic>
<sub><italic>s</italic></sub> + <italic>φ</italic>) &gt; 0) then a correction factor of +180° is applied, otherwise (i.e. sin(<italic>θ</italic>
<sub><italic>s</italic></sub> + <italic>φ</italic>) &lt; 0) a correction of -180° is needed.</p>
                <p>It is hypothesized that this PVD model can explain the perceived heading bias. This hypothesis is based on prior human behavior experiments[<xref rid="pone.0135539.ref025" ref-type="bibr">25</xref>, <xref rid="pone.0135539.ref026" ref-type="bibr">26</xref>] as well as primate neurophysiology[<xref rid="pone.0135539.ref027" ref-type="bibr">27</xref>] which suggest greater numbers of units sensitive to changes in lateral or sway heading changes for both visual and inertial headings or in terms of the current model r &gt; 1. If visual headings are perceived in retina coordinates as suggested by recordings from VIP[<xref rid="pone.0135539.ref023" ref-type="bibr">23</xref>, <xref rid="pone.0135539.ref024" ref-type="bibr">24</xref>], then the offset angle (<italic>φ</italic>) will be similar to the eye position (<xref rid="pone.0135539.g001" ref-type="fig">Fig 1B</xref>). This model was fit to the responses for each condition. This was done using the fminsearch function in Matlab (Mathworks, Natick, Massachusetts) to minimize the mean squared error (MSE) of the model predictions relative to the observed responses.</p>
                <p>Statistics was performed using JMP Pro version 11 for the Macintosh (SAS, Cary, North Carolina). A paired student’s t-test was used to determine significance between model fit parameters across the population tested. One way analysis of variance (ANOVA) was used to determine if there were significant effects of gaze/head position, stimulus types, and between subjects.</p>
              </sec>
            </sec>
            <sec sec-type="results" id="sec009">
              <title>Results</title>
              <p>Eye position at the start of the trial was within 1° of the intended target. Eye fixation remained at the intended point during motion. In conditions in which the head position was varied over ±28°, the subjects were able to do this accurately. The average error was 0.2° at the start of the trial with the maximum &lt;1°. The head also remained in position during the inertial movement. The average peak-to-peak variation in yaw head position during the inertial movement in head varied conditions was 1.9° with the standard deviation of head position averaging 0.6°.</p>
              <p>Performance of a typical subject (#4) is shown in <xref rid="pone.0135539.g002" ref-type="fig">Fig 2</xref> with the results of model fit to the data. The visual headings (<xref rid="pone.0135539.g002" ref-type="fig">Fig 2A, 2C, 2F, &amp; 2I</xref>) had heading specific biases that could be predicted by relative greater sensitivity to sway motion as the best fit ratio (r) ranged from 2.1 to 3. The heading offsets were a function of gaze position (<xref rid="pone.0135539.g002" ref-type="fig">Fig 2C &amp; 2I</xref>). Inertial headings were predicted using a sensitivity to sway closer to that of surge with the best r fit ranging from 1.2 to 1.8 (<xref rid="pone.0135539.g002" ref-type="fig">Fig 2B, 2D, 2G, &amp; 2J</xref>). Inertial heading perception was independent of gaze or head position. When visual and inertial headings were combined (<xref rid="pone.0135539.g002" ref-type="fig">Fig 2E, 2H, &amp; 2K</xref>) the perceived headings and model fits represented a response that was intermediate between the visual and inertial only conditions.</p>
              <fig id="pone.0135539.g002" orientation="portrait" position="float">
                <object-id pub-id-type="doi">10.1371/journal.pone.0135539.g002</object-id>
                <label>Fig 2</label>
                <caption>
                  <title>Sample data and model fits for a typical individual subject (#4).</title>
                  <p>The abscissa represents the stimulus heading. The ordinate represents the response error (perceived minus stimulus heading). Curves represent the best fit of the PVD model. Top row (A&amp;B): The head centered gaze centered (HCGC) condition. Second row (panels C-E): The head centered gaze varied (HCGV) condition with gaze varied by ±28°. The gaze direction strongly influenced visual heading perception as indicated by the PVD model phase/offset parameter (<italic>φ</italic>). Third row (F-H): Head position varied over ±28° with gaze centered (HVGC). Inertial headings were independent of head position. Bottom row (I-K): Varying gaze and head yielded results similar to varying gaze alone (C-E).</p>
                </caption>
                <graphic xlink:href="pone.0135539.g002"/>
              </fig>
              <p>The data were summarized using the ratio (<italic>r</italic>) and phase or offset (<italic>φ</italic>) parameters of the PVD model fit. The offset parameters that best fit each subject’s responses are shown for each subject and trial block type (<xref rid="pone.0135539.g003" ref-type="fig">Fig 3</xref>). For the HCGC conditions the offset averaged near zero (<xref rid="pone.0135539.g003" ref-type="fig">Fig 3A &amp; 3B</xref>). In visual heading conditions where gaze was varied there was a large and significant effect of gaze direction (<xref rid="pone.0135539.g003" ref-type="fig">Fig 3C &amp; 3I</xref>). The average offset was 13 ± 2° (mean ± SEM) in the direction of gaze or 46% of the gaze shift with the head centered and 17 ± 2° or 61% of the gaze shift in the HVGV condition. The mean effect was about half the size at 7.2 ± 1.4° (HCGV) and 12.8 ± 1.8° (HVGV) but remained highly significant with combined visual and inertial headings (<xref rid="pone.0135539.g003" ref-type="fig">Fig 3E &amp; 3K</xref>). In every subject, when a visual stimulus was present, gaze shifted the perceived heading estimate so that left gaze produced a positive (rightward) offset and right gaze produced a relatively negative (leftward) offset. This was consistent with retina based coordinates since a leftward gaze would cause stationary objects to appear shifted to the right and vice-versa. Shifts in gaze did not cause a significant shift in inertial heading perception with the average shift being -0.3±1.4° (HCGV). Similarly head shifts (HVGC) produced a non-significant offset of 2.0 ± 1.2°.</p>
              <fig id="pone.0135539.g003" orientation="portrait" position="float">
                <object-id pub-id-type="doi">10.1371/journal.pone.0135539.g003</object-id>
                <label>Fig 3</label>
                <caption>
                  <title>The phase/offset parameter (<italic>φ</italic>) of the PVD model for each of condition across subjects.</title>
                  <p>The average (combined) value is shown in the furthest right column marked with a C. For the HCGC conditions the average phase offset was near zero. In conditions where the trial blocks included multiple head and/or gaze positions (panels C-K) a T-test was used to determine if the values were significantly different across subjects and p-values are printed for each condition.</p>
                </caption>
                <graphic xlink:href="pone.0135539.g003"/>
              </fig>
              <p>Unlike the offset, the ratio (r) parameter of the model that best fit the data did not depend on the gaze or head position (e.g. HVGC, HCGV, etc.; ANOVA, p = 0.98, F = 0.06). The ratio depended on the stimulus type (e.g. visual, inertial, or combined; ANOVA p &lt; 0.0001, F = 32.37). The ratio did not depend on the direction of gaze or head position (ANOVA p = 0.63, F = 0.23). The mean r for a visual stimulus was 2.78 (95% CI: 1.39–5.47), for an inertial stimulus it was 1.21 (95% CI: 0.79–1.72), for the combined visual-inertial stimulus it was 1.75 (95% CI: 0.99–3.42). There was significant variation in r across subjects (ANOVA p &lt; 0.0001, F = 6.45). The ratio is shown by subject and stimulus type in <xref rid="pone.0135539.g004" ref-type="fig">Fig 4</xref>. In every subject the ratio was higher for the visual condition relative to the inertial condition. The combined stimulus yielded a ratio that had a variable relationship to the visual and inertial ratio depending on subject and could be closer to inertial (subjects 1, 6, 7), similar to visual (subject 3), or intermediate between the two (subjects 2, 4, 5).</p>
              <fig id="pone.0135539.g004" orientation="portrait" position="float">
                <object-id pub-id-type="doi">10.1371/journal.pone.0135539.g004</object-id>
                <label>Fig 4</label>
                <caption>
                  <title>The ratio parameter (<italic>r</italic>) of the PVD model.</title>
                  <p>Unlike the offset parameter, ratio was independent of gaze/head condition which where combined. The values combined across subjects are shown in the furthest right column marked C. Error bars represent the 95% CI.</p>
                </caption>
                <graphic xlink:href="pone.0135539.g004"/>
              </fig>
              <p>Once fit with the ideal ratio and offset parameters the MSE of the fit of the model to the data did not depend on gaze/head condition (ANOVA, p = 0.72) or stimulus type (ANOVA, p = 0.52).</p>
            </sec>
            <sec sec-type="conclusions" id="sec010">
              <title>Discussion</title>
              <p>The method used here to measure heading estimates is a relatively simple task which has the potential to be influenced by confounding factors including cognitive influences. Yet, the large offsets in visual heading estimates with changes in gaze position were consistently observed across subjects and predicted from the known neurophysiology. In VIP, visual motion stimuli have been shown to be represented in eye-centered coordinates[<xref rid="pone.0135539.ref023" ref-type="bibr">23</xref>, <xref rid="pone.0135539.ref024" ref-type="bibr">24</xref>]. The current data demonstrate that although visual headings are biased towards eye position, the bias in the heading was only about half of the eye position offset. This is likely because eye position is also considered in heading estimation[<xref rid="pone.0135539.ref035" ref-type="bibr">35</xref>–<xref rid="pone.0135539.ref038" ref-type="bibr">38</xref>], although the current data demonstrate that the effect of eye position is not completely corrected. Although it is possible that visual headings could be represented in body or world coordinates elsewhere in the central nervous system no such area has yet been identified. The current data suggest that no such representations of visual headings exist as subject’s reported perception of visual headings was strongly biased towards retina coordinates.</p>
              <p>The other major finding was that inertial heading was represented in a body fixed coordinate system. Even large variations in head and gaze position had no influence on the heading estimates. The current experiments did not vary the body orientation relative to the earth so an earth fixed coordinate system is also possible. However, it is clear that the inertial headings and visual headings are perceived relative to different coordinate systems. This is consistent with the neurophysiology which demonstrates that in VIP, vestibular heading representation does not change with changes in either eye or head position[<xref rid="pone.0135539.ref016" ref-type="bibr">16</xref>]. Although it has been shown in MSTd and the parientoinsular vestibular cortex vestibular headings appear to be in head centered coordinates [<xref rid="pone.0135539.ref006" ref-type="bibr">6</xref>, <xref rid="pone.0135539.ref016" ref-type="bibr">16</xref>]. It is not surprising that there are areas of the brain where inertial signals are represented in head coordinates as the vestibular organs are fixed in the head, but the current data demonstrates that inertial heading perception follows the neurophysiology of VIP most closely.</p>
              <p>Previous studies have looked at multisensory integration using visual and inertial cues[<xref rid="pone.0135539.ref003" ref-type="bibr">3</xref>, <xref rid="pone.0135539.ref039" ref-type="bibr">39</xref>–<xref rid="pone.0135539.ref041" ref-type="bibr">41</xref>]. A common model of multisensory integration is that cues are integrated in a statistically optimal way also known as an ideal observer model. This model predicts that each sensory cue will be weighted according to its relative reliability. This weighting strategy is what is predicted by Bayesian probability theory[<xref rid="pone.0135539.ref042" ref-type="bibr">42</xref>]. For visual-vestibular heading integration, the prior work in this area has focused on multisensory integration using a discrimination task (e.g. subjects report if a test stimulus is to the right or left of a reference heading). The current experiments involve a estimation task which allows the bias and reference frame to also be determined. The relative reliability of visual and inertial cues is more complex for heading estimation because the relative reliability varies not only on the stimulus but also with heading direction. The current experiments did not vary the relative reliabilities of the stimuli or offset the visual and inertial stimuli relative to each other which limits the scope of conclusions that can be made with regard to multi-sensory integration. However, the phase offsets (<xref rid="pone.0135539.g003" ref-type="fig">Fig 3</xref>) and ratios (<xref rid="pone.0135539.g004" ref-type="fig">Fig 4</xref>) calculated demonstrated that the combined condition was usually intermediate between the visual and inertial condition. In some subjects the combined stimulus was closer to inertial (e.g. subject 1) while in others it was similar to visual (e.g. subject 3). It seems clear that there is no uniform common reference frame for heading estimation, but how the intermediate reference frame is developed remains unclear.</p>
              <p>An obvious issue raised by these results is that if eye position causes large biases in heading perception, what are the implications for day-to-day activities such as ambulation and driving? It is possible that feedback could minimize the biases observed here. The effect of feedback was not studied in the current experiments and it is likely that subjects were not aware that their responses were biased. However, these subjects also had feedback during their daily activities such as driving and ambulation that did not eliminate these biases. During ambulation people tend to direct gaze in the direction of intended motion[<xref rid="pone.0135539.ref043" ref-type="bibr">43</xref>, <xref rid="pone.0135539.ref044" ref-type="bibr">44</xref>] which makes it easier to maintain an accurate heading[<xref rid="pone.0135539.ref036" ref-type="bibr">36</xref>] and this also occurs with driving[<xref rid="pone.0135539.ref045" ref-type="bibr">45</xref>–<xref rid="pone.0135539.ref047" ref-type="bibr">47</xref>]. Thus, under natural conditions, control of gaze direction may be the mechanism by which heading errors that could arise with eccentric gaze positions are minimized. When gaze is eccentric from the intended course by as little as 5° while driving, subjects shifted their position on road significantly toward the direction of gaze[<xref rid="pone.0135539.ref047" ref-type="bibr">47</xref>]. With vestibular headings in body coordinates, it is not surprising that head orientation changes do not influence ambulation direction [<xref rid="pone.0135539.ref048" ref-type="bibr">48</xref>]. The current data suggest fixed eccentric gaze while driving could lead to deviation towards the direction of gaze. Such an experiment has not to the author’s knowledge been done but, fixing gaze at a central position causes steering errors and decreased performance in driving simulation[<xref rid="pone.0135539.ref049" ref-type="bibr">49</xref>, <xref rid="pone.0135539.ref050" ref-type="bibr">50</xref>]. Thus prior behavioral data is consistent with the current findings.</p>
              <p>The current data strongly suggest that visual-vestibular heading estimation occurs in different reference frames.</p>
            </sec>
          </body>
          <back>
            <ref-list>
              <title>References</title>
              <ref id="pone.0135539.ref001">
                <label>1</label>
                <mixed-citation publication-type="journal"><name><surname>Chen</surname><given-names>A</given-names></name>, <name><surname>DeAngelis</surname><given-names>GC</given-names></name>, <name><surname>Angelaki</surname><given-names>DE</given-names></name>. <article-title>Convergence of vestibular and visual self-motion signals in an area of the posterior sylvian fissure</article-title>. <source>The Journal of neuroscience: the official journal of the Society for Neuroscience</source>. <year>2011</year>;<volume>31</volume>(<issue>32</issue>):<fpage>11617</fpage>–<lpage>27</lpage>. Epub 2011/08/13. <pub-id pub-id-type="doi">10.1523/JNEUROSCI.1266-11.2011</pub-id>
<?supplied-pmid 21832191?><pub-id pub-id-type="pmid">21832191</pub-id></mixed-citation>
              </ref>
              <ref id="pone.0135539.ref002">
                <label>2</label>
                <mixed-citation publication-type="other">DeAngelis GC, Angelaki DE. Visual-Vestibular Integration for Self-Motion Perception. In: Murray MM, Wallace MT, editors. The Neural Bases of Multisensory Processes. Frontiers in Neuroscience. Boca Raton (FL)2012.</mixed-citation>
              </ref>
              <ref id="pone.0135539.ref003">
                <label>3</label>
                <mixed-citation publication-type="journal"><name><surname>Fetsch</surname><given-names>CR</given-names></name>, <name><surname>DeAngelis</surname><given-names>GC</given-names></name>, <name><surname>Angelaki</surname><given-names>DE</given-names></name>. <article-title>Bridging the gap between theories of sensory cue integration and the physiology of multisensory neurons</article-title>. <source>Nature reviews Neuroscience</source>. <year>2013</year>;<volume>14</volume>(<issue>6</issue>):<fpage>429</fpage>–<lpage>42</lpage>. <pub-id pub-id-type="doi">10.1038/nrn3503</pub-id>
.<?supplied-pmid 23686172?><pub-id pub-id-type="pmid">23686172</pub-id></mixed-citation>
              </ref>
              <ref id="pone.0135539.ref004">
                <label>4</label>
                <mixed-citation publication-type="journal"><name><surname>Larish</surname><given-names>JF</given-names></name>, <name><surname>Flach</surname><given-names>JM</given-names></name>. <article-title>Sources of optical information useful for perception of speed of rectilinear self-motion</article-title>. <source>J Exp Psychol Hum Percept Perform</source>. <year>1990</year>;<volume>16</volume>(<issue>2</issue>):<fpage>295</fpage>–<lpage>302</lpage>. .<?supplied-pmid 2142200?><pub-id pub-id-type="pmid">2142200</pub-id></mixed-citation>
              </ref>
              <ref id="pone.0135539.ref005">
                <label>5</label>
                <mixed-citation publication-type="journal"><name><surname>Palmisano</surname><given-names>S</given-names></name>, <name><surname>Allison</surname><given-names>RS</given-names></name>, <name><surname>Pekin</surname><given-names>F</given-names></name>. <article-title>Accelerating self-motion displays produce more compelling vection in depth</article-title>. <source>Perception</source>. <year>2008</year>;<volume>37</volume>(<issue>1</issue>):<fpage>22</fpage>–<lpage>33</lpage>. Epub 2008/04/11. .<?supplied-pmid 18399245?><pub-id pub-id-type="pmid">18399245</pub-id></mixed-citation>
              </ref>
              <ref id="pone.0135539.ref006">
                <label>6</label>
                <mixed-citation publication-type="journal"><name><surname>Fetsch</surname><given-names>CR</given-names></name>, <name><surname>Wang</surname><given-names>S</given-names></name>, <name><surname>Gu</surname><given-names>Y</given-names></name>, <name><surname>Deangelis</surname><given-names>GC</given-names></name>, <name><surname>Angelaki</surname><given-names>DE</given-names></name>. <article-title>Spatial reference frames of visual, vestibular, and multimodal heading signals in the dorsal subdivision of the medial superior temporal area</article-title>. <source>J Neurosci</source>. <year>2007</year>;<volume>27</volume>(<issue>3</issue>):<fpage>700</fpage>–<lpage>12</lpage>. .<?supplied-pmid 17234602?><pub-id pub-id-type="pmid">17234602</pub-id></mixed-citation>
              </ref>
              <ref id="pone.0135539.ref007">
                <label>7</label>
                <mixed-citation publication-type="other">Roditi RE, Crane BT. Asymmetries in human vestibular perception thresholds. Association for Research in Otolarngology, 34th Annual Meeting; Baltimore, MD2011. p. 1006.</mixed-citation>
              </ref>
              <ref id="pone.0135539.ref008">
                <label>8</label>
                <mixed-citation publication-type="journal"><name><surname>Soyka</surname><given-names>F</given-names></name>, <name><surname>Giordano</surname><given-names>PR</given-names></name>, <name><surname>Barnett-Cowan</surname><given-names>M</given-names></name>, <name><surname>Bulthoff</surname><given-names>HH</given-names></name>. <article-title>Modeling direction discrimination thresholds for yaw rotations around an earth-vertical axis for arbitrary motion profiles</article-title>. <source>Exp Brain Res</source>. <year>2012</year>;<volume>220</volume>(<issue>1</issue>):<fpage>89</fpage>–<lpage>99</lpage>. Epub 2012/05/25. <pub-id pub-id-type="doi">10.1007/s00221-012-3120-x</pub-id>
<?supplied-pmid 22623095?><pub-id pub-id-type="pmid">22623095</pub-id></mixed-citation>
              </ref>
              <ref id="pone.0135539.ref009">
                <label>9</label>
                <mixed-citation publication-type="journal"><name><surname>Merfeld</surname><given-names>DM</given-names></name>, <name><surname>Park</surname><given-names>S</given-names></name>, <name><surname>Gianna-Poulin</surname><given-names>C</given-names></name>, <name><surname>Black</surname><given-names>FO</given-names></name>, <name><surname>Wood</surname><given-names>S</given-names></name>. <article-title>Vestibular perception and action employ qualitatively different mechanisms. I. Frequency response of VOR and perceptual responses during Translation and Tilt</article-title>. <source>Journal of neurophysiology</source>. <year>2005</year>;<volume>94</volume>(<issue>1</issue>):<fpage>186</fpage>–<lpage>98</lpage>. .<?supplied-pmid 15728767?><pub-id pub-id-type="pmid">15728767</pub-id></mixed-citation>
              </ref>
              <ref id="pone.0135539.ref010">
                <label>10</label>
                <mixed-citation publication-type="journal"><name><surname>McIntyre</surname><given-names>S</given-names></name>, <name><surname>Seizova-Cajic</surname><given-names>T</given-names></name>. <article-title>Neck muscle vibration in full cues affects pointing</article-title>. <source>Journal of vision</source>. <year>2007</year>;<volume>7</volume>(<issue>5</issue>):<fpage>9</fpage> 1–8. Epub 2008/01/26. <pub-id pub-id-type="doi">10.1167/7.5.9</pub-id>
.<?supplied-pmid 18217849?><pub-id pub-id-type="pmid">18217849</pub-id></mixed-citation>
              </ref>
              <ref id="pone.0135539.ref011">
                <label>11</label>
                <mixed-citation publication-type="journal"><name><surname>Ceyte</surname><given-names>H</given-names></name>, <name><surname>Cian</surname><given-names>C</given-names></name>, <name><surname>Nougier</surname><given-names>V</given-names></name>, <name><surname>Olivier</surname><given-names>I</given-names></name>, <name><surname>Roux</surname><given-names>A</given-names></name>. <article-title>Effects of neck muscles vibration on the perception of the head and trunk midline position</article-title>. <source>Exp Brain Res</source>. <year>2006</year>;<volume>170</volume>(<issue>1</issue>):<fpage>136</fpage>–<lpage>40</lpage>. Epub 2006/02/28. <pub-id pub-id-type="doi">10.1007/s00221-006-0389-7</pub-id>
.<?supplied-pmid 16501959?><pub-id pub-id-type="pmid">16501959</pub-id></mixed-citation>
              </ref>
              <ref id="pone.0135539.ref012">
                <label>12</label>
                <mixed-citation publication-type="journal"><name><surname>Cohen</surname><given-names>YE</given-names></name>, <name><surname>Andersen</surname><given-names>RA</given-names></name>. <article-title>A common reference frame for movement plans in the posterior parietal cortex</article-title>. <source>Nature reviews Neuroscience</source>. <year>2002</year>;<volume>3</volume>(<issue>7</issue>):<fpage>553</fpage>–<lpage>62</lpage>. <pub-id pub-id-type="doi">10.1038/nrn873</pub-id>
.<?supplied-pmid 12094211?><pub-id pub-id-type="pmid">12094211</pub-id></mixed-citation>
              </ref>
              <ref id="pone.0135539.ref013">
                <label>13</label>
                <mixed-citation publication-type="journal"><name><surname>Stein</surname><given-names>BE</given-names></name>, <name><surname>Meredith</surname><given-names>MA</given-names></name>, <name><surname>Wallace</surname><given-names>MT</given-names></name>. <article-title>The visually responsive neuron and beyond: multisensory integration in cat and monkey</article-title>. <source>Prog Brain Res</source>. <year>1993</year>;<volume>95</volume>:<fpage>79</fpage>–<lpage>90</lpage>. .<?supplied-pmid 8493355?><pub-id pub-id-type="pmid">8493355</pub-id></mixed-citation>
              </ref>
              <ref id="pone.0135539.ref014">
                <label>14</label>
                <mixed-citation publication-type="journal"><name><surname>Schlack</surname><given-names>A</given-names></name>, <name><surname>Sterbing-D'Angelo</surname><given-names>SJ</given-names></name>, <name><surname>Hartung</surname><given-names>K</given-names></name>, <name><surname>Hoffmann</surname><given-names>KP</given-names></name>, <name><surname>Bremmer</surname><given-names>F</given-names></name>. <article-title>Multisensory space representations in the macaque ventral intraparietal area</article-title>. <source>J Neurosci</source>. <year>2005</year>;<volume>25</volume>(<issue>18</issue>):<fpage>4616</fpage>–<lpage>25</lpage>. <pub-id pub-id-type="doi">10.1523/JNEUROSCI.0455-05.2005</pub-id>
.<?supplied-pmid 15872109?><pub-id pub-id-type="pmid">15872109</pub-id></mixed-citation>
              </ref>
              <ref id="pone.0135539.ref015">
                <label>15</label>
                <mixed-citation publication-type="journal"><name><surname>Karnath</surname><given-names>HO</given-names></name>, <name><surname>Sievering</surname><given-names>D</given-names></name>, <name><surname>Fetter</surname><given-names>M</given-names></name>. <article-title>The interactive contribution of neck muscle proprioception and vestibular stimulation to subjective "straight ahead" orientation in man</article-title>. <source>Exp Brain Res</source>. <year>1994</year>;<volume>101</volume>(<issue>1</issue>):<fpage>140</fpage>–<lpage>6</lpage>. Epub 1994/01/01. .<?supplied-pmid 7843292?><pub-id pub-id-type="pmid">7843292</pub-id></mixed-citation>
              </ref>
              <ref id="pone.0135539.ref016">
                <label>16</label>
                <mixed-citation publication-type="journal"><name><surname>Chen</surname><given-names>X</given-names></name>, <name><surname>Deangelis</surname><given-names>GC</given-names></name>, <name><surname>Angelaki</surname><given-names>DE</given-names></name>. <article-title>Diverse spatial reference frames of vestibular signals in parietal cortex</article-title>. <source>Neuron</source>. <year>2013</year>;<volume>80</volume>(<issue>5</issue>):<fpage>1310</fpage>–<lpage>21</lpage>. <pub-id pub-id-type="doi">10.1016/j.neuron.2013.09.006</pub-id> PubMed Central PMCID: PMCPMC3858444.
<?supplied-pmid 24239126?><pub-id pub-id-type="pmid">24239126</pub-id></mixed-citation>
              </ref>
              <ref id="pone.0135539.ref017">
                <label>17</label>
                <mixed-citation publication-type="journal"><name><surname>Avillac</surname><given-names>M</given-names></name>, <name><surname>Deneve</surname><given-names>S</given-names></name>, <name><surname>Olivier</surname><given-names>E</given-names></name>, <name><surname>Pouget</surname><given-names>A</given-names></name>, <name><surname>Duhamel</surname><given-names>JR</given-names></name>. <article-title>Reference frames for representing visual and tactile locations in parietal cortex</article-title>. <source>Nature neuroscience</source>. <year>2005</year>;<volume>8</volume>(<issue>7</issue>):<fpage>941</fpage>–<lpage>9</lpage>. <pub-id pub-id-type="doi">10.1038/nn1480</pub-id>
.<?supplied-pmid 15951810?><pub-id pub-id-type="pmid">15951810</pub-id></mixed-citation>
              </ref>
              <ref id="pone.0135539.ref018">
                <label>18</label>
                <mixed-citation publication-type="journal"><name><surname>Mullette-Gillman</surname><given-names>OA</given-names></name>, <name><surname>Cohen</surname><given-names>YE</given-names></name>, <name><surname>Groh</surname><given-names>JM</given-names></name>. <article-title>Eye-centered, head-centered, and complex coding of visual and auditory targets in the intraparietal sulcus</article-title>. <source>Journal of neurophysiology</source>. <year>2005</year>;<volume>94</volume>(<issue>4</issue>):<fpage>2331</fpage>–<lpage>52</lpage>. <pub-id pub-id-type="doi">10.1152/jn.00021.2005</pub-id>
.<?supplied-pmid 15843485?><pub-id pub-id-type="pmid">15843485</pub-id></mixed-citation>
              </ref>
              <ref id="pone.0135539.ref019">
                <label>19</label>
                <mixed-citation publication-type="journal"><name><surname>Chen</surname><given-names>A</given-names></name>, <name><surname>DeAngelis</surname><given-names>GC</given-names></name>, <name><surname>Angelaki</surname><given-names>DE</given-names></name>. <article-title>Representation of vestibular and visual cues to self-motion in ventral intraparietal cortex</article-title>. <source>The Journal of neuroscience: the official journal of the Society for Neuroscience</source>. <year>2011</year>;<volume>31</volume>(<issue>33</issue>):<fpage>12036</fpage>–<lpage>52</lpage>. Epub 2011/08/19. <pub-id pub-id-type="doi">10.1523/JNEUROSCI.0395-11.2011</pub-id>
<?supplied-pmid 21849564?><pub-id pub-id-type="pmid">21849564</pub-id></mixed-citation>
              </ref>
              <ref id="pone.0135539.ref020">
                <label>20</label>
                <mixed-citation publication-type="journal"><name><surname>Colby</surname><given-names>CL</given-names></name>, <name><surname>Duhamel</surname><given-names>JR</given-names></name>, <name><surname>Goldberg</surname><given-names>ME</given-names></name>. <article-title>Ventral intraparietal area of the macaque: anatomic location and visual response properties</article-title>. <source>Journal of neurophysiology</source>. <year>1993</year>;<volume>69</volume>(<issue>3</issue>):<fpage>902</fpage>–<lpage>14</lpage>. Epub 1993/03/01. .<?supplied-pmid 8385201?><pub-id pub-id-type="pmid">8385201</pub-id></mixed-citation>
              </ref>
              <ref id="pone.0135539.ref021">
                <label>21</label>
                <mixed-citation publication-type="journal"><name><surname>Maciokas</surname><given-names>JB</given-names></name>, <name><surname>Britten</surname><given-names>KH</given-names></name>. <article-title>Extrastriate area MST and parietal area VIP similarly represent forward headings</article-title>. <source>Journal of neurophysiology</source>. <year>2010</year>;<volume>104</volume>(<issue>1</issue>):<fpage>239</fpage>–<lpage>47</lpage>. Epub 2010/04/30. <pub-id pub-id-type="doi">10.1152/jn.01083.2009</pub-id>
<?supplied-pmid 20427618?><pub-id pub-id-type="pmid">20427618</pub-id></mixed-citation>
              </ref>
              <ref id="pone.0135539.ref022">
                <label>22</label>
                <mixed-citation publication-type="journal"><name><surname>Guipponi</surname><given-names>O</given-names></name>, <name><surname>Wardak</surname><given-names>C</given-names></name>, <name><surname>Ibarrola</surname><given-names>D</given-names></name>, <name><surname>Comte</surname><given-names>JC</given-names></name>, <name><surname>Sappey-Marinier</surname><given-names>D</given-names></name>, <name><surname>Pinede</surname><given-names>S</given-names></name>, <etal>et al</etal><article-title>Multimodal convergence within the intraparietal sulcus of the macaque monkey</article-title>. <source>J Neurosci</source>. <year>2013</year>;<volume>33</volume>(<issue>9</issue>):<fpage>4128</fpage>–<lpage>39</lpage>. <pub-id pub-id-type="doi">10.1523/JNEUROSCI.1421-12.2013</pub-id>
.<?supplied-pmid 23447621?><pub-id pub-id-type="pmid">23447621</pub-id></mixed-citation>
              </ref>
              <ref id="pone.0135539.ref023">
                <label>23</label>
                <mixed-citation publication-type="journal"><name><surname>Chen</surname><given-names>X</given-names></name>, <name><surname>DeAngelis</surname><given-names>GC</given-names></name>, <name><surname>Angelaki</surname><given-names>DE</given-names></name>. <article-title>Eye-centered visual receptive fields in the ventral intraparietal area</article-title>. <source>Journal of neurophysiology</source>. <year>2014</year>;<volume>112</volume>(<issue>2</issue>):<fpage>353</fpage>–<lpage>61</lpage>. <pub-id pub-id-type="doi">10.1152/jn.00057.2014</pub-id>
<?supplied-pmid 24790176?><pub-id pub-id-type="pmid">24790176</pub-id></mixed-citation>
              </ref>
              <ref id="pone.0135539.ref024">
                <label>24</label>
                <mixed-citation publication-type="journal"><name><surname>Chen</surname><given-names>X</given-names></name>, <name><surname>DeAngelis</surname><given-names>GC</given-names></name>, <name><surname>Angelaki</surname><given-names>DE</given-names></name>. <article-title>Eye-centered representation of optic flow tuning in the ventral intraparietal area</article-title>. <source>J Neurosci</source>. <year>2013</year>;<volume>33</volume>(<issue>47</issue>):<fpage>18574</fpage>–<lpage>82</lpage>. <pub-id pub-id-type="doi">10.1523/JNEUROSCI.2837-13.2013</pub-id>
<?supplied-pmid 24259579?><pub-id pub-id-type="pmid">24259579</pub-id></mixed-citation>
              </ref>
              <ref id="pone.0135539.ref025">
                <label>25</label>
                <mixed-citation publication-type="journal"><name><surname>Crane</surname><given-names>BT</given-names></name>. <article-title>Direction Specific Biases in Human Visual and Vestibular Heading Perception</article-title>. <source>PLoS One</source>. <year>2012</year>;<volume>7</volume>(<issue>12</issue>):<fpage>e51383</fpage> Epub 12/7/12. <pub-id pub-id-type="doi">10.1371/journal.pone.0051383</pub-id>
.<?supplied-pmid 23236490?><pub-id pub-id-type="pmid">23236490</pub-id></mixed-citation>
              </ref>
              <ref id="pone.0135539.ref026">
                <label>26</label>
                <mixed-citation publication-type="journal"><name><surname>Cuturi</surname><given-names>LF</given-names></name>, <name><surname>Macneilage</surname><given-names>PR</given-names></name>. <article-title>Systematic biases in human heading estimation</article-title>. <source>PLoS One</source>. <year>2013</year>;<volume>8</volume>(<issue>2</issue>):<fpage>e56862</fpage><pub-id pub-id-type="doi">10.1371/journal.pone.0056862</pub-id><?supplied-pmid 23457631?><pub-id pub-id-type="pmid">23457631</pub-id></mixed-citation>
              </ref>
              <ref id="pone.0135539.ref027">
                <label>27</label>
                <mixed-citation publication-type="journal"><name><surname>Gu</surname><given-names>Y</given-names></name>, <name><surname>Fetsch</surname><given-names>CR</given-names></name>, <name><surname>Adeyemo</surname><given-names>B</given-names></name>, <name><surname>Deangelis</surname><given-names>GC</given-names></name>, <name><surname>Angelaki</surname><given-names>DE</given-names></name>. <article-title>Decoding of MSTd population activity accounts for variations in the precision of heading perception</article-title>. <source>Neuron</source>. <year>2010</year>;<volume>66</volume>(<issue>4</issue>):<fpage>596</fpage>–<lpage>609</lpage>. Epub 2010/06/01. <pub-id pub-id-type="doi">10.1016/j.neuron.2010.04.026</pub-id>
<?supplied-pmid 20510863?><pub-id pub-id-type="pmid">20510863</pub-id></mixed-citation>
              </ref>
              <ref id="pone.0135539.ref028">
                <label>28</label>
                <mixed-citation publication-type="journal"><name><surname>MacNeilage</surname><given-names>PR</given-names></name>, <name><surname>Banks</surname><given-names>MS</given-names></name>, <name><surname>DeAngelis</surname><given-names>GC</given-names></name>, <name><surname>Angelaki</surname><given-names>DE</given-names></name>. <article-title>Vestibular heading discrimination and sensitivity to linear acceleration in head and world coordinates</article-title>. <source>J Neurosci</source>. <year>2010</year>;<volume>30</volume>(<issue>27</issue>):<fpage>9084</fpage>–<lpage>94</lpage>. Epub 2010/07/09. doi: 30/27/9084 [pii] <pub-id pub-id-type="doi">10.1523/JNEUROSCI.1304-10.2010</pub-id>
<?supplied-pmid 20610742?><pub-id pub-id-type="pmid">20610742</pub-id></mixed-citation>
              </ref>
              <ref id="pone.0135539.ref029">
                <label>29</label>
                <mixed-citation publication-type="journal"><name><surname>Grabherr</surname><given-names>L</given-names></name>, <name><surname>Nicoucar</surname><given-names>K</given-names></name>, <name><surname>Mast</surname><given-names>FW</given-names></name>, <name><surname>Merfeld</surname><given-names>DM</given-names></name>. <article-title>Vestibular thresholds for yaw rotation about an earth-vertical axis as a function of frequency</article-title>. <source>Exp Brain Res</source>. <year>2008</year>;<volume>186</volume>(<issue>4</issue>):<fpage>677</fpage>–<lpage>81</lpage>. <pub-id pub-id-type="doi">10.1007/s00221-008-1350-8</pub-id>
<?supplied-pmid 18350283?><pub-id pub-id-type="pmid">18350283</pub-id></mixed-citation>
              </ref>
              <ref id="pone.0135539.ref030">
                <label>30</label>
                <mixed-citation publication-type="journal"><name><surname>Fetsch</surname><given-names>CR</given-names></name>, <name><surname>Turner</surname><given-names>AH</given-names></name>, <name><surname>Deangelis</surname><given-names>GC</given-names></name>, <name><surname>Angelaki</surname><given-names>DE</given-names></name>. <article-title>Dynamic re-weighting of visual and vestibular cues during self-motion perception</article-title>. <source>J Neurosci</source>. <year>2009</year>;<volume>29</volume>(<issue>49</issue>):<fpage>15601</fpage>–<lpage>12</lpage>. <pub-id pub-id-type="doi">10.1523/JNEUROSCI.2574-09.2009</pub-id>
.<?supplied-pmid 20007484?><pub-id pub-id-type="pmid">20007484</pub-id></mixed-citation>
              </ref>
              <ref id="pone.0135539.ref031">
                <label>31</label>
                <mixed-citation publication-type="journal"><name><surname>Crane</surname><given-names>BT</given-names></name>. <article-title>Human Visual and Vestibular Heading Perception in the Vertical Planes</article-title>. <source>J Assoc Res Otolaryngol</source>. <year>2014</year>;<volume>15</volume>(<issue>1</issue>):<fpage>87</fpage>–<lpage>102</lpage>. Epub 2013/11/20. <pub-id pub-id-type="doi">10.1007/s10162-013-0423-y</pub-id>
.<?supplied-pmid 24249574?><pub-id pub-id-type="pmid">24249574</pub-id></mixed-citation>
              </ref>
              <ref id="pone.0135539.ref032">
                <label>32</label>
                <mixed-citation publication-type="journal"><name><surname>Crane</surname><given-names>BT</given-names></name>. <article-title>The influence of head and body tilt on human fore-aft translation perception</article-title>. <source>Exp Brain Res</source>. <year>2014</year>;<volume>232</volume>:<fpage>3897</fpage>–<lpage>905</lpage>. <pub-id pub-id-type="doi">10.1007/s00221-014-4060-4</pub-id>
.<?supplied-pmid 25160866?><pub-id pub-id-type="pmid">25160866</pub-id></mixed-citation>
              </ref>
              <ref id="pone.0135539.ref033">
                <label>33</label>
                <mixed-citation publication-type="journal"><name><surname>Roditi</surname><given-names>RE</given-names></name>, <name><surname>Crane</surname><given-names>BT</given-names></name>. <article-title>Directional asymmetries and age effects in human self-motion perception</article-title>. <source>J Assoc Res Otolaryngol</source>. <year>2012</year>;<volume>13</volume>(<issue>3</issue>):<fpage>381</fpage>–<lpage>401</lpage>. <pub-id pub-id-type="doi">10.1007/s10162-012-0318-3</pub-id>
<?supplied-pmid 22402987?><pub-id pub-id-type="pmid">22402987</pub-id></mixed-citation>
              </ref>
              <ref id="pone.0135539.ref034">
                <label>34</label>
                <mixed-citation publication-type="journal"><name><surname>Benson</surname><given-names>AJ</given-names></name>, <name><surname>Hutt</surname><given-names>EC</given-names></name>, <name><surname>Brown</surname><given-names>SF</given-names></name>. <article-title>Thresholds for the perception of whole body angular movement about a vertical axis</article-title>. <source>Aviat Space Environ Med</source>. <year>1989</year>;<volume>60</volume>(<issue>3</issue>):<fpage>205</fpage>–<lpage>13</lpage>. .<?supplied-pmid 2712798?><pub-id pub-id-type="pmid">2712798</pub-id></mixed-citation>
              </ref>
              <ref id="pone.0135539.ref035">
                <label>35</label>
                <mixed-citation publication-type="journal"><name><surname>Royden</surname><given-names>CS</given-names></name>, <name><surname>Banks</surname><given-names>MS</given-names></name>, <name><surname>Crowell</surname><given-names>JA</given-names></name>. <article-title>The perception of heading during eye movements</article-title>. <source>Nature</source>. <year>1992</year>;<volume>360</volume>(<issue>6404</issue>):<fpage>583</fpage>–<lpage>5</lpage>. .<?supplied-pmid 1461280?><pub-id pub-id-type="pmid">1461280</pub-id></mixed-citation>
              </ref>
              <ref id="pone.0135539.ref036">
                <label>36</label>
                <mixed-citation publication-type="journal"><name><surname>Wann</surname><given-names>JP</given-names></name>, <name><surname>Swapp</surname><given-names>DK</given-names></name>. <article-title>Why you should look where you are going</article-title>. <source>Nature neuroscience</source>. <year>2000</year>;<volume>3</volume>(<issue>7</issue>):<fpage>647</fpage>–<lpage>8</lpage>. Epub 2000/06/22. <pub-id pub-id-type="doi">10.1038/76602</pub-id>
.<?supplied-pmid 10862695?><pub-id pub-id-type="pmid">10862695</pub-id></mixed-citation>
              </ref>
              <ref id="pone.0135539.ref037">
                <label>37</label>
                <mixed-citation publication-type="journal"><name><surname>Warren</surname><given-names>WH</given-names><suffix>Jr</suffix></name>, <name><surname>Hannon</surname><given-names>DJ</given-names></name>. <article-title>Direction of self-motion perceived from optical flow</article-title>. <source>Nature</source>. <year>1988</year>;<volume>336</volume>:<fpage>162</fpage>–<lpage>3</lpage>.</mixed-citation>
              </ref>
              <ref id="pone.0135539.ref038">
                <label>38</label>
                <mixed-citation publication-type="journal"><name><surname>Lappe</surname><given-names>M</given-names></name>, <name><surname>Bremmer</surname><given-names>F</given-names></name>, <name><surname>van den Berg</surname><given-names>AV</given-names></name>. <article-title>Perception of self-motion from visual flow</article-title>. <source>Trends Cogn Sci</source>. <year>1999</year>;<volume>3</volume>(<issue>9</issue>):<fpage>329</fpage>–<lpage>36</lpage>. .<?supplied-pmid 10461195?><pub-id pub-id-type="pmid">10461195</pub-id></mixed-citation>
              </ref>
              <ref id="pone.0135539.ref039">
                <label>39</label>
                <mixed-citation publication-type="journal"><name><surname>Gu</surname><given-names>Y</given-names></name>, <name><surname>Angelaki</surname><given-names>DE</given-names></name>, <name><surname>Deangelis</surname><given-names>GC</given-names></name>. <article-title>Neural correlates of multisensory cue integration in macaque MSTd</article-title>. <source>Nature neuroscience</source>. <year>2008</year>;<volume>11</volume>(<issue>10</issue>):<fpage>1201</fpage>–<lpage>10</lpage>. <pub-id pub-id-type="doi">10.1038/nn.2191</pub-id>
<?supplied-pmid 18776893?><pub-id pub-id-type="pmid">18776893</pub-id></mixed-citation>
              </ref>
              <ref id="pone.0135539.ref040">
                <label>40</label>
                <mixed-citation publication-type="journal"><name><surname>de Winkel</surname><given-names>KN</given-names></name>, <name><surname>Weesie</surname><given-names>J</given-names></name>, <name><surname>Werkhoven</surname><given-names>PJ</given-names></name>, <name><surname>Groen</surname><given-names>EL</given-names></name>. <article-title>Integration of visual and inertial cues in perceived heading of self-motion</article-title>. <source>Journal of vision</source>. <year>2010</year>;<volume>10</volume>(<issue>12</issue>):<fpage>1</fpage> Epub 2010/11/05. <pub-id pub-id-type="doi">10.1167/10.12.1</pub-id>
.<?supplied-pmid 21047733?><pub-id pub-id-type="pmid">21047733</pub-id></mixed-citation>
              </ref>
              <ref id="pone.0135539.ref041">
                <label>41</label>
                <mixed-citation publication-type="journal"><name><surname>Edwards</surname><given-names>M</given-names></name>, <name><surname>O'Mahony</surname><given-names>S</given-names></name>, <name><surname>Ibbotson</surname><given-names>MR</given-names></name>, <name><surname>Kohlhagen</surname><given-names>S</given-names></name>. <article-title>Vestibular stimulation affects optic-flow sensitivity</article-title>. <source>Perception</source>. <year>2010</year>;<volume>39</volume>(<issue>10</issue>):<fpage>1303</fpage>–<lpage>10</lpage>. Epub 2010/12/25. .<?supplied-pmid 21180352?><pub-id pub-id-type="pmid">21180352</pub-id></mixed-citation>
              </ref>
              <ref id="pone.0135539.ref042">
                <label>42</label>
                <mixed-citation publication-type="journal"><name><surname>Knill</surname><given-names>DC</given-names></name>, <name><surname>Pouget</surname><given-names>A</given-names></name>. <article-title>The Bayesian brain: the role of uncertainty in neural coding and computation</article-title>. <source>Trends in neurosciences</source>. <year>2004</year>;<volume>27</volume>(<issue>12</issue>):<fpage>712</fpage>–<lpage>9</lpage>. <pub-id pub-id-type="doi">10.1016/j.tins.2004.10.007</pub-id>
.<?supplied-pmid 15541511?><pub-id pub-id-type="pmid">15541511</pub-id></mixed-citation>
              </ref>
              <ref id="pone.0135539.ref043">
                <label>43</label>
                <mixed-citation publication-type="journal"><name><surname>Hollands</surname><given-names>MA</given-names></name>, <name><surname>Marple-Horvat</surname><given-names>DE</given-names></name>, <name><surname>Henkes</surname><given-names>S</given-names></name>, <name><surname>Rowan</surname><given-names>AK</given-names></name>. <article-title>Human Eye Movements During Visually Guided Stepping</article-title>. <source>Journal of motor behavior</source>. <year>1995</year>;<volume>27</volume>(<issue>2</issue>):<fpage>155</fpage>–<lpage>63</lpage>. <pub-id pub-id-type="doi">10.1080/00222895.1995.9941707</pub-id>
.<?supplied-pmid 12736124?><pub-id pub-id-type="pmid">12736124</pub-id></mixed-citation>
              </ref>
              <ref id="pone.0135539.ref044">
                <label>44</label>
                <mixed-citation publication-type="journal"><name><surname>Patla</surname><given-names>AE</given-names></name>, <name><surname>Vickers</surname><given-names>JN</given-names></name>. <article-title>Where and when do we look as we approach and step over an obstacle in the travel path?</article-title><source>Neuroreport</source>. <year>1997</year>;<volume>8</volume>(<issue>17</issue>):<fpage>3661</fpage>–<lpage>5</lpage>. .<?supplied-pmid 9427347?><pub-id pub-id-type="pmid">9427347</pub-id></mixed-citation>
              </ref>
              <ref id="pone.0135539.ref045">
                <label>45</label>
                <mixed-citation publication-type="journal"><name><surname>Land</surname><given-names>MF</given-names></name>, <name><surname>Lee</surname><given-names>DN</given-names></name>. <article-title>Where we look when we steer</article-title>. <source>Nature</source>. <year>1994</year>;<volume>369</volume>(<issue>6483</issue>):<fpage>742</fpage>–<lpage>4</lpage>. <pub-id pub-id-type="doi">10.1038/369742a0</pub-id>
.<?supplied-pmid 8008066?><pub-id pub-id-type="pmid">8008066</pub-id></mixed-citation>
              </ref>
              <ref id="pone.0135539.ref046">
                <label>46</label>
                <mixed-citation publication-type="journal"><name><surname>Wilkie</surname><given-names>RM</given-names></name>, <name><surname>Wann</surname><given-names>JP</given-names></name>. <article-title>Driving as night falls: the contribution of retinal flow and visual direction to the control of steering</article-title>. <source>Curr Biol</source>. <year>2002</year>;<volume>12</volume>(<issue>23</issue>):<fpage>2014</fpage>–<lpage>7</lpage>. .<?supplied-pmid 12477389?><pub-id pub-id-type="pmid">12477389</pub-id></mixed-citation>
              </ref>
              <ref id="pone.0135539.ref047">
                <label>47</label>
                <mixed-citation publication-type="journal"><name><surname>Readinger</surname><given-names>WO</given-names></name>, <name><surname>Chatziastros</surname><given-names>A</given-names></name>, <name><surname>Cunningham</surname><given-names>DW</given-names></name>, <name><surname>Bülthoff</surname><given-names>HH</given-names></name>. <article-title>Driving effects of retinal-flow properties associated with eccentric gaze</article-title>. <source>Perception</source>. <year>2001</year>;<volume>30</volume>(ECVP Abstract Supplement).</mixed-citation>
              </ref>
              <ref id="pone.0135539.ref048">
                <label>48</label>
                <mixed-citation publication-type="journal"><name><surname>Cinelli</surname><given-names>M</given-names></name>, <name><surname>Warren</surname><given-names>WH</given-names></name>. <article-title>Do walkers follow their heads? Investigating the role of head rotation in locomotor control</article-title>. <source>Exp Brain Res</source>. <year>2012</year>;<volume>219</volume>(<issue>2</issue>):<fpage>175</fpage>–<lpage>90</lpage>. <pub-id pub-id-type="doi">10.1007/s00221-012-3077-9</pub-id>
<?supplied-pmid 22466410?><pub-id pub-id-type="pmid">22466410</pub-id></mixed-citation>
              </ref>
              <ref id="pone.0135539.ref049">
                <label>49</label>
                <mixed-citation publication-type="journal"><name><surname>Wilkie</surname><given-names>RM</given-names></name>, <name><surname>Wann</surname><given-names>JP</given-names></name>. <article-title>Eye-movements aid the control of locomotion</article-title>. <source>Journal of vision</source>. <year>2003</year>;<volume>3</volume>(<issue>11</issue>):<fpage>677</fpage>–<lpage>84</lpage>. Epub 2004/02/10. <pub-id pub-id-type="doi">10.1167/3.11.3</pub-id>
.<?supplied-pmid 14765952?><pub-id pub-id-type="pmid">14765952</pub-id></mixed-citation>
              </ref>
              <ref id="pone.0135539.ref050">
                <label>50</label>
                <mixed-citation publication-type="journal"><name><surname>Marple-Horvat</surname><given-names>DE</given-names></name>, <name><surname>Chattington</surname><given-names>M</given-names></name>, <name><surname>Anglesea</surname><given-names>M</given-names></name>, <name><surname>Ashford</surname><given-names>DG</given-names></name>, <name><surname>Wilson</surname><given-names>M</given-names></name>, <name><surname>Keil</surname><given-names>D</given-names></name>. <article-title>Prevention of coordinated eye movements and steering impairs driving performance</article-title>. <source>Exp Brain Res</source>. <year>2005</year>;<volume>163</volume>(<issue>4</issue>):<fpage>411</fpage>–<lpage>20</lpage>. <pub-id pub-id-type="doi">10.1007/s00221-004-2192-7</pub-id>
.<?supplied-pmid 15841399?><pub-id pub-id-type="pmid">15841399</pub-id></mixed-citation>
              </ref>
            </ref-list>
          </back>
        </article>
      </metadata>
    </record>
  </GetRecord>
</OAI-PMH>
