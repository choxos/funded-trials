<?xml version="1.0" encoding="UTF-8"?>
<OAI-PMH xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/ http://www.openarchives.org/OAI/2.0/OAI-PMH.xsd">
  <responseDate>2024-05-03T04:43:44Z</responseDate>
  <request verb="GetRecord" identifier="oai:pubmedcentral.nih.gov:6370195" metadataPrefix="pmc">https:/www.ncbi.nlm.nih.gov/pmc/oai/oai.cgi</request>
  <GetRecord>
    <record>
      <header>
        <identifier>oai:pubmedcentral.nih.gov:6370195</identifier>
        <datestamp>2019-02-22</datestamp>
        <setSpec>plosone</setSpec>
        <setSpec>pmc-open</setSpec>
      </header>
      <metadata>
        <article xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" xsi:schemaLocation="https://jats.nlm.nih.gov/ns/archiving/1.3/ https://jats.nlm.nih.gov/archiving/1.3/xsd/JATS-archivearticle1-3.xsd" article-type="research-article">
          <front>
            <journal-meta>
              <journal-id journal-id-type="nlm-ta">PLoS One</journal-id>
              <journal-id journal-id-type="iso-abbrev">PLoS ONE</journal-id>
              <journal-id journal-id-type="publisher-id">plos</journal-id>
              <journal-id journal-id-type="pmc">plosone</journal-id>
              <journal-title-group>
                <journal-title>PLoS ONE</journal-title>
              </journal-title-group>
              <issn pub-type="epub">1932-6203</issn>
              <publisher>
                <publisher-name>Public Library of Science</publisher-name>
                <publisher-loc>San Francisco, CA USA</publisher-loc>
              </publisher>
            </journal-meta>
            <article-meta>
              <article-id pub-id-type="accession">PMC6370195</article-id>
              <article-id pub-id-type="pmcid">PMC6370195</article-id>
              <article-id pub-id-type="pmc-uid">6370195</article-id>
              <article-id pub-id-type="pmid">30742639</article-id>
              <article-id pub-id-type="publisher-id">PONE-D-18-06488</article-id>
              <article-id pub-id-type="doi">10.1371/journal.pone.0210463</article-id>
              <article-categories>
                <subj-group subj-group-type="heading">
                  <subject>Research Article</subject>
                </subj-group>
                <subj-group subj-group-type="Discipline-v3">
                  <subject>Biology and Life Sciences</subject>
                  <subj-group>
                    <subject>Anatomy</subject>
                    <subj-group>
                      <subject>Integumentary System</subject>
                      <subj-group>
                        <subject>Skin</subject>
                        <subj-group>
                          <subject>Eyelids</subject>
                        </subj-group>
                      </subj-group>
                    </subj-group>
                  </subj-group>
                </subj-group>
                <subj-group subj-group-type="Discipline-v3">
                  <subject>Medicine and Health Sciences</subject>
                  <subj-group>
                    <subject>Anatomy</subject>
                    <subj-group>
                      <subject>Integumentary System</subject>
                      <subj-group>
                        <subject>Skin</subject>
                        <subj-group>
                          <subject>Eyelids</subject>
                        </subj-group>
                      </subj-group>
                    </subj-group>
                  </subj-group>
                </subj-group>
                <subj-group subj-group-type="Discipline-v3">
                  <subject>Medicine and Health Sciences</subject>
                  <subj-group>
                    <subject>Infectious Diseases</subject>
                    <subj-group>
                      <subject>Bacterial Diseases</subject>
                      <subj-group>
                        <subject>Trachoma</subject>
                      </subj-group>
                    </subj-group>
                  </subj-group>
                </subj-group>
                <subj-group subj-group-type="Discipline-v3">
                  <subject>Medicine and Health Sciences</subject>
                  <subj-group>
                    <subject>Ophthalmology</subject>
                    <subj-group>
                      <subject>Eye Diseases</subject>
                      <subj-group>
                        <subject>Trachoma</subject>
                      </subj-group>
                    </subj-group>
                  </subj-group>
                </subj-group>
                <subj-group subj-group-type="Discipline-v3">
                  <subject>Medicine and Health Sciences</subject>
                  <subj-group>
                    <subject>Tropical Diseases</subject>
                    <subj-group>
                      <subject>Neglected Tropical Diseases</subject>
                      <subj-group>
                        <subject>Trachoma</subject>
                      </subj-group>
                    </subj-group>
                  </subj-group>
                </subj-group>
                <subj-group subj-group-type="Discipline-v3">
                  <subject>Biology and Life Sciences</subject>
                  <subj-group>
                    <subject>Agriculture</subject>
                    <subj-group>
                      <subject>Crop Science</subject>
                      <subj-group>
                        <subject>Crops</subject>
                      </subj-group>
                    </subj-group>
                  </subj-group>
                </subj-group>
                <subj-group subj-group-type="Discipline-v3">
                  <subject>Computer and Information Sciences</subject>
                  <subj-group>
                    <subject>Neural Networks</subject>
                  </subj-group>
                </subj-group>
                <subj-group subj-group-type="Discipline-v3">
                  <subject>Biology and Life Sciences</subject>
                  <subj-group>
                    <subject>Neuroscience</subject>
                    <subj-group>
                      <subject>Neural Networks</subject>
                    </subj-group>
                  </subj-group>
                </subj-group>
                <subj-group subj-group-type="Discipline-v3">
                  <subject>Research and Analysis Methods</subject>
                  <subj-group>
                    <subject>Mathematical and Statistical Techniques</subject>
                    <subj-group>
                      <subject>Mathematical Functions</subject>
                      <subj-group>
                        <subject>Convolution</subject>
                      </subj-group>
                    </subj-group>
                  </subj-group>
                </subj-group>
                <subj-group subj-group-type="Discipline-v3">
                  <subject>Engineering and Technology</subject>
                  <subj-group>
                    <subject>Signal Processing</subject>
                    <subj-group>
                      <subject>Image Processing</subject>
                    </subj-group>
                  </subj-group>
                </subj-group>
                <subj-group subj-group-type="Discipline-v3">
                  <subject>Research and Analysis Methods</subject>
                  <subj-group>
                    <subject>Imaging Techniques</subject>
                  </subj-group>
                </subj-group>
                <subj-group subj-group-type="Discipline-v3">
                  <subject>Computer and Information Sciences</subject>
                  <subj-group>
                    <subject>Computer Vision</subject>
                  </subj-group>
                </subj-group>
              </article-categories>
              <title-group>
                <article-title>Sensitivity and specificity of computer vision classification of eyelid photographs for programmatic trachoma assessment</article-title>
                <alt-title alt-title-type="running-head">Computer vision classification for trachoma</alt-title>
              </title-group>
              <contrib-group>
                <contrib contrib-type="author">
                  <name>
                    <surname>Kim</surname>
                    <given-names>Matthew C.</given-names>
                  </name>
                  <role content-type="http://credit.casrai.org/">Conceptualization</role>
                  <role content-type="http://credit.casrai.org/">Formal analysis</role>
                  <role content-type="http://credit.casrai.org/">Methodology</role>
                  <role content-type="http://credit.casrai.org/">Software</role>
                  <role content-type="http://credit.casrai.org/">Writing – original draft</role>
                  <xref ref-type="aff" rid="aff001">
                    <sup>1</sup>
                  </xref>
                  <xref ref-type="aff" rid="aff003">
                    <sup>3</sup>
                  </xref>
                </contrib>
                <contrib contrib-type="author">
                  <name>
                    <surname>Okada</surname>
                    <given-names>Kazunori</given-names>
                  </name>
                  <role content-type="http://credit.casrai.org/">Conceptualization</role>
                  <role content-type="http://credit.casrai.org/">Methodology</role>
                  <role content-type="http://credit.casrai.org/">Supervision</role>
                  <role content-type="http://credit.casrai.org/">Writing – review &amp; editing</role>
                  <xref ref-type="aff" rid="aff002">
                    <sup>2</sup>
                  </xref>
                </contrib>
                <contrib contrib-type="author">
                  <name>
                    <surname>Ryner</surname>
                    <given-names>Alexander M.</given-names>
                  </name>
                  <role content-type="http://credit.casrai.org/">Software</role>
                  <role content-type="http://credit.casrai.org/">Visualization</role>
                  <xref ref-type="aff" rid="aff001">
                    <sup>1</sup>
                  </xref>
                </contrib>
                <contrib contrib-type="author">
                  <name>
                    <surname>Amza</surname>
                    <given-names>Abdou</given-names>
                  </name>
                  <role content-type="http://credit.casrai.org/">Data curation</role>
                  <role content-type="http://credit.casrai.org/">Investigation</role>
                  <role content-type="http://credit.casrai.org/">Project administration</role>
                  <role content-type="http://credit.casrai.org/">Writing – review &amp; editing</role>
                  <xref ref-type="aff" rid="aff004">
                    <sup>4</sup>
                  </xref>
                </contrib>
                <contrib contrib-type="author">
                  <name>
                    <surname>Tadesse</surname>
                    <given-names>Zerihun</given-names>
                  </name>
                  <role content-type="http://credit.casrai.org/">Data curation</role>
                  <role content-type="http://credit.casrai.org/">Investigation</role>
                  <role content-type="http://credit.casrai.org/">Project administration</role>
                  <role content-type="http://credit.casrai.org/">Writing – review &amp; editing</role>
                  <xref ref-type="aff" rid="aff005">
                    <sup>5</sup>
                  </xref>
                </contrib>
                <contrib contrib-type="author">
                  <name>
                    <surname>Cotter</surname>
                    <given-names>Sun Y.</given-names>
                  </name>
                  <role content-type="http://credit.casrai.org/">Data curation</role>
                  <role content-type="http://credit.casrai.org/">Project administration</role>
                  <role content-type="http://credit.casrai.org/">Writing – review &amp; editing</role>
                  <xref ref-type="aff" rid="aff001">
                    <sup>1</sup>
                  </xref>
                </contrib>
                <contrib contrib-type="author">
                  <name>
                    <surname>Gaynor</surname>
                    <given-names>Bruce D.</given-names>
                  </name>
                  <role content-type="http://credit.casrai.org/">Data curation</role>
                  <role content-type="http://credit.casrai.org/">Validation</role>
                  <role content-type="http://credit.casrai.org/">Writing – review &amp; editing</role>
                  <xref ref-type="aff" rid="aff001">
                    <sup>1</sup>
                  </xref>
                </contrib>
                <contrib contrib-type="author">
                  <name>
                    <surname>Keenan</surname>
                    <given-names>Jeremy D.</given-names>
                  </name>
                  <role content-type="http://credit.casrai.org/">Data curation</role>
                  <role content-type="http://credit.casrai.org/">Validation</role>
                  <role content-type="http://credit.casrai.org/">Writing – review &amp; editing</role>
                  <xref ref-type="aff" rid="aff001">
                    <sup>1</sup>
                  </xref>
                  <xref ref-type="aff" rid="aff006">
                    <sup>6</sup>
                  </xref>
                </contrib>
                <contrib contrib-type="author">
                  <name>
                    <surname>Lietman</surname>
                    <given-names>Thomas M.</given-names>
                  </name>
                  <role content-type="http://credit.casrai.org/">Data curation</role>
                  <role content-type="http://credit.casrai.org/">Validation</role>
                  <role content-type="http://credit.casrai.org/">Writing – review &amp; editing</role>
                  <xref ref-type="aff" rid="aff001">
                    <sup>1</sup>
                  </xref>
                  <xref ref-type="aff" rid="aff006">
                    <sup>6</sup>
                  </xref>
                  <xref ref-type="aff" rid="aff007">
                    <sup>7</sup>
                  </xref>
                </contrib>
                <contrib contrib-type="author">
                  <contrib-id authenticated="true" contrib-id-type="orcid">http://orcid.org/0000-0001-8813-3171</contrib-id>
                  <name>
                    <surname>Porco</surname>
                    <given-names>Travis C.</given-names>
                  </name>
                  <role content-type="http://credit.casrai.org/">Conceptualization</role>
                  <role content-type="http://credit.casrai.org/">Data curation</role>
                  <role content-type="http://credit.casrai.org/">Formal analysis</role>
                  <role content-type="http://credit.casrai.org/">Funding acquisition</role>
                  <role content-type="http://credit.casrai.org/">Methodology</role>
                  <role content-type="http://credit.casrai.org/">Project administration</role>
                  <role content-type="http://credit.casrai.org/">Supervision</role>
                  <role content-type="http://credit.casrai.org/">Writing – original draft</role>
                  <role content-type="http://credit.casrai.org/">Writing – review &amp; editing</role>
                  <xref ref-type="aff" rid="aff001">
                    <sup>1</sup>
                  </xref>
                  <xref ref-type="aff" rid="aff006">
                    <sup>6</sup>
                  </xref>
                  <xref ref-type="aff" rid="aff007">
                    <sup>7</sup>
                  </xref>
                  <xref ref-type="corresp" rid="cor001">*</xref>
                </contrib>
              </contrib-group>
              <aff id="aff001">
                <label>1</label>
                <addr-line>Francis I. Proctor Foundation, University of California San Francisco, San Francisco, CA, United States of America</addr-line>
              </aff>
              <aff id="aff002">
                <label>2</label>
                <addr-line>Department of Computer Science, San Francisco State University, San Francisco, CA, United States of America</addr-line>
              </aff>
              <aff id="aff003">
                <label>3</label>
                <addr-line>Department of Mathematics, San Francisco State University, San Francisco, CA, United States of America</addr-line>
              </aff>
              <aff id="aff004">
                <label>4</label>
                <addr-line>Programme FSS/Université Abdou Moumouni de Niamey, Programme National de Santé Oculaire, Niamey, Niger</addr-line>
              </aff>
              <aff id="aff005">
                <label>5</label>
                <addr-line>Carter Center, Addis Ababa, Ethiopia</addr-line>
              </aff>
              <aff id="aff006">
                <label>6</label>
                <addr-line>Department of Ophthalmology, University of California San Francisco, San Francisco, CA, United States of America</addr-line>
              </aff>
              <aff id="aff007">
                <label>7</label>
                <addr-line>Department of Epidemiology and Biostatistics, University of California San Francisco, San Francisco, CA, United States of America</addr-line>
              </aff>
              <contrib-group>
                <contrib contrib-type="editor">
                  <name>
                    <surname>Wiegand</surname>
                    <given-names>Ryan E.</given-names>
                  </name>
                  <role>Editor</role>
                  <xref ref-type="aff" rid="edit1"/>
                </contrib>
              </contrib-group>
              <aff id="edit1">
                <addr-line>Centers for Disease Control and Prevention, UNITED STATES</addr-line>
              </aff>
              <author-notes>
                <fn fn-type="COI-statement" id="coi001">
                  <p><bold>Competing Interests: </bold>The authors have declared that no competing interests exist.</p>
                </fn>
                <corresp id="cor001">* E-mail: <email>travis.porco@ucsf.edu</email></corresp>
              </author-notes>
              <pub-date pub-type="collection">
                <year>2019</year>
              </pub-date>
              <pub-date pub-type="epub">
                <day>11</day>
                <month>2</month>
                <year>2019</year>
              </pub-date>
              <volume>14</volume>
              <issue>2</issue>
              <elocation-id>e0210463</elocation-id>
              <history>
                <date date-type="received">
                  <day>28</day>
                  <month>2</month>
                  <year>2018</year>
                </date>
                <date date-type="accepted">
                  <day>24</day>
                  <month>12</month>
                  <year>2018</year>
                </date>
              </history>
              <permissions>
                <copyright-statement>© 2019 Kim et al</copyright-statement>
                <copyright-year>2019</copyright-year>
                <copyright-holder>Kim et al</copyright-holder>
                <license xlink:href="http://creativecommons.org/licenses/by/4.0/">
                  <license-p>This is an open access article distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution License</ext-link>, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited.</license-p>
                </license>
              </permissions>
              <self-uri content-type="pdf" xlink:href="pone.0210463.pdf"/>
              <abstract>
                <sec id="sec001">
                  <title>Background/aims</title>
                  <p>Trachoma programs base treatment decisions on the community prevalence of the clinical signs of trachoma, assessed by direct examination of the conjunctiva. Automated assessment could be more standardized and more cost-effective. We tested the hypothesis that an automated algorithm could classify eyelid photographs better than chance.</p>
                </sec>
                <sec id="sec002">
                  <title>Methods</title>
                  <p>A total of 1,656 field-collected conjunctival images were obtained from clinical trial participants in Niger and Ethiopia. Images were scored for trachomatous inflammation—follicular (TF) and trachomatous inflammation—intense (TI) according to the simplified World Health Organization grading system by expert raters. We developed an automated procedure for image enhancement followed by application of a convolutional neural net classifier for TF and separately for TI. One hundred images were selected for testing TF and TI, and these images were not used for training.</p>
                </sec>
                <sec id="sec003">
                  <title>Results</title>
                  <p>The agreement score for TF and TI tasks for the automated algorithm relative to expert graders was <italic>κ</italic> = 0.44 (95% CI: 0.26 to 0.62, <italic>P</italic> &lt; 0.001) and <italic>κ</italic> = 0.69 (95% CI: 0.55 to 0.84, <italic>P</italic> &lt; 0.001), respectively.</p>
                </sec>
                <sec id="sec004">
                  <title>Discussion</title>
                  <p>For assessing the clinical signs of trachoma, a convolutional neural net performed well above chance when tested against expert consensus. Further improvements in specificity may render this method suitable for field use.</p>
                </sec>
              </abstract>
              <funding-group>
                <award-group id="award001">
                  <funding-source>
                    <institution-wrap>
                      <institution-id institution-id-type="funder-id">http://dx.doi.org/10.13039/100000053</institution-id>
                      <institution>National Eye Institute</institution>
                    </institution-wrap>
                  </funding-source>
                  <award-id>EY016214</award-id>
                  <principal-award-recipient>
                    <name>
                      <surname>Lietman</surname>
                      <given-names>Thomas M.</given-names>
                    </name>
                  </principal-award-recipient>
                </award-group>
                <award-group id="award002">
                  <funding-source>
                    <institution-wrap>
                      <institution-id institution-id-type="funder-id">http://dx.doi.org/10.13039/100000865</institution-id>
                      <institution>Bill and Melinda Gates Foundation</institution>
                    </institution-wrap>
                  </funding-source>
                  <award-id>48027</award-id>
                  <principal-award-recipient>
                    <name>
                      <surname>Lietman</surname>
                      <given-names>Thomas M.</given-names>
                    </name>
                  </principal-award-recipient>
                </award-group>
                <award-group id="award003">
                  <funding-source>
                    <institution-wrap>
                      <institution-id institution-id-type="funder-id">http://dx.doi.org/10.13039/100010946</institution-id>
                      <institution>School of Medicine, University of California, San Francisco</institution>
                    </institution-wrap>
                  </funding-source>
                  <award-id>RAP</award-id>
                  <principal-award-recipient>
                    <contrib-id authenticated="true" contrib-id-type="orcid">http://orcid.org/0000-0001-8813-3171</contrib-id>
                    <name>
                      <surname>Porco</surname>
                      <given-names>Travis C.</given-names>
                    </name>
                  </principal-award-recipient>
                </award-group>
                <funding-statement>The authors gratefully acknowledge funding from the US National Eye Institute (NEI U10 EY016214, TANA and TIRET studies) to TML, from the Bill and Melinda Gates Foundation (grant number 48027, PRET Study) to TML, and from the UCSF Academic Senate RAP Program (“Computer vision assessment of trachoma photos”) to TCP. The UCSF Department of Ophthalmology acknowledges support from an unrestricted grant from Research to Prevent Blindness. The funders had no role in study design, data collection and analysis, decision to publish, or preparation of the manuscript.</funding-statement>
              </funding-group>
              <counts>
                <fig-count count="6"/>
                <table-count count="3"/>
                <page-count count="12"/>
              </counts>
              <custom-meta-group>
                <custom-meta id="data-availability">
                  <meta-name>Data Availability</meta-name>
                  <meta-value>Original images and underlying data for this study are available on figshare: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.6084/m9.figshare.7551053.v1">https://doi.org/10.6084/m9.figshare.7551053.v1</ext-link>.</meta-value>
                </custom-meta>
              </custom-meta-group>
            </article-meta>
            <notes>
              <title>Data Availability</title>
              <p>Original images and underlying data for this study are available on figshare: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.6084/m9.figshare.7551053.v1">https://doi.org/10.6084/m9.figshare.7551053.v1</ext-link>.</p>
            </notes>
          </front>
          <body>
            <sec sec-type="intro" id="sec005">
              <title>Introduction</title>
              <p>Millions of people are currently blind because of trachoma worldwide, a result of infection by ocular strains of <italic>Chlamydia trachomatis</italic>. [<xref rid="pone.0210463.ref001" ref-type="bibr">1</xref>, <xref rid="pone.0210463.ref002" ref-type="bibr">2</xref>] This infection is treatable using single-dose azithromycin, and mass administration of azithromycin forms the basis of the World Health Organization’s strategy for trachoma control. [<xref rid="pone.0210463.ref003" ref-type="bibr">3</xref>] Stakeholders base decisions on starting programs, stopping mass treatment, and declaring control on the clinical signs of trachoma. Yet studies show a great deal of variance between graders, or even the same grader over time. If any concerns later arise, field results are not auditable. Photographic grading appears to be as accurate as clinical grading, and could overcome other limitations. [<xref rid="pone.0210463.ref004" ref-type="bibr">4</xref>]</p>
              <p>Is it possible for an automated algorithm to clinically grade active trachoma from photographs collected in the field? We note that automated image processing is becoming useful in many medical imaging applications. [<xref rid="pone.0210463.ref005" ref-type="bibr">5</xref>–<xref rid="pone.0210463.ref007" ref-type="bibr">7</xref>] Our application differs from most in that we use images collected under field conditions (under differing lighting conditions and camera angles and distances), and in that we are conducting classifications of a subclinical condition with an ultimate goal of guiding, not individual treatment, but community-wide mass administration of azithromycin for a public health control campaign. Automated scoring would avoid human grader drift over time. [<xref rid="pone.0210463.ref008" ref-type="bibr">8</xref>, <xref rid="pone.0210463.ref009" ref-type="bibr">9</xref>] It would also permit standardization of methods between regions and countries, and could allow a higher volume of images to be scored at lower cost. Neural networks have long been useful for diagnostic tests in medicine, and in ophthalmological applications in particular. [<xref rid="pone.0210463.ref010" ref-type="bibr">10</xref>–<xref rid="pone.0210463.ref012" ref-type="bibr">12</xref>] Here, we test the hypothesis that a convolutional neural network [<xref rid="pone.0210463.ref013" ref-type="bibr">13</xref>] can classify trachoma photographs substantially better than chance.</p>
            </sec>
            <sec sec-type="materials|methods" id="sec006">
              <title>Materials and methods</title>
              <sec id="sec007">
                <title>Data</title>
                <p>Images used in this prospective study were obtained from two clinical trials: the Niger arm of the Partnership for the Rapid Elimination of Trachoma trial (PRET, <monospace>clinicaltrials.gov</monospace> accession number NCT00792922), and the Trachoma Amelioration in Northern Amhara trial (TANA, <monospace>clinicaltrials.gov</monospace> accession number NCT01202331). These trials included a total of 85550 participants, with details published elsewhere. [<xref rid="pone.0210463.ref014" ref-type="bibr">14</xref>, <xref rid="pone.0210463.ref015" ref-type="bibr">15</xref>] Verbal consent was obtained for study subjects, and ethical approval was obtained from the University of California, San Francisco, the Niger Ministry of Health, and the Ethiopian Ministry of Science and Technology.</p>
                <p>Images were taken by community health workers who were trained in field trachoma evaluation, and who were implementing the specimen collection for each trial. For each study participant, the right upper eyelid was everted and the underlying tarsal conjunctiva photographed with a single-lens reflex (SLR) camera equipped with a 105/2.8<italic>f</italic> macro lens using a standardized protocol (aperture priority, aperture <italic>f</italic>/40, ISO 400, native flash engaged, automatic white balance, at least 2 high-quality photographs taken). Images were saved in JPG format. A panel of three experts applied the WHO simplified system [<xref rid="pone.0210463.ref016" ref-type="bibr">16</xref>] to randomly selected images. The graders classified each image for the presence or absence of TF and the presence or absence of TI, independently. No qualitative evaluation of TF or TI intensity was conducted (see <xref ref-type="fig" rid="pone.0210463.g001">Fig 1</xref>). The three human experts graded images independently, each masked to the grades of the other two. A labeled data set of 1,656 digital images was obtained, considering the human consensus as the gold standard (<xref rid="pone.0210463.t001" ref-type="table">Table 1</xref>). No missing or indeterminate grades were allowed. These images were used in assessment of field grading for the clinical trials, and were the total set of available images. Each image in our dataset exhibits an everted eyelid that is approximately centered and parallel to the edge of the photograph.</p>
                <fig id="pone.0210463.g001" orientation="portrait" position="float">
                  <object-id pub-id-type="doi">10.1371/journal.pone.0210463.g001</object-id>
                  <label>Fig 1</label>
                  <caption>
                    <title>Trachoma classification of selected field collected images, according to the WHO simplified system.</title>
                    <p>TF: trachomatous inflammation—follicular; TI: trachomatous inflammation—intense [<xref rid="pone.0210463.ref016" ref-type="bibr">16</xref>].</p>
                  </caption>
                  <graphic xlink:href="pone.0210463.g001"/>
                </fig>
                <table-wrap id="pone.0210463.t001" orientation="portrait" position="float">
                  <object-id pub-id-type="doi">10.1371/journal.pone.0210463.t001</object-id>
                  <label>Table 1</label>
                  <caption>
                    <title>Distribution of clinical categories in our dataset.</title>
                  </caption>
                  <alternatives>
                    <graphic id="pone.0210463.t001g" xlink:href="pone.0210463.t001"/>
                    <table frame="box" rules="all" border="0">
                      <colgroup span="1">
                        <col align="left" valign="middle" span="1"/>
                        <col align="left" valign="middle" span="1"/>
                        <col align="left" valign="middle" span="1"/>
                        <col align="left" valign="middle" span="1"/>
                        <col align="left" valign="middle" span="1"/>
                        <col align="left" valign="middle" span="1"/>
                      </colgroup>
                      <thead>
                        <tr>
                          <th align="left" style="border-right:thick" rowspan="1" colspan="1">Label</th>
                          <th align="center" rowspan="1" colspan="1">Number</th>
                          <th align="center" rowspan="1" colspan="1">% of total</th>
                          <th align="center" colspan="3" rowspan="1">Sub categories</th>
                        </tr>
                      </thead>
                      <tbody>
                        <tr>
                          <td align="left" style="border-right:thick" rowspan="1" colspan="1">Neither TF nor TI</td>
                          <td align="center" rowspan="1" colspan="1">843</td>
                          <td align="char" char="." rowspan="1" colspan="1">51.0%</td>
                          <td align="center" colspan="3" rowspan="1"/>
                        </tr>
                        <tr>
                          <td align="left" rowspan="4" style="border-right:thick" colspan="1">Infected</td>
                          <td align="center" rowspan="4" colspan="1">813</td>
                          <td align="char" char="." rowspan="4" colspan="1">49.0%</td>
                          <td align="center" rowspan="1" colspan="1">TF</td>
                          <td align="center" rowspan="1" colspan="1">527</td>
                          <td align="char" char="." rowspan="1" colspan="1">32.0%</td>
                        </tr>
                        <tr>
                          <td align="center" rowspan="1" colspan="1">TI</td>
                          <td align="center" rowspan="1" colspan="1">272</td>
                          <td align="char" char="." rowspan="1" colspan="1">16.3%</td>
                        </tr>
                        <tr>
                          <td align="center" rowspan="1" colspan="1">TI and TF</td>
                          <td align="center" rowspan="1" colspan="1">162</td>
                          <td align="char" char="." rowspan="1" colspan="1">9.7%</td>
                        </tr>
                        <tr>
                          <td align="center" rowspan="1" colspan="1">Scarring</td>
                          <td align="center" rowspan="1" colspan="1">176</td>
                          <td align="char" char="." rowspan="1" colspan="1">10.6%</td>
                        </tr>
                      </tbody>
                    </table>
                  </alternatives>
                </table-wrap>
                <p>For both TF and TI cases, we randomly sampled 50 images from the TF or TI labeled set and another 50 images from the normal set to obtain a hold-out validation set. These images were not used to train classifiers, but were used only to produce the final performance scores.</p>
                <p>For TF classification, we utilized 477 images scored as TF and 793 normal images; for TI classification, we utilized 222 images scored as TI alone and the same 793 normal images. These constituted a random sample of images that had been prepared for trial evaluation. We estimated that inclusion of 230 images would achieve an estimated standard error of 0.05 in Cohen’s kappa, assuming <italic>κ</italic> = 0.8 and that 20% of the images were classified TF.</p>
              </sec>
              <sec id="sec008">
                <title>Machine classification</title>
                <sec id="sec009">
                  <title>Image preprocessing</title>
                  <p>Automated preprocessing was necessary, since the eyelid may have been off center or misaligned with the edge (<xref ref-type="fig" rid="pone.0210463.g002">Fig 2</xref>). The eyelid in each image was approximately centered and parallel to the main axis of the photograph as shown in <xref ref-type="fig" rid="pone.0210463.g003">Fig 3(a)</xref>. The region of interest was automatically extracted without these assumptions using a four step procedure consisting of image resizing, application of a pixel-level classifier, a corrective rotation step, and finally a crop to yield a standard size region of interest. Note that pixel classifiers have proven useful in other applications [<xref rid="pone.0210463.ref017" ref-type="bibr">17</xref>].</p>
                  <fig id="pone.0210463.g002" orientation="portrait" position="float">
                    <object-id pub-id-type="doi">10.1371/journal.pone.0210463.g002</object-id>
                    <label>Fig 2</label>
                    <caption>
                      <title>Sample image where eyelid is neither centered nor horizontally aligned.</title>
                    </caption>
                    <graphic xlink:href="pone.0210463.g002"/>
                  </fig>
                  <fig id="pone.0210463.g003" orientation="portrait" position="float">
                    <object-id pub-id-type="doi">10.1371/journal.pone.0210463.g003</object-id>
                    <label>Fig 3</label>
                    <caption>
                      <title>An illustrative example for various eyelid images in our procedural pipeline.</title>
                    </caption>
                    <graphic xlink:href="pone.0210463.g003"/>
                  </fig>
                  <p><underline>Resizing</underline>. The original raw images are in color JPEG format, which vary in dimension from 4288 × 2848 to 3008 × 2000 pixels. The first step in preprocessing was resize the images to 1024 × 680 preserving the 3:2 ratio of the digital cameras using the image resizing function in the OpenCV package [<xref rid="pone.0210463.ref018" ref-type="bibr">18</xref>] with linear interpolation. Our eyelid rectification procedure then transforms these preprocessed color images into cropped grayscale images of size 128 × 128 containing an eyelid in a standard orientation and location. This procedure consists of the three successive steps: 1) a pixel-level transformation, 2) a corrective rotation, and 3) a ROI crop selection. We explain these steps in some detail below.</p>
                  <p><underline>Classification of pixels</underline>. We used a pixel-level classifier as part of the image preprocessing; a different classifier is used for classifying the entire image into trachoma-related categories. For the first pixel-level transformation step, we build a binary classifier that maps a pixel color in RBG values into the probability of the pixel being on an eyelid or not. This classifier is then successively applied to each pixel of the 1024 × 680 preprocessed image, yielding a probabilistic image of the same size, whose pixel value represents the estimated probability that certain pixel belongs to an eyelid (See <xref ref-type="fig" rid="pone.0210463.g003">Fig 3(b)</xref> for an example).</p>
                  <p>We design this classifier with a multilayer perceptron [<xref rid="pone.0210463.ref019" ref-type="bibr">19</xref>] with two fully-connected hidden layers. Architectural overview of this network is shown in <xref ref-type="fig" rid="pone.0210463.g004">Fig 4</xref>. The input layer to the multilayer perceptron consists of three neurons (<italic>x</italic><sub>1</sub>, <italic>x</italic><sub>2</sub>, <italic>x</italic><sub>3</sub>) corresponding to a pixel’s RGB values between 0 and 255. The first and the second hidden layer includes 8 neurons <inline-formula id="pone.0210463.e001"><alternatives><graphic xlink:href="pone.0210463.e001.jpg" id="pone.0210463.e001g" mimetype="image" position="anchor" orientation="portrait"/><mml:math id="M1"><mml:mrow><mml:mo>(</mml:mo><mml:msubsup><mml:mi>a</mml:mi><mml:mrow><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mo>‥</mml:mo><mml:mo>,</mml:mo><mml:mn>8</mml:mn></mml:mrow><mml:mrow><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:msubsup><mml:mo>)</mml:mo></mml:mrow></mml:math></alternatives></inline-formula>. The final output layer consists of two neurons (<italic>y</italic><sub>1</sub>, <italic>y</italic><sub>2</sub>), representing two possible states: whether a pixel is on eyelid or not. For both the input and hidden layers the rectified linear unit (ReLu) defined as <italic>f</italic>(<italic>x</italic>) ≔ max(<italic>x</italic>, 0) is used as its non-linear activation function. The softmax function is applied to the output neurons in order to generate a two dimensional stochastic vector estimating the probability distribution of the pixel belonging to an eyelid.</p>
                  <fig id="pone.0210463.g004" orientation="portrait" position="float">
                    <object-id pub-id-type="doi">10.1371/journal.pone.0210463.g004</object-id>
                    <label>Fig 4</label>
                    <caption>
                      <title>Network architecture of multilayer perceptron-based pixel-level classifier.</title>
                    </caption>
                    <graphic xlink:href="pone.0210463.g004"/>
                  </fig>
                  <p>We train this classifier by backpropagation [<xref rid="pone.0210463.ref020" ref-type="bibr">20</xref>] with the stochastic gradient descent and the categorical cross-entropy as its loss function. [<xref rid="pone.0210463.ref021" ref-type="bibr">21</xref>] The training set, consisting of 32 million positive (i.e., eyelid) pixels and 41 million negative (i.e., non-eyelid) pixels, was prepared by hand-segmenting eyelids in 40 images randomly sampled from our training set for positives and by collecting 28 non-eyelid crops for negatives, including various types of objects such as skin, fingers, eyelashes, and insects. Our trained classifier yielded 96.7% accuracy when tested with our hold-out validation data sets for validation.</p>
                  <p><underline>Corrective rotation</underline>. For the second corrective rotation step, we first estimate an eyelid’s center and its binary shape mask from the result of the first step, then perform discrete Gabor transform on the shape mask in order to estimate the tilt-angle between the major axis of the detected eyelid and the horizontal image axis. The image is then rotated to correct this tilt, resulting in automatic alignment of the eyelid’s orientation.</p>
                  <p>Given the image of estimated probabilities from the first step, we first smooth this probabilistic field by 3 × 3 median filtering. Then we estimate the location of the everted eyelid’s center by computing the centroid of the probabilistic field. We also derive the binary eyelid shape mask (see <xref ref-type="fig" rid="pone.0210463.g003">Fig 3(c)</xref> for an example) by thresholding the probability value <italic>p</italic> at each pixel: eyelid (1) if <italic>p</italic> &gt; <italic>TH</italic> and non-eyelid (0) otherwise. We use TH = 0.6 that was empirically chosen. The resulting binary field is then smoothed by morphological closing. [<xref rid="pone.0210463.ref022" ref-type="bibr">22</xref>] We estimate the tilt-angle of the eyelid’s major axis by 1) convolving the shape mask at the center location with a bank of 18 orientation-selective discrete Gabor filters [<xref rid="pone.0210463.ref022" ref-type="bibr">22</xref>, <xref rid="pone.0210463.ref023" ref-type="bibr">23</xref>] designed in a range between <inline-formula id="pone.0210463.e002"><alternatives><graphic xlink:href="pone.0210463.e002.jpg" id="pone.0210463.e002g" mimetype="image" position="anchor" orientation="portrait"/><mml:math id="M2"><mml:mrow><mml:mo>-</mml:mo><mml:mfrac><mml:mi>π</mml:mi><mml:mn>4</mml:mn></mml:mfrac></mml:mrow></mml:math></alternatives></inline-formula> and <inline-formula id="pone.0210463.e003"><alternatives><graphic xlink:href="pone.0210463.e003.jpg" id="pone.0210463.e003g" mimetype="image" position="anchor" orientation="portrait"/><mml:math id="M3"><mml:mfrac><mml:mi>π</mml:mi><mml:mn>4</mml:mn></mml:mfrac></mml:math></alternatives></inline-formula> with an interval of <inline-formula id="pone.0210463.e004"><alternatives><graphic xlink:href="pone.0210463.e004.jpg" id="pone.0210463.e004g" mimetype="image" position="anchor" orientation="portrait"/><mml:math id="M4"><mml:mfrac><mml:mi>π</mml:mi><mml:mn>36</mml:mn></mml:mfrac></mml:math></alternatives></inline-formula>, and 2) selecting the filter that resulted in the maximum response. The angle associated with the selected maximal filter is used as our tilt-angle estimate. The original image is then rotated by the negative of this angle to align the eyelid horizontally.</p>
                  <p><underline>Crop</underline>. For the final ROI crop selection step, we first extract, from the preprocessed and rotated image, a 256 × 256 crop centered at the eyelid center estimated in the previous step. We did not estimate the eyelid size for each image since our data set came with relatively similar size of eyelids across images. The window size was empirically chosen to encompass the extent of eyelids across images. The crop is then resized to 128 × 128 and converted to gray scale. We then applied contrast limiting adaptive histogram normalization [<xref rid="pone.0210463.ref024" ref-type="bibr">24</xref>] in order to enhance and standardize grayscale contrasts. <xref ref-type="fig" rid="pone.0210463.g005">Fig 5</xref> shows an illustrative example for this final step.</p>
                  <fig id="pone.0210463.g005" orientation="portrait" position="float">
                    <object-id pub-id-type="doi">10.1371/journal.pone.0210463.g005</object-id>
                    <label>Fig 5</label>
                    <caption>
                      <title>ROI crop selection procedure.</title>
                      <p>(a) 256 × 256 crop on the rotated image. Estimated (white) and randomly perturbed eyelid centers (green) are shown. (b) Resulting 128 × 128 grayscale ROI.</p>
                    </caption>
                    <graphic xlink:href="pone.0210463.g005"/>
                  </fig>
                </sec>
                <sec id="sec010">
                  <title>Trachoma Classification</title>
                  <p>This section describes our classification model that takes the 128 × 128 region of interest from the previous eyelid rectification process as an input, and outputs the binary classification of whether an eye depicted in the input image exhibits the signs TF or TI. We designed our model with a convolutional neural network [<xref rid="pone.0210463.ref013" ref-type="bibr">13</xref>]. Our network consists of three stage convolutional layers followed by fully connected layers with two hidden layers. Note that the representational power of a convolutional neural network is not compromised by the use of relatively small (3 × 3) filters, since by stacking several convolutional layers, a much larger effective field is realized. [<xref rid="pone.0210463.ref013" ref-type="bibr">13</xref>] The Keras platform with a Theano backend was used in implementation (<ext-link ext-link-type="uri" xlink:href="https://keras.io/">https://keras.io/</ext-link>, <ext-link ext-link-type="uri" xlink:href="http://deeplearning.net/software/theano">http://deeplearning.net/software/theano</ext-link>).</p>
                  <p><xref rid="pone.0210463.t002" ref-type="table">Table 2</xref> summarizes our convolutional neural network architecture. We use convolutional filters of size 3 × 3. Each stage of the convolutional layers is augmented with a max-pooling layer with 2 × 2 size blocks, halving the size of the input after each stage. The border of input image is zero-padded before convolution of each layer. <xref ref-type="fig" rid="pone.0210463.g006">Fig 6</xref> illustrates computational procedures in the convolutinoal layer for a schematized simple example. Two fully-connected hidden-layers of our network include 512 neurons. The final output layer consists of two positive/negative neurons whose value indicates the probability that the target image manifests TF or TI, respectively. The rectified linear unit (ReLu), defined above, is used as our activation function of each layer. The softmax function was applied to the final output layer to produce a probabilistic classification. Final binary classification is then given by thresholding the probability with <italic>TH</italic> = 0.5.</p>
                  <fig id="pone.0210463.g006" orientation="portrait" position="float">
                    <object-id pub-id-type="doi">10.1371/journal.pone.0210463.g006</object-id>
                    <label>Fig 6</label>
                    <caption>
                      <title>Convolutional layer with zero-padding and a 3 × 3 filter followed by max pooling with a 2 × 2 block.</title>
                    </caption>
                    <graphic xlink:href="pone.0210463.g006"/>
                  </fig>
                  <table-wrap id="pone.0210463.t002" orientation="portrait" position="float">
                    <object-id pub-id-type="doi">10.1371/journal.pone.0210463.t002</object-id>
                    <label>Table 2</label>
                    <caption>
                      <title>Architecture of our convolutional neural network classification model.</title>
                      <p><italic>K</italic> denotes the number of filters in the first stage of the convolutional layers.</p>
                    </caption>
                    <alternatives>
                      <graphic id="pone.0210463.t002g" xlink:href="pone.0210463.t002"/>
                      <table frame="box" rules="all" border="0">
                        <colgroup span="1">
                          <col align="left" valign="middle" span="1"/>
                          <col align="left" valign="middle" span="1"/>
                        </colgroup>
                        <thead>
                          <tr>
                            <th align="left" style="border-right:thick" rowspan="1" colspan="1">Layer</th>
                            <th align="center" rowspan="1" colspan="1">Size</th>
                          </tr>
                        </thead>
                        <tbody>
                          <tr>
                            <td align="left" style="border-right:thick" rowspan="1" colspan="1">Convolution 1.1</td>
                            <td align="center" rowspan="1" colspan="1">3 × 3 × <italic>K</italic></td>
                          </tr>
                          <tr>
                            <td align="left" style="border-right:thick" rowspan="1" colspan="1">Convolution 1.2</td>
                            <td align="center" rowspan="1" colspan="1">3 × 3 × <italic>K</italic></td>
                          </tr>
                          <tr>
                            <td align="left" style="border-right:thick" rowspan="1" colspan="1">Max Pooling 1</td>
                            <td align="center" rowspan="1" colspan="1">2 × 2</td>
                          </tr>
                          <tr>
                            <td align="left" style="border-right:thick" rowspan="1" colspan="1">Convolution 2.1</td>
                            <td align="center" rowspan="1" colspan="1">3 × 3 × 2<italic>K</italic></td>
                          </tr>
                          <tr>
                            <td align="left" style="border-right:thick" rowspan="1" colspan="1">Convolution 2.2</td>
                            <td align="center" rowspan="1" colspan="1">3 × 3 × 2<italic>K</italic></td>
                          </tr>
                          <tr>
                            <td align="left" style="border-right:thick" rowspan="1" colspan="1">Max Pooling 2</td>
                            <td align="center" rowspan="1" colspan="1">2 × 2</td>
                          </tr>
                          <tr>
                            <td align="left" style="border-right:thick" rowspan="1" colspan="1">Convolution 3.1</td>
                            <td align="center" rowspan="1" colspan="1">3 × 3 × 4<italic>K</italic></td>
                          </tr>
                          <tr>
                            <td align="left" style="border-right:thick" rowspan="1" colspan="1">Convolution 3.2</td>
                            <td align="center" rowspan="1" colspan="1">3 × 3 × 4<italic>K</italic></td>
                          </tr>
                          <tr>
                            <td align="left" style="border-right:thick" rowspan="1" colspan="1">Convolution 3.3</td>
                            <td align="center" rowspan="1" colspan="1">3 × 3 × 4<italic>K</italic></td>
                          </tr>
                          <tr>
                            <td align="left" style="border-right:thick" rowspan="1" colspan="1">Max Pooling 3</td>
                            <td align="center" rowspan="1" colspan="1">2 × 2</td>
                          </tr>
                          <tr>
                            <td align="left" style="border-right:thick" rowspan="1" colspan="1">Fully-Connected Hidden 1</td>
                            <td align="center" rowspan="1" colspan="1">512</td>
                          </tr>
                          <tr>
                            <td align="left" style="border-right:thick" rowspan="1" colspan="1">Fully-Connected Hidden 2</td>
                            <td align="center" rowspan="1" colspan="1">512</td>
                          </tr>
                          <tr>
                            <td align="left" style="border-right:thick" rowspan="1" colspan="1">Fully-Connected Output</td>
                            <td align="center" rowspan="1" colspan="1">2</td>
                          </tr>
                        </tbody>
                      </table>
                    </alternatives>
                  </table-wrap>
                  <p><underline>Training</underline>. For training, we used the binary cross-entropy loss function [<xref rid="pone.0210463.ref021" ref-type="bibr">21</xref>] with respect to the gold standard labels made by our expert panels. We then trained our classification model using two learning strategies of 1) a standard stochastic gradient descent algorithm [<xref rid="pone.0210463.ref025" ref-type="bibr">25</xref>] and 2) AdaDelta [<xref rid="pone.0210463.ref026" ref-type="bibr">26</xref>] with an adaptive learning rate. Furthermore, we tested our model with the varying number of convolutional filters <italic>K</italic> set to 8, 16, 32, or 64. The best strategy with the maximum performance was determined by using a second hold-out test set which was prepared by randomly selecting 10% of the training set, described in Data section, to be used for computing performance statistics of a model trained with the remaining 90% in order to minimize over-fitting.</p>
                  <p>To further reduce the possibility of overfitting, we incorporated the following strategies during our model training. We used batch normalization [<xref rid="pone.0210463.ref027" ref-type="bibr">27</xref>] after each set of convolutional layers at training time, which normalizes batches of training images in between layers so that each pixel has a standard normal distribution over all the images in the batch. This prevents variations in the distribution of training data in deeper networks, which can slow training by forcing the later weights to accommodate a larger domain. We also employed random dropout [<xref rid="pone.0210463.ref028" ref-type="bibr">28</xref>] of 25% (i.e. 25% of weights are randomly chosen to be turned off, forcing the remaining weights to generalize faster) after each stage of convolutional layers. Training for the fully-connected layers were also subject to <italic>L</italic>2 regularization with a quadratic complexity penalty. We utilized the same strategies for building models for both TF and TI classification tasks.</p>
                  <p>We used an additional strategy to reduce overfitting. Specifically, we augmented our cropped region of interests as follows (see <xref ref-type="fig" rid="pone.0210463.g005">Fig 5(b)</xref>) As described above, we used an automated procedure which yields a single region of interest crop centered at the estimated eyelid center location. As shown in <xref ref-type="fig" rid="pone.0210463.g005">Fig 5(a)</xref>, we modified this procedure to introduce randomly generated noise to the centroid location of each eyelid. Repeating the procedure with this perturbed eyelid center yields a new cropped region of interest which exhibits a slightly translated view from the original crop. In order to effectively increase the size of the training set, we incorporated this random data perturbation between each successive epoch (i.e., iteration) during our model training, providing, in essence, a virtually unlimited stream of new training images.</p>
                </sec>
              </sec>
            </sec>
            <sec sec-type="results" id="sec011">
              <title>Results</title>
              <p>We trained eight convolutional neural network models by varying the two learning strategies and four <italic>K</italic> values for each of TF and TI classification tasks. Using the hold-out set, we ranked eight models in terms of the kappa statistic for each of the two TF and TI tasks.</p>
              <p>The model that produced the best training-time scores on the TF task was trained using AdaDelta and used <italic>K</italic> = 64 filters in its initial convolutional layer. The best performing model for the TI task was training using stochastic gradient descent and used <italic>K</italic> = 32 filters. We compared this best performing model with an ensemble classifier that averages the output probabilities estimated by the three top models for each task. Finally, for validating the best and the ensemble classifiers for the TF and TI tasks, we used the first hold-out validation set of 100 cases for each task and computed four standard performance statistic scores: sensitivity, specificity, accuracy, and Cohen’s kappa (<italic>κ</italic> [<xref rid="pone.0210463.ref029" ref-type="bibr">29</xref>]). <xref rid="pone.0210463.t003" ref-type="table">Table 3</xref> summarizes the results.</p>
              <table-wrap id="pone.0210463.t003" orientation="portrait" position="float">
                <object-id pub-id-type="doi">10.1371/journal.pone.0210463.t003</object-id>
                <label>Table 3</label>
                <caption>
                  <title>Validation scores on trained convolutional neural network models for TF and TI classification tasks.</title>
                </caption>
                <alternatives>
                  <graphic id="pone.0210463.t003g" xlink:href="pone.0210463.t003"/>
                  <table frame="box" rules="all" border="0">
                    <colgroup span="1">
                      <col align="left" valign="middle" span="1"/>
                      <col align="left" valign="middle" span="1"/>
                      <col align="left" valign="middle" span="1"/>
                      <col align="left" valign="middle" span="1"/>
                    </colgroup>
                    <thead>
                      <tr>
                        <th align="center" style="border-bottom:thick" rowspan="1" colspan="1">Class</th>
                        <th align="center" style="border-bottom:thick" rowspan="1" colspan="1">Measure</th>
                        <th align="center" style="border-bottom:thick" rowspan="1" colspan="1">Best Model</th>
                        <th align="center" style="border-bottom:thick" rowspan="1" colspan="1">Ensemble of Top-3</th>
                      </tr>
                    </thead>
                    <tbody>
                      <tr>
                        <td align="center" rowspan="4" colspan="1">TF</td>
                        <td align="center" rowspan="1" colspan="1">
                          <italic>κ</italic>
                        </td>
                        <td align="char" char="." rowspan="1" colspan="1">0.40</td>
                        <td align="char" char="." rowspan="1" colspan="1">0.44</td>
                      </tr>
                      <tr>
                        <td align="center" rowspan="1" colspan="1">Sensitivity</td>
                        <td align="char" char="." rowspan="1" colspan="1">0.92</td>
                        <td align="char" char="." rowspan="1" colspan="1">0.86</td>
                      </tr>
                      <tr>
                        <td align="center" rowspan="1" colspan="1">Specificity</td>
                        <td align="char" char="." rowspan="1" colspan="1">0.48</td>
                        <td align="char" char="." rowspan="1" colspan="1">0.58</td>
                      </tr>
                      <tr>
                        <td align="center" rowspan="1" colspan="1">Accuracy</td>
                        <td align="char" char="." rowspan="1" colspan="1">0.70</td>
                        <td align="char" char="." rowspan="1" colspan="1">0.72</td>
                      </tr>
                      <tr>
                        <td align="center" rowspan="4" colspan="1">TI</td>
                        <td align="center" rowspan="1" colspan="1">
                          <italic>κ</italic>
                        </td>
                        <td align="char" char="." rowspan="1" colspan="1">0.69</td>
                        <td align="char" char="." rowspan="1" colspan="1">0.69</td>
                      </tr>
                      <tr>
                        <td align="center" rowspan="1" colspan="1">Sensitivity</td>
                        <td align="char" char="." rowspan="1" colspan="1">0.98</td>
                        <td align="char" char="." rowspan="1" colspan="1">0.96</td>
                      </tr>
                      <tr>
                        <td align="center" rowspan="1" colspan="1">Specificity</td>
                        <td align="char" char="." rowspan="1" colspan="1">0.72</td>
                        <td align="char" char="." rowspan="1" colspan="1">0.74</td>
                      </tr>
                      <tr>
                        <td align="center" rowspan="1" colspan="1">Accuracy</td>
                        <td align="char" char="." rowspan="1" colspan="1">0.85</td>
                        <td align="char" char="." rowspan="1" colspan="1">0.85</td>
                      </tr>
                    </tbody>
                  </table>
                </alternatives>
              </table-wrap>
              <p>We observe in the results that the ensemble classifiers yield better performance than the separate classifiers, measured by the kappa, specificity, and accuracy scores for both TF and TI tasks. However the ensemble decreases the sensitivity (e.g., recall) score. The results also indicate that scores for the TI task are higher than those for the TF task for all four measures and for both best and ensemble classifiers. The agreements for TF and TI tasks by our ensemble models were <italic>κ</italic> = 0.44 (95% CI: 0.26 to 0.62, <italic>P</italic> &lt; 0.001) and <italic>κ</italic> = 0.69 (95% CI: 0.55 to 0.84, <italic>P</italic> &lt; 0.001), indicating results far better than chance.</p>
            </sec>
            <sec sec-type="conclusions" id="sec012">
              <title>Discussion</title>
              <p>We found that machine classification of field collected eyelid images can yield automated trachoma classifications with performance far better than expected from chance alone. Confining our attention to studies with digital images, we note that human grading of conjunctival photographs using the same protocol resulted in a Cohen’s kappa of 0.55 in one study, and direct conjunctival examination in the field has shown agreement in the range of 0.57 to 0.73. [<xref rid="pone.0210463.ref004" ref-type="bibr">4</xref>, <xref rid="pone.0210463.ref008" ref-type="bibr">8</xref>]</p>
              <p>Trachoma grades TF and TI are defined by the presence of specific features on eyelids (seen in the field or through photographs), as assessed by human experts. No other gold standard for the clinical grade is available. The agreements for TF and TI tasks by our ensemble models were lower and higher (respectively) than this baseline reported [<xref rid="pone.0210463.ref004" ref-type="bibr">4</xref>]. We note that in the Global Trachoma Mapping Project [<xref rid="pone.0210463.ref030" ref-type="bibr">30</xref>], the protocol required that the agreement for TF between a master grader and candidate grader trainer should be at least 0.8 for certification of the candidate grader.</p>
              <p>Our experimental results also suggest that TI classification may be easier than TF classification. The best kappa was 0.44 and 0.69 for the TF and TI tasks both by the ensemble classifiers, respectively. The network trained for the TI task outperformed that for the TF tasks for all scores. Although not currently used programmatically, the TI classification appears to be much more specific than TF, and may be more correlated with actual chlamydial infection. [<xref rid="pone.0210463.ref009" ref-type="bibr">9</xref>] Overall, these reasonably high validation scores are promising toward further improving the proposed methodologies to our goal of deploying such automated grading software for the actual field studies.</p>
              <p>We note certain limitations. We did not explore various representation of color information beyond the RGB space, nor classification models other than the chosen multilayer perceptron for the eyelid detection. We have no information on generalizability beyond the two countries examined, to archival images, or to images collected with smartphones. Our algorithm was designed for assessment of images as part of a trachoma control campaign, not for individual-level assessment. Such a classifier could enable the assessment of community and district level TF prevalence, as needed to guide intervention efforts during the WHO trachoma elimination campaign. [<xref rid="pone.0210463.ref003" ref-type="bibr">3</xref>] Thus, we have not trained the system to evaluate other features, since active trachoma is usually a subclinical condition which poses no immediate threat to vision. The proposed method, in principle, could be extended to more detailed trachoma classifications. [<xref rid="pone.0210463.ref031" ref-type="bibr">31</xref>, <xref rid="pone.0210463.ref032" ref-type="bibr">32</xref>]</p>
            </sec>
            <sec sec-type="conclusions" id="sec013">
              <title>Conclusion</title>
              <p>Although grading of field trachoma images can be challenging due to less standardization of lighting and distance than in other computer vision exercises, we showed that computer vision methods are capable of classifying field collected trachoma images better than chance. Use of newer deep learning algorithms, together with larger corpuses of labeled trachoma images which are becoming available, is expected to yield substantial improvements in specificity of classification. This may thus permit computer vision techniques to now play a practical role in preserving human vision in some of the world’s poorest communities.</p>
            </sec>
          </body>
          <back>
            <ack>
              <p>The authors gratefully acknowledge funding from the US National Eye Institute (NEI U10 EY016214, TANA and TIRET studies), from the Bill and Melinda Gates Foundation (grant number 48027, PRET Study), and from the UCSF Academic Senate RAP Program (“Computer vision assessment of trachoma photos”).</p>
            </ack>
            <ref-list>
              <title>References</title>
              <ref id="pone.0210463.ref001">
                <label>1</label>
                <mixed-citation publication-type="journal"><name><surname>Burton</surname><given-names>MJ</given-names></name>, <name><surname>Mabey</surname><given-names>DCW</given-names></name>. <article-title>The global burden of trachoma: a review</article-title>. <source>PLoS Negl Trop Dis</source>. <year>2009</year>;<volume>3</volume>(<issue>10</issue>):<fpage>e460</fpage><pub-id pub-id-type="doi">10.1371/journal.pntd.0000460</pub-id><?supplied-pmid 19859534?><pub-id pub-id-type="pmid">19859534</pub-id></mixed-citation>
              </ref>
              <ref id="pone.0210463.ref002">
                <label>2</label>
                <mixed-citation publication-type="journal"><name><surname>Ramadhani</surname><given-names>AM</given-names></name>, <name><surname>Derrick</surname><given-names>T</given-names></name>, <name><surname>Holland</surname><given-names>MJ</given-names></name>, <name><surname>Burton</surname><given-names>MJ</given-names></name>. <article-title>Blinding Trachoma: Systematic Review of Rates and Risk Factors for Progressive Disease</article-title>. <source>PLoS Negl Trop Dis</source>. <year>2016</year>;<volume>10</volume>(<issue>8</issue>):<fpage>e0004859</fpage><pub-id pub-id-type="doi">10.1371/journal.pntd.0004859</pub-id><?supplied-pmid 27483002?><pub-id pub-id-type="pmid">27483002</pub-id></mixed-citation>
              </ref>
              <ref id="pone.0210463.ref003">
                <label>3</label>
                <mixed-citation publication-type="other">World Health Organization. Trachoma control. A guide for programme managers. Geneva; 2006. Available from: <ext-link ext-link-type="uri" xlink:href="http://apps.who.int/iris/bitstream/10665/43405/1/9241546905_eng.pdf">http://apps.who.int/iris/bitstream/10665/43405/1/9241546905_eng.pdf</ext-link>.</mixed-citation>
              </ref>
              <ref id="pone.0210463.ref004">
                <label>4</label>
                <mixed-citation publication-type="journal"><name><surname>Gebresillasie</surname><given-names>S</given-names></name>, <name><surname>Tadesse</surname><given-names>Z</given-names></name>, <name><surname>Shiferaw</surname><given-names>A</given-names></name>, <name><surname>Yu</surname><given-names>SN</given-names></name>, <name><surname>Stoller</surname><given-names>NE</given-names></name>, <name><surname>Zhou</surname><given-names>Z</given-names></name>, <etal>et al</etal><article-title>Inter-Rater agreement between trachoma graders: comparison of grades given in field conditions versus grades from photographic review</article-title>. <source>Ophthalmic Epidemiol</source>. <year>2015</year>;<volume>22</volume>(<issue>3</issue>):<fpage>162</fpage>–<lpage>169</lpage>. <pub-id pub-id-type="doi">10.3109/09286586.2015.1035792</pub-id>
<?supplied-pmid 26158573?><pub-id pub-id-type="pmid">26158573</pub-id></mixed-citation>
              </ref>
              <ref id="pone.0210463.ref005">
                <label>5</label>
                <mixed-citation publication-type="other">Hann CE, Chase JG, Revie JA, Hewett D, Shaw GM. Diabetic Retinopathy Screening Using Computer Vision. In: 7th IFAC Symposium on Modelling and Control in Biomedical Systems. vol. 42; 2009. p. 298–303. Available from: <ext-link ext-link-type="uri" xlink:href="http://www.sciencedirect.com/science/article/pii/S1474667015378903">http://www.sciencedirect.com/science/article/pii/S1474667015378903</ext-link>.</mixed-citation>
              </ref>
              <ref id="pone.0210463.ref006">
                <label>6</label>
                <mixed-citation publication-type="journal"><name><surname>Sopharak</surname><given-names>A</given-names></name>, <name><surname>Uyyanonvara</surname><given-names>B</given-names></name>, <name><surname>Barman</surname><given-names>S</given-names></name>, <name><surname>Williamson</surname><given-names>TH</given-names></name>. <article-title>Automatic detection of diabetic retinopathy exudates from non-dilated retinal images using mathematical morphology methods</article-title>. <source>Comput Med Imaging Graph</source>. <year>2008</year>;<volume>32</volume>:<fpage>720</fpage>–<lpage>727</lpage>. <pub-id pub-id-type="doi">10.1016/j.compmedimag.2008.08.009</pub-id>
<?supplied-pmid 18930631?><pub-id pub-id-type="pmid">18930631</pub-id></mixed-citation>
              </ref>
              <ref id="pone.0210463.ref007">
                <label>7</label>
                <mixed-citation publication-type="journal"><name><surname>Anthimopoulos</surname><given-names>M</given-names></name>, <name><surname>Christodoulidis</surname><given-names>S</given-names></name>, <name><surname>Ebner</surname><given-names>L</given-names></name>, <name><surname>Christe</surname><given-names>A</given-names></name>, <name><surname>Mougiakakou</surname><given-names>S</given-names></name>. <article-title>Lung Pattern Classification for Interstitial Lung Diseases Using a Deep Convolutional Neural Network</article-title>. <source>IEEE Trans Med Imaging</source>. <year>2016</year>;<volume>35</volume>(<issue>5</issue>):<fpage>1207</fpage>–<lpage>1216</lpage>. <pub-id pub-id-type="doi">10.1109/TMI.2016.2535865</pub-id>
<?supplied-pmid 26955021?><pub-id pub-id-type="pmid">26955021</pub-id></mixed-citation>
              </ref>
              <ref id="pone.0210463.ref008">
                <label>8</label>
                <mixed-citation publication-type="journal"><name><surname>Miller</surname><given-names>K</given-names></name>, <name><surname>Schmidt</surname><given-names>G</given-names></name>, <name><surname>Melese</surname><given-names>M</given-names></name>, <name><surname>Alemayehu</surname><given-names>W</given-names></name>, <name><surname>Yi</surname><given-names>E</given-names></name>, <name><surname>Cevallos</surname><given-names>V</given-names></name>, <etal>et al</etal><article-title>How reliable is the clinical exam in detecting ocular chlamydial infection?</article-title><source>Ophthalmic Epidemiol</source>. <year>2004</year>;<volume>11</volume>(<issue>3</issue>):<fpage>255</fpage>–<lpage>262</lpage>. <pub-id pub-id-type="doi">10.1080/09286580490514577</pub-id>
<?supplied-pmid 15370556?><pub-id pub-id-type="pmid">15370556</pub-id></mixed-citation>
              </ref>
              <ref id="pone.0210463.ref009">
                <label>9</label>
                <mixed-citation publication-type="journal"><name><surname>See</surname><given-names>CW</given-names></name>, <name><surname>Alemayehu</surname><given-names>W</given-names></name>, <name><surname>Melese</surname><given-names>M</given-names></name>, <name><surname>Zhou</surname><given-names>Z</given-names></name>, <name><surname>Porco</surname><given-names>TC</given-names></name>, <name><surname>Shiboski</surname><given-names>S</given-names></name>, <etal>et al</etal><article-title>How reliable are tests for trachoma?—a latent class approach</article-title>. <source>Invest Ophthalmol Vis Sci</source>. <year>2011</year>;<volume>52</volume>(<issue>9</issue>):<fpage>6133</fpage>–<lpage>6137</lpage>. <pub-id pub-id-type="doi">10.1167/iovs.11-7419</pub-id>
<?supplied-pmid 21685340?><pub-id pub-id-type="pmid">21685340</pub-id></mixed-citation>
              </ref>
              <ref id="pone.0210463.ref010">
                <label>10</label>
                <mixed-citation publication-type="journal"><name><surname>Lietman</surname><given-names>T</given-names></name>, <name><surname>Eng</surname><given-names>J</given-names></name>, <name><surname>Katz</surname><given-names>J</given-names></name>, <name><surname>Quigley</surname><given-names>HA</given-names></name>. <article-title>Neural networks for visual field analysis: how do they compare with other algorithms?</article-title><source>J Glaucoma</source>. <year>1999</year>;<volume>8</volume>(<issue>1</issue>):<fpage>77</fpage>–<lpage>80</lpage>. <pub-id pub-id-type="doi">10.1097/00061198-199902000-00014</pub-id>
<?supplied-pmid 10084278?><pub-id pub-id-type="pmid">10084278</pub-id></mixed-citation>
              </ref>
              <ref id="pone.0210463.ref011">
                <label>11</label>
                <mixed-citation publication-type="journal"><name><surname>Pratt</surname><given-names>H</given-names></name>, <name><surname>Coenen</surname><given-names>F</given-names></name>, <name><surname>Broadbent</surname><given-names>DM</given-names></name>, <name><surname>Harding</surname><given-names>SP</given-names></name>, <name><surname>Zheng</surname><given-names>Y</given-names></name>. <article-title>Convolutional Neural Networks for Diabetic Retinopathy</article-title>. <source>Procedia Computer Science</source>. <year>2016</year>;<volume>90</volume>:<fpage>200</fpage>–<lpage>205</lpage>. <pub-id pub-id-type="doi">10.1016/j.procs.2016.07.014</pub-id></mixed-citation>
              </ref>
              <ref id="pone.0210463.ref012">
                <label>12</label>
                <mixed-citation publication-type="journal"><name><surname>Krizhevsky</surname><given-names>A</given-names></name>, <name><surname>Sutskever</surname><given-names>I</given-names></name>, <name><surname>Hinton</surname><given-names>GE</given-names></name>. <article-title>Imagenet classification with deep convolutional neural networks</article-title>. In: <source>Advances in neural information processing systems</source>; <year>2012</year>.</mixed-citation>
              </ref>
              <ref id="pone.0210463.ref013">
                <label>13</label>
                <mixed-citation publication-type="journal"><name><surname>Simonyan</surname><given-names>K</given-names></name>, <name><surname>Zisserman</surname><given-names>A</given-names></name>. <article-title>Very Deep Convolutional Networks for Large-Scale Image Recoginition</article-title>. <source>Intl Conf on Learning Representations (ICLR)</source>. <year>2015</year>; p. <fpage>1</fpage>–<lpage>14</lpage>.</mixed-citation>
              </ref>
              <ref id="pone.0210463.ref014">
                <label>14</label>
                <mixed-citation publication-type="journal"><name><surname>House</surname><given-names>JI</given-names></name>, <name><surname>Ayele</surname><given-names>B</given-names></name>, <name><surname>Porco</surname><given-names>TC</given-names></name>, <name><surname>Zhou</surname><given-names>Z</given-names></name>, <name><surname>Hong</surname><given-names>KC</given-names></name>, <name><surname>Gebre</surname><given-names>T</given-names></name>, <etal>et al</etal><article-title>Assessment of herd protection against trachoma due to repeated mass antibiotic distributions: a cluster-randomised trial</article-title>. <source>Lancet</source>. <year>2009</year>;<volume>373</volume>(<issue>9669</issue>):<fpage>1111</fpage>–<lpage>1118</lpage>. <pub-id pub-id-type="doi">10.1016/S0140-6736(09)60323-8</pub-id>
<?supplied-pmid 19329003?><pub-id pub-id-type="pmid">19329003</pub-id></mixed-citation>
              </ref>
              <ref id="pone.0210463.ref015">
                <label>15</label>
                <mixed-citation publication-type="journal"><name><surname>Amza</surname><given-names>A</given-names></name>, <name><surname>Kadri</surname><given-names>B</given-names></name>, <name><surname>Nassirou</surname><given-names>B</given-names></name>, <name><surname>Cotter</surname><given-names>SY</given-names></name>, <name><surname>Stoller</surname><given-names>NE</given-names></name>, <name><surname>Zhou</surname><given-names>Z</given-names></name>, <etal>et al</etal><article-title>A cluster-randomized trial to assess the efficacy of targeting trachoma treatment to children</article-title>. <source>Clin Infect Dis</source>. <year>2017</year>;<volume>64</volume>(<issue>6</issue>):<fpage>743</fpage>–<lpage>750</lpage>. <pub-id pub-id-type="doi">10.1093/cid/ciw810</pub-id>
<?supplied-pmid 27956455?><pub-id pub-id-type="pmid">27956455</pub-id></mixed-citation>
              </ref>
              <ref id="pone.0210463.ref016">
                <label>16</label>
                <mixed-citation publication-type="journal"><name><surname>Thylefors</surname><given-names>B</given-names></name>, <name><surname>Dawson</surname><given-names>CR</given-names></name>, <name><surname>Jones</surname><given-names>BR</given-names></name>, <name><surname>West</surname><given-names>SK</given-names></name>, <name><surname>Taylor</surname><given-names>HR</given-names></name>. <article-title>A simple system for the assessment of trachoma and its complications</article-title>. <source>Bull WHO</source>. <year>1987</year>;<volume>65</volume>(<issue>4</issue>):<fpage>477</fpage>–<lpage>483</lpage>. <?supplied-pmid 3500800?><pub-id pub-id-type="pmid">3500800</pub-id></mixed-citation>
              </ref>
              <ref id="pone.0210463.ref017">
                <label>17</label>
                <mixed-citation publication-type="journal"><name><surname>Phung</surname><given-names>SL</given-names></name>, <name><surname>Bouzerdoum</surname><given-names>A</given-names></name>, <name><surname>Chai</surname><given-names>D</given-names></name>. <article-title>Skin segmentation using color pixel classification: analysis and comparison</article-title>. <source>IEEE Trans Pattern Anal Mach Intell</source>. <year>2005</year>;<volume>27</volume>(<issue>1</issue>):<fpage>148</fpage>–<lpage>154</lpage>. <pub-id pub-id-type="doi">10.1109/TPAMI.2005.17</pub-id>
<?supplied-pmid 15628277?><pub-id pub-id-type="pmid">15628277</pub-id></mixed-citation>
              </ref>
              <ref id="pone.0210463.ref018">
                <label>18</label>
                <mixed-citation publication-type="book"><name><surname>Kaehler</surname><given-names>A</given-names></name>, <name><surname>Bradski</surname><given-names>G</given-names></name>. <source>Learning OpenCV 3: Computer Vision in C++ with the OpenCV Library</source>. <edition>1st ed</edition><publisher-loc>Sebastopol, CA</publisher-loc>: <publisher-name>O’Reilly Media</publisher-name>; <year>2017</year>.</mixed-citation>
              </ref>
              <ref id="pone.0210463.ref019">
                <label>19</label>
                <mixed-citation publication-type="book"><name><surname>Haykin</surname><given-names>S</given-names></name>. <source>Neural networks and learning machines</source>. <edition>3d ed</edition><publisher-loc>Upper Saddle River, NJ</publisher-loc>: <publisher-name>Pearson</publisher-name>, <year>2009</year>.</mixed-citation>
              </ref>
              <ref id="pone.0210463.ref020">
                <label>20</label>
                <mixed-citation publication-type="journal"><name><surname>Dreyfus</surname><given-names>S</given-names></name>. <article-title>The numerical solution of variational problems</article-title>. <source>J Math Anal Appl</source>. <year>1962</year>;<volume>5</volume>(<issue>1</issue>):<fpage>30</fpage>–<lpage>45</lpage>. <pub-id pub-id-type="doi">10.1016/0022-247X(62)90004-5</pub-id></mixed-citation>
              </ref>
              <ref id="pone.0210463.ref021">
                <label>21</label>
                <mixed-citation publication-type="other">Buja A, Werner S, Shen Y. Loss Functions for Binary Class Probability Estimation and Classification: Structure and Applications; 2005. <ext-link ext-link-type="uri" xlink:href="http://www-stat.wharton.upenn.edu/~buja/PAPERS/paper-proper-scoring.pdf">http://www-stat.wharton.upenn.edu/~buja/PAPERS/paper-proper-scoring.pdf</ext-link>.</mixed-citation>
              </ref>
              <ref id="pone.0210463.ref022">
                <label>22</label>
                <mixed-citation publication-type="book"><name><surname>Snyder</surname><given-names>WE</given-names></name>, <name><surname>Qi</surname><given-names>H</given-names></name>. <source>Machine vision</source>. <publisher-loc>Cambridge</publisher-loc>: <publisher-name>Cambridge University Press</publisher-name>; <year>2010</year>.</mixed-citation>
              </ref>
              <ref id="pone.0210463.ref023">
                <label>23</label>
                <mixed-citation publication-type="journal"><name><surname>Daugman</surname><given-names>JG</given-names></name>. <article-title>Two-dimensional spectral analysis of cortical receptive field profiles</article-title>. <source>Vision res</source>. <year>1980</year>;<volume>20</volume>(<issue>10</issue>):<fpage>847</fpage>–<lpage>856</lpage>. <pub-id pub-id-type="doi">10.1016/0042-6989(80)90065-6</pub-id>
<?supplied-pmid 7467139?><pub-id pub-id-type="pmid">7467139</pub-id></mixed-citation>
              </ref>
              <ref id="pone.0210463.ref024">
                <label>24</label>
                <mixed-citation publication-type="book">Pizer SM, Johnston RE, Ericksen JP, Yankaskas BC, Muller KE. Contrast-limited adaptive histogram equalization: speed and effectiveness. In: Proceedings of the first conference on Visualization in Biomedical Computing, May 22–25, Atlanta, GA. IEEE Computer Society Press; 1990. p. 337–345. <pub-id pub-id-type="doi">10.1109/VBC.1990.109340</pub-id></mixed-citation>
              </ref>
              <ref id="pone.0210463.ref025">
                <label>25</label>
                <mixed-citation publication-type="book"><name><surname>Spall</surname><given-names>JC</given-names></name>. <source>Introduction to Stochastic Search and Optimization</source>. <publisher-loc>New York</publisher-loc>: <publisher-name>Wiley</publisher-name>; <year>2003</year>.</mixed-citation>
              </ref>
              <ref id="pone.0210463.ref026">
                <label>26</label>
                <mixed-citation publication-type="other">Zeiler MD. ADADELTA: an adaptive learning rate method. arXiv preprint arXiv:12125701. 2012.</mixed-citation>
              </ref>
              <ref id="pone.0210463.ref027">
                <label>27</label>
                <mixed-citation publication-type="other">Ioffe S, Szegedy C. Batch normalization: Accelerating deep network training by reducing internal covariate shift. arXiv preprint arXiv:150203167. 2015.</mixed-citation>
              </ref>
              <ref id="pone.0210463.ref028">
                <label>28</label>
                <mixed-citation publication-type="journal"><name><surname>Srivastava</surname><given-names>N</given-names></name>, <name><surname>Hinton</surname><given-names>GE</given-names></name>, <name><surname>Krizhevsky</surname><given-names>A</given-names></name>, <name><surname>Sutskever</surname><given-names>I</given-names></name>, <name><surname>Salakhutdinov</surname><given-names>R</given-names></name>. <article-title>Dropout: a simple way to prevent neural networks from overfitting</article-title>. <source>J Mach Learn Res</source>. <year>2014</year>;<volume>15</volume>(<issue>1</issue>):<fpage>1929</fpage>–<lpage>1958</lpage>.</mixed-citation>
              </ref>
              <ref id="pone.0210463.ref029">
                <label>29</label>
                <mixed-citation publication-type="journal"><name><surname>Cohen</surname><given-names>J</given-names></name>. <article-title>A coefficient of agreement for nominal scales</article-title>. <source>Educ Psychol Meas</source>. <year>1960</year>;<volume>20</volume>:<fpage>213</fpage>–<lpage>220</lpage>. <pub-id pub-id-type="doi">10.1177/001316446002000104</pub-id></mixed-citation>
              </ref>
              <ref id="pone.0210463.ref030">
                <label>30</label>
                <mixed-citation publication-type="journal"><name><surname>Solomon</surname><given-names>AW</given-names></name>, <name><surname>Pavluck</surname><given-names>AL</given-names></name>, <name><surname>Courtright</surname><given-names>P</given-names></name>, <name><surname>Aboe</surname><given-names>A</given-names></name>, <name><surname>Adamu</surname><given-names>L</given-names></name>, <name><surname>Alemayehu</surname><given-names>W</given-names></name>, <etal>et al</etal><article-title>The Global Trachoma Mapping Project: Methodology of a 34-Country Population-Based Study</article-title>. <source>Ophthalmic Epidemiol</source>. <year>2015</year>;<volume>22</volume>(<issue>3</issue>):<fpage>214</fpage>–<lpage>225</lpage>. <pub-id pub-id-type="doi">10.3109/09286586.2015.1037401</pub-id>
<?supplied-pmid 26158580?><pub-id pub-id-type="pmid">26158580</pub-id></mixed-citation>
              </ref>
              <ref id="pone.0210463.ref031">
                <label>31</label>
                <mixed-citation publication-type="journal"><name><surname>Dawson</surname><given-names>CR</given-names></name>, <name><surname>Jones</surname><given-names>BR</given-names></name>, <name><surname>Darougar</surname><given-names>S</given-names></name>. <article-title>Blinding and non-blinding trachoma: assessment of intensity of upper tarsal inflammatory disease and disabling lesions</article-title>. <source>Bull WHO</source>. <year>1975</year>;<volume>52</volume>(<issue>3</issue>):<fpage>279</fpage>–<lpage>282</lpage>. <?supplied-pmid 1084798?><pub-id pub-id-type="pmid">1084798</pub-id></mixed-citation>
              </ref>
              <ref id="pone.0210463.ref032">
                <label>32</label>
                <mixed-citation publication-type="journal"><name><surname>Roper</surname><given-names>KG</given-names></name>, <name><surname>Taylor</surname><given-names>HR</given-names></name>. <article-title>Comparison of clinical and photographic assessment of trachoma</article-title>. <source>Br J Ophthalmol</source>. <year>2009</year>;<volume>93</volume>(<issue>6</issue>):<fpage>811</fpage>–<lpage>814</lpage>. <pub-id pub-id-type="doi">10.1136/bjo.2008.144147</pub-id>
<?supplied-pmid 19304582?><pub-id pub-id-type="pmid">19304582</pub-id></mixed-citation>
              </ref>
            </ref-list>
          </back>
        </article>
      </metadata>
    </record>
  </GetRecord>
</OAI-PMH>
