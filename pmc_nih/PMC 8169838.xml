<?xml version="1.0" encoding="UTF-8"?>
<OAI-PMH xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/ http://www.openarchives.org/OAI/2.0/OAI-PMH.xsd">
  <responseDate>2024-05-03T02:52:23Z</responseDate>
  <request verb="GetRecord" identifier="oai:pubmedcentral.nih.gov:8169838" metadataPrefix="pmc">https:/www.ncbi.nlm.nih.gov/pmc/oai/oai.cgi</request>
  <GetRecord>
    <record>
      <header>
        <identifier>oai:pubmedcentral.nih.gov:8169838</identifier>
        <datestamp>2021-06-03</datestamp>
        <setSpec>scirep</setSpec>
        <setSpec>pmc-open</setSpec>
      </header>
      <metadata>
        <article xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:mml="http://www.w3.org/1998/Math/MathML" xsi:schemaLocation="https://jats.nlm.nih.gov/ns/archiving/1.3/ https://jats.nlm.nih.gov/archiving/1.3/xsd/JATS-archivearticle1-3.xsd" article-type="research-article">
          <front>
            <journal-meta>
              <journal-id journal-id-type="nlm-ta">Sci Rep</journal-id>
              <journal-id journal-id-type="iso-abbrev">Sci Rep</journal-id>
              <journal-title-group>
                <journal-title>Scientific Reports</journal-title>
              </journal-title-group>
              <issn pub-type="epub">2045-2322</issn>
              <publisher>
                <publisher-name>Nature Publishing Group UK</publisher-name>
                <publisher-loc>London</publisher-loc>
              </publisher>
            </journal-meta>
            <article-meta>
              <article-id pub-id-type="accession">PMC8169838</article-id>
              <article-id pub-id-type="pmcid">PMC8169838</article-id>
              <article-id pub-id-type="pmc-uid">8169838</article-id>
              <article-id pub-id-type="pmid">34075169</article-id>
              <article-id pub-id-type="publisher-id">91006</article-id>
              <article-id pub-id-type="doi">10.1038/s41598-021-91006-8</article-id>
              <article-categories>
                <subj-group subj-group-type="heading">
                  <subject>Article</subject>
                </subj-group>
              </article-categories>
              <title-group>
                <article-title>Gravitational effects of scene information in object localization</article-title>
              </title-group>
              <contrib-group>
                <contrib contrib-type="author" corresp="yes">
                  <name>
                    <surname>Kosovicheva</surname>
                    <given-names>Anna</given-names>
                  </name>
                  <address>
                    <email>a.kosovicheva@utoronto.ca</email>
                  </address>
                  <xref ref-type="aff" rid="Aff1">1</xref>
                  <xref ref-type="aff" rid="Aff2">2</xref>
                </contrib>
                <contrib contrib-type="author">
                  <name>
                    <surname>Bex</surname>
                    <given-names>Peter J.</given-names>
                  </name>
                  <xref ref-type="aff" rid="Aff2">2</xref>
                </contrib>
                <aff id="Aff1"><label>1</label><institution-wrap><institution-id institution-id-type="GRID">grid.17063.33</institution-id><institution-id institution-id-type="ISNI">0000 0001 2157 2938</institution-id><institution>Department of Psychology, </institution><institution>University of Toronto Mississauga, </institution></institution-wrap>3359 Mississauga Road, Mississauga, ON L5L 1C6 Canada </aff>
                <aff id="Aff2"><label>2</label><institution-wrap><institution-id institution-id-type="GRID">grid.261112.7</institution-id><institution-id institution-id-type="ISNI">0000 0001 2173 3359</institution-id><institution>Department of Psychology, </institution><institution>Northeastern University, </institution></institution-wrap>125 Nightingale Hall, 360 Huntington Ave., Boston, MA 02115 USA </aff>
              </contrib-group>
              <pub-date pub-type="epub">
                <day>1</day>
                <month>6</month>
                <year>2021</year>
              </pub-date>
              <pub-date pub-type="pmc-release">
                <day>1</day>
                <month>6</month>
                <year>2021</year>
              </pub-date>
              <pub-date pub-type="collection">
                <year>2021</year>
              </pub-date>
              <volume>11</volume>
              <elocation-id>11520</elocation-id>
              <history>
                <date date-type="received">
                  <day>6</day>
                  <month>3</month>
                  <year>2021</year>
                </date>
                <date date-type="accepted">
                  <day>20</day>
                  <month>5</month>
                  <year>2021</year>
                </date>
              </history>
              <permissions>
                <copyright-statement>© The Author(s) 2021</copyright-statement>
                <license>
                  <ali:license_ref specific-use="textmining" content-type="ccbylicense">https://creativecommons.org/licenses/by/4.0/</ali:license_ref>
                  <license-p><bold>Open Access</bold> This article is licensed under a Creative Commons Attribution 4.0 International License, which permits use, sharing, adaptation, distribution and reproduction in any medium or format, as long as you give appropriate credit to the original author(s) and the source, provide a link to the Creative Commons licence, and indicate if changes were made. The images or other third party material in this article are included in the article's Creative Commons licence, unless indicated otherwise in a credit line to the material. If material is not included in the article's Creative Commons licence and your intended use is not permitted by statutory regulation or exceeds the permitted use, you will need to obtain permission directly from the copyright holder. To view a copy of this licence, visit <ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by/4.0/">http://creativecommons.org/licenses/by/4.0/</ext-link>.</license-p>
                </license>
              </permissions>
              <abstract id="Abs1">
                <p id="Par1">We effortlessly interact with objects in our environment, but how do we know where something is? An object’s apparent position does not simply correspond to its retinotopic location but is influenced by its surrounding context. In the natural environment, this context is highly complex, and little is known about how visual information in a scene influences the apparent location of the objects within it. We measured the influence of local image statistics (luminance, edges, object boundaries, and saliency) on the reported location of a brief target superimposed on images of natural scenes. For each image statistic, we calculated the difference between the image value at the physical center of the target and the value at its reported center, using observers’ cursor responses, and averaged the resulting values across all trials. To isolate image-specific effects, difference scores were compared to a randomly-permuted null distribution that accounted for any response biases. The observed difference scores indicated that responses were significantly biased toward darker regions, luminance edges, object boundaries, and areas of high saliency, with relatively low shared variance among these measures. In addition, we show that the same image statistics were associated with observers’ saccade errors, despite large differences in response time, and that some effects persisted when high-level scene processing was disrupted by 180° rotations and color negatives of the originals. Together, these results provide evidence for landmark effects within natural images, in which feature location reports are pulled toward low- and high-level informative content in the scene.</p>
              </abstract>
              <kwd-group kwd-group-type="npg-subject">
                <title>Subject terms</title>
                <kwd>Visual system</kwd>
                <kwd>Object vision</kwd>
                <kwd>Human behaviour</kwd>
                <kwd>Perception</kwd>
              </kwd-group>
              <funding-group>
                <award-group>
                  <funding-source>
                    <institution-wrap>
                      <institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100000002</institution-id>
                      <institution>National Institutes of Health</institution>
                    </institution-wrap>
                  </funding-source>
                  <award-id>F32 EY028814</award-id>
                  <award-id>R01 EY029713</award-id>
                  <principal-award-recipient>
                    <name>
                      <surname>Kosovicheva</surname>
                      <given-names>Anna</given-names>
                    </name>
                    <name>
                      <surname>Bex</surname>
                      <given-names>Peter J.</given-names>
                    </name>
                  </principal-award-recipient>
                </award-group>
              </funding-group>
              <custom-meta-group>
                <custom-meta>
                  <meta-name>issue-copyright-statement</meta-name>
                  <meta-value>© The Author(s) 2021</meta-value>
                </custom-meta>
              </custom-meta-group>
            </article-meta>
          </front>
          <body>
            <sec id="Sec1">
              <title>Introduction</title>
              <p id="Par2">To successfully interact with our environment, we must accurately perceive the locations of objects around us. This is central to our ability to perform a variety of tasks, such as catching a baseball or reaching for a pen. Although position information is coded retinotopically at multiple stages of visual processing, perceived location can be influenced by many other factors beyond retinal location. Factors such as eye movements<sup><xref ref-type="bibr" rid="CR1">1</xref></sup>, motion<sup><xref ref-type="bibr" rid="CR2">2</xref>–<xref ref-type="bibr" rid="CR4">4</xref></sup>, stimulus history<sup><xref ref-type="bibr" rid="CR5">5</xref>,<xref ref-type="bibr" rid="CR6">6</xref></sup>, visual attention<sup><xref ref-type="bibr" rid="CR7">7</xref></sup>, and individual differences<sup><xref ref-type="bibr" rid="CR8">8</xref></sup> can influence position assignment. Many of these phenomena may reflect mechanisms that facilitate our interaction with the environment, including maintenance of stability across eye movements<sup><xref ref-type="bibr" rid="CR1">1</xref></sup>, and over time<sup><xref ref-type="bibr" rid="CR9">9</xref></sup>.</p>
              <p id="Par3">An important contribution to position representations is the influence of nearby visual features on localization. Effects of this type include shifts in the perceived location or orientation of a static object based on the appearance of a surrounding frame<sup><xref ref-type="bibr" rid="CR10">10</xref>,<xref ref-type="bibr" rid="CR11">11</xref></sup>. Similarly, when observers report the location of a previously seen target, responses are often pulled toward other stationary landmarks or spatial references<sup><xref ref-type="bibr" rid="CR12">12</xref>–<xref ref-type="bibr" rid="CR17">17</xref></sup>. Although these effects may generally reflect remembered—rather than perceived—location, other studies have reported illusions in which the apparent position of a target is shifted toward nearby references or anchors<sup><xref ref-type="bibr" rid="CR18">18</xref>–<xref ref-type="bibr" rid="CR20">20</xref></sup>. These effects have typically been studied with simple stimulus configurations on uniform backgrounds in which patterns of error in reproducing the position of a single target are measured with or without static references. Despite the relevance for action in everyday tasks, much less is known about how these factors operate in more natural settings, or how local image features contribute to position assignment within natural scenes.</p>
              <p id="Par4">When an observer is shown a briefly flashed target within a scene, how does the information in the scene affect where they localize it? Although observers could, conceivably, ignore scene information, either localizing the briefly presented object accurately or with some error that is random with respect to the scene, this is unlikely given this previous literature. One possibility is that the types of landmark effects observed in more simple configurations may appear in more naturalistic settings and pull the reported object locations toward nearby references. Additionally, localization biases could reflect priors for expected object location, based on the spatial distribution of objects learned from experience with natural settings. For example, objects are unlikely to be suspended in midair or far from other objects, and this knowledge might bias an observer’s report of target positions in natural settings.</p>
              <p id="Par5">To identify the ‘landmarks’ that influence position judgments under uncertainty within natural images, we measured the influence of local image statistics on target localization, similar to approaches to investigate detection and identification within natural scene images<sup><xref ref-type="bibr" rid="CR21">21</xref>–<xref ref-type="bibr" rid="CR23">23</xref></sup>. On each trial, observers reported the perceived location of a briefly presented Gaussian patch by adjusting the position of a cursor to match its apparent location (Fig. <xref rid="Fig1" ref-type="fig">1</xref>A). We analyzed observers’ errors in relation to meaningful features or statistics within the images, which although not fully independent, have relatively low shared variance (see <italic>Results</italic>), and therefore have the potential to capture different information in these images.<fig id="Fig1"><label>Figure 1</label><caption><p>(<bold>A</bold>) On each trial, observers continuously fixated a dot at the center of the display for a random interval between 500 and 1250 ms. A brief (50 ms) Gaussian target patch was randomly presented at one of 72 possible peripheral locations (3 eccentricities × 24 angular locations; shown here in the upper-right portion of the image). Observers then either reported its location by moving a crosshair controlled by a mouse while maintaining fixation (Experiments 1 and 3), or made a reflexive saccade to the target (Experiment 2). (<bold>B</bold>) The images used in Experiment 3 were color negatives of the original image, and flipped upside down. (<bold>C</bold>) To measure the influence of different image features on localization errors, we calculated difference scores from the image value at the physical center of the patch location (“physical”; red) and the location reported by the observer (“response”; blue, hypothetical example shown with luminance maps). In the example, negative values indicate that responses are pulled toward darker areas in the image, and positive values indicate that responses are pulled toward lighter areas. To isolate image-specific effects from other potential reporting biases, the observed mean difference score was compared to values in a null distribution, which was calculated by randomly shuffling the physical and response coordinates in relation to other images. In the example, the images in the right column are in the same order as the original trials, but the coordinates are taken from different trials (randomly shuffled on each iteration), indicated by the gray arrows. For example, the coordinates from trial 2 were used to calculate a difference score for the image in trial 1. A mean difference score was calculated from all shuffled trials, and this procedure was repeated for 1000 iterations to produce a null distribution of difference scores.</p></caption><graphic xlink:href="41598_2021_91006_Fig1_HTML" id="MO1"/></fig></p>
              <p id="Par6">We adopted an exploratory approach, using a number of features that have been previously shown to influence detection or identification of elements<sup><xref ref-type="bibr" rid="CR21">21</xref>–<xref ref-type="bibr" rid="CR23">23</xref></sup> as well as attention or eye movements<sup><xref ref-type="bibr" rid="CR24">24</xref>,<xref ref-type="bibr" rid="CR25">25</xref></sup> within natural scenes. This included lower-level features such as luminance and luminance-defined edge information (i.e., distance from nearest edge). We also included an edge density measure, which captures information about locations where there is a high density of luminance boundaries (e.g., closely-spaced textures), and has been shown to influence detection of targets within natural scenes<sup><xref ref-type="bibr" rid="CR21">21</xref></sup>. In addition, we included higher-level information about the outlines of objects in the image by measuring errors in relation to annotated object outlines<sup><xref ref-type="bibr" rid="CR26">26</xref></sup> in the images. Finally, we included a measure of saliency<sup><xref ref-type="bibr" rid="CR27">27</xref></sup>, as a way to identify possible attentional landmarks in the images, as attention has been previously shown to influence spatial position judgments under more simple configurations<sup><xref ref-type="bibr" rid="CR7">7</xref></sup>.</p>
              <p id="Par7">In Experiment 1, we established the image-specific influences of each of these features on observers’ localization errors, demonstrating that observers’ responses were significantly biased toward darker regions, luminance edges, object boundaries, and areas of high saliency. To demonstrate that these effects were not specific to cursor responses, Experiment 2 reproduced this result by showing that similar features contributed to observers’ saccade errors. Finally, in Experiment 3 (Fig. <xref rid="Fig1" ref-type="fig">1</xref>B), we used inverted and color-negative images to demonstrate that a subset of these gravitational biases persist when normal scene processing is disrupted, separating lower-level and higher-level influences of these features on position representations.</p>
            </sec>
            <sec id="Sec2">
              <title>Methods</title>
              <sec id="Sec3">
                <title>Participants</title>
                <p id="Par8">We collected data from ten participants in Experiment 1 (6 female, 4 male, ages 18–37), twelve in Experiment 2 (9 female, 3 male, 18–33), and twelve in Experiment 3 (8 female, 4 male, ages 18–30). One of the authors participated in Experiments 1 and 2. All participants reported normal or corrected-to-normal vision, and all except the author were naïve to the purpose of the study. The experiments and their respective analyses were pre-registered on the Open Science Framework (<ext-link ext-link-type="uri" xlink:href="https://osf.io/d7qrv/registrations">https://osf.io/d7qrv/registrations</ext-link>). Procedures were approved by the Institutional Review Board at Northeastern University and the experiments were carried out in accordance with the relevant regulations regarding human subjects research. All observers gave informed consent prior to participating in the experiment.</p>
              </sec>
              <sec id="Sec4">
                <title>Eye tracking</title>
                <p id="Par9">Eye movements were recorded with an Eyelink 1000 desktop mounted infrared eye tracker (SR Research Ltd., Ottawa, Ontario, Canada), used in conjunction with the Eyelink Toolbox for Matlab<sup><xref ref-type="bibr" rid="CR28">28</xref></sup>. Gaze recordings were calibrated with a standard 9-point calibration procedure<sup><xref ref-type="bibr" rid="CR29">29</xref></sup>, and gaze position was recorded binocularly at a sampling rate of 1000 Hz. Online fixation monitoring was based on the average of the recorded gaze positions from the two eyes. Noise artifacts were reduced using Eyelink software, which applied a heuristic filtering algorithm to the raw gaze position samples (see Stampe, 1993, for details). Gaze data were classified using the Eyelink algorithm into saccades and fixations using velocity and acceleration thresholds of 30°/s and 8000°/s<sup>2</sup>, respectively.</p>
              </sec>
              <sec id="Sec5">
                <title>Stimuli</title>
                <p id="Par10">Stimuli were presented on a gamma-corrected 27″ BenQ XL2720Z LCD monitor controlled by a Dell Optiplex 9020 desktop computer with a Quadro K420 graphics card. The experiment was programmed in Matlab (The Mathworks, Inc., Natick, MA) using the Psychophysics Toolbox Version 3<sup><xref ref-type="bibr" rid="CR30">30</xref>–<xref ref-type="bibr" rid="CR32">32</xref></sup>. Display resolution was set to 1920 × 1080 and the refresh rate to 120 Hz. Observers were seated at a viewing distance of 50 cm from the display, with head position stabilized using a chinrest. At this distance, the display subtended 67.2° horizontally and 38.1° vertically. The maximum display intensity was 141 cd/m<sup>2</sup>.</p>
                <p id="Par11">Natural scene images were selected from the LabelMe image database<sup><xref ref-type="bibr" rid="CR26">26</xref></sup>. As the full database includes a much larger number of images than required for the present study, a subset was created from a benchmark dataset (<ext-link ext-link-type="uri" xlink:href="http://groups.csail.mit.edu/vision/LabelMe/Benchmarks/spain/">http://groups.csail.mit.edu/vision/LabelMe/Benchmarks/spain/</ext-link>), originally used for training a classifier to recognize different object categories, using the 4053 images listed under test and training sets (training and test images were used the same way, with no distinction between these two categories). It consisted of a variety of mostly outdoor scenes (e.g., a mix of urban and rural scenes) taken from different countries around the world, and selected in part because they were heavily annotated (median percentage of annotated area in each image = 92.9%). To maintain uniformity in stimulus dimensions across trials, we discarded images that did not have an aspect ratio of 4:3, which resulted in a final set of 3494 images. Each image was scaled to 48° in width and 36.9° in height, and the root-mean-square (RMS) contrast of each image was adjusted to a value of 0.5. On each trial, we selected a random image, without replacement, from this set. Images were displayed on a uniform gray (69.2 cd/m<sup>2</sup>) background.</p>
                <p id="Par12">Targets were 2D Gaussian patches that were intended to be highly visible across a large range of background luminance values and textures. These patches were generated by first producing an image negative of the natural scene image ([255,255,255]-[R,G,B]) within a 3.25° × 3.25° square centered on the x- and y- coordinates of the selected patch location (see <italic>Procedure</italic>). The pixel locations within this square region were then randomly scrambled and the resulting values were superimposed on the original image using alpha blending, with the alpha values defined by a 2D Gaussian profile (s = 0.85°), ranging from fully transparent to fully opaque.</p>
              </sec>
              <sec id="Sec6">
                <title>Procedure</title>
                <p id="Par13">For the duration of each trial, observers were instructed to maintain fixation on a 0.4° diameter circle (white 0.035° outline and black fill) at the center of the display. As shown in Fig. <xref rid="Fig1" ref-type="fig">1</xref>A, at the beginning of each trial, observers were shown the scene image and fixation dot. Target presentation was withheld until the observer’s registered fixation location continuously fell within a 1.5° × 1.5° square region centered on the dot for a randomly selected interval from 500 to 1250 ms. Following this interval, the target patch was presented for 50 ms at one of 72 possible stimulus locations, defined by the combination of 3 eccentricities (5°, 7.5°, 10°) and 24 angular locations (0 to 360° in 15° steps). After the target patch was removed, observers were instructed to report the location of the center of the patch by adjusting a crosshair using a mouse (0.50° × 0.50° black fill, and 0.14° stroke width, surrounded by a 0.04° white outline), while maintaining fixation at the center of the display. The crosshair was visible during the response phase only, and its position was reset to the center of the display at the beginning of the response phase on each trial. During this response phase, if the observer’s gaze position fell outside the 1.5° square centered on the fixation dot for longer than 150 ms, observers were given feedback in the form of a buzzer tone. In these instances, the trial was immediately aborted and no response was recorded. In addition, observers were instructed to press the spacebar if they did not see the patch at all, and the program advanced to the next trial without recording a response. Responses were otherwise recorded when the observer clicked to indicate their response. No feedback was provided regarding response accuracy. Trials were separated by a 500 ms blank interval with a uniform mean luminance gray screen.</p>
                <p id="Par14">Observers first completed a short block of 45 practice trials of the task, followed by 720 trials of the main experiment, which consisted of 10 trials for each possible patch location, presented in a random order. Drift correction of the eye tracker was performed every 45 trials, at which time observers were also shown a screen indicating their progress (number of trials completed) through the experiment. In addition, the eye tracker was re-calibrated at least once every 180 trials.</p>
                <p id="Par15">The procedure for Experiment 2 was identical to that of Experiment 1, except observers were instructed to saccade to the target patch as soon as it appeared. After saccade onset, once the observer’s gaze position left the central 1.5° × 1.5° square region around fixation, a crosshair was displayed at the x- and y- coordinates of the currently registered gaze position from the eye tracker and remained on the screen for 500 ms following saccade onset. The marker was a crosshair similar to that used in Experiment 1 (black fill and a white outline, or a white fill with a black outline, randomly selected with equal probability). The purpose of the crosshair was twofold – to maintain participant alertness (as there were no other visual stimuli or task instructions), and to monitor tracking accuracy throughout the course of the experiment. Participants were instructed to notify the experimenter in the unlikely event that the crosshair appeared far from their point of gaze following the saccade. If this happened, the eye tracker was recalibrated at that point in the experiment. As in Experiment 1, drift correction and recalibration were also performed every 45 trials and 180 trials, respectively.</p>
                <p id="Par16">The procedures for Experiment 3 were similar to those for Experiment 1, with a change in the appearance of the natural scene images. Images were RGB color negatives ([255,255,255]-[R,G,B]) of the originals, and rotated 180° (i.e., flipped upside down) from the originals. As in Experiment 2, the contrast polarity of the crosshair in Experiment 3 was randomized (black outline and white fill, or the reverse). Observers reported the location of the target patch by adjusting the location of the crosshair using the mouse.</p>
              </sec>
              <sec id="Sec7">
                <title>Data analysis</title>
                <p id="Par17">Aborted trials due to breaks from fixation during the response interval accounted for 5.63% and 4.34% of all responses in Experiments 1 and 3, respectively. Aborted trials due to self-reported inability to see the patch accounted for 0.19% of responses in Experiment 1. As this represented a very small number of trials, this response option was removed in Experiments 2 and 3. To remove potential lapses, we additionally removed responses with an error magnitude (distance from the center of the patch) of more than 4 standard deviations from the mean (0.29%, 2.73%, and 0.30% of trials in each of the three experiments).</p>
                <p id="Par18">The purpose of the analysis was to determine the relationship between the direction of observers’ errors in the task and the features in the image. As shown in Fig. <xref rid="Fig1" ref-type="fig">1</xref>C, the influence of image features on localization errors was calculated by subtracting the image feature value (e.g., luminance) at the location of the physical center of the patch from the value at the coordinates of the observer’s response. These values were calculated from the original scene image without the target patch. For example, if localization were systematically biased toward dark areas in the image, we would see lower luminance values at the reported location compared to the physical center of the stimulus. Difference scores between these map values were calculated for each of the image statistics described below and then averaged across trials. In Experiments 1 and 3, the response coordinates were taken from the location of the mouse click. In Experiment 2, gaze data were classified into saccades and fixations using Eyelink software, with velocity and acceleration thresholds of 30°/s and 8000°/s<sup>2</sup>, respectively. To calculate difference scores, the coordinates were taken from the <italic>x,y</italic> saccade landing location of the first saccade (greater than 2.5°) following stimulus onset. Saccade landing coordinates were averaged across the left eye and right eye recordings. Across participants, the median saccade onset latency was 195.5 ms ± 10.4 ms (SEM). Figure <xref rid="MOESM1" ref-type="media">S2</xref> in the Supplemental Materials shows the distribution of saccade latencies in Experiment 2. In addition, Fig. <xref rid="MOESM1" ref-type="media">S1</xref> shows the distribution of response errors (distances of mouse clicks and saccades from the patch center) across the three experiments.</p>
                <p id="Par19">Image value difference scores were calculated for eight image statistics (including three individual saliency channels). These measures fell into three broad information categories: luminance, edges (edge density, distance from the nearest edge, distance from labeled object boundaries), and saliency. Figure <xref rid="Fig2" ref-type="fig">2</xref>A shows 2-D maps illustrating each measure for the same image. To measure the effect of luminance on observers’ responses, we calculated difference scores based on the intensity value after blurring the image using a 2-D Gaussian kernel at each of five different standard deviations (σ): 0.25°, 0.5°, 1°, 2°, and 4°.<fig id="Fig2"><label>Figure 2</label><caption><p>(<bold>A</bold>) Influence of image features on the direction of localization errors within natural images. Bars indicate the mean difference score for each image statistic (examples shown in the left column) in each experiment (Experiments 1, 2, and 3 shown blue, red, and yellow, respectively), with scatter points representing individual observers (see text for description of the direction of each effect). To facilitate comparison across the different measures, all values are expressed as the z-score of the observed mean difference score relative to the null distribution produced by the permutation procedure illustrated in Fig. <xref rid="Fig1" ref-type="fig">1</xref>B. Asterisks on the right side of the figure panel indicate significant differences from 0 based on individual permutation tests adjusted for a false discovery rate of 0.05 using the Benjamini–Hochberg procedure (see Supplemental Fig. <xref rid="MOESM1" ref-type="media">S3</xref>B for histograms of the group null distributions). (<bold>B</bold>) Difference scores (in z-score units) for luminance, analyzed separately for each Gaussian kernel size (standard deviation, σ). (<bold>C</bold>) Difference scores for edge density at each kernel size. Error bars in each panel represent ± 1 standard error of the mean (SEM).</p></caption><graphic xlink:href="41598_2021_91006_Fig2_HTML" id="MO2"/></fig></p>
                <p id="Par20">Edge density was calculated by identifying the luminance edges in the image using the Canny edge detection algorithm, and then spatially smoothing the binary output using a Gaussian kernel with the same set of five standard deviations (0.25° through 4°). Higher values represent regions of the image with a greater density of edges. Edge distances were calculated by first extracting the luminance edges using the Canny algorithm, and then applying a Euclidean distance transform on the binary edge map to calculate the distance of each pixel from the nearest edge (i.e., higher values represent larger distances from the nearest edge). Similarly, distances to object boundaries were calculated from a binary map of all the outlines of the LabelMe annotations in the image. A Euclidean distance transform was applied to this map, producing a map of the distance of each pixel from the nearest edge.</p>
                <p id="Par21">Saliency was calculated using the procedures and parameters described in Itti, Koch, and Niebur<sup><xref ref-type="bibr" rid="CR27">27</xref></sup>, with saliency computed from the sum of three normalized conspicuity maps, representing color, intensity, and orientation contrast. To summarize, the model consisted of seven channels: two for color (red-green and blue-yellow color differences), one for intensity, and four for orientation (0°, 45°, 90°, and 135°). For each channel, maps across different spatial scales were generated using dyadic Gaussian pyramids and center-surround difference maps were calculated from paired differences across these spatial scales. We note that, although there have been many refinements to the original saliency model, with the goal of predicting fixation patterns in free-viewing of natural images (e.g.,<sup><xref ref-type="bibr" rid="CR33">33</xref></sup>), we selected this model as a simple way to summarize center-surround differences image content, and to quantify possible landmarks in the images.</p>
                <p id="Par22">Significance testing was performed using permutation tests (Fig. <xref rid="Fig1" ref-type="fig">1</xref>C), in which the observed values were compared to permuted null distributions, which account for general biases present across all images. For example, independent of image content, a subject might have a tendency to report locations that are below the target’s physical location, or positions toward the center of the image, which could coincide with changes in these image features. As shown in Fig. <xref rid="Fig1" ref-type="fig">1</xref>C, to control for these factors, each null distribution was calculated by randomly shuffling the relationship between the specific image and the response coordinates. For example, for the luminance difference score, the physical and reported patch locations on trial 1 might be used to calculate a difference score based on the luminance values on trial 7, and so on. This mapping was shuffled between trials and repeated for 1000 iterations to calculate a null distribution of differences. In order to report measurements of effect size on the same scale across the different measures, for each observer, we calculated a z-score, representing the number of standard deviations the observed value was from the center of the null distribution (see Fig. <xref rid="MOESM1" ref-type="media">S3</xref>A in the Supplemental Materials), and these values are shown in Fig. <xref rid="Fig2" ref-type="fig">2</xref>. For each measure, a group null distribution was calculated by averaging the values in the null distribution across observers, and <italic>p</italic> values were calculated from the proportion of difference scores in the resulting null distribution that were more extreme than the mean observed difference score (see Fig. <xref rid="MOESM1" ref-type="media">S3</xref>B). To correct for multiple comparisons, we ﻿controlled for a false discovery rate of 0.05 using the Benjamini–Hochberg procedure. Where difference scores were calculated for different spatial scales (luminance and edge density), we report difference scores averaged across all kernel sizes in Fig. <xref rid="Fig2" ref-type="fig">2</xref>A and separately for each spatial scale in Fig. <xref rid="Fig2" ref-type="fig">2</xref>B,C.</p>
                <p id="Par23">In addition, to visualize local variation in image statistics near the physical and reported stimulus locations, maps of each image statistic were averaged across trials. To account for variation in error direction and magnitude across trials, as shown in Fig. <xref rid="Fig3" ref-type="fig">3</xref>A, images were rotated and scaled prior to averaging, such that physical and perceived locations were aligned across trials. As shown in Fig. <xref rid="Fig3" ref-type="fig">3</xref>B, the coordinates (x = 0,y = 0) and (x = 1,y = 0) correspond to the physical and response locations, respectively. As before, to facilitate comparisons between different measures, values are reported as the z-score of the observed difference relative to the permuted null distribution.<fig id="Fig3"><label>Figure 3</label><caption><p>(<bold>A</bold>) Procedure for calculating averaged feature maps. Maps of each feature (in this example, luminance), were rotated and scaled to align the physical and response locations of each patch across all trials. The resulting maps were then averaged across trials, and across participants. (<bold>B</bold>) Averaged maps of each feature in Experiment 1. The coordinates x = 0, y = 0, indicated by the empty circle (○), correspond to the physical location of the patch, and the coordinates x = 1, y = 0, indicated by the plus symbol ( +) correspond to the location of the observer’s response. Following the procedures shown in Fig. <xref rid="Fig1" ref-type="fig">1</xref>C, values are shown as z-scores, calculated relative to a permuted null distribution of maps, where the SD for the calculation is the standard deviation of null distribution at the reported location. Asterisks indicate significant effects shown in Fig. <xref rid="Fig2" ref-type="fig">2</xref>.</p></caption><graphic xlink:href="41598_2021_91006_Fig3_HTML" id="MO3"/></fig></p>
              </sec>
            </sec>
            <sec id="Sec8">
              <title>Results</title>
              <sec id="Sec9">
                <title>Experiment 1: directional biases in observers’ localization errors</title>
                <p id="Par24">Figures <xref rid="MOESM1" ref-type="media">S1</xref> and <xref rid="MOESM1" ref-type="media">S2</xref> in the Supplemental Materials show the mean error magnitudes and latencies across the each of the three experiments. In each experiment, median localization errors (distance from the center of the patch ± SEM) were 1.08° ± 0.08°, 1.24° ± 0.07°, and 0.98° ± 0.05°, respectively. The primary goal of the analyses was to determine the influence of each image feature on these errors, by calculating a difference score based on the value of each feature at the physical stimulus location and the reported location. We isolated the image-specific effects using a procedure in which we compared the observed difference score to a permuted null distribution (Fig. <xref rid="Fig1" ref-type="fig">1</xref>C), produced by shuffling the mapping between images and their patch and response coordinates. This controlled for general biases (i.e., nonspecific effects) that might influence the difference scores—for example, a general downward bias<sup><xref ref-type="bibr" rid="CR34">34</xref></sup>, or idiosyncratic errors<sup><xref ref-type="bibr" rid="CR8">8</xref></sup>, which could coincide with changes in luminance, edge information, or saliency. The effects of any such biases would be present in the null distribution; comparing the observed difference score to this baseline controls for these non-specific effects.</p>
                <p id="Par25">Figure <xref rid="Fig2" ref-type="fig">2</xref> shows the observed difference score relative to this null distribution, expressed as a z-score, corresponding to the number of standard deviations of observed value from the center of the null distribution, and <italic>p</italic> values were calculated from null distributions observed at the group level (see Fig. <xref rid="MOESM1" ref-type="media">S3</xref> in the Supplemental Materials for the individual distributions). In Experiment 1 (mouse click responses, shown in blue), these revealed significant effects related to luminance, edge information, and saliency. Specifically, observers’ responses were pulled toward the darker areas in the images, with negative values in the graph indicating lower luminance values at the response location compared to the physical location of the target patch (mean of individual observer z-scores: − 2.18, permutation test: <italic>p</italic> &lt; 0.001). This analysis also revealed spatial tuning across different blur levels (Fig. <xref rid="Fig2" ref-type="fig">2</xref>B), with the largest effect observed with a Gaussian kernel with a standard deviation of 1° (mean z-score = -2.73). In other words, observers localized patches toward darker areas in the image, at a relatively coarse spatial scale, which may be a consequence of reduced acuity in the periphery. This is further supported by an additional analysis (Fig. <xref rid="MOESM1" ref-type="media">S5</xref> in the Supplemental Materials) which shows different spatial tuning for the patch eccentricity conditions. At 5° and 7.5° eccentricity, the largest effect was observed with a kernel size of 1°, while the largest effect at 10° eccentricity was observed with a kernel size of 4°.</p>
                <p id="Par26">In addition, we observed effects associated with edge information. For edge density, positive values in the graph indicate a bias toward areas of higher edge density (i.e., a higher edge density value at the response location compared to the physical location; z = 0.74, <italic>p</italic> = 0.003). As shown in Fig. <xref rid="Fig2" ref-type="fig">2</xref>B, this effect was largest at small kernel sizes. In addition, we observed significant effects of edge distance. Here, <italic>negative</italic> difference scores indicate lower edge distance values at the response location compared to the patch location, consistent with a bias <italic>toward</italic> edges. These values were significantly negative for edge distances based on both luminance-defined edges (z =  − 1.20; <italic>p</italic> &lt; 0.001), and edges defined by annotated objects in the LabelMe database (z =  − 1.14; <italic>p</italic> &lt; 0.001).</p>
                <p id="Par27">Finally, we observed significant effects of saliency, in which responses were pulled toward more salient regions of the image (z = 2.67). We tested whether this effect was driven by one saliency channel (color, intensity, or orientation) or whether these effects were seen across all channels. When the difference scores were calculated individually for each channel, in Experiment 1, we observed significant effects in each channel (color: z = 1.15, <italic>p</italic> = 0.001, intensity: z = 1.40, <italic>p</italic> &lt; 0.001, orientation: z = 0.86, <italic>p</italic> = 0.006).</p>
              </sec>
              <sec id="Sec10">
                <title>Experiment 2: Changing the response modality</title>
                <p id="Par28">One possibility is that these biases may be a direct or indirect consequence of the response modality used to measure position judgments. For example, they may be influenced by observers’ preference for cursor placement (e.g., the cursor may be more visible on certain areas). In addition, with manual responses, observers’ reports are somewhat delayed from stimulus presentation. Even though observers were able to initiate their responses once the target disappeared, the time required to perform the task resulted in a mean response latency of 987.3 ms. This may introduce additional distortions related to the memory of the target’s location. To test whether such factors could account for the observed biases, we carried out a second experiment in which observers were instructed to saccade to the patch as quickly as possible, and we analyzed the landing coordinates of the saccade in place of observers’ mouse clicks. This reduced the possibility of observers introducing biases related to cursor adjustment, and greatly reduced the response latency (with a median saccade onset latency of 195.5 ms).</p>
                <p id="Par29">Here we observed very similar effects to those in Experiment 1 (Fig. <xref rid="Fig2" ref-type="fig">2</xref>; red bars). Observers’ responses were significantly biased toward darker areas (z =  − 2.58, <italic>p</italic> &lt; 0.001), areas of high edge density (z = 1.44, <italic>p</italic> &lt; 0.001), as well as toward luminance edges (z =  − 0.96, <italic>p</italic> = 0.001), and object boundaries (z =  − 0.79, <italic>p</italic> = 0.005). The pattern of tuning for edge density was different for saccades compared to mouse clicks, with the largest effect at observed with a Gaussian kernel with a standard deviation of 2° (Fig. <xref rid="Fig2" ref-type="fig">2</xref>B), but nevertheless, this effect was robust for a range of kernel sizes. As in Experiment 1, observers’ responses were also pulled toward more salient regions of the image (z = 1.80, <italic>p</italic> &lt; 0.001). As in Experiment 1, these effects were significant when analyzed separately for each saliency channel (color: z = 0.69, <italic>p</italic> = 0.02, intensity: z = 0.64, <italic>p</italic> = 0.028, and orientation: z = 1.33, <italic>p</italic> &lt; 0.001, respectively).</p>
              </sec>
              <sec id="Sec11">
                <title>Experiment 3: Inversion</title>
                <p id="Par30">One possibility is that these effects may depend on high-level scene processing. We tested whether these effects persist when scene processing is disrupted, by making the scene more difficult to recognize. Previous work has shown that inversion interferes with object recognition<sup><xref ref-type="bibr" rid="CR35">35</xref></sup> and high-level scene processing across a number of experimental paradigms<sup><xref ref-type="bibr" rid="CR36">36</xref>–<xref ref-type="bibr" rid="CR38">38</xref></sup>. In addition, scene categorization performance is reduced in abnormally-colored images<sup><xref ref-type="bibr" rid="CR39">39</xref></sup>. The procedures in Experiment 3 were the same as Experiment 1, except images were color negatives of the originals that were presented upside-down (rotated 180°; see Fig. <xref rid="Fig1" ref-type="fig">1</xref>B). Using negatives also had the effect of changing the correlations between some of the image features (e.g., edge density and luminance; see <italic>Correlations between features</italic>). With these inverted images, we observed very similar effects of both luminance and saliency (Fig. <xref rid="Fig2" ref-type="fig">2</xref>; yellow bars), with responses pulled toward darker areas (z =  − 1.88, <italic>p</italic> &lt; 0.001) and more salient areas of the image (z = 2.20; <italic>p</italic> &lt; 0.001 for individual channels, color: z = 0.49, <italic>p</italic> = 0.10, intensity: z = 1.47, <italic>p</italic> &lt; 0.001, and orientation: z = 0.43, <italic>p</italic> = 0.15). Although the sign of the difference scores for some of the measures associated with edge information stayed in the same direction as observed in the other two experiments, two of the effects were no longer significant (z =  − 0.07, and z =  − 0.53 for edge density and object distance, respectively, <italic>p</italic> values ≥ 0.069, while the effect of luminance edge distance was significant (z =  − 0.66, <italic>p</italic> = 0.02).</p>
              </sec>
              <sec id="Sec12">
                <title>Mapping feature influences</title>
                <p id="Par31">The main analysis in Fig. <xref rid="Fig2" ref-type="fig">2</xref> shows the influence of local features using a two-point comparison based on the difference in feature values between the physical and reported locations of the patch. However, this only partially captures information about how these features influence position judgments within natural scene images. Do the changes in image statistics between the physical and reported locations of the patch follow a gradient (for example, uniformly decreasing luminance values between these two points), or are there more complex spatial patterns underlying these effects? For example, these effects could be driven by local minima/maxima of a given feature (e.g., small dark spot consistently surrounded by a light background), or there could be an interstitial feature (such as an edge) that appears consistently between the location of the patch and the response. Although such patterns are unlikely (as they would have to occur consistently across trials in order to produce these effects), we nevertheless created maps of feature influences for each effect shown in Fig. <xref rid="Fig3" ref-type="fig">3</xref>, to more clearly visualize our results.</p>
                <p id="Par32">To align images across trials, feature maps (e.g., luminance maps) were rotated and scaled such that the physical and reported locations of the target patch were co-registered across images, as shown in Fig. <xref rid="Fig3" ref-type="fig">3</xref>A. Figure <xref rid="Fig3" ref-type="fig">3</xref>B shows the final averaged maps for Experiment 1 (see Fig. <xref rid="MOESM1" ref-type="media">S4</xref> in the Supplemental Materials for the image maps for the other experiments), Within each panel, the coordinates x = 0, y = 0, indicated by the empty circle, correspond to the physical center of the patch, and x = 1, y = 0, indicated by the + symbol, correspond to the reported location. To facilitate comparisons with the bar graphs, values are expressed as z-scores relative to a permuted null distribution of mean feature maps, where the value at the physical location of the patch is fixed at zero, and the value shown at the reported location of the patch is the same as the values reported in Fig. <xref rid="Fig2" ref-type="fig">2</xref>. As shown in Fig. <xref rid="Fig3" ref-type="fig">3</xref>B, the spatial distribution of the image features around the physical and reported locations generally follows either an increasing or decreasing gradient. This indicates that the difference score based on the image values at these two points is generally sufficient to summarize the effects associated with each feature, and excludes the possibility of more unusual spatial patterns (e.g., small local minima/maxima, or interstitial features) underlying these effects.</p>
              </sec>
              <sec id="Sec13">
                <title>Correlations between features</title>
                <p id="Par33">How much do these features capture redundant information about image content? We would expect a high degree of redundancy among the measures that are within the general categories tested (luminance, edges, saliency). For example, edge density and edge distance are derived from the same measure, and the individual saliency channels are necessarily associated with the overall measure of saliency. How much redundancy is there within these different categories, versus between them? Fig. <xref rid="Fig4" ref-type="fig">4</xref> shows the correlations between the difference scores for individual trials for each of the measures shown in Fig. <xref rid="Fig2" ref-type="fig">2</xref> (N = 6760, N = 8403, and N = 8239 in Experiments 1, 2, and 3, respectively). As expected, difference scores that were within these general categories (e.g., within the edge-related measures and within the saliency channels) were highly correlated. Overall, however, we observed relatively low, but statistically significant correlations across pairs of difference scores across all the measures used (mean of absolute Fisher z-transformed correlation values = 0.13, or R<sup>2</sup> = 0.017). Are these internal correlations large enough to drive the patterns of results in Fig. <xref rid="Fig2" ref-type="fig">2</xref>? For example, edge density was significantly and negatively correlated with the intensity saliency channel (r =  − 0.19, <italic>p</italic> &lt; 0.0001), even though the effects for these two measures go in the same direction (both positive z-scores) in Experiment 1. This indicates that these low correlations between image features may not be sufficient to account for all of the effects that we observed.<fig id="Fig4"><label>Figure 4</label><caption><p>Correlation between image difference scores. Correlation values were calculated across all pairs of difference scores from all analyzed trials in each experiment (N = 6760, N = 8403, and N = 8239 in Experiments 1, 2, and 3, respectively). Diamond symbols indicate statistically significant correlations after controlling for a false discovery rate of 0.05 using the Benjamini–Hochberg procedure. Half-filled diamond symbols indicate correlations that are significantly above (upper-half filled) or below (lower-half filled) those in the permuted null distribution.</p></caption><graphic xlink:href="41598_2021_91006_Fig4_HTML" id="MO4"/></fig></p>
                <p id="Par34">We therefore directly tested whether these correlations drive the biases we observed using two different approaches. First, we isolated effects that were specific to the observer’s responses within the images, using the same permutation procedure described earlier. In other words, we compared the values in Fig. <xref rid="Fig4" ref-type="fig">4</xref> to those calculated from difference scores in the permuted null distribution, allowing us to identify effects that were specific to the images shown. We note that the correlations in the null distribution generally preserve the relationship between image features; in other words, features that were highly correlated within these images (e.g., correlations within the saliency channels) are also well-correlated in the null distribution. However, as shown in Fig. <xref rid="Fig4" ref-type="fig">4</xref>, we identified a small subset of correlation values (one pair of features in Experiment 1 and four in Experiment 3) where the observed correlation was significantly different from the null.</p>
                <p id="Par35">Second, if the correlations between pairs of difference scores derived from different measures (Fig. <xref rid="Fig4" ref-type="fig">4</xref>) are largely responsible for the effects we see in Fig. <xref rid="Fig2" ref-type="fig">2</xref>, we would expect this to emerge at the individual observer level as well. In other words, for pairs of difference scores that are positively correlated, observers who exhibit a larger effect in one measure (e.g., luminance) should also show a larger effect in another (e.g., edge distance). Figures <xref rid="Fig5" ref-type="fig">5</xref> and <xref rid="Fig6" ref-type="fig">6</xref> show the same correlations between pairs of measures at the observer level. Across these analyses, at the individual participant level, only three of the correlations were significant (between saliency and two of saliency channels). In addition, a number of the correlations at the observer level were in the opposite direction from those calculated from the individual difference scores. Together, this suggests that the effects we observe are likely due to a constellation of different influences rather than exclusively driven by a single feature.<fig id="Fig5"><label>Figure 5</label><caption><p>Between-participant correlations for all pairwise image features in Experiments 1 (N = 10) and 2 (N = 12). Each symbol represents the trial-averaged difference score (in z-units) for one participant in Experiments 1 (empty circle) and 2 (plus symbol). The correlations for Experiment 3, which used color negative images, are shown separately in Fig. <xref rid="Fig6" ref-type="fig">6</xref>. Filled oval regions represent 95% error ellipses, using the same color scale shown in Fig. <xref rid="Fig4" ref-type="fig">4</xref>. The asterisks (in two of the saliency-related measures) indicate a statistically significant correlation after controlling for a false discovery rate of 0.05 using the Benjamini–Hochberg procedure.</p></caption><graphic xlink:href="41598_2021_91006_Fig5_HTML" id="MO5"/></fig><fig id="Fig6"><label>Figure 6</label><caption><p>Between-participant correlations for all pairwise image features in Experiment 3 (N = 12). Each symbol represents the trial-averaged difference score (in z-units) for one participant. Filled oval regions represent 95% error ellipses, using the same color scale shown in Fig. <xref rid="Fig4" ref-type="fig">4</xref>. The asterisk (for the correlation between Saliency and Saliency: Intensity) indicates a statistically significant correlation after controlling for a false discovery rate of 0.05 using the Benjamini–Hochberg procedure.</p></caption><graphic xlink:href="41598_2021_91006_Fig6_HTML" id="MO6"/></fig></p>
              </sec>
            </sec>
            <sec id="Sec14">
              <title>Discussion</title>
              <p id="Par36">Our representation of an object’s location is heavily influenced by its visual context; however, much of the literature to date has examined these contextual influences with simple configurations and a limited number of objects. In contrast, this study measured the influence of local image features on observers’ position judgments of brief targets embedded within natural scene images, probing for these contextual effects across a number of perceptually relevant dimensions. The results of Experiment 1 indicated that observers’ responses were pulled toward darker image regions, as well as toward edges, and more salient regions of the images (Fig. <xref rid="Fig2" ref-type="fig">2</xref>). Critically, a follow-up experiment indicated that these errors were consistent across response modalities, even when responses occurred at very different time scales (987 ms on average for cursor adjustment versus 196 ms for saccades). In addition, a number of the reported effects persisted when the images were replaced with color negatives of the originals and physically inverted, manipulations which interfere with recognition<sup><xref ref-type="bibr" rid="CR35">35</xref>,<xref ref-type="bibr" rid="CR37">37</xref>–<xref ref-type="bibr" rid="CR39">39</xref></sup>, and which changed the internal correlations between some of the image features (Fig. <xref rid="Fig4" ref-type="fig">4</xref>). Across all three experiments, we controlled for general directional biases in observers’ responses that coincide with changes in image features using a permutation-based analysis. This approach allows us to account for non-specific effects (e.g., response biases towards the center of the image, or towards the ground) and isolate those that are particular to the images seen on each trial.</p>
              <p id="Par37">Collectively, these results point to gravitational effects within natural scenes that are analogous to those observed with more simple configurations. Importantly, the gravitational landmarks are sources of information in the image—as we observed in each experiment, observers’ responses were generally biased away from sparse or featureless regions of the images, and toward edges or other informative content in the scene. In describing these effects, we generally refer to them as pull ‘toward’ rather than ‘away’ from specific features (e.g., a pull toward dark areas), to match the language used in other research describing landmark or anchor effects. However, we note that in principle, it could be described as biases away from other features (e.g., a pull away from bright areas). One possibility is that this constellation of effects may reflect priors for where objects are likely to be located within a scene. In many everyday tasks, observers need to make decisions regarding object location under conditions of noise or uncertainty, and these decisions could reflect the underlying assumption that a brief target would most likely be physically near other objects in the environment. The effects that we observed may point to some of the perceptually-relevant features that underlie these assumptions.</p>
              <p id="Par38">Although the effects of edges and saliency are largely consistent with an explanation based on local landmarks, the effects associated with luminance are more surprising. One possible account is that this may reflect a general assumption that darker regions are also associated with objects rather than empty space. However, this tendency for observers to report that the target was located on a darker portion of the image persisted when color negatives were used. Here, portions of the image that were previously higher luminance were now lower luminance in this experiment, indicating that this bias was independent of the identity of the objects in the image. Another possible explanation for this error pattern is that it may be related to previously reported black-white asymmetries seen in a number of different visual processes, including detection, motion perception, and grouping and object perception<sup><xref ref-type="bibr" rid="CR40">40</xref>–<xref ref-type="bibr" rid="CR42">42</xref></sup>, which generally show that luminance decrements are processed more effectively than increments. In addition, we note that there are other image-related factors, such as our use of a linearized monitor, as well as specific photographic attributes (e.g., exposure, dynamic range), that could have enhanced the subjective saliency of dark regions and contributed to the observed bias. Differentiating between these possible accounts would require further investigation.</p>
              <p id="Par39">Although we observed some of these effects (e.g., luminance) consistently across all experiments, not all effects survived the image manipulations in Experiment 3, in which the scene photographs were inverted and color-negatives of the originals. Specifically, gravitational biases toward two of the edge-related measures (edge density and annotated edge distance) were not observed in Experiment 3. One possibility is that these effects may depend on high-level object recognition, and therefore may be more likely to be disrupted by the inversion and color manipulations, particularly if identifying the outline of an annotated object is linked to recognition. Further work comparing the sizes of these effects within the same observers will be important to establish whether disrupting object recognition significantly changes the gravitational biases we observed.</p>
              <p id="Par40">Taken together, these results also complement previous findings in the literature, showing landmark effects in pointing or cursor movements. As we discuss, when reproducing the locations of previously seen targets, observers’ responses are pulled toward nearby landmarks or anchors, although more complex patterns can emerge when multiple landmarks are present<sup><xref ref-type="bibr" rid="CR13">13</xref>,<xref ref-type="bibr" rid="CR43">43</xref></sup>. This landmark effect in pointing or cursor responses also resembles another common observation in saccades—the global effect, or the tendency for saccades endpoints to fall near the center of mass when multiple objects are present<sup><xref ref-type="bibr" rid="CR44">44</xref>–<xref ref-type="bibr" rid="CR46">46</xref></sup>. Here, we observed that saccade landing locations were directed to the same set of features as observers’ cursor responses, toward edges or other informative content in the scene, which could be a result of averaging or assimilation between the target and these features in the environment. However, previous work has also shown that the global effect varies with the timing of the target and distractor, with no global effect observed for saccades to a brief target when the distractor is continuously visible prior to target onset<sup><xref ref-type="bibr" rid="CR47">47</xref></sup>, as was the case in our experiments. It is therefore possible that the saccade errors observed here reflect different mechanisms than the global effect in saccades.</p>
              <p>In addition, our results demonstrating that saccade landing positions are directed to more salient regions of the image are broadly consistent with saliency models to predict eye movements during free-viewing or more naturalistic tasks<sup><xref ref-type="bibr" rid="CR27">27</xref>,<xref ref-type="bibr" rid="CR33">33</xref></sup>, however this work has not generally examined the relationship between saliency and error in reflexive saccades to unrelated targets. We also note that we observed effects of saliency on observers’ responses across both short and long time scales (i.e., saccades and mouse clicks), whereas previous work has shown that the effects of saliency on eye movements are larger at shorter timescales<sup><xref ref-type="bibr" rid="CR48">48</xref>–<xref ref-type="bibr" rid="CR50">50</xref></sup>. However, in all our experiments, observers previewed the scene while maintaining central fixation for 500–1250 ms prior to patch onset (Fig. <xref rid="Fig1" ref-type="fig">1</xref>A), and it is possible that removing or shortening this initial fixation in future work might reveal variation in the effect of saliency or other visual features on observers’ responses at different time scales.</p>
              <p id="Par41">Another related line of work has used similar approaches to identify category-based biases in reporting the remembered location of an object. Specifically, the category adjustment model of spatial memory<sup><xref ref-type="bibr" rid="CR12">12</xref></sup> posits that memory-based reports of an object’s spatial location are weighted by categorical information, where position judgments are biased toward central, or prototypical locations (e.g., the center of a quadrant within a circle). Holden et al.<sup><xref ref-type="bibr" rid="CR51">51</xref></sup> demonstrated that this model can be applied to natural scene images. When reporting the locations of a target dot previously shown within a scene image in a delayed response task (with an intervening image shown during retention), observers’ errors were pulled toward the center of color-defined clusters in the images. Unlike the present study, these effects were altered when inverted and color negative images were used, suggesting that those effects may depend on higher-level processes related to the extraction of semantic information from those images. More recently, using a serial reproduction technique, Langlois et al.<sup><xref ref-type="bibr" rid="CR52">52</xref></sup> demonstrate that memory-based spatial judgments are pulled toward regions in natural images where position discrimination accuracy is high. While it remains to be seen to what extent memory is involved here, we note that we observed similar effects for immediate saccades, which were on a very rapid timescale, and where any influence of memory is likely to be minimal. As the targets here were intentionally selected to be brief and diffuse to produce a high degree of uncertainty in position judgments, these responses necessarily occurred following stimulus disappearance. Regardless of whether memory is involved, the effects we observed occurred for immediate judgments of spatial location, which are highly relevant for localization in more naturalistic settings.</p>
              <p id="Par42">While these results indicate an assimilation or gravitational effect in natural scene images similar to those observed in more simple configurations, one possible interpretation is that biases may be an artifact of poor visibility of the targets on some of the backgrounds we used. The procedure for creating the target patches (using color negatives and scrambling the pixels) was intended to produce high visibility across a range of backgrounds, and the very low percentage of trials in which observers indicated not seeing the patch (less than 1% of trials) suggests that this was the case. Nevertheless, it is possible that for a given target patch, different portions may be more or less visible (e.g., a patch that straddles the boundary between two different surfaces in the image) which would bias the apparent location of its center. However, this account mostly generates predictions <italic>opposite</italic> to the effects observed in the present experiments. Targets would be expected to be less visible or identifiable in areas where the density of local features is higher<sup><xref ref-type="bibr" rid="CR21">21</xref>,<xref ref-type="bibr" rid="CR22">22</xref></sup>. For example, a target patch positioned at the boundary of a densely-textured portion of the image and a relatively sparse region would be more visible in the sparely-textured region, biasing position judgments <italic>away</italic> from areas where it is less visible. These effects are therefore unlikely to be a result of visibility-related stimulus artifacts.</p>
              <p id="Par43">Together, these results identify features within natural scene images that influence observers’ immediate reports of spatial location. Analogous to classic position landmark or assimilation effects, observers’ responses were pulled toward salient or informative landmarks within natural scene images, in a manner that was image-specific and independent of response modality, and robust to manipulations that interfered with identification, but preserved the spatial structure of the scene. While we observed these effects for immediate responses, further work will be required to determine the extent to which these relate to category-based effects in spatial memory, or priors for the distribution of objects within natural scenes. Another remaining question is the extent to which these effects depend on the identity of the target. Although the targets used in this study were defined by low-level features, using annotated objects as targets in future studies would allow examination of how identity-specific priors for target location might interact with scene information to influence position judgments.</p>
            </sec>
            <sec sec-type="supplementary-material">
              <title>Supplementary Information</title>
              <sec id="Sec15">
                <p>
                  <supplementary-material content-type="local-data" id="MOESM1">
                    <media xlink:href="41598_2021_91006_MOESM1_ESM.docx">
                      <caption>
                        <p>Supplementary Information.</p>
                      </caption>
                    </media>
                  </supplementary-material>
                </p>
              </sec>
            </sec>
          </body>
          <back>
            <fn-group>
              <fn>
                <p>
                  <bold>Publisher's note</bold>
                </p>
                <p>Springer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations.</p>
              </fn>
            </fn-group>
            <sec>
              <title>Supplementary Information</title>
              <p>The online version contains supplementary material available at 10.1038/s41598-021-91006-8.</p>
            </sec>
            <ack>
              <title>Acknowledgements</title>
              <p>This work was supported by funding from the National Institutes of Health (R01 EY029713 to P. J. B. and F32 EY028814 to A. K.). The authors thank Benjamin Wolfe for his comments on an earlier draft of the manuscript, as well as Leah Brodie for her assistance with data collection and analysis, and Koushik Sridhar for his assistance with programming and data collection as part of the Research Science Institute (RSI).</p>
            </ack>
            <notes notes-type="author-contribution">
              <title>Author contributions</title>
              <p>A.K. and P.J.B. developed the study concept and designed the experiments. A.K. programmed the experiments, collected the data, and analyzed the results. A.K. and P.J.B. drafted and edited the manuscript.</p>
            </notes>
            <notes notes-type="data-availability">
              <title>Data availability</title>
              <p>All data and materials for the study are available on the Open Science Framework online at: <ext-link ext-link-type="uri" xlink:href="https://osf.io/d7qrv/">https://osf.io/d7qrv/</ext-link>.</p>
            </notes>
            <notes id="FPar1" notes-type="COI-statement">
              <title>Competing interests</title>
              <p id="Par44">The authors declare no competing interests.</p>
            </notes>
            <ref-list id="Bib1">
              <title>References</title>
              <ref id="CR1">
                <label>1.</label>
                <element-citation publication-type="journal">
                  <person-group person-group-type="author">
                    <name>
                      <surname>Ross</surname>
                      <given-names>J</given-names>
                    </name>
                    <name>
                      <surname>Morrone</surname>
                      <given-names>MC</given-names>
                    </name>
                    <name>
                      <surname>Burr</surname>
                      <given-names>DC</given-names>
                    </name>
                  </person-group>
                  <article-title>Compression of visual space before saccades</article-title>
                  <source>Nature</source>
                  <year>1997</year>
                  <volume>386</volume>
                  <fpage>598</fpage>
                  <lpage>601</lpage>
                  <pub-id pub-id-type="doi">10.1038/386598a0</pub-id>
                  <?supplied-pmid 9121581?>
                  <pub-id pub-id-type="pmid">9121581</pub-id>
                </element-citation>
              </ref>
              <ref id="CR2">
                <label>2.</label>
                <element-citation publication-type="journal">
                  <person-group person-group-type="author">
                    <name>
                      <surname>De Valois</surname>
                      <given-names>RL</given-names>
                    </name>
                    <name>
                      <surname>De Valois</surname>
                      <given-names>KK</given-names>
                    </name>
                  </person-group>
                  <article-title>Vernier acuity with stationary moving Gabors</article-title>
                  <source>Vis. Res.</source>
                  <year>1991</year>
                  <volume>31</volume>
                  <fpage>1619</fpage>
                  <lpage>1626</lpage>
                  <pub-id pub-id-type="doi">10.1016/0042-6989(91)90138-U</pub-id>
                  <?supplied-pmid 1949630?>
                  <pub-id pub-id-type="pmid">1949630</pub-id>
                </element-citation>
              </ref>
              <ref id="CR3">
                <label>3.</label>
                <element-citation publication-type="journal">
                  <person-group person-group-type="author">
                    <name>
                      <surname>Ramachandran</surname>
                      <given-names>VS</given-names>
                    </name>
                    <name>
                      <surname>Anstis</surname>
                      <given-names>SM</given-names>
                    </name>
                  </person-group>
                  <article-title>Illusory displacement of equiluminous kinetic edges</article-title>
                  <source>Perception</source>
                  <year>1990</year>
                  <volume>19</volume>
                  <fpage>611</fpage>
                  <lpage>616</lpage>
                  <pub-id pub-id-type="doi">10.1068/p190611</pub-id>
                  <?supplied-pmid 2102995?>
                  <pub-id pub-id-type="pmid">2102995</pub-id>
                </element-citation>
              </ref>
              <ref id="CR4">
                <label>4.</label>
                <element-citation publication-type="journal">
                  <person-group person-group-type="author">
                    <name>
                      <surname>Whitney</surname>
                      <given-names>D</given-names>
                    </name>
                    <name>
                      <surname>Cavanagh</surname>
                      <given-names>P</given-names>
                    </name>
                  </person-group>
                  <article-title>Motion distorts visual space: Shifting the perceived position of remote stationary objects</article-title>
                  <source>Nat. Neurosci.</source>
                  <year>2000</year>
                  <volume>3</volume>
                  <fpage>954</fpage>
                  <lpage>959</lpage>
                  <pub-id pub-id-type="doi">10.1038/78878</pub-id>
                  <?supplied-pmid 10966628?>
                  <pub-id pub-id-type="pmid">10966628</pub-id>
                </element-citation>
              </ref>
              <ref id="CR5">
                <label>5.</label>
                <element-citation publication-type="journal">
                  <person-group person-group-type="author">
                    <name>
                      <surname>Whitaker</surname>
                      <given-names>D</given-names>
                    </name>
                    <name>
                      <surname>McGraw</surname>
                      <given-names>PV</given-names>
                    </name>
                    <name>
                      <surname>Levi</surname>
                      <given-names>DM</given-names>
                    </name>
                  </person-group>
                  <article-title>The influence of adaptation on perceived visual location</article-title>
                  <source>Vis. Res.</source>
                  <year>1997</year>
                  <volume>37</volume>
                  <fpage>2207</fpage>
                  <lpage>2216</lpage>
                  <pub-id pub-id-type="doi">10.1016/S0042-6989(97)00030-8</pub-id>
                  <?supplied-pmid 9578903?>
                  <pub-id pub-id-type="pmid">9578903</pub-id>
                </element-citation>
              </ref>
              <ref id="CR6">
                <label>6.</label>
                <element-citation publication-type="journal">
                  <person-group person-group-type="author">
                    <name>
                      <surname>Manassi</surname>
                      <given-names>M</given-names>
                    </name>
                    <name>
                      <surname>Liberman</surname>
                      <given-names>A</given-names>
                    </name>
                    <name>
                      <surname>Kosovicheva</surname>
                      <given-names>A</given-names>
                    </name>
                    <name>
                      <surname>Zhang</surname>
                      <given-names>K</given-names>
                    </name>
                    <name>
                      <surname>Whitney</surname>
                      <given-names>D</given-names>
                    </name>
                  </person-group>
                  <article-title>Serial dependence in position occurs at the time of perception</article-title>
                  <source>Psychon. Bull. Rev.</source>
                  <year>2018</year>
                  <volume>25</volume>
                  <fpage>2245</fpage>
                  <lpage>2253</lpage>
                  <pub-id pub-id-type="doi">10.3758/s13423-018-1454-5</pub-id>
                  <?supplied-pmid 29582377?>
                  <pub-id pub-id-type="pmid">29582377</pub-id>
                </element-citation>
              </ref>
              <ref id="CR7">
                <label>7.</label>
                <element-citation publication-type="journal">
                  <person-group person-group-type="author">
                    <name>
                      <surname>Suzuki</surname>
                      <given-names>S</given-names>
                    </name>
                    <name>
                      <surname>Cavanagh</surname>
                      <given-names>P</given-names>
                    </name>
                  </person-group>
                  <article-title>Focused attention distorts visual space: An attentional repulsion effect</article-title>
                  <source>J. Exp. Psychol. Hum. Percept. Perform.</source>
                  <year>1997</year>
                  <volume>23</volume>
                  <fpage>443</fpage>
                  <lpage>463</lpage>
                  <pub-id pub-id-type="doi">10.1037/0096-1523.23.2.443</pub-id>
                  <?supplied-pmid 9104004?>
                  <pub-id pub-id-type="pmid">9104004</pub-id>
                </element-citation>
              </ref>
              <ref id="CR8">
                <label>8.</label>
                <element-citation publication-type="journal">
                  <person-group person-group-type="author">
                    <name>
                      <surname>Kosovicheva</surname>
                      <given-names>A</given-names>
                    </name>
                    <name>
                      <surname>Whitney</surname>
                      <given-names>D</given-names>
                    </name>
                  </person-group>
                  <article-title>Stable individual signatures in object localization</article-title>
                  <source>Curr. Biol.</source>
                  <year>2017</year>
                  <volume>27</volume>
                  <fpage>R700</fpage>
                  <lpage>R701</lpage>
                  <pub-id pub-id-type="doi">10.1016/j.cub.2017.06.001</pub-id>
                  <?supplied-pmid 28743014?>
                  <pub-id pub-id-type="pmid">28743014</pub-id>
                </element-citation>
              </ref>
              <ref id="CR9">
                <label>9.</label>
                <element-citation publication-type="journal">
                  <person-group person-group-type="author">
                    <name>
                      <surname>Fischer</surname>
                      <given-names>J</given-names>
                    </name>
                    <name>
                      <surname>Whitney</surname>
                      <given-names>D</given-names>
                    </name>
                  </person-group>
                  <article-title>Serial dependence in visual perception</article-title>
                  <source>Nat. Neurosci.</source>
                  <year>2014</year>
                  <volume>17</volume>
                  <fpage>738</fpage>
                  <lpage>743</lpage>
                  <pub-id pub-id-type="doi">10.1038/nn.3689</pub-id>
                  <?supplied-pmid 24686785?>
                  <pub-id pub-id-type="pmid">24686785</pub-id>
                </element-citation>
              </ref>
              <ref id="CR10">
                <label>10.</label>
                <mixed-citation publication-type="other">Rock, I. The frame of reference. in <italic>The Legacy of Solomon Asch</italic> 243–68 (1990).</mixed-citation>
              </ref>
              <ref id="CR11">
                <label>11.</label>
                <element-citation publication-type="journal">
                  <person-group person-group-type="author">
                    <name>
                      <surname>Roelofs</surname>
                      <given-names>CO</given-names>
                    </name>
                  </person-group>
                  <article-title>Die optische Lokalisation</article-title>
                  <source>Arch. für Augenheilkd.</source>
                  <year>1935</year>
                  <volume>109</volume>
                  <fpage>395</fpage>
                  <lpage>415</lpage>
                </element-citation>
              </ref>
              <ref id="CR12">
                <label>12.</label>
                <element-citation publication-type="journal">
                  <person-group person-group-type="author">
                    <name>
                      <surname>Huttenlocher</surname>
                      <given-names>J</given-names>
                    </name>
                    <name>
                      <surname>Hedges</surname>
                      <given-names>LV</given-names>
                    </name>
                    <name>
                      <surname>Duncan</surname>
                      <given-names>S</given-names>
                    </name>
                  </person-group>
                  <article-title>Categories and particulars: prototype effects in estimating spatial location</article-title>
                  <source>Psychol. Rev.</source>
                  <year>1991</year>
                  <volume>98</volume>
                  <fpage>352</fpage>
                  <lpage>376</lpage>
                  <pub-id pub-id-type="doi">10.1037/0033-295X.98.3.352</pub-id>
                  <?supplied-pmid 1891523?>
                  <pub-id pub-id-type="pmid">1891523</pub-id>
                </element-citation>
              </ref>
              <ref id="CR13">
                <label>13.</label>
                <element-citation publication-type="journal">
                  <person-group person-group-type="author">
                    <name>
                      <surname>Diedrichsen</surname>
                      <given-names>J</given-names>
                    </name>
                    <name>
                      <surname>Werner</surname>
                      <given-names>S</given-names>
                    </name>
                    <name>
                      <surname>Schmidt</surname>
                      <given-names>T</given-names>
                    </name>
                    <name>
                      <surname>Trommershauser</surname>
                      <given-names>J</given-names>
                    </name>
                  </person-group>
                  <article-title>Immediate spatial distortions of pointing movements induced by visual landmarks</article-title>
                  <source>Percept. Psychophys.</source>
                  <year>2004</year>
                  <volume>66</volume>
                  <fpage>89</fpage>
                  <lpage>103</lpage>
                  <pub-id pub-id-type="doi">10.3758/BF03194864</pub-id>
                  <?supplied-pmid 15095943?>
                  <pub-id pub-id-type="pmid">15095943</pub-id>
                </element-citation>
              </ref>
              <ref id="CR14">
                <label>14.</label>
                <element-citation publication-type="journal">
                  <person-group person-group-type="author">
                    <name>
                      <surname>Werner</surname>
                      <given-names>S</given-names>
                    </name>
                    <name>
                      <surname>Diedrichsen</surname>
                      <given-names>J</given-names>
                    </name>
                  </person-group>
                  <article-title>The time course of spatial memory distortions</article-title>
                  <source>Mem. Cognit.</source>
                  <year>2002</year>
                  <volume>30</volume>
                  <fpage>718</fpage>
                  <lpage>730</lpage>
                  <pub-id pub-id-type="doi">10.3758/BF03196428</pub-id>
                  <?supplied-pmid 12219889?>
                  <pub-id pub-id-type="pmid">12219889</pub-id>
                </element-citation>
              </ref>
              <ref id="CR15">
                <label>15.</label>
                <element-citation publication-type="journal">
                  <person-group person-group-type="author">
                    <name>
                      <surname>Prinzmetal</surname>
                      <given-names>W</given-names>
                    </name>
                  </person-group>
                  <article-title>Location perception: the X-Files parable</article-title>
                  <source>Percept. Psychophys.</source>
                  <year>2005</year>
                  <volume>67</volume>
                  <fpage>48</fpage>
                  <lpage>71</lpage>
                  <pub-id pub-id-type="doi">10.3758/BF03195012</pub-id>
                  <?supplied-pmid 15912872?>
                  <pub-id pub-id-type="pmid">15912872</pub-id>
                </element-citation>
              </ref>
              <ref id="CR16">
                <label>16.</label>
                <element-citation publication-type="journal">
                  <person-group person-group-type="author">
                    <name>
                      <surname>Sheth</surname>
                      <given-names>BR</given-names>
                    </name>
                    <name>
                      <surname>Shimojo</surname>
                      <given-names>S</given-names>
                    </name>
                  </person-group>
                  <article-title>Compression of space in visual memory</article-title>
                  <source>Vis. Res.</source>
                  <year>2001</year>
                  <volume>41</volume>
                  <fpage>329</fpage>
                  <lpage>341</lpage>
                  <pub-id pub-id-type="doi">10.1016/S0042-6989(00)00230-3</pub-id>
                  <?supplied-pmid 11164448?>
                  <pub-id pub-id-type="pmid">11164448</pub-id>
                </element-citation>
              </ref>
              <ref id="CR17">
                <label>17.</label>
                <element-citation publication-type="journal">
                  <person-group person-group-type="author">
                    <name>
                      <surname>Hubbard</surname>
                      <given-names>TL</given-names>
                    </name>
                    <name>
                      <surname>Ruppel</surname>
                      <given-names>SE</given-names>
                    </name>
                  </person-group>
                  <article-title>Spatial memory averaging, the landmark attraction effect, and representational gravity</article-title>
                  <source>Psychol. Res.</source>
                  <year>2000</year>
                  <volume>64</volume>
                  <fpage>41</fpage>
                  <lpage>55</lpage>
                  <pub-id pub-id-type="doi">10.1007/s004260000029</pub-id>
                  <?supplied-pmid 11109866?>
                  <pub-id pub-id-type="pmid">11109866</pub-id>
                </element-citation>
              </ref>
              <ref id="CR18">
                <label>18.</label>
                <element-citation publication-type="journal">
                  <person-group person-group-type="author">
                    <name>
                      <surname>Makovski</surname>
                      <given-names>T</given-names>
                    </name>
                    <name>
                      <surname>Swallow</surname>
                      <given-names>KM</given-names>
                    </name>
                    <name>
                      <surname>Jiang</surname>
                      <given-names>YV</given-names>
                    </name>
                  </person-group>
                  <article-title>The visual attractor illusion</article-title>
                  <source>J. Vis.</source>
                  <year>2010</year>
                  <volume>10</volume>
                  <fpage>1</fpage>
                  <lpage>16</lpage>
                  <pub-id pub-id-type="doi">10.1167/10.1.1</pub-id>
                  <?supplied-pmid 20143894?>
                  <pub-id pub-id-type="pmid">20143894</pub-id>
                </element-citation>
              </ref>
              <ref id="CR19">
                <label>19.</label>
                <mixed-citation publication-type="other">Naito, S. &amp; Cole, J. B. The Gravity Lens Illusion and its Mathematical Model. 39–50 (1994). 10.1007/978-1-4612-4308-3_3.</mixed-citation>
              </ref>
              <ref id="CR20">
                <label>20.</label>
                <element-citation publication-type="journal">
                  <person-group person-group-type="author">
                    <name>
                      <surname>Zimmermann</surname>
                      <given-names>E</given-names>
                    </name>
                    <name>
                      <surname>Fink</surname>
                      <given-names>G</given-names>
                    </name>
                    <name>
                      <surname>Cavanagh</surname>
                      <given-names>P</given-names>
                    </name>
                  </person-group>
                  <article-title>Perifoveal spatial compression</article-title>
                  <source>J. Vis.</source>
                  <year>2013</year>
                  <volume>13</volume>
                  <fpage>1</fpage>
                  <lpage>9</lpage>
                  <pub-id pub-id-type="doi">10.1167/13.5.21</pub-id>
                </element-citation>
              </ref>
              <ref id="CR21">
                <label>21.</label>
                <element-citation publication-type="journal">
                  <person-group person-group-type="author">
                    <name>
                      <surname>Bex</surname>
                      <given-names>PJ</given-names>
                    </name>
                    <name>
                      <surname>Solomon</surname>
                      <given-names>SG</given-names>
                    </name>
                    <name>
                      <surname>Dakin</surname>
                      <given-names>SC</given-names>
                    </name>
                  </person-group>
                  <article-title>Contrast sensitivity in natural scenes depends on edge as well as spatial frequency structure</article-title>
                  <source>J. Vis.</source>
                  <year>2009</year>
                  <volume>9</volume>
                  <fpage>1</fpage>
                  <lpage>19</lpage>
                  <pub-id pub-id-type="doi">10.1167/9.10.1</pub-id>
                  <?supplied-pmid 20053108?>
                  <pub-id pub-id-type="pmid">20053108</pub-id>
                </element-citation>
              </ref>
              <ref id="CR22">
                <label>22.</label>
                <element-citation publication-type="journal">
                  <person-group person-group-type="author">
                    <name>
                      <surname>Wallis</surname>
                      <given-names>TSA</given-names>
                    </name>
                    <name>
                      <surname>Bex</surname>
                      <given-names>PJ</given-names>
                    </name>
                  </person-group>
                  <article-title>Image correlates of crowding in natural scenes</article-title>
                  <source>J. Vis.</source>
                  <year>2012</year>
                  <volume>12</volume>
                  <fpage>6</fpage>
                  <lpage>6</lpage>
                  <pub-id pub-id-type="doi">10.1167/12.7.6</pub-id>
                  <?supplied-pmid 22798053?>
                  <pub-id pub-id-type="pmid">22798053</pub-id>
                </element-citation>
              </ref>
              <ref id="CR23">
                <label>23.</label>
                <element-citation publication-type="journal">
                  <person-group person-group-type="author">
                    <name>
                      <surname>Bex</surname>
                      <given-names>PJ</given-names>
                    </name>
                  </person-group>
                  <article-title>(In) Sensitivity to spatial distortion in natural scenes</article-title>
                  <source>J. Vis.</source>
                  <year>2010</year>
                  <volume>10</volume>
                  <fpage>1</fpage>
                  <lpage>15</lpage>
                  <pub-id pub-id-type="doi">10.1167/10.2.23</pub-id>
                  <?supplied-pmid 20462324?>
                  <pub-id pub-id-type="pmid">20462324</pub-id>
                </element-citation>
              </ref>
              <ref id="CR24">
                <label>24.</label>
                <element-citation publication-type="journal">
                  <person-group person-group-type="author">
                    <name>
                      <surname>Parkhurst</surname>
                      <given-names>D</given-names>
                    </name>
                    <name>
                      <surname>Law</surname>
                      <given-names>K</given-names>
                    </name>
                    <name>
                      <surname>Niebur</surname>
                      <given-names>E</given-names>
                    </name>
                  </person-group>
                  <article-title>Modeling the role of salience in the allocation of overt visual attention</article-title>
                  <source>Vis. Res.</source>
                  <year>2002</year>
                  <volume>42</volume>
                  <fpage>107</fpage>
                  <lpage>123</lpage>
                  <pub-id pub-id-type="doi">10.1016/S0042-6989(01)00250-4</pub-id>
                  <?supplied-pmid 11804636?>
                  <pub-id pub-id-type="pmid">11804636</pub-id>
                </element-citation>
              </ref>
              <ref id="CR25">
                <label>25.</label>
                <element-citation publication-type="journal">
                  <person-group person-group-type="author">
                    <name>
                      <surname>Itti</surname>
                      <given-names>L</given-names>
                    </name>
                    <name>
                      <surname>Koch</surname>
                      <given-names>C</given-names>
                    </name>
                  </person-group>
                  <article-title>A saliency-based search mechanism for overt and covert shifts of visual attention</article-title>
                  <source>Vis. Res.</source>
                  <year>2000</year>
                  <volume>40</volume>
                  <fpage>1489</fpage>
                  <lpage>1506</lpage>
                  <pub-id pub-id-type="doi">10.1016/S0042-6989(99)00163-7</pub-id>
                  <?supplied-pmid 10788654?>
                  <pub-id pub-id-type="pmid">10788654</pub-id>
                </element-citation>
              </ref>
              <ref id="CR26">
                <label>26.</label>
                <element-citation publication-type="journal">
                  <person-group person-group-type="author">
                    <name>
                      <surname>Russell</surname>
                      <given-names>BC</given-names>
                    </name>
                    <name>
                      <surname>Torralba</surname>
                      <given-names>A</given-names>
                    </name>
                    <name>
                      <surname>Murphy</surname>
                      <given-names>KP</given-names>
                    </name>
                    <name>
                      <surname>Freeman</surname>
                      <given-names>WT</given-names>
                    </name>
                  </person-group>
                  <article-title>LabelMe: A database and web-based tool for image annotation</article-title>
                  <source>Int. J. Comput. Vis.</source>
                  <year>2008</year>
                  <volume>77</volume>
                  <fpage>157</fpage>
                  <lpage>173</lpage>
                  <pub-id pub-id-type="doi">10.1007/s11263-007-0090-8</pub-id>
                </element-citation>
              </ref>
              <ref id="CR27">
                <label>27.</label>
                <element-citation publication-type="journal">
                  <person-group person-group-type="author">
                    <name>
                      <surname>Itti</surname>
                      <given-names>L</given-names>
                    </name>
                    <name>
                      <surname>Koch</surname>
                      <given-names>C</given-names>
                    </name>
                    <name>
                      <surname>Niebur</surname>
                      <given-names>E</given-names>
                    </name>
                  </person-group>
                  <article-title>A model of saliency-based visual attention for rapid scene analysis</article-title>
                  <source>IEEE Trans. Pattern Anal. Mach. Intell.</source>
                  <year>1998</year>
                  <volume>20</volume>
                  <fpage>1254</fpage>
                  <lpage>1259</lpage>
                  <pub-id pub-id-type="doi">10.1109/34.730558</pub-id>
                </element-citation>
              </ref>
              <ref id="CR28">
                <label>28.</label>
                <element-citation publication-type="journal">
                  <person-group person-group-type="author">
                    <name>
                      <surname>Cornelissen</surname>
                      <given-names>FW</given-names>
                    </name>
                    <name>
                      <surname>Peters</surname>
                      <given-names>EM</given-names>
                    </name>
                    <name>
                      <surname>Palmer</surname>
                      <given-names>J</given-names>
                    </name>
                  </person-group>
                  <article-title>The Eyelink toolbox: eye tracking with MATLAB and the psychophysics toolbox</article-title>
                  <source>Behav. Res. Methods Instrum. Comput.</source>
                  <year>2002</year>
                  <volume>34</volume>
                  <fpage>613</fpage>
                  <lpage>617</lpage>
                  <pub-id pub-id-type="doi">10.3758/BF03195489</pub-id>
                  <?supplied-pmid 12564564?>
                  <pub-id pub-id-type="pmid">12564564</pub-id>
                </element-citation>
              </ref>
              <ref id="CR29">
                <label>29.</label>
                <element-citation publication-type="journal">
                  <person-group person-group-type="author">
                    <name>
                      <surname>Stampe</surname>
                      <given-names>DM</given-names>
                    </name>
                  </person-group>
                  <article-title>Heuristic filtering and reliable calibration methods for video-based pupil-tracking systems</article-title>
                  <source>Behav. Res. Methods Instrum. Comput.</source>
                  <year>1993</year>
                  <volume>25</volume>
                  <fpage>137</fpage>
                  <lpage>142</lpage>
                  <pub-id pub-id-type="doi">10.3758/BF03204486</pub-id>
                </element-citation>
              </ref>
              <ref id="CR30">
                <label>30.</label>
                <element-citation publication-type="journal">
                  <person-group person-group-type="author">
                    <name>
                      <surname>Kleiner</surname>
                      <given-names>M</given-names>
                    </name>
                    <name>
                      <surname>Brainard</surname>
                      <given-names>D</given-names>
                    </name>
                    <name>
                      <surname>Pelli</surname>
                      <given-names>DG</given-names>
                    </name>
                  </person-group>
                  <article-title>What’s new in Psychtoolbox-3?</article-title>
                  <source>Percept. 36 ECVP Abstr. Suppl.</source>
                  <year>2007</year>
                  <volume>36</volume>
                  <fpage>1</fpage>
                </element-citation>
              </ref>
              <ref id="CR31">
                <label>31.</label>
                <element-citation publication-type="journal">
                  <person-group person-group-type="author">
                    <name>
                      <surname>Pelli</surname>
                      <given-names>DG</given-names>
                    </name>
                  </person-group>
                  <article-title>The VideoToolbox software for visual psychophysics: transforming numbers into movies</article-title>
                  <source>Spat. Vis.</source>
                  <year>1997</year>
                  <volume>10</volume>
                  <fpage>437</fpage>
                  <lpage>442</lpage>
                  <pub-id pub-id-type="doi">10.1163/156856897X00366</pub-id>
                  <?supplied-pmid 9176953?>
                  <pub-id pub-id-type="pmid">9176953</pub-id>
                </element-citation>
              </ref>
              <ref id="CR32">
                <label>32.</label>
                <element-citation publication-type="journal">
                  <person-group person-group-type="author">
                    <name>
                      <surname>Brainard</surname>
                      <given-names>DH</given-names>
                    </name>
                  </person-group>
                  <article-title>The psychophysics toolbox</article-title>
                  <source>Spat. Vis.</source>
                  <year>1997</year>
                  <volume>10</volume>
                  <fpage>433</fpage>
                  <lpage>436</lpage>
                  <pub-id pub-id-type="doi">10.1163/156856897X00357</pub-id>
                  <pub-id pub-id-type="pmid">9176952</pub-id>
                </element-citation>
              </ref>
              <ref id="CR33">
                <label>33.</label>
                <mixed-citation publication-type="other">Harel, J., Koch, C. &amp; Perona, P. Graph-Based Visual Saliency. in <italic>Advances in Neural Information Processing Systems</italic><bold>19</bold>, 545–552 (MIT Press, 2007).</mixed-citation>
              </ref>
              <ref id="CR34">
                <label>34.</label>
                <element-citation publication-type="journal">
                  <person-group person-group-type="author">
                    <name>
                      <surname>Schurgin</surname>
                      <given-names>MW</given-names>
                    </name>
                    <name>
                      <surname>Flombaum</surname>
                      <given-names>JI</given-names>
                    </name>
                  </person-group>
                  <article-title>How undistorted spatial memories can produce distorted responses</article-title>
                  <source>Attent. Percept. Psychophys.</source>
                  <year>2014</year>
                  <volume>76</volume>
                  <fpage>1371</fpage>
                  <lpage>1380</lpage>
                  <pub-id pub-id-type="doi">10.3758/s13414-014-0647-x</pub-id>
                </element-citation>
              </ref>
              <ref id="CR35">
                <label>35.</label>
                <element-citation publication-type="journal">
                  <person-group person-group-type="author">
                    <name>
                      <surname>Rock</surname>
                      <given-names>I</given-names>
                    </name>
                  </person-group>
                  <article-title>The perception of disoriented figures</article-title>
                  <source>Sci. Am.</source>
                  <year>1974</year>
                  <volume>230</volume>
                  <fpage>78</fpage>
                  <lpage>85</lpage>
                  <pub-id pub-id-type="doi">10.1038/scientificamerican0174-78</pub-id>
                  <?supplied-pmid 4808785?>
                  <pub-id pub-id-type="pmid">4808785</pub-id>
                </element-citation>
              </ref>
              <ref id="CR36">
                <label>36.</label>
                <element-citation publication-type="journal">
                  <person-group person-group-type="author">
                    <name>
                      <surname>Intraub</surname>
                      <given-names>H</given-names>
                    </name>
                  </person-group>
                  <article-title>Conceptual masking: the effects of subsequent visual events on memory for pictures</article-title>
                  <source>J. Exp. Psychol. Learn. Mem. Cogn.</source>
                  <year>1984</year>
                  <volume>10</volume>
                  <fpage>115</fpage>
                  <lpage>125</lpage>
                  <pub-id pub-id-type="doi">10.1037/0278-7393.10.1.115</pub-id>
                  <?supplied-pmid 6242731?>
                  <pub-id pub-id-type="pmid">6242731</pub-id>
                </element-citation>
              </ref>
              <ref id="CR37">
                <label>37.</label>
                <element-citation publication-type="journal">
                  <person-group person-group-type="author">
                    <name>
                      <surname>Brockmole</surname>
                      <given-names>JR</given-names>
                    </name>
                    <name>
                      <surname>Henderson</surname>
                      <given-names>JM</given-names>
                    </name>
                  </person-group>
                  <article-title>Using real-world scenes as contextual cues for search</article-title>
                  <source>Vis. cogn.</source>
                  <year>2006</year>
                  <volume>13</volume>
                  <fpage>99</fpage>
                  <lpage>108</lpage>
                  <pub-id pub-id-type="doi">10.1080/13506280500165188</pub-id>
                </element-citation>
              </ref>
              <ref id="CR38">
                <label>38.</label>
                <element-citation publication-type="journal">
                  <person-group person-group-type="author">
                    <name>
                      <surname>Shore</surname>
                      <given-names>DI</given-names>
                    </name>
                    <name>
                      <surname>Klein</surname>
                      <given-names>RM</given-names>
                    </name>
                  </person-group>
                  <article-title>The effects of scene inversion on change blindness</article-title>
                  <source>J. Gen. Psychol.</source>
                  <year>2000</year>
                  <volume>127</volume>
                  <fpage>27</fpage>
                  <lpage>43</lpage>
                  <pub-id pub-id-type="doi">10.1080/00221300009598569</pub-id>
                  <?supplied-pmid 10695950?>
                  <pub-id pub-id-type="pmid">10695950</pub-id>
                </element-citation>
              </ref>
              <ref id="CR39">
                <label>39.</label>
                <element-citation publication-type="journal">
                  <person-group person-group-type="author">
                    <name>
                      <surname>Goffaux</surname>
                      <given-names>V</given-names>
                    </name>
                    <etal/>
                  </person-group>
                  <article-title>Diagnostic colours contribute to the early stages of scene categorization: behavioural and neurophysiological evidence</article-title>
                  <source>Vis. Cogn.</source>
                  <year>2005</year>
                  <volume>12</volume>
                  <fpage>878</fpage>
                  <lpage>892</lpage>
                  <pub-id pub-id-type="doi">10.1080/13506280444000562</pub-id>
                </element-citation>
              </ref>
              <ref id="CR40">
                <label>40.</label>
                <element-citation publication-type="journal">
                  <person-group person-group-type="author">
                    <name>
                      <surname>Short</surname>
                      <given-names>AD</given-names>
                    </name>
                  </person-group>
                  <article-title>Decremental and incremental visual thresholds</article-title>
                  <source>J. Physiol.</source>
                  <year>1966</year>
                  <volume>185</volume>
                  <fpage>646</fpage>
                  <lpage>654</lpage>
                  <pub-id pub-id-type="doi">10.1113/jphysiol.1966.sp008007</pub-id>
                  <?supplied-pmid 5918061?>
                  <pub-id pub-id-type="pmid">5918061</pub-id>
                </element-citation>
              </ref>
              <ref id="CR41">
                <label>41.</label>
                <element-citation publication-type="journal">
                  <person-group person-group-type="author">
                    <name>
                      <surname>Bowen</surname>
                      <given-names>RW</given-names>
                    </name>
                    <name>
                      <surname>Pokorny</surname>
                      <given-names>J</given-names>
                    </name>
                    <name>
                      <surname>Smith</surname>
                      <given-names>VC</given-names>
                    </name>
                  </person-group>
                  <article-title>Sawtooth contrast sensitivity: Decrements have the edge</article-title>
                  <source>Vision Res.</source>
                  <year>1989</year>
                  <volume>29</volume>
                  <fpage>IN1</fpage>
                  <pub-id pub-id-type="doi">10.1016/0042-6989(89)90169-7</pub-id>
                </element-citation>
              </ref>
              <ref id="CR42">
                <label>42.</label>
                <element-citation publication-type="journal">
                  <person-group person-group-type="author">
                    <name>
                      <surname>Lu</surname>
                      <given-names>ZL</given-names>
                    </name>
                    <name>
                      <surname>Sperling</surname>
                      <given-names>G</given-names>
                    </name>
                  </person-group>
                  <article-title>Black-white asymmetry in visual perception</article-title>
                  <source>J. Vis.</source>
                  <year>2012</year>
                  <volume>12</volume>
                  <fpage>1</fpage>
                  <lpage>21</lpage>
                </element-citation>
              </ref>
              <ref id="CR43">
                <label>43.</label>
                <element-citation publication-type="journal">
                  <person-group person-group-type="author">
                    <name>
                      <surname>Schmidt</surname>
                      <given-names>T</given-names>
                    </name>
                    <name>
                      <surname>Werner</surname>
                      <given-names>S</given-names>
                    </name>
                    <name>
                      <surname>Diedrichsen</surname>
                      <given-names>J</given-names>
                    </name>
                  </person-group>
                  <article-title>Spatial distortions induced by multiple visual landmarks: how local distortions combine to produce complex distortion patterns</article-title>
                  <source>Percept. Psychophys.</source>
                  <year>2003</year>
                  <volume>65</volume>
                  <fpage>861</fpage>
                  <lpage>873</lpage>
                  <pub-id pub-id-type="doi">10.3758/BF03194820</pub-id>
                  <?supplied-pmid 14528896?>
                  <pub-id pub-id-type="pmid">14528896</pub-id>
                </element-citation>
              </ref>
              <ref id="CR44">
                <label>44.</label>
                <element-citation publication-type="journal">
                  <person-group person-group-type="author">
                    <name>
                      <surname>Coren</surname>
                      <given-names>S</given-names>
                    </name>
                    <name>
                      <surname>Hoenig</surname>
                      <given-names>P</given-names>
                    </name>
                  </person-group>
                  <article-title>Effect of non-target stimuli upon length of voluntary saccades</article-title>
                  <source>Percept. Mot. Skills</source>
                  <year>1972</year>
                  <volume>34</volume>
                  <fpage>499</fpage>
                  <lpage>508</lpage>
                  <pub-id pub-id-type="doi">10.2466/pms.1972.34.2.499</pub-id>
                  <?supplied-pmid 5063190?>
                  <pub-id pub-id-type="pmid">5063190</pub-id>
                </element-citation>
              </ref>
              <ref id="CR45">
                <label>45.</label>
                <element-citation publication-type="journal">
                  <person-group person-group-type="author">
                    <name>
                      <surname>Findlay</surname>
                      <given-names>JM</given-names>
                    </name>
                  </person-group>
                  <article-title>Global visual processing for saccadic eye movements</article-title>
                  <source>Vis. Res.</source>
                  <year>1982</year>
                  <volume>22</volume>
                  <fpage>1033</fpage>
                  <lpage>1045</lpage>
                  <pub-id pub-id-type="doi">10.1016/0042-6989(82)90040-2</pub-id>
                  <?supplied-pmid 7135840?>
                  <pub-id pub-id-type="pmid">7135840</pub-id>
                </element-citation>
              </ref>
              <ref id="CR46">
                <label>46.</label>
                <element-citation publication-type="journal">
                  <person-group person-group-type="author">
                    <name>
                      <surname>He</surname>
                      <given-names>P</given-names>
                    </name>
                    <name>
                      <surname>Kowler</surname>
                      <given-names>E</given-names>
                    </name>
                  </person-group>
                  <article-title>The role of location probability in the programming of saccades: Implications for ‘center-of-gravity’ tendencies</article-title>
                  <source>Vis. Res.</source>
                  <year>1989</year>
                  <volume>29</volume>
                  <fpage>1165</fpage>
                  <lpage>1181</lpage>
                  <pub-id pub-id-type="doi">10.1016/0042-6989(89)90063-1</pub-id>
                  <?supplied-pmid 2617863?>
                  <pub-id pub-id-type="pmid">2617863</pub-id>
                </element-citation>
              </ref>
              <ref id="CR47">
                <label>47.</label>
                <element-citation publication-type="journal">
                  <person-group person-group-type="author">
                    <name>
                      <surname>Choi</surname>
                      <given-names>WY</given-names>
                    </name>
                    <name>
                      <surname>Viswanathan</surname>
                      <given-names>J</given-names>
                    </name>
                    <name>
                      <surname>Barton</surname>
                      <given-names>JJS</given-names>
                    </name>
                  </person-group>
                  <article-title>The temporal dynamics of the distractor in the global effect</article-title>
                  <source>Exp. Brain Res.</source>
                  <year>2016</year>
                  <volume>234</volume>
                  <fpage>2457</fpage>
                  <lpage>2463</lpage>
                  <pub-id pub-id-type="doi">10.1007/s00221-016-4650-4</pub-id>
                  <?supplied-pmid 27086262?>
                  <pub-id pub-id-type="pmid">27086262</pub-id>
                </element-citation>
              </ref>
              <ref id="CR48">
                <label>48.</label>
                <element-citation publication-type="journal">
                  <person-group person-group-type="author">
                    <name>
                      <surname>Mackay</surname>
                      <given-names>M</given-names>
                    </name>
                    <name>
                      <surname>Cerf</surname>
                      <given-names>M</given-names>
                    </name>
                    <name>
                      <surname>Koch</surname>
                      <given-names>C</given-names>
                    </name>
                  </person-group>
                  <article-title>Evidence for two distinct mechanisms directing gaze in natural scenes</article-title>
                  <source>J. Vis.</source>
                  <year>2012</year>
                  <volume>12</volume>
                  <fpage>1</fpage>
                  <lpage>12</lpage>
                  <pub-id pub-id-type="doi">10.1167/12.4.9</pub-id>
                </element-citation>
              </ref>
              <ref id="CR49">
                <label>49.</label>
                <element-citation publication-type="journal">
                  <person-group person-group-type="author">
                    <name>
                      <surname>Anderson</surname>
                      <given-names>NC</given-names>
                    </name>
                    <name>
                      <surname>Ort</surname>
                      <given-names>E</given-names>
                    </name>
                    <name>
                      <surname>Kruijne</surname>
                      <given-names>W</given-names>
                    </name>
                    <name>
                      <surname>Meeter</surname>
                      <given-names>M</given-names>
                    </name>
                    <name>
                      <surname>Donk</surname>
                      <given-names>M</given-names>
                    </name>
                  </person-group>
                  <article-title>It depends on when you look at it: Salience influences eye movements in natural scene viewing and search early in time</article-title>
                  <source>J. Vis.</source>
                  <year>2015</year>
                  <volume>15</volume>
                  <fpage>1</fpage>
                  <lpage>22</lpage>
                </element-citation>
              </ref>
              <ref id="CR50">
                <label>50.</label>
                <element-citation publication-type="journal">
                  <person-group person-group-type="author">
                    <name>
                      <surname>Foulsham</surname>
                      <given-names>T</given-names>
                    </name>
                    <name>
                      <surname>Underwood</surname>
                      <given-names>G</given-names>
                    </name>
                  </person-group>
                  <article-title>What can saliency models predict about eye movements? Spatial and sequential aspects of fixations during encoding and recognition</article-title>
                  <source>J. Vis.</source>
                  <year>2008</year>
                  <volume>8</volume>
                  <fpage>1</fpage>
                  <lpage>17</lpage>
                  <pub-id pub-id-type="doi">10.1167/8.2.6</pub-id>
                  <?supplied-pmid 18318632?>
                  <pub-id pub-id-type="pmid">18318632</pub-id>
                </element-citation>
              </ref>
              <ref id="CR51">
                <label>51.</label>
                <element-citation publication-type="journal">
                  <person-group person-group-type="author">
                    <name>
                      <surname>Holden</surname>
                      <given-names>MP</given-names>
                    </name>
                    <name>
                      <surname>Curby</surname>
                      <given-names>KM</given-names>
                    </name>
                    <name>
                      <surname>Newcombe</surname>
                      <given-names>NS</given-names>
                    </name>
                    <name>
                      <surname>Shipley</surname>
                      <given-names>TF</given-names>
                    </name>
                  </person-group>
                  <article-title>A Category Adjustment Approach to Memory for Spatial Location in Natural Scenes</article-title>
                  <source>J. Exp. Psychol. Learn. Mem. Cogn.</source>
                  <year>2010</year>
                  <volume>36</volume>
                  <fpage>590</fpage>
                  <lpage>604</lpage>
                  <pub-id pub-id-type="doi">10.1037/a0019293</pub-id>
                  <?supplied-pmid 20438259?>
                  <pub-id pub-id-type="pmid">20438259</pub-id>
                </element-citation>
              </ref>
              <ref id="CR52">
                <label>52.</label>
                <element-citation publication-type="journal">
                  <person-group person-group-type="author">
                    <name>
                      <surname>Langlois</surname>
                      <given-names>TA</given-names>
                    </name>
                    <name>
                      <surname>Jacoby</surname>
                      <given-names>N</given-names>
                    </name>
                    <name>
                      <surname>Suchow</surname>
                      <given-names>JW</given-names>
                    </name>
                    <name>
                      <surname>Griffiths</surname>
                      <given-names>TL</given-names>
                    </name>
                  </person-group>
                  <article-title>Serial reproduction reveals the geometry of visuospatial representations</article-title>
                  <source>Proc. Natl. Acad. Sci.</source>
                  <year>2021</year>
                  <volume>118</volume>
                  <fpage>1</fpage>
                  <lpage>56</lpage>
                  <pub-id pub-id-type="doi">10.1073/pnas.2012938118</pub-id>
                </element-citation>
              </ref>
            </ref-list>
          </back>
        </article>
      </metadata>
    </record>
  </GetRecord>
</OAI-PMH>
