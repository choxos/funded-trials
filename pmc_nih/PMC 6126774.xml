<?xml version="1.0" encoding="UTF-8"?>
<OAI-PMH xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/ http://www.openarchives.org/OAI/2.0/OAI-PMH.xsd">
  <responseDate>2024-05-03T05:57:45Z</responseDate>
  <request verb="GetRecord" identifier="oai:pubmedcentral.nih.gov:6126774" metadataPrefix="pmc">https:/www.ncbi.nlm.nih.gov/pmc/oai/oai.cgi</request>
  <GetRecord>
    <record>
      <header>
        <identifier>oai:pubmedcentral.nih.gov:6126774</identifier>
        <datestamp>2018-09-07</datestamp>
        <setSpec>pnas</setSpec>
        <setSpec>pmc-open</setSpec>
      </header>
      <metadata>
        <article xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" xsi:schemaLocation="https://jats.nlm.nih.gov/ns/archiving/1.3/ https://jats.nlm.nih.gov/archiving/1.3/xsd/JATS-archivearticle1-3.xsd" article-type="research-article">
          <front>
            <journal-meta>
              <journal-id journal-id-type="nlm-ta">Proc Natl Acad Sci U S A</journal-id>
              <journal-id journal-id-type="iso-abbrev">Proc. Natl. Acad. Sci. U.S.A</journal-id>
              <journal-id journal-id-type="hwp">pnas</journal-id>
              <journal-id journal-id-type="pmc">pnas</journal-id>
              <journal-id journal-id-type="publisher-id">PNAS</journal-id>
              <journal-title-group>
                <journal-title>Proceedings of the National Academy of Sciences of the United States of America</journal-title>
              </journal-title-group>
              <issn pub-type="ppub">0027-8424</issn>
              <issn pub-type="epub">1091-6490</issn>
              <publisher>
                <publisher-name>National Academy of Sciences</publisher-name>
              </publisher>
            </journal-meta>
            <article-meta>
              <article-id pub-id-type="accession">PMC6126774</article-id>
              <article-id pub-id-type="pmcid">PMC6126774</article-id>
              <article-id pub-id-type="pmc-uid">6126774</article-id>
              <article-id pub-id-type="pmid">30104363</article-id>
              <article-id pub-id-type="pmid">30104363</article-id>
              <article-id pub-id-type="publisher-id">201719397</article-id>
              <article-id pub-id-type="doi">10.1073/pnas.1719397115</article-id>
              <article-categories>
                <subj-group subj-group-type="heading">
                  <subject>Biological Sciences</subject>
                  <subj-group>
                    <subject>Neuroscience</subject>
                  </subj-group>
                </subj-group>
                <subj-group subj-group-type="heading">
                  <subject>Social Sciences</subject>
                  <subj-group>
                    <subject>Psychological and Cognitive Sciences</subject>
                  </subj-group>
                </subj-group>
              </article-categories>
              <title-group>
                <article-title>Recurrent computations for visual pattern completion</article-title>
              </title-group>
              <contrib-group>
                <contrib contrib-type="author">
                  <name>
                    <surname>Tang</surname>
                    <given-names>Hanlin</given-names>
                  </name>
                  <xref ref-type="aff" rid="aff1">
                    <sup>a</sup>
                  </xref>
                  <xref ref-type="aff" rid="aff2">
                    <sup>b</sup>
                  </xref>
                  <xref ref-type="author-notes" rid="fn1">
                    <sup>1</sup>
                  </xref>
                </contrib>
                <contrib contrib-type="author">
                  <name>
                    <surname>Schrimpf</surname>
                    <given-names>Martin</given-names>
                  </name>
                  <xref ref-type="aff" rid="aff2">
                    <sup>b</sup>
                  </xref>
                  <xref ref-type="aff" rid="aff3">
                    <sup>c</sup>
                  </xref>
                  <xref ref-type="aff" rid="aff4">
                    <sup>d</sup>
                  </xref>
                  <xref ref-type="aff" rid="aff5">
                    <sup>e</sup>
                  </xref>
                  <xref ref-type="author-notes" rid="fn1">
                    <sup>1</sup>
                  </xref>
                </contrib>
                <contrib contrib-type="author">
                  <name>
                    <surname>Lotter</surname>
                    <given-names>William</given-names>
                  </name>
                  <xref ref-type="aff" rid="aff1">
                    <sup>a</sup>
                  </xref>
                  <xref ref-type="aff" rid="aff2">
                    <sup>b</sup>
                  </xref>
                  <xref ref-type="aff" rid="aff6">
                    <sup>f</sup>
                  </xref>
                  <xref ref-type="author-notes" rid="fn1">
                    <sup>1</sup>
                  </xref>
                </contrib>
                <contrib contrib-type="author">
                  <name>
                    <surname>Moerman</surname>
                    <given-names>Charlotte</given-names>
                  </name>
                  <xref ref-type="aff" rid="aff2">
                    <sup>b</sup>
                  </xref>
                </contrib>
                <contrib contrib-type="author">
                  <name>
                    <surname>Paredes</surname>
                    <given-names>Ana</given-names>
                  </name>
                  <xref ref-type="aff" rid="aff2">
                    <sup>b</sup>
                  </xref>
                </contrib>
                <contrib contrib-type="author">
                  <name>
                    <surname>Ortega Caro</surname>
                    <given-names>Josue</given-names>
                  </name>
                  <xref ref-type="aff" rid="aff2">
                    <sup>b</sup>
                  </xref>
                </contrib>
                <contrib contrib-type="author">
                  <name>
                    <surname>Hardesty</surname>
                    <given-names>Walter</given-names>
                  </name>
                  <xref ref-type="aff" rid="aff2">
                    <sup>b</sup>
                  </xref>
                </contrib>
                <contrib contrib-type="author">
                  <name>
                    <surname>Cox</surname>
                    <given-names>David</given-names>
                  </name>
                  <xref ref-type="aff" rid="aff6">
                    <sup>f</sup>
                  </xref>
                </contrib>
                <contrib contrib-type="author">
                  <contrib-id contrib-id-type="orcid" authenticated="false">http://orcid.org/0000-0003-3505-8475</contrib-id>
                  <name>
                    <surname>Kreiman</surname>
                    <given-names>Gabriel</given-names>
                  </name>
                  <xref ref-type="aff" rid="aff2">
                    <sup>b</sup>
                  </xref>
                  <xref ref-type="corresp" rid="cor1">
                    <sup>2</sup>
                  </xref>
                </contrib>
                <aff id="aff1"><sup>a</sup>Program in Biophysics, <institution>Harvard University</institution>, Boston, <addr-line>MA</addr-line> 02115;</aff>
                <aff id="aff2"><sup>b</sup><institution>Children’s Hospital, Harvard Medical School</institution>, Boston, <addr-line>MA</addr-line> 02115;</aff>
                <aff id="aff3"><sup>c</sup>Program in Software Engineering, <institution>Institut für Informatik, Universität Augsburg</institution>, 86159 Augsburg, <addr-line>Germany</addr-line>;</aff>
                <aff id="aff4"><sup>d</sup>Program in Software Engineering, <institution>Institut für Informatik, Ludwig-Maximilians-Universität München</institution>, 80538 München, <addr-line>Germany</addr-line>;</aff>
                <aff id="aff5"><sup>e</sup>Program in Software Engineering, Fakultät für Informatik, <institution>Technische Universität München</institution>, 85748 Garching, <addr-line>Germany</addr-line>;</aff>
                <aff id="aff6"><sup>f</sup>Molecular and Cellular Biology, <institution>Harvard University</institution>, Cambridge, <addr-line>MA</addr-line> 02138</aff>
              </contrib-group>
              <author-notes>
                <corresp id="cor1"><sup>2</sup>To whom correspondence should be addressed. Email: <email>gabriel.kreiman@tch.harvard.edu</email>.</corresp>
                <fn fn-type="edited-by">
                  <p>Edited by Terrence J. Sejnowski, Salk Institute for Biological Studies, La Jolla, CA, and approved July 20, 2018 (received for review November 10, 2017)</p>
                </fn>
                <fn fn-type="con">
                  <p>Author contributions: H.T., M.S., W.L., C.M., D.C., and G.K. designed research; H.T., M.S., W.L., C.M., A.P., J.O.C., W.H., and G.K. performed research; H.T., M.S., W.L., C.M., and G.K. analyzed data; and H.T., M.S., W.L., and G.K. wrote the paper.</p>
                </fn>
                <fn fn-type="equal" id="fn1">
                  <p><sup>1</sup>H.T., M.S., and W.L. contributed equally to this work.</p>
                </fn>
              </author-notes>
              <pub-date pub-type="ppub">
                <day>28</day>
                <month>8</month>
                <year>2018</year>
              </pub-date>
              <pub-date pub-type="epub">
                <day>13</day>
                <month>8</month>
                <year>2018</year>
              </pub-date>
              <pub-date pub-type="pmc-release">
                <day>13</day>
                <month>8</month>
                <year>2018</year>
              </pub-date>
              <!-- PMC Release delay is 0 months and 0 days and was based on the <pub-date pub-type="epub"/>. -->
              <volume>115</volume>
              <issue>35</issue>
              <fpage>8835</fpage>
              <lpage>8840</lpage>
              <permissions>
                <copyright-statement>Copyright © 2018 the Author(s). Published by PNAS.</copyright-statement>
                <copyright-year>2018</copyright-year>
                <license license-type="open-access" xlink:href="https://creativecommons.org/licenses/by-nc-nd/4.0/">
                  <ali:license_ref specific-use="vor"/>
                  <license-p>This open access article is distributed under <ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by-nc-nd/4.0/">Creative Commons Attribution-NonCommercial-NoDerivatives License 4.0 (CC BY-NC-ND)</ext-link>.</license-p>
                </license>
              </permissions>
              <self-uri xlink:title="pdf" xlink:href="pnas.201719397.pdf"/>
              <abstract abstract-type="executive-summary">
                <title>Significance</title>
                <p>The ability to complete patterns and interpret partial information is a central property of intelligence. Deep convolutional network architectures have proved successful in labeling whole objects in images and capturing the initial 150 ms of processing along the ventral visual cortex. This study shows that human object recognition abilities remain robust when only small amounts of information are available due to heavy occlusion, but the performance of bottom-up computational models is impaired under limited visibility. The results provide combined behavioral, neurophysiological, and modeling insights showing how recurrent computations may help the brain solve the fundamental challenge of pattern completion.</p>
              </abstract>
              <abstract>
                <p>Making inferences from partial information constitutes a critical aspect of cognition. During visual perception, pattern completion enables recognition of poorly visible or occluded objects. We combined psychophysics, physiology, and computational models to test the hypothesis that pattern completion is implemented by recurrent computations and present three pieces of evidence that are consistent with this hypothesis. First, subjects robustly recognized objects even when they were rendered &lt;15% visible, but recognition was largely impaired when processing was interrupted by backward masking. Second, invasive physiological responses along the human ventral cortex exhibited visually selective responses to partially visible objects that were delayed compared with whole objects, suggesting the need for additional computations. These physiological delays were correlated with the effects of backward masking. Third, state-of-the-art feed-forward computational architectures were not robust to partial visibility. However, recognition performance was recovered when the model was augmented with attractor-based recurrent connectivity. The recurrent model was able to predict which images of heavily occluded objects were easier or harder for humans to recognize, could capture the effect of introducing a backward mask on recognition behavior, and was consistent with the physiological delays along the human ventral visual stream. These results provide a strong argument of plausibility for the role of recurrent computations in making visual inferences from partial information.</p>
              </abstract>
              <kwd-group>
                <kwd>visual object recognition</kwd>
                <kwd>computational neuroscience</kwd>
                <kwd>pattern completion</kwd>
                <kwd>artificial intelligence</kwd>
                <kwd>machine learning</kwd>
              </kwd-group>
              <funding-group>
                <award-group id="gs1">
                  <funding-source id="sp1">HHS | NIH | National Eye Institute (NEI)<named-content content-type="funder-id">100000053</named-content></funding-source>
                  <award-id rid="sp1">R01EY026025</award-id>
                  <principal-award-recipient>Gabriel Kreiman</principal-award-recipient>
                </award-group>
                <award-group id="gs2">
                  <funding-source id="sp2">National Science Foundation (NSF)<named-content content-type="funder-id">100000001</named-content></funding-source>
                  <award-id rid="sp2">1231216</award-id>
                  <principal-award-recipient>Gabriel Kreiman</principal-award-recipient>
                </award-group>
              </funding-group>
              <counts>
                <page-count count="6"/>
              </counts>
            </article-meta>
          </front>
          <body>
            <p content-type="flushleft">Humans and other animals have a remarkable ability to make inferences from partial data across all cognitive domains. This inference capacity is ubiquitously illustrated during pattern completion to recognize objects that are partially visible due to noise, limited viewing angles, poor illumination, or occlusion. There has been significant progress in describing the neural machinery along the ventral visual stream responsible for recognizing whole objects (<xref rid="r1" ref-type="bibr">1</xref><xref rid="r2" ref-type="bibr"/><xref rid="r3" ref-type="bibr"/><xref rid="r4" ref-type="bibr"/>–<xref rid="r5" ref-type="bibr">5</xref>). Computational models instantiating biologically plausible algorithms for pattern recognition of whole objects typically consist of a sequence of filtering and nonlinear pooling operations. The concatenation of these operations transforms pixel inputs into a feature representation amenable for linear decoding of object labels. Such feed-forward algorithms perform well in large-scale computer vision experiments for pattern recognition (<xref rid="r6" ref-type="bibr">6</xref><xref rid="r7" ref-type="bibr"/><xref rid="r8" ref-type="bibr"/>–<xref rid="r9" ref-type="bibr">9</xref>) and provide a first-order approximation to describe the activity of cortical neurons (e.g., ref. <xref rid="r10" ref-type="bibr">10</xref>).</p>
            <p>Spatial and temporal integration play an important role in pattern completion mechanisms (<xref rid="r11" ref-type="bibr">11</xref><xref rid="r12" ref-type="bibr"/><xref rid="r13" ref-type="bibr"/>–<xref rid="r14" ref-type="bibr">14</xref>). When an object is occluded, there are infinitely many possible contours that could join the object’s parts together. However, the brain typically manages to integrate those parts to correctly recognize the occluded object. Multiple studies have highlighted the importance of temporal integration by demonstrating that recognizing partially visible objects takes more time than recognizing fully visible ones at the behavioral (<xref rid="r11" ref-type="bibr">11</xref>, <xref rid="r15" ref-type="bibr">15</xref>) and physiological (<xref rid="r12" ref-type="bibr">12</xref>, <xref rid="r13" ref-type="bibr">13</xref>) levels. We conjectured that within-layer and top-down recurrent computations are involved in implementing the spatial and temporal integrative mechanisms underlying pattern completion. Recurrent connections can link signals over space within a layer and provide specific top-down modulation from neurons with larger receptive fields (<xref rid="r16" ref-type="bibr">16</xref>, <xref rid="r17" ref-type="bibr">17</xref>). Additionally, recurrent signals temporally lag behind their feed-forward counterparts, and therefore provide an ideal way to incorporate temporal integration mechanisms.</p>
            <p>To examine plausible mechanisms involved in pattern completion, we combined psychophysics, neurophysiology (<xref rid="r13" ref-type="bibr">13</xref>), and computational modeling to evaluate recognition of partially visible objects. We show that humans robustly recognize objects even from a limited amount of information, but performance rapidly deteriorates when computations are interrupted by a noise mask. On an image-by-image basis, the behavioral effect of such backward masking correlates with an increase in latency in neurophysiological intracranial field potentials along the ventral visual stream. A family of modern feed-forward convolutional hierarchical models is not robust to occlusion. We extend previous notions of attractor dynamics by adding recurrence to such bottom-up models and providing a proof-of-concept model that captures the essence of human pattern completion behavior.</p>
            <sec sec-type="results" id="s1">
              <title>Results</title>
              <sec id="s2">
                <title>Robust Recognition of Partially Visible Objects.</title>
                <p>Subjects performed a recognition task (<xref ref-type="fig" rid="fig01">Fig. 1 <italic>A</italic> and <italic>B</italic></xref>) involving categorization of objects that were either partially visible (Partial in <xref ref-type="fig" rid="fig01">Fig. 1<italic>C</italic></xref>, <italic>Right</italic>) or fully visible (Whole in <xref ref-type="fig" rid="fig01">Fig. 1<italic>C</italic></xref>, <italic>Left</italic>). Images were followed by either a gray screen (“unmasked” in <xref ref-type="fig" rid="fig01">Fig. 1<italic>A</italic></xref>) or a spatially overlapping noise pattern (“masked” in <xref ref-type="fig" rid="fig01">Fig. 1<italic>B</italic></xref>). The image presentation time, referred to as stimulus onset asynchrony (SOA), ranged from 25 to 150 ms in randomly ordered trials. Stimuli consisted of 325 objects belonging to five categories: animals, chairs, faces, fruits, and vehicles. The parts revealed for each object were chosen randomly. There were 40 images per object, comprising a total of 13,000 images of partial objects (<xref ref-type="sec" rid="s9"><italic>Methods</italic></xref>).</p>
                <fig id="fig01" fig-type="featured" orientation="portrait" position="float">
                  <label>Fig. 1.</label>
                  <caption>
                    <p>Backward masking disrupts recognition of partially visible objects. (<italic>A</italic> and <italic>B</italic>) Forced-choice categorization task (<italic>n</italic> = 21 subjects). After 500 ms of fixation, stimuli were presented for variable exposure times (SOA from 25 to 150 ms), followed by a gray screen (<italic>A</italic>) or a noise mask (<italic>B</italic>) for 500 ms. Stimuli were presented unaltered (Whole; <italic>C</italic>, <italic>Left</italic> and <italic>D</italic>, <italic>Left</italic>), rendered partially visible (Partial; <italic>C</italic>, <italic>Right</italic>), or rendered occluded (<italic>D</italic>, <italic>Right</italic>) (<ext-link ext-link-type="uri" xlink:href="http://www.pnas.org/lookup/suppl/doi:10.1073/pnas.1719397115/-/DCSupplemental" xlink:show="new"><italic>SI Appendix</italic>, Fig. S1</ext-link>). (<italic>E</italic>) Experimental variation with novel objects (<ext-link ext-link-type="uri" xlink:href="http://www.pnas.org/lookup/suppl/doi:10.1073/pnas.1719397115/-/DCSupplemental" xlink:show="new"><italic>SI Appendix</italic>, Fig. S8</ext-link>). Behavioral performance is shown as a function of visibility for the unmasked (<italic>F</italic>) and masked (<italic>G</italic>) trials. Colors denote different SOAs. Error bars denote SEM. The horizontal dashed line indicates chance level (20%). Bin size = 2.5%. Note the discontinuity in the <italic>x</italic> axis to report performance at 100% visibility. (<italic>H</italic>) Average recognition performance as a function of SOA for partial objects (same data replotted from <italic>F</italic> and <italic>G</italic>, excluding 100% visibility). Performance was significantly degraded by masking (solid gray line) compared with the unmasked trials (dotted gray line) (<italic>P</italic> &lt; 0.001, χ<sup>2</sup> test; df = 4). (<italic>I</italic>) Performance versus SOA for the occluded stimuli in <italic>D</italic> (note: chance = 25% here) (<ext-link ext-link-type="uri" xlink:href="http://www.pnas.org/lookup/suppl/doi:10.1073/pnas.1719397115/-/DCSupplemental" xlink:show="new"><italic>SI Appendix</italic>, Fig. S1</ext-link>). (<italic>J</italic>) Performance versus SOA for the novel object stimuli in <italic>E</italic>.</p>
                  </caption>
                  <graphic id="gra1" xlink:href="pnas.1719397115fig01"/>
                </fig>
                <p>For whole objects and without a mask, behavioral performance was near ceiling, as expected (100% visible in <xref ref-type="fig" rid="fig01">Fig. 1<italic>F</italic></xref>). Subjects robustly recognized partial objects across a wide range of visibility levels despite the limited information provided (<xref ref-type="fig" rid="fig01">Fig. 1<italic>F</italic></xref>). Although poor visibility degraded performance, subjects still showed 80 ± 3% performance at 35 ± 2.5% visibility (partial versus whole objects: <italic>P</italic> &lt; 10<sup>−10</sup>, two-sided <italic>t</italic> test). Even for images with 10 ± 2.5% visibility, performance was well above chance levels (59 ± 2%; <italic>P</italic> &lt; 10<sup>−10</sup>, two-sided <italic>t</italic> test; chance = 20%). There was a small but significant improvement in performance at longer SOAs for partially visible objects (dashed lines in <xref ref-type="fig" rid="fig01">Fig. 1<italic>H</italic></xref>; Pearson <italic>r</italic> = 0.56; <italic>P</italic> &lt; 0.001, permutation test).</p>
                <p>In a separate experiment, we generated images where objects appeared behind a black surface occluder (<xref ref-type="fig" rid="fig01">Fig. 1<italic>D</italic></xref>). Consistent with previous studies (e.g., ref. <xref rid="r14" ref-type="bibr">14</xref>), recognition was also robust when using heavily occluded images (<xref ref-type="fig" rid="fig01">Fig. 1<italic>I</italic></xref>). The presence of an occluder improved recognition performance with respect to partial objects (compare <ext-link ext-link-type="uri" xlink:href="http://www.pnas.org/lookup/suppl/doi:10.1073/pnas.1719397115/-/DCSupplemental" xlink:show="new"><italic>SI Appendix</italic>, Fig. S1 <italic>A</italic></ext-link> versus <ext-link ext-link-type="uri" xlink:href="http://www.pnas.org/lookup/suppl/doi:10.1073/pnas.1719397115/-/DCSupplemental" xlink:show="new"><italic>B</italic></ext-link>; <italic>P</italic> &lt; 10<sup>−4</sup>, χ<sup>2</sup> test). We focused next on the essential aspects of pattern completion by considering the more challenging condition of partially visible objects, without help from other cues such as occluders.</p>
                <p>While subjects had not seen any of the specific images in this experiment before, they had had extensive experience with fully visible and occluded versions of other images of animals, faces, fruits, chairs, and vehicles. We conducted a separate experiment with novel shapes (<xref ref-type="fig" rid="fig01">Fig. 1<italic>E</italic></xref> and <ext-link ext-link-type="uri" xlink:href="http://www.pnas.org/lookup/suppl/doi:10.1073/pnas.1719397115/-/DCSupplemental" xlink:show="new"><italic>SI Appendix</italic>, Fig. S8<italic>A</italic></ext-link>) to assess whether robustness to limited visibility (<xref ref-type="fig" rid="fig01">Fig. 1 <italic>F</italic>, <italic>H</italic>, and <italic>I</italic></xref>) extended to unfamiliar objects. Visual categorization of such novel objects was also robust to limited visibility (<xref ref-type="fig" rid="fig01">Fig. 1<italic>J</italic></xref> and <ext-link ext-link-type="uri" xlink:href="http://www.pnas.org/lookup/suppl/doi:10.1073/pnas.1719397115/-/DCSupplemental" xlink:show="new"><italic>SI Appendix</italic>, Fig. S8<italic>B</italic></ext-link>).</p>
              </sec>
              <sec id="s3">
                <title>Backward Masking Disrupts Recognition of Partially Visible Objects.</title>
                <p>Behavioral (<xref rid="r18" ref-type="bibr">18</xref>), physiological (<xref rid="r19" ref-type="bibr">19</xref>, <xref rid="r20" ref-type="bibr">20</xref>), and computational studies (<xref rid="r3" ref-type="bibr">3</xref>, <xref rid="r4" ref-type="bibr">4</xref>, <xref rid="r10" ref-type="bibr">10</xref>) suggest that recognition of whole isolated objects can be described by rapid, largely feed-forward, mechanisms. Several investigators have used backward masking to force visual recognition to operate in a fast regime with minimal influences from recurrent signals (<xref rid="r21" ref-type="bibr">21</xref>): When an image is rapidly followed by a spatially overlapping mask, the high-contrast noise mask interrupts any additional, presumably recurrent, processing of the original image (<xref rid="r22" ref-type="bibr">22</xref><xref rid="r23" ref-type="bibr"/>–<xref rid="r24" ref-type="bibr">24</xref>). We asked whether this fast, essentially feed-forward, recognition regime imposed by backward masking is sufficient for robust recognition of partially visible objects by randomly interleaving trials with a mask (<xref ref-type="fig" rid="fig01">Fig. 1<italic>B</italic></xref>).</p>
                <p>Performance for whole images was affected by the mask only for the shortest SOA values (compare <xref ref-type="fig" rid="fig01">Fig. 1 <italic>F</italic></xref> versus <xref ref-type="fig" rid="fig01"><italic>G</italic></xref> at 100% visibility; <italic>P</italic> &lt; 0.01, two-sided <italic>t</italic> test). When partial objects were followed by a backward mask, performance was severely impaired (compare <xref ref-type="fig" rid="fig01">Fig. 1 <italic>F</italic></xref> versus <xref ref-type="fig" rid="fig01"><italic>G</italic></xref>). A two-way ANOVA on performance with SOA and masking as factors revealed a significant interaction (<italic>P</italic> &lt; 10<sup>−8</sup>). The behavioral consequences of shortening SOA were significantly stronger in the presence of backward masking (compare solid versus dashed lines in <xref ref-type="fig" rid="fig01">Fig. 1<italic>H</italic></xref>). Additionally, backward masking disrupted performance across a wide range of visibility levels for SOAs that were ≤100 ms (<xref ref-type="fig" rid="fig01">Fig. 1 <italic>G</italic> and <italic>H</italic></xref>). Similar effects of backward masking were observed when using occluded objects (<xref ref-type="fig" rid="fig01">Fig. 1<italic>I</italic></xref>; <italic>P</italic> &lt; 0.001, two-way ANOVA) as well as when using novel objects (<xref ref-type="fig" rid="fig01">Fig. 1<italic>J</italic></xref> and <ext-link ext-link-type="uri" xlink:href="http://www.pnas.org/lookup/suppl/doi:10.1073/pnas.1719397115/-/DCSupplemental" xlink:show="new"><italic>SI Appendix</italic>, Fig. S8 <italic>C</italic> and <italic>D</italic></ext-link>; <italic>P</italic> &lt; 0.0001, two-way ANOVA). In sum, interrupting processing via backward masking led to a large reduction in the ability for recognition of partially visible objects, occluded images, and partially visible novel objects across a wide range of SOA values and visibility levels.</p>
              </sec>
              <sec id="s4">
                <title>Images More Susceptible to Backward Masking Elicited Longer Neural Delays Along Human Ventral Visual Cortex.</title>
                <p>In a recent study, we recorded invasive physiological signals throughout the ventral visual stream in human patients with epilepsy while they performed an experiment similar to the one in <xref ref-type="fig" rid="fig01">Fig. 1<italic>A</italic></xref> (<xref rid="r13" ref-type="bibr">13</xref>). This experiment included 25 objects presented for 150 ms without any masking, with random bubble positions in each trial. For whole objects, neural signals along the ventral visual stream showed rapid selective responses to different categories, as shown for an example electrode in the left fusiform gyrus in <xref ref-type="fig" rid="fig02">Fig. 2 <italic>A</italic> and <italic>B</italic></xref>. When presenting partially visible objects, the neural signals remained visually selective (<xref ref-type="fig" rid="fig02">Fig. 2 <italic>C</italic> and <italic>D</italic></xref>). The visually selective signals elicited by the partial objects were significantly delayed with respect to the responses to whole objects (compare the neural latency, defined here as the single-trial time of peak responses, in <xref ref-type="fig" rid="fig02">Fig. 2 <italic>C</italic> and <italic>D</italic></xref> with the time of peak response before 200 ms in <xref ref-type="fig" rid="fig02">Fig. 2 <italic>A</italic> and <italic>B</italic></xref>). Because the visible features varied from trial to trial, different renderings of the same object elicited a wide distribution in the neural latencies (<xref ref-type="fig" rid="fig02">Fig. 2 <italic>C</italic> and <italic>D</italic></xref>). For example, the peak voltage occurred at 206 ms after stimulus onset in response to the first image in <xref ref-type="fig" rid="fig02">Fig. 2<italic>C</italic></xref> and at 248 ms in response to the last image in <xref ref-type="fig" rid="fig02">Fig. 2<italic>C</italic></xref>.</p>
                <fig id="fig02" fig-type="figure" orientation="portrait" position="float">
                  <label>Fig. 2.</label>
                  <caption>
                    <p>Behavioral effect of masking correlated with the neural response latency on an image-by-image basis. (<italic>A</italic>) Intracranial field potential (IFP) responses from an electrode in the left fusiform gyrus averaged across five categories of whole objects while a subject was performing the task described in <xref ref-type="fig" rid="fig01">Fig. 1</xref> (no masking, 150-ms presentation time). This electrode showed a stronger response to faces (green). The gray rectangle indicates the stimulus presentation time (150 ms). The shaded area indicates SEM (details are provided in ref. <xref rid="r13" ref-type="bibr">13</xref>). (<italic>B</italic>) IFP responses for one of the whole objects for the electrode in <italic>A</italic> showing single-trial responses (gray, <italic>n</italic> = 9) and average response (green). The latency of the peak response is marked on the <italic>x</italic> axis. (<italic>C</italic>) Single-trial responses (<italic>n</italic> = 1) to four partial images of the same object in <italic>B</italic>. (<italic>D</italic>) New stimulus set for psychophysics experiments was constructed from the images in 650 trials from two electrodes in the physiology experiments. A raster of the neural responses for the example electrode in <italic>A</italic>, one trial per line, from partial image trials selected for psychophysics is shown. These trials elicited strong physiological responses with a wide distribution of response latencies (sorted by the neural latency). The color indicates the voltage (color scale on bottom). (<italic>Right</italic>, <italic>Inset</italic>) Zoomed-in view of the responses to the 82 trials in the preferred category. (<italic>E</italic>) We measured the effect of backward masking at various SOAs for each of the same partial exemplar images used in the physiology experiment (<italic>n</italic> = 33 subjects) and computed an MI for each image (<xref ref-type="sec" rid="s9"><italic>Methods</italic></xref>). The larger the MI for a given image, the stronger was the effect of masking. (<italic>F</italic>) Correlation between the effect of backward masking (<italic>y</italic> axis, MI as defined in <italic>E</italic>) and the neural response latency (<italic>x</italic> axis, as defined in <italic>B</italic> and <italic>C</italic>). Each dot is a single partial object from the preferred category for electrode 1 (blue) or 2 (gray). Error bars for the MI are based on half-split reliability (<ext-link ext-link-type="uri" xlink:href="http://www.pnas.org/lookup/suppl/doi:10.1073/pnas.1719397115/-/DCSupplemental" xlink:show="new"><italic>SI Appendix</italic>, Fig. S2</ext-link>), and the neural latency values are based on single trials. There was a significant correlation (Pearson <italic>r</italic> = 0.37; <italic>P</italic> = 0.004, linear regression, permutation test). cc, correlation coefficient.</p>
                  </caption>
                  <graphic id="gra2" xlink:href="pnas.1719397115fig02"/>
                </fig>
                <p>Heterogeneity across different renderings of the same object was also evident in the range of effects of backward masking at the behavioral level in the experiment in <xref ref-type="fig" rid="fig01">Fig. 1 <italic>G</italic> and <italic>H</italic></xref>. We hypothesized that those images that elicited longer neural delays would also be more susceptible to backward masking. To test this hypothesis, we selected two electrodes in the neurophysiological study showing strong visually selective signals (<xref ref-type="sec" rid="s9"><italic>Methods</italic></xref>; one of these sites is shown in <xref ref-type="fig" rid="fig02">Fig. 2 <italic>A</italic>–<italic>D</italic></xref>). We considered 650 images of partially visible objects corresponding to the 25 objects from the neurophysiology experiment. Using the same images (i.e., the exact same features revealed for each object), we conducted a separate psychophysics experiment to evaluate the effect of backward masking on each individual image (<italic>n</italic> = 33 subjects). This experiment allowed us to construct a curve of behavioral performance, as a function of SOA during backward masking, for each of the selected images from the neurophysiology experiment (<xref ref-type="fig" rid="fig02">Fig. 2<italic>E</italic></xref>). To quantify the effect of backward masking for each individual image, we defined a masking index (MI), 1 − AUC, where AUC is the normalized area under the curve in the performance versus SOA plot (gray area in <xref ref-type="fig" rid="fig02">Fig. 2<italic>E</italic></xref>). Larger MI values correspond to larger effects of backward masking: the MI ranges from 0 (no effect of backward masking) to 0.8 (backward masking leads to chance performance). For example, in <xref ref-type="fig" rid="fig02">Fig. 2<italic>C</italic></xref>, the first image was less affected by backward masking than the last image, particularly at short SOA values (MI values of 3% and 20%, respectively).</p>
                <p>For those images from the preferred category for each of the two electrodes, the MI showed a weak but significant correlation with the neural response latency, even after accounting for image difficulty and recording site differences (<xref ref-type="fig" rid="fig02">Fig. 2<italic>F</italic></xref>; Pearson <italic>r</italic> = 0.37; <italic>P</italic> = 0.004, permutation test; <xref ref-type="sec" rid="s9"><italic>Methods</italic></xref>). This effect was stimulus selective: The MI was not correlated with the neural response latency for images from the nonpreferred categories (<italic>P</italic> = 0.33, permutation test). The neural latencies are noisy measures based on single trials (<xref ref-type="sec" rid="s9"><italic>Methods</italic></xref> and <xref ref-type="fig" rid="fig02">Fig. 2<italic>C</italic></xref>), the physiology and behavioral experiments were conducted in different subjects, and there was variability across subjects in the MI (<xref ref-type="fig" rid="fig02">Fig. 2<italic>F</italic></xref> and <ext-link ext-link-type="uri" xlink:href="http://www.pnas.org/lookup/suppl/doi:10.1073/pnas.1719397115/-/DCSupplemental" xlink:show="new"><italic>SI Appendix</italic>, Fig. S2</ext-link>). However, despite all of these sources of noise, images that led to longer neural response latencies were associated with a stronger effect of interrupting computations via backward masking.</p>
              </sec>
              <sec id="s5">
                <title>Standard Feed-Forward Models Are Not Robust to Occlusion.</title>
                <p>We next investigated the potential computational mechanisms responsible for the behavioral and physiological observations in <xref ref-type="fig" rid="fig01">Figs. 1</xref> and <xref ref-type="fig" rid="fig02">2</xref>. We began by considering state-of-the-art implementations of purely feed-forward computational models of visual recognition. These computational models are characterized by hierarchical, feed-forward processing with progressive increases in the size of receptive fields, degree of selectivity, and tolerance to object transformations (e.g., refs. <xref rid="r2" ref-type="bibr">2</xref><xref rid="r3" ref-type="bibr"/>–<xref rid="r4" ref-type="bibr">4</xref>). Such models have been successfully used to describe rapid recognition of whole objects at the behavioral level (e.g., ref. <xref rid="r4" ref-type="bibr">4</xref>) and neuronal firing rates in area V4 and the inferior temporal cortex in macaque monkeys (e.g., ref. <xref rid="r10" ref-type="bibr">10</xref>). Additionally, these deep convolutional network architectures achieve high performance in computer vision competitions evaluating object recognition capabilities (e.g., refs. <xref rid="r6" ref-type="bibr">6</xref>, <xref rid="r7" ref-type="bibr">7</xref>).</p>
                <p>We evaluated the performance of these feed-forward models in recognition of partially visible objects using the same 325 objects (13,000 trials) in <xref ref-type="fig" rid="fig01">Fig. 1</xref>. As a representative of this family of models, we considered AlexNet (<xref rid="r6" ref-type="bibr">6</xref>), an eight-layer convolutional neural network trained via back-propagation on ImageNet, a large corpus of natural images (<xref rid="r9" ref-type="bibr">9</xref>). We used as features either activity in the last fully connected layer before readout (fc7; 4,096 units) or activity in the last retinotopic layer (pool5; 9,216 units). To measure the effect of low-level differences between categories (e.g., contrast, object area), we also considered raw pixels as baseline performance (256 × 256 = 65,536 features).</p>
                <p>We sought to measure the robustness of these networks to partial object visibility in the same way that tolerance to other transformations such as size and position changes is evaluated [i.e., by training a decision boundary on one condition such as specific size, viewpoint, or whole objects and testing on the other conditions such as other sizes, viewpoints, or occlusion (e.g., refs. <xref rid="r2" ref-type="bibr">2</xref>, <xref rid="r4" ref-type="bibr">4</xref>)]. It is not fair to compare models trained with occluded objects versus models trained exclusively with whole objects; therefore, we do not include occluded objects in the training set. Furthermore, the results in <xref ref-type="fig" rid="fig01">Fig. 1<italic>J</italic></xref> and <ext-link ext-link-type="uri" xlink:href="http://www.pnas.org/lookup/suppl/doi:10.1073/pnas.1719397115/-/DCSupplemental" xlink:show="new"><italic>SI Appendix</italic>, Fig. S8</ext-link> show that humans can perform pattern completion for novel objects without any prior training with occluded versions of those objects. We trained a support vector machine (SVM) classifier (linear kernel) on the features of whole objects and tested object categorization performance on the representation of images of partial objects. Importantly, all of the models were trained exclusively with whole objects, and performance was evaluated in images with partially visible objects. Cross-validation was performed over objects: Objects used to train the decision boundary did not appear as partial objects in the test set. The performance of raw pixels was essentially at chance level (<xref ref-type="fig" rid="fig03">Fig. 3<italic>A</italic></xref>). In contrast, the other models performed well above chance (<italic>P</italic> &lt; 10<sup>−10</sup>, two-sided <italic>t</italic> test; also <ext-link ext-link-type="uri" xlink:href="http://www.pnas.org/lookup/suppl/doi:10.1073/pnas.1719397115/-/DCSupplemental" xlink:show="new"><italic>SI Appendix</italic>, Fig. S4</ext-link>). While feed-forward models performed well above chance, there was a significant gap with respect to human performance at all visibility levels below 40% (<italic>P</italic> &lt; 0.001, χ<sup>2</sup> test; <xref ref-type="fig" rid="fig03">Fig. 3<italic>A</italic></xref>). These results are consistent with those reported in other simulations with occluded objects and similar networks (<xref rid="r25" ref-type="bibr">25</xref>). The decrease in performance of feed-forward models compared with humans depends strongly on the stimuli and on the amount of information available: Bottom-up models were comparable to humans at full visibility (<xref rid="r26" ref-type="bibr">26</xref>) (<ext-link ext-link-type="uri" xlink:href="http://www.pnas.org/lookup/suppl/doi:10.1073/pnas.1719397115/-/DCSupplemental" xlink:show="new"><italic>SI Appendix</italic>, Fig. S3</ext-link>).</p>
                <fig id="fig03" fig-type="figure" orientation="portrait" position="float">
                  <label>Fig. 3.</label>
                  <caption>
                    <p>Standard feed-forward models were not robust to occlusion. (<italic>A</italic>) Performance of feed-forward computational models (colors) compared with humans (black) (also <ext-link ext-link-type="uri" xlink:href="http://www.pnas.org/lookup/suppl/doi:10.1073/pnas.1719397115/-/DCSupplemental" xlink:show="new"><italic>SI Appendix</italic>, Figs. S4, S5, and S9<italic>A</italic></ext-link>). We used the feature representation of a subset of whole objects to train an SVM classifier and evaluated the model’s performance on the feature representation of partial objects (<xref ref-type="sec" rid="s9"><italic>Methods</italic></xref>). The objects used to train the classifier did not appear as partial objects in the test set. Human performance is shown here (150-ms SOA) for the same set of images. Error bars denote SEM (fivefold cross-validation). The single-trial neural latency for each image (<xref ref-type="fig" rid="fig02">Fig. 2<italic>B</italic></xref>) was correlated with the distance of each partial object to its whole object category center for AlexNet pool5 (<italic>B</italic>) and AlexNet fc7 (<italic>C</italic>). Each dot represents a partial object, with responses recorded from either electrode 1 (blue dots) or electrode 2 (gray dots). The correlation coefficients (cc) and <italic>P</italic> values from the permutation test are shown for each subplot.</p>
                  </caption>
                  <graphic id="gra3" xlink:href="pnas.1719397115fig03"/>
                </fig>
                <p>The decline in performance with low visibility was not specific to the set of images used in this study: AlexNet pool5 and fc7 also performed below human levels when considering novel objects (<ext-link ext-link-type="uri" xlink:href="http://www.pnas.org/lookup/suppl/doi:10.1073/pnas.1719397115/-/DCSupplemental" xlink:show="new"><italic>SI Appendix</italic>, Fig. S9<italic>A</italic></ext-link>). The decline in performance with low visibility was not specific to using pixels or the AlexNet pool5 or fc7 layer. All of the feed-forward models that we tested led to the same conclusions, including different layers of VGG16, VGG19 (<xref rid="r7" ref-type="bibr">7</xref>), InceptionV3 (<xref rid="r8" ref-type="bibr">8</xref>), and ResNet50 (<ext-link ext-link-type="uri" xlink:href="http://www.pnas.org/lookup/suppl/doi:10.1073/pnas.1719397115/-/DCSupplemental" xlink:show="new"><italic>SI Appendix</italic>, Fig. S4</ext-link>). Among these models, the VGG16 architecture provided slightly better recognition performance in the low-visibility regime.</p>
                <p>The models shown in <xref ref-type="fig" rid="fig03">Fig. 3<italic>A</italic></xref> and <ext-link ext-link-type="uri" xlink:href="http://www.pnas.org/lookup/suppl/doi:10.1073/pnas.1719397115/-/DCSupplemental" xlink:show="new"><italic>SI Appendix</italic>, Fig. S4</ext-link> were trained to optimize object classification performance in the ImageNet 2012 dataset (<xref rid="r9" ref-type="bibr">9</xref>) without any specific training for the set of objects used in our study, except for the SVM classifier. To assess whether fine-tuning the model’s weights could alleviate the challenges with limited visibility, we fine-trained AlexNet via back-propagation using the 325 whole objects and then retested this fine-tuned model on the images with limited visibility. Fine-tuning the AlexNet architecture did not lead to improvements at low visibility (<ext-link ext-link-type="uri" xlink:href="http://www.pnas.org/lookup/suppl/doi:10.1073/pnas.1719397115/-/DCSupplemental" xlink:show="new"><italic>SI Appendix</italic>, Fig. S5</ext-link>). These results are consistent with a previous computational study using feed-forward models similar to the ones in the current work and evaluating a more extensive image dataset (<xref rid="r25" ref-type="bibr">25</xref>).</p>
                <p>We used stochastic neighborhood embedding to project the AlexNet fc7 layer features onto two dimensions and to visualize the effects of occlusion on the model (<xref ref-type="fig" rid="fig04">Fig. 4<italic>C</italic></xref>). The representation of whole objects (open circles) showed a clear separation among categories, but partial objects from different categories (filled circles) were more similar to each other than to their whole object counterparts. Therefore, decision boundaries trained on whole objects did not generalize to categorization of partial objects (<xref ref-type="fig" rid="fig03">Fig. 3<italic>A</italic></xref>). Despite the success of purely feed-forward models in recognition of whole objects, these models were not robust under limited visibility.</p>
                <p>We next sought to further understand the breakdown in the models’ representations of objects under partial visibility. Removing large amounts of pixels from the objects pushed the model’s representation of the partially visible object away from their whole counterparts (<xref ref-type="fig" rid="fig04">Fig. 4<italic>C</italic></xref>). The distance between the representation of a partially visible object and the corresponding whole object category mean is indicative of the impact of partial visibility. We evaluated whether this distortion was correlated with the latencies in the neural recordings from <xref ref-type="fig" rid="fig02">Fig. 2</xref>. We reasoned that images of partial objects whose model representation was more distorted would lead to longer neural response latencies. We computed the Euclidean distance between the representation of each partial object and the whole object category mean. We found a modest but significant correlation at the object-by-object level between the computational distance to the category mean and the neural response latency for the pool5 (<xref ref-type="fig" rid="fig03">Fig. 3<italic>B</italic></xref>) and fc7 (<xref ref-type="fig" rid="fig03">Fig. 3<italic>C</italic></xref>) features. The statistical significance of these correlations was assessed by regressing the distance to category mean against the neural latency, along with the following additional predictors to account for potential confounds: (<italic>i</italic>) the percentage of object visibility and pixel distance to regress out any variation explained by low-level effects of occlusion and difficulty, (<italic>ii</italic>) the electrode number to account for the interelectrode variability in our dataset, and (<italic>iii</italic>) the MI (<xref ref-type="fig" rid="fig02">Fig. 2<italic>E</italic></xref>) to control for overall recognition difficulty. The model distance to category mean in the pool5 and fc7 layers correlated with the response latency beyond what could be explained by these additional factors (pool 5: Pearson <italic>r</italic> = 0.27; <italic>P</italic> = 0.004, permutation test; fc7: Pearson <italic>r</italic> = 0.3; <italic>P</italic> = 0.001, permutation test). In sum, state-of-the-art feed-forward architectures did not robustly extrapolate from whole to partially visible objects and failed to reach human-level performance in recognition of partially visible objects. As the difference in the representation of whole and partial objects increased, the time it took for a selective neural response to evolve for the partial objects was longer.</p>
              </sec>
              <sec id="s6">
                <title>Recurrent Neural Networks Improve Recognition of Partially Visible Objects.</title>
                <p>The behavioral, neural, and modeling results presented above suggest a need for additional computational steps beyond those present in feed-forward architectures to build a robust representation for partially visible objects. Several computational ideas, originating from models proposed by Hopfield (<xref rid="r27" ref-type="bibr">27</xref>), have shown that attractor networks can perform pattern completion. In the Hopfield network, units are connected in an all-to-all fashion with weights defining fixed attractor points dictated by the whole objects to be represented. Images that are pushed farther away by limited visibility would require more processing time to converge to the appropriate attractor, consistent with the behavioral and physiological observations. As a proof of principle, we augmented the feed-forward models discussed in the previous section with recurrent connections to generate a robust representation through an attractor-like mechanism (<xref ref-type="fig" rid="fig04">Fig. 4<italic>A</italic></xref>), with one attractor for each whole object. We used the AlexNet architecture with fixed feed-forward weights from pretraining on ImageNet (as in <xref ref-type="fig" rid="fig03">Fig. 3</xref>) and added recurrent connections to the fc7 layer. Recurrent connectivity is ubiquitous throughout all visual neocortical areas in biological systems. The motivation to include recurrent connectivity only in the fc7 layer was to examine first a simple and possibly minimal extension to the existing architectures (<xref ref-type="sec" rid="s8"><italic>Discussion</italic></xref>).</p>
                <fig id="fig04" fig-type="figure" orientation="portrait" position="float">
                  <label>Fig. 4.</label>
                  <caption>
                    <p>Dynamic RNN showed improved performance over time and was impaired by backward masking. (<italic>A</italic>) Top-level representation in AlexNet (fc7) receives inputs from fc6, governed by weights <bold>W</bold><sub>6→7</sub>. We added a recurrent loop within the top-level representation (RNN). The weight matrix <bold>W</bold><sub><italic>h</italic></sub> governs the temporal evolution of the fc7 representation (<xref ref-type="sec" rid="s9"><italic>Methods</italic></xref>). (<italic>B</italic>) Performance of the RNN<sub>h</sub> (blue) as a function of visibility. The RNN<sub>h</sub> approached human performance (black curve) and represented a significant improvement over the original fc7 layer (red curve). The red and black curves are copied from <xref ref-type="fig" rid="fig03">Fig. 3<italic>A</italic></xref> for comparison. Error bars denote SEM. (<italic>C</italic>) Temporal evolution of the feature representation for RNN<sub>h</sub> as visualized with stochastic neighborhood embedding. Over time, the representation of partial objects approaches the correct category in the clusters of whole images. (<italic>D</italic>) Overall performance of the RNN<sub>h</sub> as a function of recurrent time step compared with humans (top dashed line) and chance (bottom dashed line). Error bars denote SEM (five-way cross-validation; <xref ref-type="sec" rid="s9"><italic>Methods</italic></xref>). (<italic>E</italic>) Correlation (Corr.) in the classification of each object between the RNN<sub>h</sub> and humans. The dashed line indicates the upper bound of human–human similarity obtained by computing how well half of the subject pool correlates with the other half. Regressions were computed separately for each category, followed by averaging the correlation coefficients across categories. Over time, the model becomes more human-like (<ext-link ext-link-type="uri" xlink:href="http://www.pnas.org/lookup/suppl/doi:10.1073/pnas.1719397115/-/DCSupplemental" xlink:show="new"><italic>SI Appendix</italic>, Fig. S6</ext-link>). Error bars denote SD across categories. (<italic>F</italic>) Effect of backward masking. The same backward mask used in the psychophysics experiments was fed to the RNN<sub>h</sub> model at different SOA values (<italic>x</italic> axis). Error bars denote SEM (five-way cross-validation). Performance improved with increasing SOA values (<ext-link ext-link-type="uri" xlink:href="http://www.pnas.org/lookup/suppl/doi:10.1073/pnas.1719397115/-/DCSupplemental" xlink:show="new"><italic>SI Appendix</italic>, Fig. S10</ext-link>).</p>
                  </caption>
                  <graphic id="gra4" xlink:href="pnas.1719397115fig04"/>
                </fig>
                <p>We denote the activity of the fc7 layer at time <italic>t</italic> as the 4,096-dimensional feature vector <bold>h</bold><sub><italic>t</italic></sub>. At each time step, <bold>h</bold><sub><italic>t</italic></sub> was determined by a combination of the activity from the previous time step <bold>h</bold><sub><italic>t−</italic>1</sub> and the constant input from the previous layer <bold>x</bold>: <inline-formula><mml:math id="i1"><mml:mrow><mml:msub><mml:mi mathvariant="bold">h</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mi>f</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi mathvariant="bold">W</mml:mi><mml:mi>h</mml:mi></mml:msub><mml:msub><mml:mi mathvariant="bold">h</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>, where <italic>f</italic> introduces a nonlinearity (<xref ref-type="sec" rid="s9"><italic>Methods</italic></xref>). The input from the previous layer, fc6, was kept constant and identical to that in the feed-forward AlexNet. <bold>W</bold><sub><italic>h</italic></sub> is a weight matrix that governs the temporal evolution of the fc7 layer. We considered a Hopfield recurrent neural network (RNN<sub>h</sub>) without introducing any free parameters that depended on the partial objects, where <bold>W</bold><sub><italic>h</italic></sub> was a symmetrical weight matrix dictated by the fc7 representation of the whole objects, using the implementation of Li et al. (<xref rid="r28" ref-type="bibr">28</xref>). The initial state of the network was given by the activity in the previous layer, <inline-formula><mml:math id="i2"><mml:mrow><mml:msub><mml:mi mathvariant="bold">h</mml:mi><mml:mn>0</mml:mn></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mi mathvariant="bold">W</mml:mi><mml:mrow><mml:mn>6</mml:mn><mml:mo>→</mml:mo><mml:mn>7</mml:mn></mml:mrow></mml:msub><mml:mi>f</mml:mi><mml:mi>c</mml:mi><mml:mn>6</mml:mn></mml:mrow></mml:math></inline-formula>, followed by binarization. The state of the network evolved over time according to <inline-formula><mml:math id="i3"><mml:mrow><mml:msub><mml:mi mathvariant="bold">h</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mi>s</mml:mi><mml:mi>a</mml:mi><mml:mi>t</mml:mi><mml:mi>l</mml:mi><mml:mi>i</mml:mi><mml:mi>n</mml:mi><mml:mi>s</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi mathvariant="bold">W</mml:mi><mml:mi>h</mml:mi></mml:msub><mml:msub><mml:mi mathvariant="bold">h</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>, where <italic>satlins</italic> is a saturating nonlinearity (<xref ref-type="sec" rid="s9"><italic>Methods</italic></xref>). We verified that the whole objects constituted an attractor point in the network by ensuring that their representation did not change over time when used as inputs to the model. We next evaluated the responses of RNN<sub>h</sub> to all of the images containing partial objects. The model was run until convergence (i.e., until none of the feature signs changed between consecutive time steps). Based on the final time point, we evaluated the performance in recognizing partially visible objects. The RNN<sub>h</sub> model demonstrated a significant improvement over the AlexNet fc7 layer (<xref ref-type="fig" rid="fig04">Fig. 4<italic>B</italic></xref>; 57 ± 0.4%; <italic>P</italic> &lt; 0.001, χ<sup>2</sup> test).</p>
                <p>The dynamic trajectory of the representation of whole and partial objects in the fc7 layer of the RNN<sub>h</sub> model is visualized in <xref ref-type="fig" rid="fig04">Fig. 4<italic>C</italic></xref>. Before any recurrent computations have taken place, at <italic>t</italic> = 0 (<xref ref-type="fig" rid="fig04">Fig. 4<italic>C</italic></xref>, <italic>Left</italic>), the representations of partial objects were clustered together (closed circles in <xref ref-type="fig" rid="fig04">Fig. 4<italic>C</italic></xref>) and separated from the clusters of whole objects in each category (open circles in <xref ref-type="fig" rid="fig04">Fig. 4<italic>C</italic></xref>). As time progressed, the cluster of partial objects was pulled apart and moved toward their respective categories. For example, at <italic>t</italic> = 16 (<xref ref-type="fig" rid="fig04">Fig. 4<italic>C</italic></xref>, <italic>Center</italic>) and <italic>t</italic> = 256 (<xref ref-type="fig" rid="fig04">Fig. 4<italic>C</italic></xref>, <italic>Right</italic>), the representation of partial chairs (closed blue circles in <xref ref-type="fig" rid="fig04">Fig. 4<italic>C</italic></xref>) largely overlapped with the cluster of whole chairs (open blue circles in <xref ref-type="fig" rid="fig04">Fig. 4<italic>C</italic></xref>). Concomitant with this dynamic transformation in the representation of partial objects, the overall performance of the RNN<sub>h</sub> model improved over time (<xref ref-type="fig" rid="fig04">Fig. 4<italic>D</italic></xref>).</p>
                <p>In addition to the average performance reported in <xref ref-type="fig" rid="fig04">Fig. 4 <italic>B</italic> and <italic>D</italic></xref>, we directly compared performance at the object-by-object level between humans and the RNN<sub>h</sub> model (<ext-link ext-link-type="uri" xlink:href="http://www.pnas.org/lookup/suppl/doi:10.1073/pnas.1719397115/-/DCSupplemental" xlink:show="new"><italic>SI Appendix</italic>, Fig. S6</ext-link>). There were notable differences across categories [e.g., humans were much better than the model in detecting faces (green circles in <ext-link ext-link-type="uri" xlink:href="http://www.pnas.org/lookup/suppl/doi:10.1073/pnas.1719397115/-/DCSupplemental" xlink:show="new"><italic>SI Appendix</italic>, Fig. S6</ext-link>)]. For this reason, we first compared models and humans at the object-by-object level within each category and then averaged the results across categories. Over time, the RNN<sub>h</sub> behaved more like humans at the object-by-object level (<xref ref-type="fig" rid="fig04">Fig. 4<italic>E</italic></xref>). For each time step in the model, we computed the average correct rate on partial objects for each object, from each of the five categories, and correlated this vector with the pattern of human performance (<ext-link ext-link-type="uri" xlink:href="http://www.pnas.org/lookup/suppl/doi:10.1073/pnas.1719397115/-/DCSupplemental" xlink:show="new"><italic>SI Appendix</italic>, Fig. S6</ext-link>). The upper bound (dashed line in <xref ref-type="fig" rid="fig04">Fig. 4<italic>E</italic></xref>) represents human–human similarity, defined as the correlation in the response patterns between half of the subject pool and the other half. Over time, the recurrent model–human correlation increased toward the human–human upper bound. Additionally, there was a parallel between human performance across different SOAs and the model performance across varying recurrent time steps (<ext-link ext-link-type="uri" xlink:href="http://www.pnas.org/lookup/suppl/doi:10.1073/pnas.1719397115/-/DCSupplemental" xlink:show="new"><italic>SI Appendix</italic>, Fig. S12</ext-link>): At fewer recurrent steps, the model showed a higher correlation with human performance at short SOAs, whereas with more recurrent steps, the model showed a higher correlation with human performance at long SOAs.</p>
                <p>Adding a Hopfield-like recurrent architecture to AlexNet also improved performance in recognition of the novel objects (<ext-link ext-link-type="uri" xlink:href="http://www.pnas.org/lookup/suppl/doi:10.1073/pnas.1719397115/-/DCSupplemental" xlink:show="new"><italic>SI Appendix</italic>, Figs. S8<italic>A</italic> and S9 <italic>B</italic>–<italic>D</italic></ext-link>). Similar conclusions were obtained when considering the VGG16 architecture and adding Hopfield-like recurrent connections to the fc1 layer (<ext-link ext-link-type="uri" xlink:href="http://www.pnas.org/lookup/suppl/doi:10.1073/pnas.1719397115/-/DCSupplemental" xlink:show="new"><italic>SI Appendix</italic>, Fig. S7</ext-link>).</p>
                <p>In sum, implementing recurrent connections in an attractor-like fashion at the top of a feed-forward hierarchical model significantly improved the model’s performance in pattern completion, and the additional computations were consistent with temporal delays described at the behavioral and neural levels.</p>
              </sec>
              <sec id="s7">
                <title>Backward Masking Impaired RNN Model Performance.</title>
                <p>We reasoned that the backward mask introduced in the experiment discussed in <xref ref-type="fig" rid="fig01">Fig. 1 <italic>B</italic>, <italic>G</italic>, and <italic>H</italic></xref> impaired performance by interrupting processing, and we set out to investigate whether we could reproduce this effect in the RNN<sub>h</sub> model. We computed the responses of the AlexNet model to the mask and fed the fc6 features for the mask to the RNN<sub>h</sub> model after a certain number of time steps. Switching the mask on at earlier time points was meant to mimic shorter SOAs in the psychophysical experiments. We read out performance based on the resulting fc7 activity combining the partial object and the mask at different time points (<xref ref-type="fig" rid="fig04">Fig. 4<italic>F</italic></xref>). Presenting the mask reduced performance from 58 ± 2% (SOA = 256 time steps) to 37 ± 2% (SOA = two time steps). Although we cannot directly compare SOAs in milliseconds with time steps in the model, these results are qualitatively consistent with the behavioral effects of backward masking (<xref ref-type="fig" rid="fig01">Fig. 1<italic>H</italic></xref>; a side-by-side comparison of the physiological, behavioral, and computational dynamics is shown in <ext-link ext-link-type="uri" xlink:href="http://www.pnas.org/lookup/suppl/doi:10.1073/pnas.1719397115/-/DCSupplemental" xlink:show="new"><italic>SI Appendix</italic>, Fig. S10</ext-link>).</p>
              </sec>
            </sec>
            <sec sec-type="discussion" id="s8">
              <title>Discussion</title>
              <p content-type="flushleft">It is routinely necessary to recognize objects that are partially visible due to occlusion and poor illumination. The visual system is capable of making inferences even when only 10–20% of the object is visible (<xref ref-type="fig" rid="fig01">Fig. 1<italic>F</italic></xref>), and even for novel objects (<xref ref-type="fig" rid="fig01">Fig. 1<italic>J</italic></xref>). We investigated the mechanisms underlying such robust recognition of partially visible objects (referred to as pattern completion) at the behavioral, physiological, and computational levels. Backward masking impaired recognition of briefly presented partial images (25 ms ≤ SOA ≤ 100 ms) (<xref ref-type="fig" rid="fig01">Fig. 1 <italic>G</italic>–<italic>J</italic></xref>). The strength of the disruptive effect of backward masking was correlated with the neural delays from invasive recordings along the ventral visual stream (<xref rid="r13" ref-type="bibr">13</xref>) (<xref ref-type="fig" rid="fig02">Fig. 2</xref>). State-of-the-art bottom-up computational architectures trained on whole objects failed to achieve robustness in recognition of partially visible objects (<xref ref-type="fig" rid="fig03">Fig. 3<italic>A</italic></xref> and <ext-link ext-link-type="uri" xlink:href="http://www.pnas.org/lookup/suppl/doi:10.1073/pnas.1719397115/-/DCSupplemental" xlink:show="new"><italic>SI Appendix</italic>, Figs. S4 and S5</ext-link>). The introduction of proof-of-principle recurrent connections (<xref ref-type="fig" rid="fig04">Fig. 4<italic>A</italic></xref>) led to significant improvement in recognition of partially visible objects at the average level (<xref ref-type="fig" rid="fig04">Fig. 4<italic>B</italic></xref> and <ext-link ext-link-type="uri" xlink:href="http://www.pnas.org/lookup/suppl/doi:10.1073/pnas.1719397115/-/DCSupplemental" xlink:show="new"><italic>SI Appendix</italic>, Figs. S7 and S9<italic>B</italic></ext-link>) and also in explaining which images were easier or harder for humans at the object-by-object level (<xref ref-type="fig" rid="fig04">Fig. 4<italic>E</italic></xref> and <ext-link ext-link-type="uri" xlink:href="http://www.pnas.org/lookup/suppl/doi:10.1073/pnas.1719397115/-/DCSupplemental" xlink:show="new"><italic>SI Appendix</italic>, Figs. S6 and S12</ext-link>). The RNN<sub>h</sub> model had no free parameters that depended on the partial objects: All of the weights were determined by the whole objects. The increase in performance involved recurrent computations evolving over time that were interrupted by the introduction of a mask (<xref ref-type="fig" rid="fig04">Fig. 4<italic>F</italic></xref>).</p>
              <p>Recognition of partially visible objects requires longer reaction times (<xref rid="r11" ref-type="bibr">11</xref>, <xref rid="r15" ref-type="bibr">15</xref>) and delayed neural representation with respect to that of whole objects (<xref rid="r12" ref-type="bibr">12</xref>, <xref rid="r13" ref-type="bibr">13</xref>). These delays suggest the need for additional computations to interpret partially visible images. Interrupting those additional computations by a mask impairs recognition (<xref ref-type="fig" rid="fig01">Fig. 1 <italic>G</italic>–<italic>J</italic></xref>). Backward masking disproportionately affects recurrent computations (<xref rid="r22" ref-type="bibr">22</xref><xref rid="r23" ref-type="bibr"/>–<xref rid="r24" ref-type="bibr">24</xref>). Accordingly, we conjectured that the disruptive effect of backward masking during pattern completion could be ascribed to the impairment of such recurrent computations. The rapid and selective signals along the ventral visual stream that enable recognition of whole objects within ∼150 ms have been interpreted to reflect largely bottom-up processing (<xref rid="r2" ref-type="bibr">2</xref><xref rid="r3" ref-type="bibr"/>–<xref rid="r4" ref-type="bibr">4</xref>, <xref rid="r10" ref-type="bibr">10</xref>, <xref rid="r18" ref-type="bibr">18</xref><xref rid="r19" ref-type="bibr"/>–<xref rid="r20" ref-type="bibr">20</xref>; however, ref. <xref rid="r29" ref-type="bibr">29</xref>) Physiological delays of ∼50 ms during recognition of partial objects (<xref rid="r12" ref-type="bibr">12</xref>, <xref rid="r13" ref-type="bibr">13</xref>) provide ample time for recurrent connections to exert their effects during pattern completion. These delays could involve recruitment of lateral connections (<xref rid="r16" ref-type="bibr">16</xref>) and/or top-down signals from higher visual areas (<xref rid="r30" ref-type="bibr">30</xref>).</p>
              <p>Humans are constantly exposed to partially visible objects. While subjects had not previously seen the specific experiment images, they had had experience with occluded animals, chairs, faces, fruits, and vehicles. To evaluate whether category-specific experience with occluded objects is required for pattern completion, we conducted an experiment with completely novel objects (<xref ref-type="fig" rid="fig01">Fig. 1<italic>J</italic></xref> and <ext-link ext-link-type="uri" xlink:href="http://www.pnas.org/lookup/suppl/doi:10.1073/pnas.1719397115/-/DCSupplemental" xlink:show="new"><italic>SI Appendix</italic>, Figs. S8 and S9</ext-link>). Subjects robustly categorized novel objects under low visibility even when they had never seen those heavily occluded objects or similar ones before.</p>
              <p>There exist infinitely many possible bottom-up models. Even though we examined multiple state-of-the-art models that are successful in object recognition (AlexNet, VGG16, VGG19, InceptionV3, and ResNet50), their failure to account for the behavioral and physiological results (<xref rid="r25" ref-type="bibr">25</xref>, <xref rid="r26" ref-type="bibr">26</xref>) (<xref ref-type="fig" rid="fig03">Fig. 3</xref> and <ext-link ext-link-type="uri" xlink:href="http://www.pnas.org/lookup/suppl/doi:10.1073/pnas.1719397115/-/DCSupplemental" xlink:show="new"><italic>SI Appendix</italic>, Fig. S4</ext-link>) should be interpreted with caution. We do not imply that it is impossible for any bottom-up architecture to recognize partially visible objects. In fact, a recurrent network with a finite number of time steps can be unfolded into a bottom-up model by creating an additional layer for each time step. However, there are several advantages to recurrent architectures, including a reduction in the number of units and weights. Furthermore, such unfolding of time into layers is only applicable when we know a priori the fixed number of computational steps, whereas recurrent architectures allow an arbitrary and dynamically flexible number of computations.</p>
              <p>The RNN dynamics involve temporal evolution of the features (<xref ref-type="fig" rid="fig04">Fig. 4 <italic>C</italic>–<italic>F</italic></xref>), bringing the representation of partial objects closer to that of whole objects. These computational dynamics, map onto the temporal lags observed at the behavioral and physiological levels. The RNN<sub>h</sub> model’s performance and correlation with humans saturates at around 10–20 time steps (<xref ref-type="fig" rid="fig04">Fig. 4 <italic>C</italic>–<italic>F</italic></xref>); a combination of feed-forward signals and recurrent computations is consistent with the physiological responses to heavily occluded objects arising at around 200 ms (<xref ref-type="fig" rid="fig02">Fig. 2<italic>D</italic></xref>). The RNN<sub>h</sub> model uses discrete time steps, but a more realistic implementation should be based on spikes and continuous dynamics. A continuous time implementation of recurrent interactions shows that information can be added rapidly, particularly under noisy input conditions, and consistently with the empirically observed delays of ∼50–100 ms in <xref ref-type="fig" rid="fig01">Figs. 1</xref> and <xref ref-type="fig" rid="fig02">2</xref> (<xref rid="r29" ref-type="bibr">29</xref>). Furthermore, these dynamics are interrupted by the presentation of a backward mask in close temporal proximity to the image (<xref ref-type="fig" rid="fig01">Figs. 1 <italic>G</italic>–<italic>J</italic></xref> and <xref ref-type="fig" rid="fig04">4<italic>F</italic></xref> and <ext-link ext-link-type="uri" xlink:href="http://www.pnas.org/lookup/suppl/doi:10.1073/pnas.1719397115/-/DCSupplemental" xlink:show="new"><italic>SI Appendix</italic>, Fig. S10</ext-link>).</p>
              <p>Multiple other cues can aid recognition of partially visible objects, including relative positions, segmentation, movement, illumination, and stereopsis. Additionally, during learning, children often encounter partially visible objects that they can continuously explore from different angles. It will be interesting to integrate these additional sources of information and to understand how they contribute to pattern completion. The convergence of behavioral, physiological, and theoretical evidence presented here provides insight into the human visual recognition neural circuitry and a biologically constrained hypothesis to understand the role of recurrent computations during pattern completion.</p>
            </sec>
            <sec sec-type="methods" id="s9">
              <title>Methods</title>
              <p content-type="flushleft">An expanded version is presented in <ext-link ext-link-type="uri" xlink:href="http://www.pnas.org/lookup/suppl/doi:10.1073/pnas.1719397115/-/DCSupplemental" xlink:show="new"><italic>SI Appendix</italic></ext-link>.</p>
              <sec id="s10">
                <title>Psychophysics.</title>
                <p>A total of 106 volunteers (62 female, aged 18–34 y) participated in the behavioral experiments. We performed an experiment with partially visible objects rendered through bubbles (<xref ref-type="fig" rid="fig01">Fig. 1</xref>) and three variations with occluded objects (<xref ref-type="fig" rid="fig01">Fig. 1</xref> and <ext-link ext-link-type="uri" xlink:href="http://www.pnas.org/lookup/suppl/doi:10.1073/pnas.1719397115/-/DCSupplemental" xlink:show="new"><italic>SI Appendix</italic>, Fig. S1</ext-link>), novel objects (<xref ref-type="fig" rid="fig01">Fig. 1</xref> and <ext-link ext-link-type="uri" xlink:href="http://www.pnas.org/lookup/suppl/doi:10.1073/pnas.1719397115/-/DCSupplemental" xlink:show="new"><italic>SI Appendix</italic>, Figs. S8 and S9</ext-link>), and stimuli matched to a previous neurophysiological experiment (<xref rid="r13" ref-type="bibr">13</xref>) (<xref ref-type="fig" rid="fig02">Fig. 2</xref>). All subjects gave informed consent and the studies were approved by the Institutional Review Board at Children’s Hospital, Harvard Medical School.</p>
              </sec>
              <sec id="s11">
                <title>Neurophysiology Experiments.</title>
                <p>The neurophysiological intracranial field potential data in <xref ref-type="fig" rid="fig02">Figs. 2</xref> and <xref ref-type="fig" rid="fig03">3</xref> were taken from a study by Tang et al. (<xref rid="r13" ref-type="bibr">13</xref>). The neural latency for each image was defined as the time of the peak response in the intracranial field potential and was calculated in single trials (e.g., <xref ref-type="fig" rid="fig02">Fig. 2<italic>C</italic></xref>).</p>
              </sec>
              <sec id="s12">
                <title>Computational Models.</title>
                <p>We tested state-of-the-art feed-forward vision models, focusing on AlexNet (<xref rid="r6" ref-type="bibr">6</xref>) (<xref ref-type="fig" rid="fig03">Fig. 3</xref>; other models are shown in <ext-link ext-link-type="uri" xlink:href="http://www.pnas.org/lookup/suppl/doi:10.1073/pnas.1719397115/-/DCSupplemental" xlink:show="new"><italic>SI Appendix</italic></ext-link> and <ext-link ext-link-type="uri" xlink:href="http://www.pnas.org/lookup/suppl/doi:10.1073/pnas.1719397115/-/DCSupplemental" xlink:show="new"><italic>SI Appendix</italic>, Fig. S4</ext-link>), with weights pretrained on ImageNet (<xref rid="r6" ref-type="bibr">6</xref>, <xref rid="r9" ref-type="bibr">9</xref>). As a proof of principle, we proposed an RNN model by adding all-to-all recurrent connections to the top feature layer of AlexNet (<xref ref-type="fig" rid="fig04">Fig. 4<italic>A</italic></xref>). The RNN model was defined using only information about the whole objects by setting the recurrent weights based on a Hopfield attractor network (<xref rid="r27" ref-type="bibr">27</xref>), as implemented in MATLAB’s newhop function (<xref rid="r28" ref-type="bibr">28</xref>).</p>
              </sec>
            </sec>
            <sec sec-type="supplementary-material">
              <title>Supplementary Material</title>
              <supplementary-material content-type="local-data">
                <label>Supplementary File</label>
                <media xlink:href="pnas.1719397115.sapp.pdf"/>
              </supplementary-material>
            </sec>
          </body>
          <back>
            <ack>
              <p>We thank Carlos Ponce, Alan Yuille, Siamak Sorooshyari, and Guy Ben-Yosef for useful discussions and comments. This work was supported by a fellowship from the FITweltweit Programme of the German Academic Exchange Service (to M.S.), National Science Foundation Science and Technology Center Award CCF-123121 (to G.K.), and NIH Award R01EY026025 (to G.K.).</p>
            </ack>
            <fn-group>
              <fn fn-type="COI-statement">
                <p>The authors declare no conflict of interest.</p>
              </fn>
              <fn fn-type="other">
                <p>This article is a PNAS Direct Submission.</p>
              </fn>
              <fn fn-type="other">
                <p>Data deposition: All data and code (including image databases, behavioral measurements, physiological measurements, and computational algorithms) have been deposited on GitHub and is available at <ext-link ext-link-type="uri" xlink:href="https://github.com/kreimanlab/occlusion-classification" xlink:show="new">https://github.com/kreimanlab/occlusion-classification</ext-link>.</p>
              </fn>
              <fn fn-type="supplementary-material">
                <p>This article contains supporting information online at <ext-link ext-link-type="uri" xlink:href="http://www.pnas.org/lookup/suppl/doi:10.1073/pnas.1719397115/-/DCSupplemental" xlink:show="new">www.pnas.org/lookup/suppl/doi:10.1073/pnas.1719397115/-/DCSupplemental</ext-link>.</p>
              </fn>
            </fn-group>
            <ref-list>
              <ref id="r1">
                <label>1</label>
                <element-citation publication-type="journal">
                  <person-group person-group-type="author">
                    <name name-style="western">
                      <surname>Logothetis</surname>
                      <given-names>NK</given-names>
                    </name>
                    <name name-style="western">
                      <surname>Sheinberg</surname>
                      <given-names>DL</given-names>
                    </name>
                  </person-group>
                  <year>1996</year>
                  <article-title>Visual object recognition</article-title>
                  <source>Annu Rev Neurosci</source>
                  <volume>19</volume>
                  <fpage>577</fpage>
                  <lpage>621</lpage>
                  <pub-id pub-id-type="pmid">8833455</pub-id>
                </element-citation>
              </ref>
              <ref id="r2">
                <label>2</label>
                <element-citation publication-type="journal">
                  <person-group person-group-type="author">
                    <name name-style="western">
                      <surname>DiCarlo</surname>
                      <given-names>JJ</given-names>
                    </name>
                    <name name-style="western">
                      <surname>Cox</surname>
                      <given-names>DD</given-names>
                    </name>
                  </person-group>
                  <year>2007</year>
                  <article-title>Untangling invariant object recognition</article-title>
                  <source>Trends Cogn Sci</source>
                  <volume>11</volume>
                  <fpage>333</fpage>
                  <lpage>341</lpage>
                  <pub-id pub-id-type="pmid">17631409</pub-id>
                </element-citation>
              </ref>
              <ref id="r3">
                <label>3</label>
                <element-citation publication-type="journal">
                  <person-group person-group-type="author">
                    <name name-style="western">
                      <surname>Riesenhuber</surname>
                      <given-names>M</given-names>
                    </name>
                    <name name-style="western">
                      <surname>Poggio</surname>
                      <given-names>T</given-names>
                    </name>
                  </person-group>
                  <year>1999</year>
                  <article-title>Hierarchical models of object recognition in cortex</article-title>
                  <source>Nat Neurosci</source>
                  <volume>2</volume>
                  <fpage>1019</fpage>
                  <lpage>1025</lpage>
                  <pub-id pub-id-type="pmid">10526343</pub-id>
                </element-citation>
              </ref>
              <ref id="r4">
                <label>4</label>
                <element-citation publication-type="journal">
                  <person-group person-group-type="author">
                    <name name-style="western">
                      <surname>Serre</surname>
                      <given-names>T</given-names>
                    </name>
                    <etal/>
                  </person-group>
                  <year>2007</year>
                  <article-title>A quantitative theory of immediate visual recognition</article-title>
                  <source>Prog Brain Res</source>
                  <volume>165</volume>
                  <fpage>33</fpage>
                  <lpage>56</lpage>
                  <pub-id pub-id-type="pmid">17925239</pub-id>
                </element-citation>
              </ref>
              <ref id="r5">
                <label>5</label>
                <element-citation publication-type="journal">
                  <person-group person-group-type="author">
                    <name name-style="western">
                      <surname>Connor</surname>
                      <given-names>CE</given-names>
                    </name>
                    <name name-style="western">
                      <surname>Brincat</surname>
                      <given-names>SL</given-names>
                    </name>
                    <name name-style="western">
                      <surname>Pasupathy</surname>
                      <given-names>A</given-names>
                    </name>
                  </person-group>
                  <year>2007</year>
                  <article-title>Transformation of shape information in the ventral pathway</article-title>
                  <source>Curr Opin Neurobiol</source>
                  <volume>17</volume>
                  <fpage>140</fpage>
                  <lpage>147</lpage>
                  <pub-id pub-id-type="pmid">17369035</pub-id>
                </element-citation>
              </ref>
              <ref id="r6">
                <label>6</label>
                <element-citation publication-type="book">
                  <person-group person-group-type="author">
                    <name name-style="western">
                      <surname>Krizhevsky</surname>
                      <given-names>A</given-names>
                    </name>
                    <name name-style="western">
                      <surname>Sutskever</surname>
                      <given-names>I</given-names>
                    </name>
                    <name name-style="western">
                      <surname>Hinton</surname>
                      <given-names>G</given-names>
                    </name>
                  </person-group>
                  <year>2012</year>
                  <source>ImageNet Classification with Deep Convolutional Neural Networks</source>
                  <publisher-name>NIPS</publisher-name>
                  <publisher-loc>Montreal</publisher-loc>
                </element-citation>
              </ref>
              <ref id="r7">
                <label>7</label>
                <element-citation publication-type="other">
                  <person-group person-group-type="author">
                    <name name-style="western">
                      <surname>Simonyan</surname>
                      <given-names>K</given-names>
                    </name>
                    <name name-style="western">
                      <surname>Zisserman</surname>
                      <given-names>A</given-names>
                    </name>
                  </person-group>
                  <year>2014</year>
                  <comment>Very deep convolutional networks for large-scale image recognition. arXiv:1409.1556</comment>
                </element-citation>
              </ref>
              <ref id="r8">
                <label>8</label>
                <element-citation publication-type="other">
                  <person-group person-group-type="author">
                    <name name-style="western">
                      <surname>Szegedy</surname>
                      <given-names>C</given-names>
                    </name>
                    <name name-style="western">
                      <surname>Vanhoucke</surname>
                      <given-names>V</given-names>
                    </name>
                    <name name-style="western">
                      <surname>Ioffe</surname>
                      <given-names>S</given-names>
                    </name>
                    <name name-style="western">
                      <surname>Shlens</surname>
                      <given-names>J</given-names>
                    </name>
                    <name name-style="western">
                      <surname>Wojna</surname>
                      <given-names>Z</given-names>
                    </name>
                  </person-group>
                  <year>2015</year>
                  <comment>Rethinking the inception architecture for computer vision. arXiv:1512.005673v3</comment>
                </element-citation>
              </ref>
              <ref id="r9">
                <label>9</label>
                <element-citation publication-type="other">
                  <person-group person-group-type="author">
                    <name name-style="western">
                      <surname>Russakovsky</surname>
                      <given-names>O</given-names>
                    </name>
                    <etal/>
                  </person-group>
                  <year>2014</year>
                  <comment>ImageNet large scale visual recognition challenge. arXiv:1409.0575</comment>
                </element-citation>
              </ref>
              <ref id="r10">
                <label>10</label>
                <element-citation publication-type="journal">
                  <person-group person-group-type="author">
                    <name name-style="western">
                      <surname>Yamins</surname>
                      <given-names>DL</given-names>
                    </name>
                    <etal/>
                  </person-group>
                  <year>2014</year>
                  <article-title>Performance-optimized hierarchical models predict neural responses in higher visual cortex</article-title>
                  <source>Proc Natl Acad Sci USA</source>
                  <volume>111</volume>
                  <fpage>8619</fpage>
                  <lpage>8624</lpage>
                  <pub-id pub-id-type="pmid">24812127</pub-id>
                </element-citation>
              </ref>
              <ref id="r11">
                <label>11</label>
                <element-citation publication-type="journal">
                  <person-group person-group-type="author">
                    <name name-style="western">
                      <surname>Murray</surname>
                      <given-names>RF</given-names>
                    </name>
                    <name name-style="western">
                      <surname>Sekuler</surname>
                      <given-names>AB</given-names>
                    </name>
                    <name name-style="western">
                      <surname>Bennett</surname>
                      <given-names>PJ</given-names>
                    </name>
                  </person-group>
                  <year>2001</year>
                  <article-title>Time course of amodal completion revealed by a shape discrimination task</article-title>
                  <source>Psychon Bull Rev</source>
                  <volume>8</volume>
                  <fpage>713</fpage>
                  <lpage>720</lpage>
                  <pub-id pub-id-type="pmid">11848590</pub-id>
                </element-citation>
              </ref>
              <ref id="r12">
                <label>12</label>
                <element-citation publication-type="journal">
                  <person-group person-group-type="author">
                    <name name-style="western">
                      <surname>Kosai</surname>
                      <given-names>Y</given-names>
                    </name>
                    <name name-style="western">
                      <surname>El-Shamayleh</surname>
                      <given-names>Y</given-names>
                    </name>
                    <name name-style="western">
                      <surname>Fyall</surname>
                      <given-names>AM</given-names>
                    </name>
                    <name name-style="western">
                      <surname>Pasupathy</surname>
                      <given-names>A</given-names>
                    </name>
                  </person-group>
                  <year>2014</year>
                  <article-title>The role of visual area V4 in the discrimination of partially occluded shapes</article-title>
                  <source>J Neurosci</source>
                  <volume>34</volume>
                  <fpage>8570</fpage>
                  <lpage>8584</lpage>
                  <pub-id pub-id-type="pmid">24948811</pub-id>
                </element-citation>
              </ref>
              <ref id="r13">
                <label>13</label>
                <element-citation publication-type="journal">
                  <person-group person-group-type="author">
                    <name name-style="western">
                      <surname>Tang</surname>
                      <given-names>H</given-names>
                    </name>
                    <etal/>
                  </person-group>
                  <year>2014</year>
                  <article-title>Spatiotemporal dynamics underlying object completion in human ventral visual cortex</article-title>
                  <source>Neuron</source>
                  <volume>83</volume>
                  <fpage>736</fpage>
                  <lpage>748</lpage>
                  <pub-id pub-id-type="pmid">25043420</pub-id>
                </element-citation>
              </ref>
              <ref id="r14">
                <label>14</label>
                <element-citation publication-type="journal">
                  <person-group person-group-type="author">
                    <name name-style="western">
                      <surname>Johnson</surname>
                      <given-names>JS</given-names>
                    </name>
                    <name name-style="western">
                      <surname>Olshausen</surname>
                      <given-names>BA</given-names>
                    </name>
                  </person-group>
                  <year>2005</year>
                  <article-title>The recognition of partially visible natural objects in the presence and absence of their occluders</article-title>
                  <source>Vision Res</source>
                  <volume>45</volume>
                  <fpage>3262</fpage>
                  <lpage>3276</lpage>
                  <pub-id pub-id-type="pmid">16043208</pub-id>
                </element-citation>
              </ref>
              <ref id="r15">
                <label>15</label>
                <element-citation publication-type="journal">
                  <person-group person-group-type="author">
                    <name name-style="western">
                      <surname>Brown</surname>
                      <given-names>JM</given-names>
                    </name>
                    <name name-style="western">
                      <surname>Koch</surname>
                      <given-names>C</given-names>
                    </name>
                  </person-group>
                  <year>2000</year>
                  <article-title>Influences of occlusion, color, and luminance on the perception of fragmented pictures</article-title>
                  <source>Percept Mot Skills</source>
                  <volume>90</volume>
                  <fpage>1033</fpage>
                  <lpage>1044</lpage>
                  <pub-id pub-id-type="pmid">10883794</pub-id>
                </element-citation>
              </ref>
              <ref id="r16">
                <label>16</label>
                <element-citation publication-type="journal">
                  <person-group person-group-type="author">
                    <name name-style="western">
                      <surname>Gilbert</surname>
                      <given-names>CD</given-names>
                    </name>
                    <name name-style="western">
                      <surname>Li</surname>
                      <given-names>W</given-names>
                    </name>
                  </person-group>
                  <year>2013</year>
                  <article-title>Top-down influences on visual processing</article-title>
                  <source>Nat Rev Neurosci</source>
                  <volume>14</volume>
                  <fpage>350</fpage>
                  <lpage>363</lpage>
                  <pub-id pub-id-type="pmid">23595013</pub-id>
                </element-citation>
              </ref>
              <ref id="r17">
                <label>17</label>
                <element-citation publication-type="journal">
                  <person-group person-group-type="author">
                    <name name-style="western">
                      <surname>Angelucci</surname>
                      <given-names>A</given-names>
                    </name>
                    <name name-style="western">
                      <surname>Bressloff</surname>
                      <given-names>PC</given-names>
                    </name>
                  </person-group>
                  <year>2006</year>
                  <article-title>Contribution of feedforward, lateral and feedback connections to the classical receptive field center and extra-classical receptive field surround of primate V1 neurons</article-title>
                  <source>Prog Brain Res</source>
                  <volume>154</volume>
                  <fpage>93</fpage>
                  <lpage>120</lpage>
                  <pub-id pub-id-type="pmid">17010705</pub-id>
                </element-citation>
              </ref>
              <ref id="r18">
                <label>18</label>
                <element-citation publication-type="journal">
                  <person-group person-group-type="author">
                    <name name-style="western">
                      <surname>Kirchner</surname>
                      <given-names>H</given-names>
                    </name>
                    <name name-style="western">
                      <surname>Thorpe</surname>
                      <given-names>SJ</given-names>
                    </name>
                  </person-group>
                  <year>2006</year>
                  <article-title>Ultra-rapid object detection with saccadic eye movements: Visual processing speed revisited</article-title>
                  <source>Vision Res</source>
                  <volume>46</volume>
                  <fpage>1762</fpage>
                  <lpage>1776</lpage>
                  <pub-id pub-id-type="pmid">16289663</pub-id>
                </element-citation>
              </ref>
              <ref id="r19">
                <label>19</label>
                <element-citation publication-type="journal">
                  <person-group person-group-type="author">
                    <name name-style="western">
                      <surname>Keysers</surname>
                      <given-names>C</given-names>
                    </name>
                    <name name-style="western">
                      <surname>Xiao</surname>
                      <given-names>DK</given-names>
                    </name>
                    <name name-style="western">
                      <surname>Földiák</surname>
                      <given-names>P</given-names>
                    </name>
                    <name name-style="western">
                      <surname>Perrett</surname>
                      <given-names>DI</given-names>
                    </name>
                  </person-group>
                  <year>2001</year>
                  <article-title>The speed of sight</article-title>
                  <source>J Cogn Neurosci</source>
                  <volume>13</volume>
                  <fpage>90</fpage>
                  <lpage>101</lpage>
                  <pub-id pub-id-type="pmid">11224911</pub-id>
                </element-citation>
              </ref>
              <ref id="r20">
                <label>20</label>
                <element-citation publication-type="journal">
                  <person-group person-group-type="author">
                    <name name-style="western">
                      <surname>Liu</surname>
                      <given-names>H</given-names>
                    </name>
                    <name name-style="western">
                      <surname>Agam</surname>
                      <given-names>Y</given-names>
                    </name>
                    <name name-style="western">
                      <surname>Madsen</surname>
                      <given-names>JR</given-names>
                    </name>
                    <name name-style="western">
                      <surname>Kreiman</surname>
                      <given-names>G</given-names>
                    </name>
                  </person-group>
                  <year>2009</year>
                  <article-title>Timing, timing, timing: Fast decoding of object information from intracranial field potentials in human visual cortex</article-title>
                  <source>Neuron</source>
                  <volume>62</volume>
                  <fpage>281</fpage>
                  <lpage>290</lpage>
                  <pub-id pub-id-type="pmid">19409272</pub-id>
                </element-citation>
              </ref>
              <ref id="r21">
                <label>21</label>
                <element-citation publication-type="journal">
                  <person-group person-group-type="author">
                    <name name-style="western">
                      <surname>Serre</surname>
                      <given-names>T</given-names>
                    </name>
                    <name name-style="western">
                      <surname>Oliva</surname>
                      <given-names>A</given-names>
                    </name>
                    <name name-style="western">
                      <surname>Poggio</surname>
                      <given-names>T</given-names>
                    </name>
                  </person-group>
                  <year>2007</year>
                  <article-title>Feedforward theories of visual cortex account for human performance in rapid categorization</article-title>
                  <source>Proc Natl Acad Sci USA</source>
                  <volume>104</volume>
                  <fpage>6424</fpage>
                  <lpage>6429</lpage>
                  <pub-id pub-id-type="pmid">17404214</pub-id>
                </element-citation>
              </ref>
              <ref id="r22">
                <label>22</label>
                <element-citation publication-type="journal">
                  <person-group person-group-type="author">
                    <name name-style="western">
                      <surname>Lamme</surname>
                      <given-names>VA</given-names>
                    </name>
                    <name name-style="western">
                      <surname>Zipser</surname>
                      <given-names>K</given-names>
                    </name>
                    <name name-style="western">
                      <surname>Spekreijse</surname>
                      <given-names>H</given-names>
                    </name>
                  </person-group>
                  <year>2002</year>
                  <article-title>Masking interrupts figure-ground signals in V1</article-title>
                  <source>J Cogn Neurosci</source>
                  <volume>14</volume>
                  <fpage>1044</fpage>
                  <lpage>1053</lpage>
                  <pub-id pub-id-type="pmid">12419127</pub-id>
                </element-citation>
              </ref>
              <ref id="r23">
                <label>23</label>
                <element-citation publication-type="journal">
                  <person-group person-group-type="author">
                    <name name-style="western">
                      <surname>Kovács</surname>
                      <given-names>G</given-names>
                    </name>
                    <name name-style="western">
                      <surname>Vogels</surname>
                      <given-names>R</given-names>
                    </name>
                    <name name-style="western">
                      <surname>Orban</surname>
                      <given-names>GA</given-names>
                    </name>
                  </person-group>
                  <year>1995</year>
                  <article-title>Cortical correlate of pattern backward masking</article-title>
                  <source>Proc Natl Acad Sci USA</source>
                  <volume>92</volume>
                  <fpage>5587</fpage>
                  <lpage>5591</lpage>
                  <pub-id pub-id-type="pmid">7777553</pub-id>
                </element-citation>
              </ref>
              <ref id="r24">
                <label>24</label>
                <element-citation publication-type="journal">
                  <person-group person-group-type="author">
                    <name name-style="western">
                      <surname>Keysers</surname>
                      <given-names>C</given-names>
                    </name>
                    <name name-style="western">
                      <surname>Perrett</surname>
                      <given-names>DI</given-names>
                    </name>
                  </person-group>
                  <year>2002</year>
                  <article-title>Visual masking and RSVP reveal neural competition</article-title>
                  <source>Trends Cogn Sci</source>
                  <volume>6</volume>
                  <fpage>120</fpage>
                  <lpage>125</lpage>
                  <pub-id pub-id-type="pmid">11861189</pub-id>
                </element-citation>
              </ref>
              <ref id="r25">
                <label>25</label>
                <element-citation publication-type="other">
                  <person-group person-group-type="author">
                    <name name-style="western">
                      <surname>Pepik</surname>
                      <given-names>B</given-names>
                    </name>
                    <name name-style="western">
                      <surname>Benenson</surname>
                      <given-names>R</given-names>
                    </name>
                    <name name-style="western">
                      <surname>Ritschel</surname>
                      <given-names>T</given-names>
                    </name>
                    <name name-style="western">
                      <surname>Schiele</surname>
                      <given-names>B</given-names>
                    </name>
                  </person-group>
                  <year>2015</year>
                  <comment>What is holding back convnets for detection? arXiv:1508.02844</comment>
                </element-citation>
              </ref>
              <ref id="r26">
                <label>26</label>
                <element-citation publication-type="journal">
                  <person-group person-group-type="author">
                    <name name-style="western">
                      <surname>Wyatte</surname>
                      <given-names>D</given-names>
                    </name>
                    <name name-style="western">
                      <surname>Curran</surname>
                      <given-names>T</given-names>
                    </name>
                    <name name-style="western">
                      <surname>O’Reilly</surname>
                      <given-names>R</given-names>
                    </name>
                  </person-group>
                  <year>2012</year>
                  <article-title>The limits of feedforward vision: Recurrent processing promotes robust object recognition when objects are degraded</article-title>
                  <source>J Cogn Neurosci</source>
                  <volume>24</volume>
                  <fpage>2248</fpage>
                  <lpage>2261</lpage>
                  <pub-id pub-id-type="pmid">22905822</pub-id>
                </element-citation>
              </ref>
              <ref id="r27">
                <label>27</label>
                <element-citation publication-type="journal">
                  <person-group person-group-type="author">
                    <name name-style="western">
                      <surname>Hopfield</surname>
                      <given-names>JJ</given-names>
                    </name>
                  </person-group>
                  <year>1982</year>
                  <article-title>Neural networks and physical systems with emergent collective computational abilities</article-title>
                  <source>Proc Natl Acad Sci USA</source>
                  <volume>79</volume>
                  <fpage>2554</fpage>
                  <lpage>2558</lpage>
                  <pub-id pub-id-type="pmid">6953413</pub-id>
                </element-citation>
              </ref>
              <ref id="r28">
                <label>28</label>
                <element-citation publication-type="journal">
                  <person-group person-group-type="author">
                    <name name-style="western">
                      <surname>Li</surname>
                      <given-names>J</given-names>
                    </name>
                    <name name-style="western">
                      <surname>Michel</surname>
                      <given-names>A</given-names>
                    </name>
                    <name name-style="western">
                      <surname>Porod</surname>
                      <given-names>W</given-names>
                    </name>
                  </person-group>
                  <year>1989</year>
                  <article-title>Analysis and synthesis of a class of neural networks: Linear systems operating on a closed hypercube</article-title>
                  <source>IEEE Trans Circuits Syst</source>
                  <volume>36</volume>
                  <fpage>1405</fpage>
                  <lpage>1422</lpage>
                </element-citation>
              </ref>
              <ref id="r29">
                <label>29</label>
                <element-citation publication-type="journal">
                  <person-group person-group-type="author">
                    <name name-style="western">
                      <surname>Panzeri</surname>
                      <given-names>S</given-names>
                    </name>
                    <name name-style="western">
                      <surname>Rolls</surname>
                      <given-names>ET</given-names>
                    </name>
                    <name name-style="western">
                      <surname>Battaglia</surname>
                      <given-names>F</given-names>
                    </name>
                    <name name-style="western">
                      <surname>Lavis</surname>
                      <given-names>R</given-names>
                    </name>
                  </person-group>
                  <year>2001</year>
                  <article-title>Speed of feedforward and recurrent processing in multilayer networks of integrate-and-fire neurons</article-title>
                  <source>Network</source>
                  <volume>12</volume>
                  <fpage>423</fpage>
                  <lpage>440</lpage>
                  <pub-id pub-id-type="pmid">11762898</pub-id>
                </element-citation>
              </ref>
              <ref id="r30">
                <label>30</label>
                <element-citation publication-type="journal">
                  <person-group person-group-type="author">
                    <name name-style="western">
                      <surname>Fyall</surname>
                      <given-names>AM</given-names>
                    </name>
                    <name name-style="western">
                      <surname>El-Shamayleh</surname>
                      <given-names>Y</given-names>
                    </name>
                    <name name-style="western">
                      <surname>Choi</surname>
                      <given-names>H</given-names>
                    </name>
                    <name name-style="western">
                      <surname>Shea-Brown</surname>
                      <given-names>E</given-names>
                    </name>
                    <name name-style="western">
                      <surname>Pasupathy</surname>
                      <given-names>A</given-names>
                    </name>
                  </person-group>
                  <year>2017</year>
                  <article-title>Dynamic representation of partially occluded objects in primate prefrontal and visual cortex</article-title>
                  <source>eLife</source>
                  <volume>6</volume>
                  <fpage>e25784</fpage>
                  <pub-id pub-id-type="pmid">28925354</pub-id>
                </element-citation>
              </ref>
            </ref-list>
          </back>
        </article>
      </metadata>
    </record>
  </GetRecord>
</OAI-PMH>
