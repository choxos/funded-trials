<?xml version="1.0" encoding="UTF-8"?>
<OAI-PMH xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/ http://www.openarchives.org/OAI/2.0/OAI-PMH.xsd">
  <responseDate>2024-05-03T06:03:39Z</responseDate>
  <request verb="GetRecord" identifier="oai:pubmedcentral.nih.gov:3541363" metadataPrefix="pmc">https:/www.ncbi.nlm.nih.gov/pmc/oai/oai.cgi</request>
  <GetRecord>
    <record>
      <header>
        <identifier>oai:pubmedcentral.nih.gov:3541363</identifier>
        <datestamp>2013-01-16</datestamp>
        <setSpec>plosone</setSpec>
        <setSpec>pmc-open</setSpec>
      </header>
      <metadata>
        <article xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" xsi:schemaLocation="https://jats.nlm.nih.gov/ns/archiving/1.3/ https://jats.nlm.nih.gov/archiving/1.3/xsd/JATS-archivearticle1-3.xsd" article-type="research-article">
          <front>
            <journal-meta>
              <journal-id journal-id-type="nlm-ta">PLoS One</journal-id>
              <journal-id journal-id-type="iso-abbrev">PLoS ONE</journal-id>
              <journal-id journal-id-type="publisher-id">plos</journal-id>
              <journal-id journal-id-type="pmc">plosone</journal-id>
              <journal-title-group>
                <journal-title>PLoS ONE</journal-title>
              </journal-title-group>
              <issn pub-type="epub">1932-6203</issn>
              <publisher>
                <publisher-name>Public Library of Science</publisher-name>
                <publisher-loc>San Francisco, USA</publisher-loc>
              </publisher>
            </journal-meta>
            <article-meta>
              <article-id pub-id-type="accession">PMC3541363</article-id>
              <article-id pub-id-type="pmcid">PMC3541363</article-id>
              <article-id pub-id-type="pmc-uid">3541363</article-id>
              <article-id pub-id-type="pmid">23326353</article-id>
              <article-id pub-id-type="pmid">23326353</article-id>
              <article-id pub-id-type="publisher-id">PONE-D-12-07446</article-id>
              <article-id pub-id-type="doi">10.1371/journal.pone.0052737</article-id>
              <article-categories>
                <subj-group subj-group-type="heading">
                  <subject>Research Article</subject>
                </subj-group>
                <subj-group subj-group-type="Discipline-v2">
                  <subject>Biology</subject>
                  <subj-group>
                    <subject>Neuroscience</subject>
                    <subj-group>
                      <subject>Sensory Systems</subject>
                      <subj-group>
                        <subject>Visual System</subject>
                      </subj-group>
                    </subj-group>
                    <subj-group>
                      <subject>Sensory Perception</subject>
                    </subj-group>
                  </subj-group>
                </subj-group>
                <subj-group subj-group-type="Discipline-v2">
                  <subject>Medicine</subject>
                  <subj-group>
                    <subject>Mental Health</subject>
                    <subj-group>
                      <subject>Psychology</subject>
                      <subj-group>
                        <subject>Sensory Perception</subject>
                      </subj-group>
                    </subj-group>
                  </subj-group>
                </subj-group>
                <subj-group subj-group-type="Discipline-v2">
                  <subject>Social and Behavioral Sciences</subject>
                  <subj-group>
                    <subject>Psychology</subject>
                    <subj-group>
                      <subject>Behavior</subject>
                      <subj-group>
                        <subject>Attention (Behavior)</subject>
                        <subject>Emotions</subject>
                      </subj-group>
                    </subj-group>
                    <subj-group>
                      <subject>Neuropsychology</subject>
                      <subject>Sensory Perception</subject>
                    </subj-group>
                  </subj-group>
                </subj-group>
              </article-categories>
              <title-group>
                <article-title>Emotions' Impact on Viewing Behavior under Natural Conditions</article-title>
                <alt-title alt-title-type="running-head">Emotions' Impact on Viewing Behavior</alt-title>
              </title-group>
              <contrib-group>
                <contrib contrib-type="author">
                  <name>
                    <surname>Kaspar</surname>
                    <given-names>Kai</given-names>
                  </name>
                  <xref ref-type="aff" rid="aff1">
                    <sup>1</sup>
                  </xref>
                  <xref ref-type="aff" rid="aff2">
                    <sup>2</sup>
                  </xref>
                  <xref ref-type="corresp" rid="cor1">
                    <sup>*</sup>
                  </xref>
                </contrib>
                <contrib contrib-type="author">
                  <name>
                    <surname>Hloucal</surname>
                    <given-names>Teresa-Maria</given-names>
                  </name>
                  <xref ref-type="aff" rid="aff1">
                    <sup>1</sup>
                  </xref>
                </contrib>
                <contrib contrib-type="author">
                  <name>
                    <surname>Kriz</surname>
                    <given-names>Jürgen</given-names>
                  </name>
                  <xref ref-type="aff" rid="aff1">
                    <sup>1</sup>
                  </xref>
                </contrib>
                <contrib contrib-type="author">
                  <name>
                    <surname>Canzler</surname>
                    <given-names>Sonja</given-names>
                  </name>
                  <xref ref-type="aff" rid="aff2">
                    <sup>2</sup>
                  </xref>
                </contrib>
                <contrib contrib-type="author">
                  <name>
                    <surname>Gameiro</surname>
                    <given-names>Ricardo Ramos</given-names>
                  </name>
                  <xref ref-type="aff" rid="aff2">
                    <sup>2</sup>
                  </xref>
                </contrib>
                <contrib contrib-type="author">
                  <name>
                    <surname>Krapp</surname>
                    <given-names>Vanessa</given-names>
                  </name>
                  <xref ref-type="aff" rid="aff2">
                    <sup>2</sup>
                  </xref>
                </contrib>
                <contrib contrib-type="author">
                  <name>
                    <surname>König</surname>
                    <given-names>Peter</given-names>
                  </name>
                  <xref ref-type="aff" rid="aff2">
                    <sup>2</sup>
                  </xref>
                  <xref ref-type="aff" rid="aff3">
                    <sup>3</sup>
                  </xref>
                </contrib>
              </contrib-group>
              <aff id="aff1">
                <label>1</label>
                <addr-line>Institute of Psychology, University of Osnabrück, Osnabrück, Germany</addr-line>
              </aff>
              <aff id="aff2">
                <label>2</label>
                <addr-line>Institute of Cognitive Science, University of Osnabrück, Osnabrück, Germany</addr-line>
              </aff>
              <aff id="aff3">
                <label>3</label>
                <addr-line>Department of Neurophysiology and Pathophysiology, University Medical Center Hamburg-Eppendorf, Hamburg, Germany</addr-line>
              </aff>
              <contrib-group>
                <contrib contrib-type="editor">
                  <name>
                    <surname>Penney</surname>
                    <given-names>Trevor Bruce</given-names>
                  </name>
                  <role>Editor</role>
                  <xref ref-type="aff" rid="edit1"/>
                </contrib>
              </contrib-group>
              <aff id="edit1">
                <addr-line>National University of Singapore, Singapore</addr-line>
              </aff>
              <author-notes>
                <corresp id="cor1">* E-mail: <email>kkaspar@uos.de</email></corresp>
                <fn fn-type="COI-statement">
                  <p><bold>Competing Interests: </bold>The authors have declared that no competing interests exist.</p>
                </fn>
                <fn fn-type="con">
                  <p>Conceived and designed the experiments: KK TMH JK PK. Performed the experiments: KK RRG SC VK. Analyzed the data: KK. Contributed reagents/materials/analysis tools: KK PK. Wrote the paper: KK TMH JK PK.</p>
                </fn>
              </author-notes>
              <pub-date pub-type="collection">
                <year>2013</year>
              </pub-date>
              <pub-date pub-type="epub">
                <day>9</day>
                <month>1</month>
                <year>2013</year>
              </pub-date>
              <volume>8</volume>
              <issue>1</issue>
              <elocation-id>e52737</elocation-id>
              <history>
                <date date-type="received">
                  <day>14</day>
                  <month>3</month>
                  <year>2012</year>
                </date>
                <date date-type="accepted">
                  <day>21</day>
                  <month>11</month>
                  <year>2012</year>
                </date>
              </history>
              <permissions>
                <copyright-statement>© 2013 Kaspar et al</copyright-statement>
                <copyright-year>2013</copyright-year>
                <copyright-holder>Kaspar et al</copyright-holder>
                <license xlink:href="http://creativecommons.org/licenses/by/4.0/">
                  <license-p>This is an open-access article distributed under the terms of the Creative Commons Attribution License, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are properly credited.</license-p>
                </license>
              </permissions>
              <abstract>
                <p>Human overt attention under natural conditions is guided by stimulus features as well as by higher cognitive components, such as task and emotional context. In contrast to the considerable progress regarding the former, insight into the interaction of emotions and attention is limited. Here we investigate the influence of the current emotional context on viewing behavior under natural conditions.</p>
                <p>In two eye-tracking studies participants freely viewed complex scenes embedded in sequences of emotion-laden images. The latter primes constituted specific emotional contexts for neutral target images.</p>
                <p>Viewing behavior toward target images embedded into sets of primes was affected by the current emotional context, revealing the intensity of the emotional context as a significant moderator. The primes themselves were not scanned in different ways when presented within a block (Study 1), but when presented individually, negative primes were more actively scanned than positive primes (Study 2). These divergent results suggest an interaction between emotional priming and further context factors. Additionally, in most cases primes were scanned more actively than target images. Interestingly, the mere presence of emotion-laden stimuli in a set of images of different categories slowed down viewing activity overall, but the known effect of image category was not affected. Finally, viewing behavior remained largely constant on single images as well as across the targets' post-prime positions (Study 2).</p>
                <p>We conclude that the emotional context significantly influences the exploration of complex scenes and the emotional context has to be considered in predictions of eye-movement patterns.</p>
              </abstract>
              <funding-group>
                <funding-statement>The work was supported by grant FP7-ICT-270212 Extending Sensorimotor Contingencies to Cognition (Peter König); Community Research and Development Information Service. URL: <ext-link ext-link-type="uri" xlink:href="http://cordis.europa.eu/">http://cordis.europa.eu/</ext-link>. The funders had no role in study design, data collection and analysis, decision to publish, or preparation of the manuscript.</funding-statement>
              </funding-group>
              <counts>
                <page-count count="14"/>
              </counts>
            </article-meta>
          </front>
          <body>
            <sec id="s1">
              <title>Introduction</title>
              <p>In everyday life visual attention is guided by stimulus features, often labeled as the bottom-up way of attention control, as well as by individual cognitive components, such as motivation, knowledge, and emotional status that affect attention processes in a top-down manner.</p>
              <p>The stimulus-dependent, bottom-up component of overt visual attention, the process of actively directing the gaze, has been widely studied by detecting, analyzing, and predicting the correlations between fixation behavior and image features <xref rid="pone.0052737-Henderson1" ref-type="bibr">[1]</xref>, <xref rid="pone.0052737-Baddeley1" ref-type="bibr">[2]</xref>, <xref rid="pone.0052737-Reinagel1" ref-type="bibr">[3]</xref>, <xref rid="pone.0052737-Krieger1" ref-type="bibr">[4]</xref>. However, this type of analysis mostly focuses on low-level features and geometrical properties. As a result, the variance explained is limited <xref rid="pone.0052737-Tatler1" ref-type="bibr">[5]</xref>, <xref rid="pone.0052737-Willming1" ref-type="bibr">[6]</xref>. Furthermore, the causal influence of low-level features on the selection of fixated regions is at the center of a continuing controversy <xref rid="pone.0052737-Tatler1" ref-type="bibr">[5]</xref>, <xref rid="pone.0052737-Carmi1" ref-type="bibr">[7]</xref>, <xref rid="pone.0052737-Einhuser1" ref-type="bibr">[8]</xref>, <xref rid="pone.0052737-Henderson2" ref-type="bibr">[9]</xref>, <xref rid="pone.0052737-Kaspar1" ref-type="bibr">[10]</xref>. Furthermore, it has been demonstrated that cognitive influences can override the stimulus saliency and thereby limit its predictive capability <xref rid="pone.0052737-Henderson3" ref-type="bibr">11</xref>,<xref rid="pone.0052737-Underwood1" ref-type="bibr">12</xref>. A hierarchical model preferring cognitive top-down influences, if present <xref rid="pone.0052737-Acik1" ref-type="bibr">[13]</xref>, or a combination of both stimulus-driven and cognitive high-level aspects <xref rid="pone.0052737-Cerf1" ref-type="bibr">[14]</xref>, <xref rid="pone.0052737-Peters1" ref-type="bibr">[15]</xref>, <xref rid="pone.0052737-Einhuser2" ref-type="bibr">[16]</xref>, emphasizes the need to consider top-down influences on attention processes. Such research on the impact of higher cognitive functions on attention concentrates on visual search, memory, or previous knowledge <xref rid="pone.0052737-Henderson2" ref-type="bibr">[9]</xref>, <xref rid="pone.0052737-Peters1" ref-type="bibr">[15]</xref>, <xref rid="pone.0052737-Ballard1" ref-type="bibr">[17]</xref>, <xref rid="pone.0052737-Chen1" ref-type="bibr">[18]</xref>, <xref rid="pone.0052737-DeAngelus1" ref-type="bibr">[19]</xref>, <xref rid="pone.0052737-Einhuser3" ref-type="bibr">[20]</xref>, <xref rid="pone.0052737-Kaspar2" ref-type="bibr">[21]</xref>, <xref rid="pone.0052737-Hamborg1" ref-type="bibr">[22]</xref>, <xref rid="pone.0052737-Navalpakkam1" ref-type="bibr">[23]</xref>, <xref rid="pone.0052737-Navalpakkam2" ref-type="bibr">[24]</xref>, <xref rid="pone.0052737-Pomplun1" ref-type="bibr">[25]</xref>. Remarkably, investigations on the impact of the emotional state as an integral part of cognition <xref rid="pone.0052737-Phelps1" ref-type="bibr">[26]</xref> on visual perception are rare.</p>
              <p>The International Affective Picture System (IAPS) is a set of natural visual stimuli with validated emotional valence and arousal <xref rid="pone.0052737-Lang1" ref-type="bibr">[27]</xref>, <xref rid="pone.0052737-Calvo1" ref-type="bibr">[28]</xref>, <xref rid="pone.0052737-Calvo2" ref-type="bibr">[29]</xref>, <xref rid="pone.0052737-Nummenmaa1" ref-type="bibr">[30]</xref>. It is a most appropriate tool to study visual processing in the context of emotionally relevant stimuli in clinical <xref rid="pone.0052737-Gotlib1" ref-type="bibr">[31]</xref>, <xref rid="pone.0052737-Joormann1" ref-type="bibr">[32]</xref>, <xref rid="pone.0052737-Joormann2" ref-type="bibr">[33]</xref> and non-clinical samples <xref rid="pone.0052737-Palermo1" ref-type="bibr">[34]</xref>, <xref rid="pone.0052737-Schub1" ref-type="bibr">[35]</xref>. Nummenmaa, Hyönä, and Calvo <xref rid="pone.0052737-Nummenmaa1" ref-type="bibr">[30]</xref> found a higher probability to fixate on emotional pictures first when presented simultaneously with neutral stimuli, which is in line with the results of Calvo and Lang <xref rid="pone.0052737-Calvo1" ref-type="bibr">[28]</xref>, <xref rid="pone.0052737-Calvo2" ref-type="bibr">[29]</xref>. Face-perception studies substantiate these results. Schubö, Gendolla, Meinecke, and Abele <xref rid="pone.0052737-Schub1" ref-type="bibr">[35]</xref> demonstrated that threatening faces were detected more quickly than friendly ones, supporting the notion of a threat-detection advantage. Interestingly, the perception of emotional faces is not simply predictable on the basis of the features of the presented stimuli, but the emotional state of the individual is important as well. Depressed patients, for instance, correctly identify sad and angry faces with a lesser intensity of facial expression than do subjects without psychopathological symptoms <xref rid="pone.0052737-Joormann1" ref-type="bibr">[32]</xref>. The impact of the subjects' emotional state on attention processes was impressively shown in a study with arachnophobic patients <xref rid="pone.0052737-Peira1" ref-type="bibr">[36]</xref>: viewing behavior was not explicitly based on stimulus saliency and general emotional relevance of the stimuli. Rather, the subjects' emotional state, in this case spider-phobia, was the crucial factor for an increased attention to suddenly appearing objects (spiders) in the scene. Similar results were obtained with snake-phobic patients <xref rid="pone.0052737-hmann1" ref-type="bibr">[37]</xref>. Thus, available results indicate that emotionally relevant stimuli capture attention rapidly, and visual attention is guided by emotional processes even outside of the context of emotional stimuli.</p>
              <p>These results raise the questions to what extent viewing behavior is affected by the emotional content of complex scenes, and whether the current emotional context influences human viewing behavior under natural conditions. These two aspects have been completely neglected in past eye-tracking studies on emotion and attention. Actually, the impact of emotional priming on the viewing behavior of complex, neutral scenes has not been directly addressed. Instead, some studies that did not apply eye-tracking technology offer interesting references to emotional priming: Flaisch, Stockburger, and Schupp <xref rid="pone.0052737-Flaisch1" ref-type="bibr">[38]</xref> studied changes in event-related potentials (ERP) while watching emotional primes and neutral target pictures, and they found a reduced, late positive potential in both primes and targets. Hence, psycho-physiological activation due to emotional stimuli seems to influence perception of neutral stimuli as well. However, Most, Chun, Widders, and Zald <xref rid="pone.0052737-Most1" ref-type="bibr">[39]</xref> also found an interference effect of emotional and perception processes, supporting the notion of prolonged processes in target perception. Consequently, when the emotional state affects neutral target processing, the rate of eye movements may increase or decrease.</p>
              <p>Here we investigate the potential impact of the emotional context on viewing behavior. For this purpose we measured eye-movement data of subjects viewing emotion-laden as well as neutral complex scenes in different free-viewing conditions. As an important cognitive modulator, we manipulated the emotional state of the participants by using the standardized pictures of the IAPS as primes <xref rid="pone.0052737-Lang1" ref-type="bibr">[27]</xref>. We provide information about subjects' viewing behavior by analyzing the duration of fixations, saccade amplitudes, and the spread of fixation distributions by means of a progressive entropy analysis. According to the main null hypothesis, we expected the measured parameters to remain stable across the various trials. Alternatively, if the emotional context influenced free-viewing behavior in a top-down manner, the measured eye-movement parameters should be affected.</p>
              <p>In order to scrutinize these research questions we conducted two studies: in Study 1 we investigated whether (1) negative, neutral, or positive complex scenes elicit discriminative viewing behavior, and (2) whether the kind of emotional context constituted by these scenes affects how people observe nature landscape images. In Study 2, we implemented a more sophisticated design to (1) replicate the findings of Study 1, (2) enlarge the findings on neutral target images differing in gist, and (3) enable a comparison between the data of the present study and one of our previous studies <xref rid="pone.0052737-Kaspar2" ref-type="bibr">[21]</xref>. The latter study serves as the baseline condition in which subjects observed images under the same natural conditions, but without an emotional context</p>
            </sec>
            <sec id="s2">
              <title>Study 1</title>
              <sec id="s2a">
                <title>Methods</title>
                <sec id="s2a1">
                  <title>Ethics statement</title>
                  <p>Both Study 1 and Study 2 conformed to the Code of Ethics of the American Psychological Association, to the Declaration of Helsinki, and to national guidelines. Written, informed consent was obtained from all participants. The study was approved by the ethics committee of the University Osnabrück.</p>
                </sec>
                <sec id="s2a2">
                  <title>Participants</title>
                  <p>Thirty-six students (majoring in Psychology and Cognitive Science) participated in the study for course credits. Ages ranged from 18–30 years with a mean of 21.78 (SD = 2.47; 19 female participants). They had normal or corrected-to-normal visual acuity and were naive to the purpose of the study as well as to the experimental setup. All subjects signed a written consent form to participate in the experiment.</p>
                </sec>
                <sec id="s2a3">
                  <title>Stimuli</title>
                  <p>In Study 1 participants observed neutral target images embedded into different emotional contexts (negative, neutral, and positive). Hence, we used four sets of complex images in this study: (1) positive images constituting an emotional context of positive valence, (2) negative images constituting a corresponding emotional context, (3) fractal images constituting a neutral emotional context, and (4) neutral target images showing nature landscapes.</p>
                  <p>Overall 80 pictures from the international affective picture system (IAPS <xref rid="pone.0052737-Lang2" ref-type="bibr">[40]</xref>) served as emotion-laden stimuli (also denoted as “primes”) and were used to influence participants' emotional states. Positive and negative IAPS images were selected with respect to high differences in valence ratings comparable to previous studies <xref rid="pone.0052737-Keil1" ref-type="bibr">[41]</xref>, <xref rid="pone.0052737-Bradley1" ref-type="bibr">[42]</xref>, <xref rid="pone.0052737-Bradley2" ref-type="bibr">[43]</xref>. Thereby, 40 IAPS pictures with low valence values (all images &lt;3 on a scale ranging from 0–9; image numbers in the IAPS are: 2053, 2205, 2345.1, 2456, 2683, 2688, 2703, 2710, 2800, 2900, 3180, 3181, 3230, 3300, 3350, 3500, 3530, 3550.1, 6212, 6315, 6520, 6540, 6550, 6821, 8485, 9041, 9075, 9140, 9163, 9181, 9183, 9184, 9187, 9250, 9254, 9332, 9410, 9414, 9419, 9921) served as negative emotional primes and created a negative context for embedded, neutral target images. The positive-context condition was developed using 40 IAPS pictures with high valence values (&gt;7; image numbers in the IAPS: 1340, 1441, 1460, 1500, 1604, 1610, 1630, 1710, 1721, 1722, 1750, 1920, 1999, 2035, 2045, 2057, 2070, 2080, 2156, 2165, 2222, 2260, 2274, 2300, 2311, 2314, 2332, 2341, 2345, 2360, 2370, 2388, 2391, 2530, 2540, 2550, 2650, 2660, , 4599, 4641), excluding pictures showing erotic content. Negative and positive primes differed in their mean arousal ratings [negative primes: M = 5.90, SD = 0.727; positive primes: M = 4.419, SD = 0.642; t-test: p&lt;.001].</p>
                  <p>For neutral target images we used 30 complex nature images from the McGill Calibrated Colour Image Database <xref rid="pone.0052737-Olmos1" ref-type="bibr">[44]</xref> depicting bushes, trees, or meadows without man-made objects. These images have been used in previous studies <xref rid="pone.0052737-Kaspar1" ref-type="bibr">[10]</xref>, <xref rid="pone.0052737-Acik1" ref-type="bibr">[13]</xref>, <xref rid="pone.0052737-Kaspar2" ref-type="bibr">[21]</xref>, <xref rid="pone.0052737-Acik2" ref-type="bibr">[45]</xref>.</p>
                  <p>In order to build a neutral emotional context, we used 40 fractal images that were borrowed from the online fractal database, chaotic n-space network (<ext-link ext-link-type="uri" xlink:href="http://www.cnspace.net/html/fractals.html">http://www.cnspace.net/html/fractals.html</ext-link>). This choice is important from two aspects:</p>
                  <p>First, nature images themselves are not suitable to build an appropriate neutral context, because they are identical to the type of target images we used. In contrast, the emotion-laden IAPS images (i.e., primes) in the positive and negative context conditions substantially differ from nature target images. Hence, a potential oddball-effect could be present in the positive and negative context conditions <xref rid="pone.0052737-Pariyadath1" ref-type="bibr">[46]</xref>. This is, when a unique stimulus (i.e., the oddball stimulus; e.g., a flower) is embedded in a train of repeated or similar stimuli (e.g., humans or animals as depicted in IAPS images), the oddball stimulus can seem to persist for a longer duration than the stimuli constituting the context. As a consequence, such an oddball effect could affect eye-movement parameters. Therefore, the primes constituting the neutral emotional context also have to be different from the target images to rule out potential context-dependent differences in viewing behavior that are signatures of oddball vs. non-oddball effects.</p>
                  <p>Second, the type of neutral primes should be of similar complexity compared to emotion-laden primes, and they should differ significantly in their visual appearance. Hence, monochrome or white-noise images are not appropriate. Even pink-noise images containing natural second-order statistics <xref rid="pone.0052737-Einhuser4" ref-type="bibr">[47]</xref>, <xref rid="pone.0052737-Kayser1" ref-type="bibr">[48]</xref> are not suitable, because they evoke fatigue in the observer and produce viewing behavior differing significantly from what can be found on complex scenes of other categories <xref rid="pone.0052737-Kaspar2" ref-type="bibr">[21]</xref>. Consequently, from the current insights in viewing behavior of complex scenes, fractal images are the first choice to build a neutral emotional context. All images were scaled down or cropped to a resolution of 1280×960 pixels (4∶3) and converted to bitmap format.</p>
                </sec>
                <sec id="s2a4">
                  <title>Apparatuses</title>
                  <p>The laboratory and experimental setup were identical to those realized in our recent studies <xref rid="pone.0052737-Kaspar1" ref-type="bibr">[10]</xref>, <xref rid="pone.0052737-Kaspar2" ref-type="bibr">[21]</xref> in order to allow the comparison of present data with the viewing behavior previously observed. For a detailed description of the system setup, see <xref rid="pone.0052737-Kaspar2" ref-type="bibr">[21]</xref>. In short: the experiment was conducted in a darkened room, providing constant background lighting conditions. Stimuli were presented on a 21-inch Samsung SyncMaster 1100 DF 2004 CRT Monitor (Samsung Electronics, Seoul, South Korea). The display resolution was 1280×960 pixels, the refresh rate was 85 Hz, and the distance to the screen was set at 80 cm without headrest to facilitate normal viewing conditions. The computer running the experiment was connected to the host computer (Pentium 4, Dell Inc., Round Rock, TX, USA) with EyeLink software via a local network.</p>
                  <p>Eye movements were recorded via the EyeLink II system (SR Research, Ontario, Canada), which uses infrared pupil tracking at a sampling rate of 500 Hz and compensates for head movements. Spatial resolution is better than 0.01° visual angle. To calibrate, participants made saccades to a grid of 13 fixation spots on the screen, which appeared one by one in a random order. Tracking of the eye, giving the lower validation error, started as soon as this value was below 0.35°. After each stimulus presentation, a fixation spot appeared in the middle of the screen to control for slow drifts in measured eye movements. Calibration and validation were repeated in cases of an error larger than 1°. Fixations and saccades were detected and parameterized automatically by the eye-tracker. The first fixation of each trial was excluded from analysis, because its localization was determined by the preceding fixation spot used for drift correction.</p>
                </sec>
                <sec id="s2a5">
                  <title>Procedure</title>
                  <p>Participants first were informed about the study and had to sign a consent form. Specifically, subjects were explicitly informed about the “unpleasant” stimuli used in the experiment. The possibility to end the experiment whenever desired was acknowledged by signing an additional consent form. Then, all participants had to pass the Ishihara Test for Color Blindness <xref rid="pone.0052737-Ishihara1" ref-type="bibr">[49]</xref>.</p>
                  <p>After the calibration and validation procedures described above, the eye-tracking session proper started. The sequence of emotional contexts (negative, neutral, and positive) was counterbalanced across subjects. Moreover, the sequence of primes constituting these emotional contexts was randomized within each context. Finally, each of the 30 neutral target images was assigned equally often to each of the three emotional contexts, whereby the order of target images within a context was randomly generated for each participant. Besides all these balancing arrangements preventing sequence effects, we also pseudo-randomly varied the position of primes and targets within a block so that a target image followed after three to six primes (see <xref ref-type="fig" rid="pone-0052737-g001">Fig. 1</xref> for an exemplary sequence). As a result, the occurrence of target images was unpredictable, and expectation effects on viewing behavior can be ruled out.</p>
                  <fig id="pone-0052737-g001" orientation="portrait" position="float">
                    <object-id pub-id-type="doi">10.1371/journal.pone.0052737.g001</object-id>
                    <label>Figure 1</label>
                    <caption>
                      <title>Sequence of images within an emotional context condition.</title>
                      <p>The emotional context was built by positive or negative IAPS images, or by neutral fractal images. Primes were presented in a block intermingled by nature target images, whereby the occurrence of target images was unpredictable.</p>
                    </caption>
                    <graphic xlink:href="pone.0052737.g001"/>
                  </fig>
                  <p>Furthermore, the minimum number of three primes (on average four primes) preceding a target provided an emotional priming of adequate intensity. Previous studies showed that the effect of high-valence IAPS images on papillary responses, heart rate, and skin conductance <xref rid="pone.0052737-Bradley2" ref-type="bibr">[43]</xref>, as well as on ERP amplitudes <xref rid="pone.0052737-Cuthbert1" ref-type="bibr">[50]</xref>, is significantly different from the effect of neutral images and that these differences were sustained for most of the presentation duration of images. In contrast to these previous studies, several IAPS images were successively presented before target onset in the present study, so that the effect of single IAPS primes should be much more sustainable and, hence, create an intensive emotional context. In accordance with previous studies <xref rid="pone.0052737-Keil1" ref-type="bibr">[41]</xref>, <xref rid="pone.0052737-Bradley1" ref-type="bibr">[42]</xref>, <xref rid="pone.0052737-Bradley2" ref-type="bibr">[43]</xref>, <xref rid="pone.0052737-Cuthbert1" ref-type="bibr">[50]</xref>, the presentation duration of each picture was six seconds. This duration ensured direct comparability to our previous eye-tracking studies with free-viewing tasks <xref rid="pone.0052737-Kaspar1" ref-type="bibr">[10]</xref>, <xref rid="pone.0052737-Kaspar2" ref-type="bibr">[21]</xref>. At the end, subjects were informed about the purpose of the study.</p>
                </sec>
                <sec id="s2a6">
                  <title>Saccade parameters</title>
                  <p>Eye-tracking data were analyzed to investigate the fixation duration and the visual step sizes by means of saccade lengths. For all subsequent analyses, the first fixation was excluded, since it was solely a result of the preceding fixation spot used for drift correction. Moreover, in order to prevent data biases, we excluded fixation durations that were shorter than 40 ms or that were more than two standard deviations above the grand mean. The lower temporal bound of 40 ms was chosen in accordance with the literature <xref rid="pone.0052737-Henderson1" ref-type="bibr">[1]</xref>, <xref rid="pone.0052737-Rayner1" ref-type="bibr">[51]</xref>. The fixation duration was calculated online by the eye-tracking system. Saccade frequency was computed as the number of valid saccades per time unit. Saccade length was operationalized by the Euclidean distance between two consecutive fixations marked by their two-dimensional coordinates in image space. To prevent data biases in the saccade length computation, we excluded all saccades that led to fixation locations beyond the screen boundary.</p>
                </sec>
                <sec id="s2a7">
                  <title>Fixation distribution analysis</title>
                  <p>To investigate the spread of fixation distributions independent of specific geometrical arrangements, we employed the concept of entropy by following a technique applied recently <xref rid="pone.0052737-Kaspar2" ref-type="bibr">[21]</xref>, <xref rid="pone.0052737-Kaspar3" ref-type="bibr">[52]</xref>. An entropy value quantifies the spread of a given fixation distribution without any prior assumption about the geometric properties of the resulting distribution. Higher values indicate a more spread out distribution. The minimum value occurs for a singular distribution; the maximum for a flat distribution. The absolute entropy values, however, depend on image resolution as well as on the size of the Gaussian kernel used for convolution and are not relevant in the present context. For an example of the effect size indicated by numerical differences in entropy values, please see <xref rid="pone.0052737-Kaspar3" ref-type="bibr">[52]</xref>. Hence, for the present purpose, entropy is a suitable signature of the explorativeness of an observer's viewing behavior.</p>
                  <p>Importantly, estimators of entropy are influenced by sample size <xref rid="pone.0052737-Miller1" ref-type="bibr">[53]</xref>, <xref rid="pone.0052737-Hausser1" ref-type="bibr">[54]</xref>. As no general unbiased estimator is available, we equalized the bias using a bootstrapping technique <xref rid="pone.0052737-Willming1" ref-type="bibr">[6]</xref>, <xref rid="pone.0052737-Kaspar2" ref-type="bibr">[21]</xref>, <xref rid="pone.0052737-Kaspar3" ref-type="bibr">[52]</xref>. We analyzed the spread of fixation distributions on a fine-grained level (i.e., single subjects observing single images) as follows: the fixation distribution map of subject <italic>s</italic> viewing image <italic>i</italic> was convolved with a Gaussian kernel, resulting in a fixation density map (FDM). The full width at half maximum (FWHM) of the Gaussian kernel defining the size of the patch was set to 1° of visual angle. In order to address the link between fixation numbers and entropy values, we used a bootstrapping correction by randomly sampling nine fixation points out of the pool of all fixations made by subject <italic>s</italic> on image <italic>i</italic>. Then the entropy <italic>E</italic> of the resulted FDM was calculated according to<disp-formula id="pone.0052737.e001"><graphic xlink:href="pone.0052737.e001.jpg" position="anchor" orientation="portrait"/></disp-formula>where <italic>x</italic> indicates the position in image space. This procedure was done with 100 repetitions, and finally, the mean entropy was calculated for subject <italic>s</italic> observing image <italic>i</italic>. These bootstrapping-corrected entropy values were subsequently averaged across all images of a category (negative, neutral, or positive primes, as well as target images). All cases in which subjects scanned single images with less than 9 fixations (&lt;5%) were excluded from this bootstrapping analysis to prevent a loss of statistical power and to allow a direct comparison with our previous study <xref rid="pone.0052737-Kaspar2" ref-type="bibr">[21]</xref>.</p>
                </sec>
                <sec id="s2a8">
                  <title>Statistical testing</title>
                  <p>In all statistical tests we chose a significance level of 5% and applied Bonferroni-correction where appropriate. Before applying parametrical tests, Kolmogorov-Smirnov tests were calculated to check normal distribution of data, and homogeneity of variances was tested by means of Levene's test. Effect sizes as indicators of practical significance are reported by partial eta squared η<sub>p</sub>
<sup>2</sup> [0.01 = small; 0.06 = medium; 0.14 = large] in the case of an analysis of variance (Greenhouse-Geisser applied) and by Cohen's d [0.3 = small; 0.5 = medium; 0.8 = large] in the case of a t-test <xref rid="pone.0052737-Cohen1" ref-type="bibr">[55]</xref>.</p>
                </sec>
              </sec>
              <sec id="s2b">
                <title>Results</title>
                <sec id="s2b1">
                  <title>Viewing behavior on emotional primes</title>
                  <p>First, we analyzed eye-movement parameters by computing analyses of variance (ANOVA) for repeated measures in order to compare negative, neutral, and positive primes. Eye-movement parameters were normally distributed in all conditions as revealed by Kolmogorov-Smirnov tests [all p≥0.561]. With respect to the mean fixation duration (averaged across all fixations made on a single image), no difference was found between positive IAPS images, negative IAPS images, and neutral fractal images [F(1.869, 65.419) = 0.398; p = 0.659; η<sub>p</sub>
<sup>2</sup> = 0.011] (<xref ref-type="fig" rid="pone-0052737-g002">Fig. 2-A</xref>). Regarding the mean saccade length on images, we also found no effect of the prime category [F(1.803, 63.094) = 0.271; p = 0.741; η<sub>p</sub>
<sup>2</sup> = 0.008] (<xref ref-type="fig" rid="pone-0052737-g002">Fig. 2-B</xref>). Finally, no dependence of entropy on prime categories was found [F(1.952, 68.304) = 0.843; p = 0.432; η<sub>p</sub>
<sup>2</sup> = 0.024] (<xref ref-type="fig" rid="pone-0052737-g002">Fig. 2-C</xref>).</p>
                  <fig id="pone-0052737-g002" orientation="portrait" position="float">
                    <object-id pub-id-type="doi">10.1371/journal.pone.0052737.g002</object-id>
                    <label>Figure 2</label>
                    <caption>
                      <title>Differences in eye-movement parameters between emotional primes (negative, neutral, and positive) and targets embedded into corresponding emotional contexts.</title>
                      <p>(A) mean fixation duration, (B) mean saccade length, and (C) mean entropy quantifying the spread of fixation distributions. On the left side of each figure, emotional primes are contrasted. On the right side, nature target images embedded into different emotional contexts are contrasted. Vertical lines on top of bars indicate standard error of the mean.</p>
                    </caption>
                    <graphic xlink:href="pone.0052737.g002"/>
                  </fig>
                  <p>We additionally investigated if the similarity in saccade length and fixation duration between prime types was stable across the observation time of an image. For that purpose, the duration of fixations was scrutinized for the first nine valid fixations as well as for the length of the first nine valid saccades (only &lt;5% of trials showed a fixation number below nine and, hence, were excluded from this analysis). Eye-movement parameters on primes were normally distributed independent of the current fixation or saccade, respectively [all p≥0.260].</p>
                  <p>A 3×9 ANOVA (prime type×fixation number) for repeated measures revealed a main effect of the current fixation [F(5.269, 184.417) = 45.811; p&lt;0.001; η<sub>p</sub>
<sup>2</sup> = 0.567] with a large increase from the first to the second fixation [Bonferroni-adjusted t-test: p&lt;0.001]. The fixation duration remained constant from the second fixation as revealed by pairwise comparisons between fixations [all p≥0.284] (see <xref ref-type="fig" rid="pone-0052737-g003">Fig. 3</xref>, left side). Again, no effect of the prime type on fixation duration was found [F(1.883, 65.895) = 0.387; p = 0.668; η<sub>p</sub>
<sup>2</sup> = 0.011]. Moreover, no significant interaction between the current fixation and the prime type was revealed [F(9.460, 331.101) = 1.624; p = 0.103; η<sub>p</sub>
<sup>2</sup> = 0.044]. With respect to the length of the first nine valid saccades, a 3×9 ANOVA (prime type×saccade number) revealed a significant change over time [F(6.385, 223.485) = 12.892; p&lt;0.001; η<sub>p</sub>
<sup>2</sup> = 0.269], whereby only a significant increase from the first to the second saccade caused this effect [p&lt;0.001]. Saccade lengths remained constant from the second saccade [all p&gt;0.999] (<xref ref-type="fig" rid="pone-0052737-g003">Fig. 3</xref>, right side). Moreover, neither an effect of the prime type was found [F(1.852, 64.806) = 0.072; p = 0.919; η<sub>p</sub>
<sup>2</sup> = 0.002], nor an interaction [F(10.293, 360.242) = 1.097; p = 0.363; η<sub>p</sub>
<sup>2</sup> = 0.030].</p>
                  <fig id="pone-0052737-g003" orientation="portrait" position="float">
                    <object-id pub-id-type="doi">10.1371/journal.pone.0052737.g003</object-id>
                    <label>Figure 3</label>
                    <caption>
                      <title>Time dependency of fixation duration and saccade length.</title>
                      <p>Fixation duration (left side) and mean saccade length (right side) for negative, neutral, and positive primes as well as for nature target images depending on the emotional context in which they were embedded. Vertical lines on data points indicate standard error of the mean.</p>
                    </caption>
                    <graphic xlink:href="pone.0052737.g003"/>
                  </fig>
                  <p>To conclude, the primes constituting the emotional contexts did not evoke different viewing behavior by means of fixation duration, visual step size, or spatial explorativeness.</p>
                </sec>
                <sec id="s2b2">
                  <title>Viewing behavior on target images</title>
                  <p>In the second step, we analyzed eye-movement parameters on target images embedded into the different emotional contexts. Eye-movement parameters on targets were normally distributed in all context conditions [Kolmogorov-Smirnov tests: all p≥0.238].</p>
                  <p>One-way ANOVAs for repeated measures revealed a significant influence of the emotional context on how participants scanned the target images. With respect to fixation duration, a significant effect of emotional priming was observable [F(1.896, 66.374) = 6.645; p = 0.003; η<sub>p</sub>
<sup>2</sup> = 0.160] (<xref ref-type="fig" rid="pone-0052737-g002">Fig. 2-A</xref>). Subsequently, we calculated Bonferroni-adjusted t-tests to pairwisely compare the priming conditions: target images were observed with fixations of longer duration in the negative context compared to the neutral and positive emotional contexts [both p≤0.035]. The neutral and positive contexts did not differ in their influence on target images [p&gt;0.999]. With respect to the mean saccade length on target images, we also found a significant effect of the emotional context [F(1.810, 63.355) = 3.870; p = 0.030; η<sub>p</sub>
<sup>2</sup> = 0.100], whereby the negative emotional context evoked shorter saccades than the neutral context [p = 0.053] (<xref ref-type="fig" rid="pone-0052737-g002">Fig. 2-B</xref>). No difference was found between the positive context and the other two priming conditions [both p≥0.401]. Finally, an effect of the priming conditions was found regarding entropy [F(1.875, 65.634) = 4.392; p = 0.018; η<sub>p</sub>
<sup>2</sup> = 0.111] (<xref ref-type="fig" rid="pone-0052737-g002">Fig. 2-C</xref>). Thereby, target images presented within the negative context were scanned less extensively than target images embedded into the neutral context [p = 0.033]. Again, no difference was found between the positive context and the other two priming conditions [both p≥0.160]. Consequently, the increased fixation duration and reduced saccade length on target images evoked by the negative context were paralleled by lowered entropy.</p>
                  <p>As done for the primes, we also analyzed viewing behavior on target images on the level of single fixations and saccades. Fixation durations and saccade lengths were normally distributed in all conditions [Kolmogorov-Smirnov-tests: all p≥0.087].</p>
                  <p>A 3×9 ANOVA (emotional context×fixation number) for repeated measures revealed a main effect of the current fixation [F(5.652, 197.834) = 3.550; p = 0.003; η<sub>p</sub>
<sup>2</sup> = 0.092]: the duration of the first fixation was significantly shorter than several subsequent fixations [three p≤0.021], which themselves did not significantly differ from each other [all p&gt;0.999] (<xref ref-type="fig" rid="pone-0052737-g003">Fig. 3</xref>, left side). Also when limited to the first nine fixations, a significant effect of the emotional context on target observation was found [F(1.887, 66.047) = 5.378; p = 0.008; η<sub>p</sub>
<sup>2</sup> = 0.133]. Bonferroni-adjusted pairwise comparisons of emotional contexts showed shorter fixations on targets embedded into a negative emotional context compared to a positive context [p = 0.013], as well as (by trend) compared to a neutral context [p = 0.081]. No interaction between the current fixation and the emotional context was found [F(9.772, 342.033) = 1.233; p = 0.270; η<sub>p</sub>
<sup>2</sup> = 0.034]. This is the context effect sustained over time, though it was revealed with a short temporal delay, as shown by <xref ref-type="fig" rid="pone-0052737-g003">Fig. 3</xref> (left side).</p>
                  <p>With respect to the length of the first nine valid saccades, a 3×9 ANOVA (emotional context×saccade number) revealed a change over time [F(6.421, 224.725) = 2.164; p = 0.043; η<sub>p</sub>
<sup>2</sup> = 0.058]. Thereby, the length of the first saccade was minimal and differed significantly from the third saccade [p = 0.021], which had a maximal average length. After the third saccade, the lengths continuously decreased, but Bonferroni-adjusted pairwise comparisons of saccade numbers did not reveal further significant differences [all p≥0.113] (<xref ref-type="fig" rid="pone-0052737-g003">Fig. 3</xref>, right side). Furthermore, an effect of the emotional context was revealed by trend [F(1.920, 67.204) = 0.086; p = 0.086; η<sub>p</sub>
<sup>2</sup> = 0.069], with longer saccades on target images embedded into the neutral context, in contrast to the positive and negative contexts. No significant interaction was found [F(10.509, 367.829) = 1.479; p = 0.141; η<sub>p</sub>
<sup>2</sup> = 0.041], and hence, the context effect was relatively constant over time.</p>
                  <p>To sum up, although different prime types were identically explored, target images embedded into the emotional contexts were observed differentially, depending on the prime type. Hence, no simple transfer effect from the primes to the targets was found on the level of eye movements; rather, viewing behavior on targets seemed to be influenced by the effect of the emotional context on the inner state of the observer. Thereby, the context effect on fixation duration and saccade length was revealed with a short temporal delay after stimulus onset, but was sustained over time.</p>
                </sec>
                <sec id="s2b3">
                  <title>Differences between emotion-laden primes and neutral target images</title>
                  <p>In the last step, we compared the emotion-laden primes (negative, neutral, and positive) with the target images presented in three different emotional contexts regarding fixation duration, saccade length, and entropy. <xref ref-type="fig" rid="pone-0052737-g002">Fig. 2</xref> depicts the corresponding means. Because of multiple testing, we refer here to a Bonferroni-adjusted significance level of <inline-formula><inline-graphic xlink:href="pone.0052737.e002.jpg"/></inline-formula>.</p>
                  <p>Overall, fixation durations were significantly shorter on emotion-laden primes (independent of prime type) compared to neutral target images embedded into the three different emotional contexts, as revealed by t-test for paired samples [all p≤0.001; all d≥0.672]. Moreover, saccade lengths were longer on targets embedded into a neutral context compared to primes independent of the primes' emotional category [all p≤0.006; all d≥0.522]. Targets presented in a positive context were also scanned with longer saccades than neutral and positive primes [both p≤0.027; all both≥0.406], but targets in a positive context did not differ from negative primes with respect to saccade length [p = 0.150; d = 0.255]. No difference was found between prime types and targets embedded into a negative emotional context [all p≥0.564; all d≤0.102]. Finally, entropy values were higher on target images than on primes independent of the emotional context and the prime type [all p≤0.006; all d≥0.493], except the comparison of positive primes vs. targets in a negative context [p = 0.046; d = 0.345] and except a non-significant difference between negative primes and targets embedded into them [p = 0.187; all d = 0.255].</p>
                  <p>To sum up, primes were scanned with fixations of shorter durations, with particularly shorter saccades, and were less spatially extensive in terms of entropy. Consequently, participants observed the primes more actively, but less spatially extensively.</p>
                </sec>
              </sec>
              <sec id="s2c">
                <title>Discussion of Study 1</title>
                <p>Study 1 investigated two mostly disregarded aspects of overt visual attention: (1) Do complex visual scenes differing in their emotional content elicit discriminative viewing behavior, and (2) does the current emotional context specifically affect how observers explore subsequently presented nature images? To answer these questions, neutral target images depicting nature landscapes were embedded in a train of emotion-laden (negative or positive) or neutral primes. Participants were instructed to freely observe the images. We report three central results:</p>
                <list list-type="order">
                  <list-item>
                    <p>The primes constituting a specific emotional context were scanned in a standard fashion. That is, independent of the primes' emotional content, they were observed with fixations of identical duration, with saccades of identical length, and equally spatially extensively.</p>
                  </list-item>
                  <list-item>
                    <p>Overall, the primes were scanned more actively but less spatially extensively than the target images. Our participants used a higher saccade frequency to scan the primes (i.e., fixations of shorter duration). Interestingly, the speeded up pace neither corresponded to a more spread out fixation distribution nor to a longer visual step size. Previous studies have already shown that a higher pace of saccades is not necessarily accompanied by a more visually extensive image scanning or longer saccades, respectively <xref rid="pone.0052737-Kaspar2" ref-type="bibr">[21]</xref>, <xref rid="pone.0052737-Kaspar3" ref-type="bibr">[52]</xref>. Participants showed a kind of active tunnel vision on primes, independent of prime type. Perhaps the fact that similar primes were presented in a train evoked a characteristic reduction of the focus of attention; however, thereby, the viewing activity was on a high level, which contradicts the hypothesis that participants had been bored by the repeating image type. In a recent study <xref rid="pone.0052737-Kaspar2" ref-type="bibr">[21]</xref> we found that boredom, as elicited by the repetition of pink-noise images, was expressed by a significantly reduced viewing activity paralleled by a tunnel vision. Probably the target images embedded into the sets of primes appeared as unique stimuli (“oddballs,” according to Pariyadath and Eagleman <xref rid="pone.0052737-Pariyadath1" ref-type="bibr">[46]</xref>) and hence evoked a more explorative viewing behavior than the repeated prime category. It is possible that the cognitive affordances to process the occasional target images were enhanced in contrast to the similar primes. This interpretation is supported by a study of Velichkovsky, Sprenger, and Pomplun <xref rid="pone.0052737-Velichkovsky1" ref-type="bibr">[56]</xref>, which found a link between the increase of fixation duration and a rise of engagement in cognitive processes. This difference between primes and targets is very interesting, since even the primes differ substantially in their content. However, similarity of images seems not to be limited to the gist of visual scenes, but rather the emotional content could be a strong promoter of the similarity of complex scenes. In any case, the missing difference in viewing behavior on different primes facilitates the interpretation of eye movements on target images.</p>
                  </list-item>
                  <list-item>
                    <p>Viewing behavior on neutral target images embedded into the sets of primes was significantly affected by the current emotional context. When target images were shown within a train of negative primes, the viewing behavior was slowed down and less spatially extensive, as indicated by longer fixation durations, shorter saccades, and lower entropy in contrast to a neutral emotional context. The impact of the positive emotional context was in between with respect to saccade length and entropy (except regarding fixation duration) and did not statistically differ from the impact of the neutral and negative emotional contexts. Moreover, the context effect on fixation duration and saccade length was revealed with a short temporal delay after stimulus onset, but was sustained over time, as shown by a more detailed analysis on the level of single fixations and saccades.</p>
                  </list-item>
                </list>
                <p>All in all, these results clearly indicate that no simple transfer effect from the primes to the targets occurred, but rather viewing behavior on targets seemed to be influenced by the effect of the current emotional context on the inner state of the observer.</p>
              </sec>
            </sec>
            <sec id="s3">
              <title>Study 2</title>
              <p>The design of Study 1 was optimized to first demonstrate a potential effect of emotional priming on the observer's viewing behavior on neutral target images. The results demonstrate the viability of this concept; however, further questions arose: can we find an effect of emotional priming even when the intensity of priming is reduced? In Study 1 a target image was on average preceded by 4 primes. In Study 2 we reduced the priming to a single image. Moreover, what effect does the type of target image have on eye-movement parameters? In Study 1, the neutral target images were limited to landscape scenes without man-made objects. However, previous studies showed significant differences in viewing behavior between several types of complex scenes <xref rid="pone.0052737-Kaspar1" ref-type="bibr">[10]</xref>, <xref rid="pone.0052737-Acik1" ref-type="bibr">[13]</xref>, <xref rid="pone.0052737-Kaspar2" ref-type="bibr">[21]</xref>, <xref rid="pone.0052737-Acik2" ref-type="bibr">[45]</xref>, <xref rid="pone.0052737-Frey1" ref-type="bibr">[57]</xref>. Therefore, we extended the set of nature target images by two further target types in order to reveal potential effects of the target category on eye-movement parameters. Finally, does the post-prime position of target images, i.e., the number of intervening neutral targets, influence the way in which they are observed? Therefore, we conducted a follow-up study that addressed all of these issues in terms of a more elaborate experimental design. In more detail, we investigate the following questions:</p>
              <list list-type="order">
                <list-item>
                  <p>Can we replicate the non-existing effect between negative and positive emotion-laden images (i.e. primes) on viewing behavior within a new experimental design?</p>
                </list-item>
                <list-item>
                  <p>Do single, emotion-laden images have a measureable priming effect on neutral target images compared to the more intense priming in Study 1?</p>
                </list-item>
                <list-item>
                  <p>Do emotion-laden primes of high valence differ from neutral target images regarding eye-movement parameters, as shown in Study 1?</p>
                </list-item>
                <list-item>
                  <p>Does the post-prime position of neutral images influence the way in which the images are observed?</p>
                </list-item>
                <list-item>
                  <p>Finally, which impact does the category of the neutral target images have in a certain emotional context constituted by emotion-laden primes? Can we replicate the signature of eye-movement parameters on complex images differing in scene gist <xref rid="pone.0052737-Kaspar2" ref-type="bibr">[21]</xref>? In our previous study, participants repeatedly observed complex images of different categories. In the first presentation run, all images were presented once and without an emotional context. Hence, this first presentation run served as baseline condition that gave us the opportunity to compare these data with the present ones.</p>
                </list-item>
              </list>
              <sec id="s3a">
                <title>Method</title>
                <sec id="s3a1">
                  <title>Participants</title>
                  <p>Twenty-four students (Psychology and Cognitive Science) participated in the study for course credits. Ages ranged from 18–32 years with a mean of 23.67 (SD = 3.76; 16 female participants). All subjects had normal or corrected-to-normal visual acuity, and they were naive to the purpose of the study as well as to the experimental setup. A written consent form to participate in the experiment was signed by all subjects. As in Study 1, ethical standards of the APA, the German Psychological Society (DGPs) and the German Psychological Association (BDP) were considered.</p>
                </sec>
                <sec id="s3a2">
                  <title>Apparatuses and stimuli</title>
                  <p>The laboratory and experimental setup in Study 2 was identical to those used in Study 1. Overall, 36 IAPS images were used as emotional primes to influence the participants' emotional states. All negative images had a valence value below 3 (image numbers in the IAPS: 2683, 2811, 3530, 6212, 6313, 6350, 6520, 6550, 6570, 6821, 8485, 9163, 9183, 9187, 9254, 9410, 9414, 9921); all positive images were rated with 7 or higher (image numbers: 1441, 1460, 1710, 1750, 1999, 2045, 2057, 2070, 2080, 2260, 2274, 2311, 2347, 2530, 2540, 2550, 2650, 2660). Negative and positive primes differed in their arousal ratings [negative primes: M = 6.584, SD = 0.367; positive primes: M = 4.531, SD = 0.500; t-test: p&lt;.001]. No neutral primes were included.</p>
                  <p>One hundred and eight complex images formed the set of neutral target stimuli. See <xref ref-type="fig" rid="pone-0052737-g004">Fig. 4</xref> for examples. They belonged to three categories: the first category (nature) contained 36 images from of the McGill Calibrated Colour Image Database <xref rid="pone.0052737-Olmos1" ref-type="bibr">[44]</xref>, as in Study 1. The second category (urban) comprised 36 images, such as streets, vehicles, and house exteriors. These pictures were taken with a high-resolution camera (Nikon D2X) and were unfamiliar to the participants. Nature and urban images were free of people or writing. The third category (fractal) consisted of 36 software-generated fractal pictures taken from the online fractal database, chaotic n-space network (<ext-link ext-link-type="uri" xlink:href="http://www.cnspace.net/html/fractals.html">http://www.cnspace.net/html/fractals.html</ext-link>). Four, eight, and eight images of the nature, urban und fractal categories, respectively, were also used in previous studies <xref rid="pone.0052737-Kaspar1" ref-type="bibr">[10]</xref>, <xref rid="pone.0052737-Kaspar2" ref-type="bibr">[21]</xref>. All images from these three categories were scaled down or cropped to a resolution of 1280×960 pixels (4∶3) and converted to bitmap format.</p>
                  <fig id="pone-0052737-g004" orientation="portrait" position="float">
                    <object-id pub-id-type="doi">10.1371/journal.pone.0052737.g004</object-id>
                    <label>Figure 4</label>
                    <caption>
                      <title>Sequence of images (example).</title>
                      <p>One positive or negative IAPS image serving as prime was followed by three neutral target images belonging to the three different image categories (nature, urban, fractal), respectively. The emotional content of primes as well as the order of target images were randomized.</p>
                    </caption>
                    <graphic xlink:href="pone.0052737.g004"/>
                  </fig>
                </sec>
                <sec id="s3a3">
                  <title>Procedure</title>
                  <p>Participants were first informed about the study and signed a consent form. Subjects were explicitly informed about “unpleasant” stimuli used in the experiment. Then, they had to pass the Ishihara Test for Color Blindness <xref rid="pone.0052737-Ishihara1" ref-type="bibr">[49]</xref>. After the common calibration and validation procedure, the eye-tracking session started. The emotional IAPS images (denoted as “primes”) were presented randomly. In this way, participants were not able to predict the following prime type. After one prime, three neutral target images were presented, whereby each target image derived from another category (nature, urban, or fractal), but their order was randomized (see <xref ref-type="fig" rid="pone-0052737-g004">Fig. 4</xref>). To prevent any potential bias, the sequence of images was balanced within individual subjects and across all subjects. As a consequence, each target image occurred equally often at a certain post-prime position, while also considering the emotional content of the preceding prime. All images were presented for 6 seconds.</p>
                </sec>
                <sec id="s3a4">
                  <title>Data analysis</title>
                  <p>The analysis of fixation duration, saccade frequency, and entropy was identical with the procedure applied in Study 1 as described above.</p>
                </sec>
              </sec>
              <sec id="s3b">
                <title>Results</title>
                <sec id="s3b1">
                  <title>Viewing behavior on emotion-laden images: positive versus negative primes (question 1)</title>
                  <p>Viewing behavior on negative and positive primes was statistically compared by t-tests for paired samples. All eye-movement parameters were normally distributed [Kolmogorov-Smirnov tests: all p≥0.639]. With respect to fixation durations we found a small- to medium-sized difference: positive IAPS images, compared to their negative counterparts, elicited significantly longer fixations by trend [t(23) = 1.995; p = 0.058; d = 0.407] (<xref ref-type="fig" rid="pone-0052737-g005">Fig. 5</xref>, left side). This difference was paralleled by shorter saccades used to scan positive primes [t(23) = −2.516; p = 0.19; d = 0.514,] and by the mean entropy of fixations made on a single image [t(23) = −9.128; p&lt;0.001; d = 1.865], indicating a less extensive spatial exploration of positive primes in contrast to negative ones (<xref ref-type="fig" rid="pone-0052737-g005">Fig. 5</xref>, left side). Consequently, negative IAPS images were scanned more actively and spatially extensively than positive images.</p>
                  <fig id="pone-0052737-g005" orientation="portrait" position="float">
                    <object-id pub-id-type="doi">10.1371/journal.pone.0052737.g005</object-id>
                    <label>Figure 5</label>
                    <caption>
                      <title>Eye-movement parameters on emotion-laden and neutral images.</title>
                      <p>Eye-movement parameters on negative and positive IAPS images serving as emotion-laden primes (left side) and parameters on neutral target images (nature, urban, and fractal) depending on target type and emotional context in which target images were embedded (right side). Vertical lines above bars indicate standard error of the mean. Note: The axis of ordinates differs between primes and targets with respect to the scaling. Moreover, the experimental setups of Studies 1 and 2 were identical, and therefore, absolute parameter values are comparable between both studies.</p>
                    </caption>
                    <graphic xlink:href="pone.0052737.g005"/>
                  </fig>
                  <p>As in Study 1, we additionally scrutinized the duration of fixations for the first nine valid fixations, as well the length of the first nine saccades [Kolmogorov-Smirnov tests: all p≥0.157]. With respect to fixation duration, a 2×9 ANOVA (prime type×fixation number) for repeated measures revealed a main effect of the current fixation [F(5.607, 128.956)  = 13.479; p&lt;0.001; η<sub>p</sub>
<sup>2</sup> = 0.370]. We again found longer fixations on positive primes compared to negative primes by trend when analyzing only the first nine fixations [F(1, 23) = 3.429; p = 0.077; η<sub>p</sub>
<sup>2</sup> = 0.130]. No significant interaction was revealed [F(5.546, 127.553) = 1.164; p = 0.330; η<sub>p</sub>
<sup>2</sup> = 0.048], though <xref ref-type="fig" rid="pone-0052737-g006">Fig. 6</xref> (left side) shows that the difference between positive and negative IAPS images increased with the current fixation number. With respect to the length of the first nine valid saccades, a 2×9 ANOVA (prime type×saccade number) revealed a significant increase in saccade length over time [F(5.460, 125.571) = 5.073; p&lt;0.001; η<sub>p</sub>
<sup>2</sup> = 0.181], longer saccades on negative IAPS images [F(1, 23) = 7.174; p = 0.013; η<sub>p</sub>
<sup>2</sup> = 0.238], but no significant interaction [F(6.251, 143.780) = 0.684; p = 0.669; η<sub>p</sub>
<sup>2</sup> = 0.029]. The first saccade, however, did not differ between positive and negative primes (<xref ref-type="fig" rid="pone-0052737-g006">Fig. 6</xref>, right side).</p>
                  <fig id="pone-0052737-g006" orientation="portrait" position="float">
                    <object-id pub-id-type="doi">10.1371/journal.pone.0052737.g006</object-id>
                    <label>Figure 6</label>
                    <caption>
                      <title>Time dependency of fixation duration and saccade length.</title>
                      <p>Fixation duration (left side) and mean saccade length (right side) for negative and positive primes as well as for nature target images depending on the emotional context in which they were embedded. Vertical lines on data points indicate standard error of the mean.</p>
                    </caption>
                    <graphic xlink:href="pone.0052737.g006"/>
                  </fig>
                </sec>
                <sec id="s3b2">
                  <title>Viewing behavior on neutral target images depending on the emotional context (question 2)</title>
                  <p>In order to answer whether, compared to Study 1, less intensive emotional priming in Study 2 also specifically affected viewing behavior on neutral target images, a 3×3 ANOVA (emotional context×target type) for repeated measures was calculated for all three eye-movement parameters [Kolmogorov-Smirnov tests: all p≥0.258]. This analysis simultaneously addressed research question 5 (the impact of target type on viewing behavior). As no interaction was found with regard to any of these parameters [all F(3.163, 72.758) ≤1.368; all p≥0.259; all η<sub>p</sub>
<sup>2</sup>≤0.056], only the main effect of the emotional context is reported in this section and the main effect of the target type will be reported in the result section addressing research question 5.</p>
                  <p>With respect to the mean fixation duration on neutral target images embedded into different emotional contexts (negative, neutral, and positive), we found a medium-sized, but not statistically significant, difference between the emotional contexts when fixation durations were averaged across target types [F(1.564, 35.974) = 2.256; p = 0.130; η<sub>p</sub>
<sup>2</sup> = 0.089]. This context effect was mostly driven by an effect on nature targets as revealed by a one-way ANOVA [F(1.885, 43.349) = 2.635; p = 0.086; η<sub>p</sub>
<sup>2</sup> = 0.103], whereby the negative context evoked longer fixations than the neutral context, as was also found in Study 1 (compare <xref ref-type="fig" rid="pone-0052737-g005">Fig. 5</xref>, right side, and <xref ref-type="fig" rid="pone-0052737-g002">Fig. 2</xref>). No context effect was found for urban and fractal images [both F≤1.172; both p≥0.314; both η<sub>p</sub>
<sup>2</sup>≤0.048].</p>
                  <p>With respect to the duration of the first nine fixations made on target images [Kolmogorov-Smirnov tests: all p≥0.113], the context effect for nature images became statistically significant, as revealed by a 3×9 ANOVA (emotional context×fixation number) [F(1.829, 42.076) = 3.455; p = 0.045; η<sub>p</sub>
<sup>2</sup> = 0.131] (<xref ref-type="fig" rid="pone-0052737-g006">Fig. 6</xref>, left side). In contrast, on urban and fractal images no effect of the emotional context on fixation duration was revealed for the first nine fixations [both F≤0.777; both p≥0.444; both η<sub>p</sub>
<sup>2</sup>≤0.033] (not depicted). Moreover, no interaction between the emotional context and the fixation number was found for any target type [all F≤1.199; all p≥0.302; all η<sub>p</sub>
<sup>2</sup>≤0.050]. However, fixation durations showed an increase during the trajectory on nature images (<xref ref-type="fig" rid="pone-0052737-g006">Fig. 6</xref>, left side), as well as on urban and fractal target images [all F≥3.909; all p≤0.003; all η<sub>p</sub>
<sup>2</sup>≥0.145].</p>
                  <p>With respect to the mean saccade length [Kolmogorov-Smirnov tests: all p≥0.122], no significant effect of the emotional context was revealed by the 3×3 ANOVA (emotional context×target type) i.e. when saccade lengths were averaged across target types [F(1.828, 42.034) = 1.010; p = 0.367; η<sub>p</sub>
<sup>2</sup> = 0.042]. However, one-way ANOVAs for each target type [all F≤1.956; all p≥0.090; all η<sub>p</sub>
<sup>2</sup>≤0.100] revealed a medium- to large-sized effect of the emotional context on saccade lengths for fractal images [η<sub>p</sub>
<sup>2</sup> = 0.100] (<xref ref-type="fig" rid="pone-0052737-g005">Fig. 5</xref>, right side).</p>
                  <p>We also analyzed the first nine valid saccades by a 3×9 ANOVA (emotional context×saccade number) for each target type, but neither a main effect of the emotional context was found [all F≤2.131; all p≥0.132; all η<sub>p</sub>
<sup>2</sup>≤0.085], nor an interaction between the emotional context and the current saccade [all F≤.984; all p≥0.455; all η<sub>p</sub>
<sup>2</sup>≤0.041]. However, the increase of fixation duration from stimulus onset found on nature images was paralleled by an increase of saccade length on nature targets [F(5.830, 134.088) = 2.850; p = 0.013; η<sub>p</sub>
<sup>2</sup> = 0.110] (<xref ref-type="fig" rid="pone-0052737-g006">Fig. 6</xref>, right side), but no difference between saccades existed for fractal and urban target images [both F≤1.439; both p≥0.204; both η<sub>p</sub>
<sup>2</sup>≤0.059] (not depicted).</p>
                  <p>With respect to entropy, the 3×3 ANOVA (emotional context×target type) did not show an effect of the emotional context on neutral target images [F(1.673, 38.471) = 0.315; p = 0.693; η<sub>p</sub>
<sup>2</sup> = 0.014] (<xref ref-type="fig" rid="pone-0052737-g005">Fig. 5</xref>, right side).</p>
                  <p>Consequently, the strong effect of the emotional context on fixation durations found in Study 1, which exclusively used nature targets, was also found in Study 2 using less intensive emotional priming. However, the signature of the context effect partially differed from those in Study 1. In contrast to Study 1, the impact of the emotional context (positive or negative) regarding saccade length and entropy was small and not statistically significant, in general.</p>
                </sec>
                <sec id="s3b3">
                  <title>Differences between emotion-laden primes and neutral target images (question 3)</title>
                  <p>As in Study 1, we compared the emotion-laden primes (positive and negative) with the neutral target images presented in three different emotional contexts regarding fixation duration, saccade length, and entropy. This comparison was done separately for each emotional context in which the target images were presented. Because of multiple testing, we refer here to a Bonferroni-adjusted significance level of <inline-formula><inline-graphic xlink:href="pone.0052737.e003.jpg"/></inline-formula>. <xref ref-type="fig" rid="pone-0052737-g005">Fig. 5</xref> depicts the corresponding means for each of the five image sets. Fixation durations were significantly shorter on emotion-laden primes (negative and positive) compared to the nature target images embedded into three different emotional contexts [all p≤0.006; all d≥0.625], and hence, the results of Study 1 were replicated. Fixation durations on fractal targets were also longer than on both prime types [all p≤0.007; all d≥0.602]. The effect reversed on urban targets, whereby the differences to positive primes was larger [all p≤0.002; all d≥0.693] than the differences to negative primes [all p≤0.093; all d≥0.357].</p>
                  <p>With respect to the mean saccade length, nature targets were scanned with longer saccades than emotion-laden primes [all p≤.019; all d≥0.513], except nature targets in positive context vs. negative primes [p = 0.148; all d = 0.305]. Urban target images were scanned with longer saccades than positive primes [all p≤0.008; all d≥0.597], but the mean saccade length did not differ between negative primes and urban targets independent of the emotional context in which they were embedded [all p≥0.167; all d≤0.291]. Fractal targets were scanned with longer saccades than positive primes when targets were presented in a neutral context [p = 0.002; all d = 0.719], but apart from that, no difference in saccade length was found between emotion-laden primes and fractal images [all p≥0.059; all d≤0.407].</p>
                  <p>Regarding entropy, we found higher values on nature targets (independent of emotional context) than on primes (independent of prime type) [all p≤0.001; all d≥1.246]. Furthermore, urban targets in all emotional contexts [all p≤0.001; all d≥0.985], as well as fractal targets [all p≤0.025; all d≥0.495], were scanned more spatially extensively than negative and positive primes.</p>
                  <p>To conclude, targets were scanned more spatially extensively and with longer saccades than primes. Saccade frequency, however, was higher on emotion-laden primes in contrast to nature and fractal images, but the effect reversed on urban images.</p>
                </sec>
                <sec id="s3b4">
                  <title>The effect of the post-prime position of neutral targets on viewing behavior (question 4)</title>
                  <p>The design of Study 2 allowed investigating the effect of the post-prime position of neutral target images on eye-movement parameters [Kolmogorov-Smirnov tests: all p≥0.312]. We first averaged across target types. Subsequently, viewing behavior on targets was analyzed depending on the targets' post-prime position by means of a 2×3 ANOVA (prime type×post-prime position of targets).</p>
                  <p>The mean fixation duration slightly decreased with an increasing post-prime position of the target; however, it did not reach significance [F(1.657, 38.118) = 2.624; p = 0.095; η<sub>p</sub>
<sup>2</sup> = 0.102]. We found neither an effect of the prime type on fixation duration [F(1, 23) = 1.708; p = 0.204; η<sub>p</sub>
<sup>2</sup> = 0.069], nor an interaction [F(1.716, 39.472) = 0.076; p = 0.902; η<sub>p</sub>
<sup>2</sup> = 0.003].</p>
                  <p>Regarding the mean saccade length, the ANOVA did not reveal any effect [all F≤1.049; all p≥0.353; all η<sub>p</sub>
<sup>2</sup>≤0.044]. Moreover, no effect was found with regard to entropy [all F≤0.888; all p≥0.356; all η<sub>p</sub>
<sup>2</sup>≤0.037]. When introducing the target type as an additional factor in the variance analytic design, no moderating effect of target type was revealed. Hence, eye-movement parameters differed significantly between primes and targets (see research question 3 above); however, none of the three eye-movement parameters revealed a significant dependence on the post-prime position of the targets.</p>
                </sec>
                <sec id="s3b5">
                  <title>The effect of target type on viewing behavior in the context of emotional priming (question 5)</title>
                  <p>In this section, we address the main effect of the target type revealed by the 3×3 ANOVA (emotional context×target type), which was above reported with respect to the main effect of the emotional context on viewing behavior (research question 2).</p>
                  <p>The target type (nature, urban, and fractal) showed a significant impact on the mean fixation duration [F(1.978, 45.503) = 30.241; p&lt;0.001; η<sub>p</sub>
<sup>2</sup> = 0.568], whereby post-hoc t-tests (alpha-adjusted) revealed longer durations on nature as well as on fractal images, in contrast to urban images [both p&lt;.001], but no difference between fractal and nature images [p&gt;0.999]. This result pattern perfectly replicates the effect of the image type on fixation duration as reported before <xref rid="pone.0052737-Kaspar2" ref-type="bibr">[21]</xref> (<xref ref-type="fig" rid="pone-0052737-g007">Fig. 7</xref>, left); however, fixation durations were generally longer in the present study, as well as the absolute differences between the image types. Therefore, when presented in the context of emotional stimuli, these target images are observed with lower saccade frequency.</p>
                  <fig id="pone-0052737-g007" orientation="portrait" position="float">
                    <object-id pub-id-type="doi">10.1371/journal.pone.0052737.g007</object-id>
                    <label>Figure 7</label>
                    <caption>
                      <title>Eye movement parameters on neutral targets of three categories in Study 2 and in our previous study <xref rid="pone.0052737-Kaspar2" ref-type="bibr">[<bold>21</bold>]</xref>.</title>
                      <p>In Study 2, the sequence of target images was intermingled with emotion-laden IAPS images, while in the previous study the targets were presented solely. Note: We only report the results of the first presentation run of <xref rid="pone.0052737-Kaspar2" ref-type="bibr">[21]</xref>. The experimental setup and the analysis of parameters were identical in both studies.</p>
                    </caption>
                    <graphic xlink:href="pone.0052737.g007"/>
                  </fig>
                  <p>With respect to the mean saccade length on target images, no significant effect of target type was found [F(1.702, 39.136) = 2.082; p = 0.145; η<sub>p</sub>
<sup>2</sup> = 0.083]. The mean saccade length on the targets, however, was generally shorter than in our recent study <xref rid="pone.0052737-Kaspar2" ref-type="bibr">[21]</xref>, as shown by <xref ref-type="fig" rid="pone-0052737-g007">Fig. 7</xref> (middle position).</p>
                  <p>Finally, target types differed significantly regarding entropy [F(1.906, 43.828) = 8.276; p&lt;0.001; η<sub>p</sub>
<sup>2</sup> = 0.265]. Bonferroni-adjusted t-tests for paired samples revealed a more extensive exploration of nature and urban images in contrast to fractal images [both p≤0.046], but no difference between nature and urban images [p = 0.503]. This image-type effect on entropy is a perfect replication of the corresponding effect reported in <xref rid="pone.0052737-Kaspar2" ref-type="bibr">[21]</xref>, whereby the absolute entropy values are smaller (see <xref ref-type="fig" rid="pone-0052737-g007">Fig. 7</xref>, right).</p>
                  <p>Consequently, the relative impact of different target types remained even when targets were presented in an emotional context; however, viewing activity was slowed down in general by the emotional contexts, as indicated by longer fixations, shorter saccades, and reduced entropy.</p>
                </sec>
              </sec>
              <sec id="s3c">
                <title>Discussion of Study 2</title>
                <p>Study 2 addresses a total of five questions and replicates and extends results of the effect of emotional priming on viewing behavior, as reported previously in Study 1. The set of nature target images was extended by two further target types, namely, urban and fractal images, to generalize (or not) the conclusions. Additionally, we focused on the potential impact of the targets' post-prime position on viewing behavior, as well as on a comparison with our recent study <xref rid="pone.0052737-Kaspar2" ref-type="bibr">[21]</xref> serving as a control condition here. Several important results are discussed in the following with respect to the research questions (1–5):</p>
                <p>(1) The data show that, in contrast to Study 1, negative IAPS images were scanned more actively and spatially extensively than positive images, expressed by shorter fixation durations, longer saccades, and a more spread out fixation distribution. That is, different prime types were not scanned in different ways when presented within a block (Study 1), but negative primes were more actively and spatially extensively scanned than positive primes when presented individually (Study 2). Importantly, 77.8% of the negative primes and 88.9% of the positive primes were identical in both studies, and hence, differences in the stimuli material are not a reason for the varying results between the studies. Rather, this difference supported the idea of a characteristic viewing behavior elicited when similar images are presented in sequence (see discussion of Study 1). In Study 1, no effect of the primes' emotional content was found on exploratory viewing behavior. In Study 2, in contrast, effects on viewing behavior were dependent on the prime type. We speculate that this results from different sensitivities to positive and negative emotional content and a ceiling effect (due to intense priming) in Study 1. The more active exploration of negative IAPS images compared to positive ones (shorter fixations, longer saccades, and higher entropy) supports the classification of IAPS pictures along the arousal axis where positive stimuli elicit less arousal than negative ones <xref rid="pone.0052737-Bradley3" ref-type="bibr">[58]</xref>. Alternatively, the higher viewing activity found on negative primes may be derived from a faster neuronal processing of visual input, as suggested by van Merle, Hermans, Qin, and Fernández <xref rid="pone.0052737-VanMerle1" ref-type="bibr">[59]</xref> who found that stress amplified sensory processing in early visual regions.</p>
                <p>We specified previous findings stating that both negative and positive emotional stimuli are associated with enhanced visual attention processes <xref rid="pone.0052737-Calvo1" ref-type="bibr">[28]</xref>, <xref rid="pone.0052737-Calvo2" ref-type="bibr">[29]</xref>, <xref rid="pone.0052737-Nummenmaa1" ref-type="bibr">[30]</xref> when presented simultaneously with neutral targets. Highly negative emotional stimuli are possibly of greater emotional relevance, motivated by an underlying defensive system responsible for efficient reactions to harmful stimuli. For this reason, attention to negative stimuli prevails <xref rid="pone.0052737-Ito1" ref-type="bibr">[60]</xref>. This result, however, has been specified by research on personality traits, indicating that optimistic people orientate more to positive stimuli <xref rid="pone.0052737-Segerstrom1" ref-type="bibr">[61]</xref>, and the effect appears not to be stable across the adult's lifespan. Older adults also tend to focus more on positive information <xref rid="pone.0052737-Isaacowitz1" ref-type="bibr">[62]</xref> and rate differences between good and bad as less severe in specific contexts <xref rid="pone.0052737-Kaspar4" ref-type="bibr">[63]</xref>. Moreover, Mather and Sutherland <xref rid="pone.0052737-Mather1" ref-type="bibr">[64]</xref> pointed out that exposure to positive stimuli can broaden cognitive processes, including one's attentional scope. Therefore, no overall advantage for positive or negative stimuli can be derived from present research. Rather, perception of emotional information depends on the individual's current state, which is consistent with related research <xref rid="pone.0052737-Kaspar2" ref-type="bibr">[21]</xref> indicating that personality states and traits are relevant top-down influences in the process of visual attention. Nevertheless, Study 2 shows that, under certain circumstances, the emotional content of images specifically influences the way in which those images are explored.</p>
                <p>In this context, the analysis of the first nine saccades and the duration of the respective fixations provided further important insights: in Study 2, the difference between positive and negative primes was not present from stimulus onset, but increased over time. Participants obviously had to recognize the gist and valence of the scene first, which was unpredictable, before switching to a specific viewing behavior.</p>
                <p>(2) Moreover, the less intensive emotional priming in Study 2 (compared to Study 1) also specifically affected viewing behavior on neutral target images. Thereby, the effect of the emotional context was small to moderate in general. However, on nature targets, a medium- to large-sized context effect on fixation duration was found partially replicating the corresponding signature in Study 1. Furthermore, a medium- to large-sized context effect on saccade lengths was found on fractal images. Independent of the emotional context, fixation durations increased over time on all target types, but saccade lengths increased after stimulus onset only on nature targets. Interestingly, the spread of fixation distributions, i.e., participants' explorativeness on neutral scenes, was not affected by the emotional context in which they were embedded. Obviously, the emotional priming by single stimuli was too weak to elicit changes in entropy.</p>
                <p>(3) In addition to that, fixation durations were shorter on emotion-laden primes compared to nature and fractal target scenes, replicating the result of Study 1. The effect reversed on urban targets. Moreover, all types of neutral targets were scanned with longer saccades than emotion-laden primes, but the effects were larger between targets and positive primes. Finally, entropy values were higher on targets than on primes, independent of the current emotional context and the primes' category. These results are mostly congruent with the corresponding results of Study 1. Obviously, images of high valence elicit a very different viewing behavior than neutral images do. However, in Study 1 fractal images that served as neutral primes were observed just like emotion-laden IAPS images. Once more, this non-effect of prime type probably resulted from the repetition of similar primes within a block, i.e., emotional context.</p>
                <p>(4) It is also important to note that these differences between primes and targets in eye-movement parameters remained constant over time, because viewing behavior remained largely constant on targets across three post-prime positions. Only the mean fixation duration on target images slightly decreased with an increasing post-prime position of targets.</p>
                <p>(5) Finally, we replicated the impact of target type on viewing behavior previously found <xref rid="pone.0052737-Kaspar2" ref-type="bibr">[21]</xref>. Hence, the impact of image type on viewing behavior seems to be strong, even when emotional priming is introduced. However, viewing activity was slowed down in general by the emotional contexts, as indicated by longer fixations, shorter saccades, and reduced entropy in the present Study 2. This indicates an additional influence of the emotional priming on viewing behavior. Obviously, neutral images were scanned less actively when presented in a context of highly emotional images than when presented alone. This suggests that the reduced viewing activity could be a possible result of interference between the emotional engagement owing to preceding emotional pictures and the processing of the actual neutral target. In other words, even the less intensive emotional priming in Study 2 influenced the emotional state of the observer, which, in turn, prolonged the processing of neutral target stimuli.</p>
              </sec>
            </sec>
            <sec id="s4">
              <title>Conclusion</title>
              <p>In summary, Study 2 provides evidence for an externally located impact of emotion on viewing behavior under natural conditions <xref rid="pone.0052737-Kaspar5" ref-type="bibr">[65]</xref>. This is the effect of the emotional content of complex scenes (IAPS images) on the way in which they are observed. However, Study 1 showed that this emotion effect diminished when similar images were presented in a train, although image properties did not change. Neutral fractal images, which were used as primes in Study 1, showed the same effect on viewing behavior as high-valence IAPS images. This result is very surprising, but highlights the necessity to consider several context factors that can affect eye movements in a top-down matter. Obviously, the emotional component of stimuli interacts with further context conditions; for example, the density of similar images in a set of images. Future studies should address these and other potential high-level interactions influencing viewing behavior.</p>
              <p>In addition to that, we also found evidence for an internally located impact of emotion on how people observe complex stimuli, i.e., the emotional context seems to affect the inner state of the observer, which, in turn, influences eye-movement parameters. Against this background, future studies should further scrutinize the impact of certain emotional states on eye-movement parameters under natural conditions. We conclude that the emotional state of an observer influences his current viewing behavior; however, attention has to be paid to the kind of emotion-related processes that are manipulated in experimental studies. For example, it seems critical to distinguish between emotion and arousal <xref rid="pone.0052737-Kaspar5" ref-type="bibr">[65]</xref>. According to Kensinger <xref rid="pone.0052737-Kensinger1" ref-type="bibr">[66]</xref> (p. 241), “a widely-accepted framework proposes that affective experiences are best characterized in a two-dimensional space.” In one dimension, valence ranges from highly negative to highly positive, and in a second dimension, arousal ranges from calming to exciting. In the present study, IAPS images were selected according to their valence ratings, but arousal values were not considered because of the small number of appropriate images of positive and negative valence that correspond regarding arousal values on the one hand, but which are suitable for eye-tracking studies on the other hand. In the IAPS very few positive images are included that evoke high arousal aside from erotic stimuli, which elicit a highly specific viewing behavior <xref rid="pone.0052737-Lykins1" ref-type="bibr">[67]</xref>. Hence, it would be fruitful for vision research to create and validate more emotional stimuli that are suitable for eye-tracking experiments in order to disentangle the influences of emotional valence and emotional arousal on viewing behavior under natural conditions.</p>
              <p>Both present studies revealed that primes which constitute an emotional context were viewed at an accelerated pace compared to neutral target images (except urban images). Although the emotional effect was long-lasting, viewing of subsequent neutral stimuli exhibited a rebound with decelerated viewing. This deceleration was paralleled by an expanded scope of exploration independent of the intensity of the emotional priming. Moreover, the mean visual step size is enlarged on target images compared to primes.</p>
              <p>Finally, the intensity of the emotional context was revealed as a significant factor on viewing behavior under natural conditions. The effect of the emotional priming on the observation of target images was larger overall in the case of a strong emotional context (Study 1). However, the mere presence of emotion-laden IAPS images in a set of different complex scenes (Study 2) slowed down viewing activity in general in contrast to a set of comparable images, but without emotional stimuli <xref rid="pone.0052737-Kaspar2" ref-type="bibr">[21]</xref>. In this context, the remaining effect of the category of target images is also remarkable. All in all, our results suggest that approaches of visual perception and perception in general should focus more on the individual cognitive aspects of these processes, as well as on specific context factors.</p>
            </sec>
          </body>
          <back>
            <ref-list>
              <title>References</title>
              <ref id="pone.0052737-Henderson1">
                <label>1</label>
                <mixed-citation publication-type="journal"><name><surname>Henderson</surname><given-names>J</given-names></name>, <name><surname>Hollingworth</surname><given-names>A</given-names></name> (<year>1999</year>) <article-title>High-level scene perception</article-title>. <source>Ann Rev Psychol</source>
<volume>50</volume>: <fpage>243</fpage>–<lpage>271</lpage>.<pub-id pub-id-type="pmid">10074679</pub-id></mixed-citation>
              </ref>
              <ref id="pone.0052737-Baddeley1">
                <label>2</label>
                <mixed-citation publication-type="journal"><name><surname>Baddeley</surname><given-names>RJ</given-names></name>, <name><surname>Tatler</surname><given-names>BW</given-names></name> (<year>2006</year>) <article-title>High frequency edges (but not contrast) predict where we fixate: a Bayesian system identification analysis</article-title>. <source>Vision Res</source>
<volume>46</volume>: <fpage>2824</fpage>–<lpage>2833</lpage>.<pub-id pub-id-type="pmid">16647742</pub-id></mixed-citation>
              </ref>
              <ref id="pone.0052737-Reinagel1">
                <label>3</label>
                <mixed-citation publication-type="journal"><name><surname>Reinagel</surname><given-names>P</given-names></name>, <name><surname>Zador</surname><given-names>AM</given-names></name> (<year>1999</year>) <article-title>Natural scene statistics at the centre of gaze</article-title>. <source>Network: Comput Neural Syst</source>
<volume>10</volume>: <fpage>1</fpage>–<lpage>10</lpage>.</mixed-citation>
              </ref>
              <ref id="pone.0052737-Krieger1">
                <label>4</label>
                <mixed-citation publication-type="journal"><name><surname>Krieger</surname><given-names>G</given-names></name>, <name><surname>Rentschler</surname><given-names>I</given-names></name>, <name><surname>Hauske</surname><given-names>G</given-names></name>, <name><surname>Schill</surname><given-names>K</given-names></name>, <name><surname>Zetzsche</surname><given-names>C</given-names></name> (<year>2000</year>) <article-title>Object and scene analysis by saccadic eye-movements: an investigation with higher-order statistics</article-title>. <source>Spatial Vision</source>
<volume>13</volume>: <fpage>201</fpage>–<lpage>214</lpage>.<pub-id pub-id-type="pmid">11198232</pub-id></mixed-citation>
              </ref>
              <ref id="pone.0052737-Tatler1">
                <label>5</label>
                <mixed-citation publication-type="journal"><name><surname>Tatler</surname><given-names>B</given-names></name> (<year>2007</year>) <article-title>The central fixation bias in scene viewing: selecting an optimal viewing position independently of motor biases and image feature distributions</article-title>. <source>J Vision</source>
<volume>7</volume>: <fpage>1</fpage>–<lpage>17</lpage>.</mixed-citation>
              </ref>
              <ref id="pone.0052737-Willming1">
                <label>6</label>
                <mixed-citation publication-type="journal"><name><surname>Willming</surname><given-names>N</given-names></name>, <name><surname>Betz</surname><given-names>T</given-names></name>, <name><surname>Kietzmann</surname><given-names>T</given-names></name>, <name><surname>König</surname><given-names>P</given-names></name> (<year>2011</year>) <article-title>Measures and limits of models of fixation selection</article-title>. <source>PloS One</source>
<volume>6</volume>: <fpage>e24038</fpage>.<pub-id pub-id-type="pmid">21931638</pub-id></mixed-citation>
              </ref>
              <ref id="pone.0052737-Carmi1">
                <label>7</label>
                <mixed-citation publication-type="journal"><name><surname>Carmi</surname><given-names>R</given-names></name>, <name><surname>Itti</surname><given-names>L</given-names></name> (<year>2006</year>) <article-title>Visual causes versus correlates of attentional selection in dynamic scenes</article-title>. <source>Vision Res</source>
<volume>46</volume>: <fpage>4333</fpage>–<lpage>4345</lpage>.<pub-id pub-id-type="pmid">17052740</pub-id></mixed-citation>
              </ref>
              <ref id="pone.0052737-Einhuser1">
                <label>8</label>
                <mixed-citation publication-type="journal"><name><surname>Einhäuser</surname><given-names>W</given-names></name>, <name><surname>König</surname><given-names>P</given-names></name> (<year>2003</year>) <article-title>Does luminance-contrast contribute to a saliency map for overt visual attention?</article-title>
<source>Eur J Neurosci</source>
<volume>17</volume>: <fpage>1089</fpage>–<lpage>1097</lpage>.<pub-id pub-id-type="pmid">12653985</pub-id></mixed-citation>
              </ref>
              <ref id="pone.0052737-Henderson2">
                <label>9</label>
                <mixed-citation publication-type="other">Henderson J, Brockmole J, Castelhano M, Mack M (2005) Visual saliency does not account for eye movements during visual search in real-world scenes. In: van Gompel RPG, Fischer MH, Murray WS, Hill RL, editors. Eye movement research: Insights into mind and brain. Amsterdam: Elsevier. pp. 2–41.</mixed-citation>
              </ref>
              <ref id="pone.0052737-Kaspar1">
                <label>10</label>
                <mixed-citation publication-type="journal"><name><surname>Kaspar</surname><given-names>K</given-names></name>, <name><surname>König</surname><given-names>P</given-names></name> (<year>2011b</year>) <article-title>Viewing behavior and the impact of low-level image properties across repeated presentations of complex scenes</article-title>. <source>J Vision</source>
<volume>11</volume>: <fpage>1</fpage>–<lpage>29</lpage>.</mixed-citation>
              </ref>
              <ref id="pone.0052737-Henderson3">
                <label>11</label>
                <mixed-citation publication-type="journal"><name><surname>Henderson</surname><given-names>J</given-names></name> (<year>2003</year>) <article-title>Human gaze control during real-world scene perception</article-title>. <source>Trends Cogn Sci</source>
<volume>7</volume>: <fpage>498</fpage>–<lpage>504</lpage>.<pub-id pub-id-type="pmid">14585447</pub-id></mixed-citation>
              </ref>
              <ref id="pone.0052737-Underwood1">
                <label>12</label>
                <mixed-citation publication-type="journal"><name><surname>Underwood</surname><given-names>G</given-names></name>, <name><surname>Foulsham</surname><given-names>T</given-names></name>, <name><surname>van Loon</surname><given-names>E</given-names></name> (<year>2006</year>) <article-title>Eye movements during scene inspection: a test of the saliency map hypothesis</article-title>. <source>Eur J Cogn Psychol</source>
<volume>18</volume>: <fpage>321</fpage>–<lpage>342</lpage>.</mixed-citation>
              </ref>
              <ref id="pone.0052737-Acik1">
                <label>13</label>
                <mixed-citation publication-type="journal"><name><surname>Acik</surname><given-names>A</given-names></name>, <name><surname>Onat</surname><given-names>S</given-names></name>, <name><surname>Schumann</surname><given-names>F</given-names></name>, <name><surname>Einhäuser</surname><given-names>W</given-names></name>, <name><surname>König</surname><given-names>P</given-names></name> (<year>2009</year>) <article-title>Effects of luminance contrast and its modifications on fixation behavior during free-viewing of images from different categories</article-title>. <source>Vision Res</source>
<volume>49</volume>: <fpage>1541</fpage>–<lpage>1553</lpage>.<pub-id pub-id-type="pmid">19306892</pub-id></mixed-citation>
              </ref>
              <ref id="pone.0052737-Cerf1">
                <label>14</label>
                <mixed-citation publication-type="journal"><name><surname>Cerf</surname><given-names>M</given-names></name>, <name><surname>Harel</surname><given-names>J</given-names></name>, <name><surname>Einhäuser</surname><given-names>W</given-names></name>, <name><surname>Koch</surname><given-names>C</given-names></name> (<year>2008</year>) <article-title>Predicting human gaze using low-level saliency combined with face detection</article-title>. <source>Adv Neur In</source>
<volume>20</volume>.</mixed-citation>
              </ref>
              <ref id="pone.0052737-Peters1">
                <label>15</label>
                <mixed-citation publication-type="journal"><name><surname>Peters</surname><given-names>RJ</given-names></name>, <name><surname>Itti</surname><given-names>L</given-names></name> (<year>2007</year>) <article-title>Beyond bottom-up: incorporating task-dependent influences into a computational model of spatial attention</article-title>. <source>Proc CVPR IEEE</source>
<fpage>1</fpage>–<lpage>7</lpage>.</mixed-citation>
              </ref>
              <ref id="pone.0052737-Einhuser2">
                <label>16</label>
                <mixed-citation publication-type="journal"><name><surname>Einhäuser</surname><given-names>W</given-names></name>, <name><surname>König</surname><given-names>P</given-names></name> (<year>2010</year>) <article-title>Getting real: sensory processing of natural stimuli</article-title>. <source>Curr Opin Neurobiol</source>
<volume>20</volume>: <fpage>389</fpage>–<lpage>395</lpage>.<pub-id pub-id-type="pmid">20434327</pub-id></mixed-citation>
              </ref>
              <ref id="pone.0052737-Ballard1">
                <label>17</label>
                <mixed-citation publication-type="journal"><name><surname>Ballard</surname><given-names>DH</given-names></name>, <name><surname>Hayho</surname><given-names>MM</given-names></name> (<year>2009</year>) <article-title>Modeling the role of task in the control of gaze</article-title>. <source>Vis Cogn</source>
<volume>17</volume>: <fpage>1185</fpage>–<lpage>1204</lpage>.<pub-id pub-id-type="pmid">20411027</pub-id></mixed-citation>
              </ref>
              <ref id="pone.0052737-Chen1">
                <label>18</label>
                <mixed-citation publication-type="journal"><name><surname>Chen</surname><given-names>X</given-names></name>, <name><surname>Zelinsky</surname><given-names>G</given-names></name> (<year>2006</year>) <article-title>Real-world visual search is dominated by top-down guidance</article-title>. <source>Vision Res</source>
<volume>46</volume>: <fpage>4118</fpage>–<lpage>4133</lpage>.<pub-id pub-id-type="pmid">17005231</pub-id></mixed-citation>
              </ref>
              <ref id="pone.0052737-DeAngelus1">
                <label>19</label>
                <mixed-citation publication-type="journal"><name><surname>DeAngelus</surname><given-names>M</given-names></name>, <name><surname>Pelz</surname><given-names>JB</given-names></name> (<year>2009</year>) <article-title>Top-down control of eye movements: Yarbus revisited</article-title>. <source>Vis Cogn</source>
<volume>17</volume>: <fpage>790</fpage>–<lpage>811</lpage>.</mixed-citation>
              </ref>
              <ref id="pone.0052737-Einhuser3">
                <label>20</label>
                <mixed-citation publication-type="journal"><name><surname>Einhäuser</surname><given-names>W</given-names></name>, <name><surname>Rutishauser</surname><given-names>U</given-names></name>, <name><surname>Koch</surname><given-names>C</given-names></name> (<year>2008</year>) <article-title>Task-demands can immediately reverse the effects of sensory-driven saliency in complex visual stimuli</article-title>. <source>J Vision</source>
<volume>8</volume>: <fpage>1</fpage>–<lpage>19</lpage>.</mixed-citation>
              </ref>
              <ref id="pone.0052737-Kaspar2">
                <label>21</label>
                <mixed-citation publication-type="journal"><name><surname>Kaspar</surname><given-names>K</given-names></name>, <name><surname>König</surname><given-names>P</given-names></name> (<year>2011a</year>) <article-title>Overt attention and context factors: the impact of repeated presentation, image type and individual motivation</article-title>. <source>PloS One</source>
<volume>6</volume> (<issue>7</issue>) <fpage>e21719</fpage>.<pub-id pub-id-type="pmid">21750726</pub-id></mixed-citation>
              </ref>
              <ref id="pone.0052737-Hamborg1">
                <label>22</label>
                <mixed-citation publication-type="journal"><name><surname>Hamborg</surname><given-names>KC</given-names></name>, <name><surname>Bruns</surname><given-names>M</given-names></name>, <name><surname>Ollermann</surname><given-names>F</given-names></name>, <name><surname>Kaspar</surname><given-names>K</given-names></name> (<year>2012</year>) <article-title>The effect of banner animation on fixation behavior and recall performance in search tasks</article-title>. <source>Comput Hum Behav</source>
<volume>28</volume>: <fpage>576</fpage>–<lpage>582</lpage>.</mixed-citation>
              </ref>
              <ref id="pone.0052737-Navalpakkam1">
                <label>23</label>
                <mixed-citation publication-type="journal"><name><surname>Navalpakkam</surname><given-names>V</given-names></name>, <name><surname>Itti</surname><given-names>L</given-names></name> (<year>2005</year>) <article-title>Modeling the influence of task on attention</article-title>. <source>Vision Res</source>
<volume>45</volume>: <fpage>205</fpage>–<lpage>231</lpage>.<pub-id pub-id-type="pmid">15581921</pub-id></mixed-citation>
              </ref>
              <ref id="pone.0052737-Navalpakkam2">
                <label>24</label>
                <mixed-citation publication-type="journal"><name><surname>Navalpakkam</surname><given-names>V</given-names></name>, <name><surname>Itti</surname><given-names>L</given-names></name> (<year>2006</year>) <article-title>Top-down selection is fine grained</article-title>. <source>J Vision</source>
<volume>6</volume>: <fpage>1180</fpage>–<lpage>1193</lpage>.</mixed-citation>
              </ref>
              <ref id="pone.0052737-Pomplun1">
                <label>25</label>
                <mixed-citation publication-type="journal"><name><surname>Pomplun</surname><given-names>M</given-names></name> (<year>2006</year>) <article-title>Saccadic selectivity in complex visual search displays</article-title>. <source>Vision Res</source>
<volume>46</volume>: <fpage>1886</fpage>–<lpage>1900</lpage>.<pub-id pub-id-type="pmid">16445960</pub-id></mixed-citation>
              </ref>
              <ref id="pone.0052737-Phelps1">
                <label>26</label>
                <mixed-citation publication-type="journal"><name><surname>Phelps</surname><given-names>E</given-names></name> (<year>2006</year>) <article-title>Emotion and cognition: insights from studies of the human amygdala</article-title>. <source>Ann Rev Psychol</source>
<volume>57</volume>: <fpage>27</fpage>–<lpage>53</lpage>.<pub-id pub-id-type="pmid">16318588</pub-id></mixed-citation>
              </ref>
              <ref id="pone.0052737-Lang1">
                <label>27</label>
                <mixed-citation publication-type="other">Lang PJ, Bradley MM, Cuthbert BN (2005) International affective picture system (IAPS): Affective ratings of pictures and instruction manual. Technical report A-6. Gainesville FL: University of Florida.</mixed-citation>
              </ref>
              <ref id="pone.0052737-Calvo1">
                <label>28</label>
                <mixed-citation publication-type="journal"><name><surname>Calvo</surname><given-names>MG</given-names></name>, <name><surname>Lang</surname><given-names>PJ</given-names></name> (<year>2004</year>) <article-title>Gaze patterns when looking at emotional pictures: motivationally biased attention</article-title>. <source>Motiv Emotion</source>
<volume>28</volume>: <fpage>221</fpage>–<lpage>243</lpage>.</mixed-citation>
              </ref>
              <ref id="pone.0052737-Calvo2">
                <label>29</label>
                <mixed-citation publication-type="journal"><name><surname>Calvo</surname><given-names>MG</given-names></name>, <name><surname>Lang</surname><given-names>PJ</given-names></name> (<year>2005</year>) <article-title>Parafoveal semantic processing of emotional visual scenes</article-title>. <source>J Exp Psychol</source>
<volume>31</volume>: <fpage>502</fpage>–<lpage>519</lpage>.</mixed-citation>
              </ref>
              <ref id="pone.0052737-Nummenmaa1">
                <label>30</label>
                <mixed-citation publication-type="journal"><name><surname>Nummenmaa</surname><given-names>L</given-names></name>, <name><surname>Hyönä</surname><given-names>J</given-names></name>, <name><surname>Calvo</surname><given-names>MG</given-names></name> (<year>2006</year>) <article-title>Preferential selective attention to emotional pictures: an eye movement study</article-title>. <source>Emotion</source>
<volume>6</volume>: <fpage>257</fpage>–<lpage>268</lpage>.<pub-id pub-id-type="pmid">16768558</pub-id></mixed-citation>
              </ref>
              <ref id="pone.0052737-Gotlib1">
                <label>31</label>
                <mixed-citation publication-type="journal"><name><surname>Gotlib</surname><given-names>IH</given-names></name>, <name><surname>Krasnoperova</surname><given-names>E</given-names></name>, <name><surname>Yue</surname><given-names>DN</given-names></name>, <name><surname>Joormann</surname><given-names>J</given-names></name> (<year>2004</year>) <article-title>Attentional biases for negative interpersonal stimuli in clinical depression</article-title>. <source>J Abnorm Psychol</source>
<volume>113</volume>: <fpage>127</fpage>–<lpage>135</lpage>.</mixed-citation>
              </ref>
              <ref id="pone.0052737-Joormann1">
                <label>32</label>
                <mixed-citation publication-type="journal"><name><surname>Joormann</surname><given-names>J</given-names></name>, <name><surname>Gotlib</surname><given-names>IH</given-names></name> (<year>2006</year>) <article-title>Is this happiness I see? Biases in the identification of emotional facial expressions in depression and social phobia</article-title>. <source>J Abnorm Psychol</source>
<volume>115</volume>: <fpage>705</fpage>–<lpage>714</lpage>.<pub-id pub-id-type="pmid">17100528</pub-id></mixed-citation>
              </ref>
              <ref id="pone.0052737-Joormann2">
                <label>33</label>
                <mixed-citation publication-type="journal"><name><surname>Joormann</surname><given-names>J</given-names></name>, <name><surname>Gotlib</surname><given-names>IH</given-names></name> (<year>2007</year>) <article-title>Selective attention to emotional faces following recovery from depression</article-title>. <source>J Abnorm Psychol</source>
<volume>116</volume>: <fpage>80</fpage>–<lpage>85</lpage>.<pub-id pub-id-type="pmid">17324018</pub-id></mixed-citation>
              </ref>
              <ref id="pone.0052737-Palermo1">
                <label>34</label>
                <mixed-citation publication-type="journal"><name><surname>Palermo</surname><given-names>R</given-names></name>, <name><surname>Rhodes</surname><given-names>G</given-names></name> (<year>2007</year>) <article-title>Are you always on my mind? A review of how face perception and attention interacts</article-title>. <source>Neuropsychologia</source>
<volume>45</volume>: <fpage>75</fpage>–<lpage>92</lpage>.<pub-id pub-id-type="pmid">16797607</pub-id></mixed-citation>
              </ref>
              <ref id="pone.0052737-Schub1">
                <label>35</label>
                <mixed-citation publication-type="journal"><name><surname>Schubö</surname><given-names>A</given-names></name>, <name><surname>Gendolla</surname><given-names>AE</given-names></name>, <name><surname>Meinecke</surname><given-names>C</given-names></name>, <name><surname>Abele</surname><given-names>AE</given-names></name> (<year>2006</year>) <article-title>Detecting emotional faces and features in a visual search paradigm: are faces special?</article-title>
<source>Emotion</source>
<volume>6</volume>: <fpage>246</fpage>–<lpage>256</lpage>.<pub-id pub-id-type="pmid">16768557</pub-id></mixed-citation>
              </ref>
              <ref id="pone.0052737-Peira1">
                <label>36</label>
                <mixed-citation publication-type="journal"><name><surname>Peira</surname><given-names>N</given-names></name>, <name><surname>Golkar</surname><given-names>A</given-names></name>, <name><surname>Larsson</surname><given-names>M</given-names></name>, <name><surname>Wiens</surname><given-names>S</given-names></name> (<year>2010</year>) <article-title>What you fear will appear: detection of schematic spiders in spider fear</article-title>. <source>Exp Psychol</source>
<volume>57</volume>: <fpage>470</fpage>–<lpage>475</lpage>.<pub-id pub-id-type="pmid">20371426</pub-id></mixed-citation>
              </ref>
              <ref id="pone.0052737-hmann1">
                <label>37</label>
                <mixed-citation publication-type="journal"><name><surname>Öhmann</surname><given-names>A</given-names></name>, <name><surname>Flykt</surname><given-names>A</given-names></name>, <name><surname>Esteves</surname><given-names>F</given-names></name> (<year>2001</year>) <article-title>Emotion drives attention: detecting the snake in the grass</article-title>. <source>J Exp Psychol Gen</source>
<volume>130</volume>: <fpage>466</fpage>–<lpage>478</lpage>.<pub-id pub-id-type="pmid">11561921</pub-id></mixed-citation>
              </ref>
              <ref id="pone.0052737-Flaisch1">
                <label>38</label>
                <mixed-citation publication-type="journal"><name><surname>Flaisch</surname><given-names>T</given-names></name>, <name><surname>Stockburger</surname><given-names>J</given-names></name>, <name><surname>Schupp</surname><given-names>HT</given-names></name> (<year>2008</year>) <article-title>Affective prime and target picture processing: an ERP analysis of early and late interference effects</article-title>. <source>Brain Topogr</source>
<volume>20</volume>: <fpage>183</fpage>–<lpage>191</lpage>.<pub-id pub-id-type="pmid">18335309</pub-id></mixed-citation>
              </ref>
              <ref id="pone.0052737-Most1">
                <label>39</label>
                <mixed-citation publication-type="journal"><name><surname>Most</surname><given-names>SB</given-names></name>, <name><surname>Chun</surname><given-names>MM</given-names></name>, <name><surname>Widders</surname><given-names>DM</given-names></name>, <name><surname>Zald</surname><given-names>DH</given-names></name> (<year>2005</year>) <article-title>Attentional rubbernecking: cognitive control and personality in emotion-induced blindness</article-title>. <source>Psychon B Rev</source>
<volume>12</volume>: <fpage>654</fpage>–<lpage>661</lpage>.</mixed-citation>
              </ref>
              <ref id="pone.0052737-Lang2">
                <label>40</label>
                <mixed-citation publication-type="other">Lang PJ, Bradley MM, Cuthbert BN (2008) International affective picture system (IAPS): Affective ratings of pictures and instruction manual. Technical report A-8. Gainesville, FL: University of Florida.</mixed-citation>
              </ref>
              <ref id="pone.0052737-Keil1">
                <label>41</label>
                <mixed-citation publication-type="journal"><name><surname>Keil</surname><given-names>A</given-names></name>, <name><surname>Bradley</surname><given-names>MM</given-names></name>, <name><surname>Hauk</surname><given-names>O</given-names></name>, <name><surname>Rockstroh</surname><given-names>B</given-names></name>, <name><surname>Elbert</surname><given-names>T</given-names></name>, <etal>et al</etal> (<year>2002</year>) <article-title>Large-scale neural correlates of affective picture processing</article-title>. <source>Psychophysiology</source>
<volume>39</volume>: <fpage>641</fpage>–<lpage>649</lpage>.<pub-id pub-id-type="pmid">12236331</pub-id></mixed-citation>
              </ref>
              <ref id="pone.0052737-Bradley1">
                <label>42</label>
                <mixed-citation publication-type="journal"><name><surname>Bradley</surname><given-names>MM</given-names></name>, <name><surname>Hamby</surname><given-names>S</given-names></name>, <name><surname>Löw</surname><given-names>A</given-names></name>, <name><surname>Lang</surname><given-names>PJ</given-names></name> (<year>2007</year>) <article-title>Brain potentials in perception: pictures complexity and emotional arousal</article-title>. <source>Psychophysiology</source>
<volume>44</volume>: <fpage>364</fpage>–<lpage>373</lpage>.<pub-id pub-id-type="pmid">17433095</pub-id></mixed-citation>
              </ref>
              <ref id="pone.0052737-Bradley2">
                <label>43</label>
                <mixed-citation publication-type="journal"><name><surname>Bradley</surname><given-names>MM</given-names></name>, <name><surname>Miccoli</surname><given-names>L</given-names></name>, <name><surname>Escrig</surname><given-names>MA</given-names></name>, <name><surname>Lang</surname><given-names>PJ</given-names></name> (<year>2008</year>) <article-title>The pupil as a measure of emotional arousal and autonomic activation</article-title>. <source>Psychophysiology</source>
<volume>45</volume>: <fpage>602</fpage>–<lpage>607</lpage>.<pub-id pub-id-type="pmid">18282202</pub-id></mixed-citation>
              </ref>
              <ref id="pone.0052737-Olmos1">
                <label>44</label>
                <mixed-citation publication-type="other">Olmos A, Kingdom FAA (2004) McGill calibrated colour image database. Available: <ext-link ext-link-type="uri" xlink:href="http://tabby.vision.mcgill.ca">http://tabby.vision.mcgill.ca</ext-link>
</mixed-citation>
              </ref>
              <ref id="pone.0052737-Acik2">
                <label>45</label>
                <mixed-citation publication-type="journal"><name><surname>Acik</surname><given-names>A</given-names></name>, <name><surname>Sarwary</surname><given-names>A</given-names></name>, <name><surname>Schultze-Kraft</surname><given-names>R</given-names></name>, <name><surname>Onat</surname><given-names>S</given-names></name>, <name><surname>König</surname><given-names>P</given-names></name> (<year>2010</year>) <article-title>Developmental changes in natural viewing behavior: bottom-up and top-down differences between children, young adults and older adults</article-title>. <source>Front Percept Sci</source>
<volume>1</volume>: <fpage>Article 27</fpage>.</mixed-citation>
              </ref>
              <ref id="pone.0052737-Pariyadath1">
                <label>46</label>
                <mixed-citation publication-type="journal"><name><surname>Pariyadath</surname><given-names>V</given-names></name>, <name><surname>Eagleman</surname><given-names>D</given-names></name> (<year>2007</year>) <article-title>The effect of predictability on subjective duration</article-title>. <source>PLoS ONE</source>
<volume>2</volume>: <fpage>e1264</fpage>.<pub-id pub-id-type="pmid">18043760</pub-id></mixed-citation>
              </ref>
              <ref id="pone.0052737-Einhuser4">
                <label>47</label>
                <mixed-citation publication-type="journal"><name><surname>Einhäuser</surname><given-names>W</given-names></name>, <name><surname>Rutishauser</surname><given-names>U</given-names></name>, <name><surname>Frady</surname><given-names>EP</given-names></name>, <name><surname>Nadler</surname><given-names>S</given-names></name>, <name><surname>König</surname><given-names>P</given-names></name>, <etal>et al</etal> (<year>2006</year>) <article-title>The relation of phase noise and luminance contrast to overt attention in complex visual stimuli</article-title>. <source>J Vision</source>
<volume>6</volume>: <fpage>1148</fpage>–<lpage>1158</lpage>
<comment>doi:<ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1167/6.11.1">10.1167/6.11.1</ext-link></comment>.</mixed-citation>
              </ref>
              <ref id="pone.0052737-Kayser1">
                <label>48</label>
                <mixed-citation publication-type="journal"><name><surname>Kayser</surname><given-names>C</given-names></name>, <name><surname>Nielsen</surname><given-names>KJ</given-names></name>, <name><surname>Logothetis</surname><given-names>NK</given-names></name> (<year>2006</year>) <article-title>Fixations in natural scenes: interaction of image structure and image content</article-title>. <source>Vision Res</source>
<volume>46</volume>: <fpage>2535</fpage>–<lpage>2545</lpage>.<pub-id pub-id-type="pmid">16545420</pub-id></mixed-citation>
              </ref>
              <ref id="pone.0052737-Ishihara1">
                <label>49</label>
                <mixed-citation publication-type="other">Ishihara S (2005) Ishihara's tests for colour deficiency. Tokyo: Kanehara Trading Inc.</mixed-citation>
              </ref>
              <ref id="pone.0052737-Cuthbert1">
                <label>50</label>
                <mixed-citation publication-type="journal"><name><surname>Cuthbert</surname><given-names>BN</given-names></name>, <name><surname>Schupp</surname><given-names>HT</given-names></name>, <name><surname>Bradley</surname><given-names>MM</given-names></name>, <name><surname>Birbaumer</surname><given-names>N</given-names></name>, <name><surname>Lang</surname><given-names>PJ</given-names></name> (<year>2000</year>) <article-title>Brain potentials in affective picture processing: covariation with autonomic arousal and affective report</article-title>. <source>Biol Psychol</source>
<volume>52</volume>: <fpage>95</fpage>–<lpage>111</lpage>.<pub-id pub-id-type="pmid">10699350</pub-id></mixed-citation>
              </ref>
              <ref id="pone.0052737-Rayner1">
                <label>51</label>
                <mixed-citation publication-type="journal"><name><surname>Rayner</surname><given-names>K</given-names></name> (<year>1998</year>) <article-title>Eye movements in reading and information processing: 20 years of research</article-title>. <source>Psychol Bull</source>
<volume>124</volume>: <fpage>372</fpage>–<lpage>422</lpage>.<pub-id pub-id-type="pmid">9849112</pub-id></mixed-citation>
              </ref>
              <ref id="pone.0052737-Kaspar3">
                <label>52</label>
                <mixed-citation publication-type="journal"><name><surname>Kaspar</surname><given-names>K</given-names></name>, <name><surname>Ollermann</surname><given-names>F</given-names></name>, <name><surname>Hamborg</surname><given-names>KC</given-names></name> (<year>2011</year>) <article-title>Time-dependent changes in viewing behavior on similarly structured web pages</article-title>. <source>J Eye Mov Res</source>
<volume>4</volume>: <fpage>1</fpage>–<lpage>16</lpage>.<pub-id pub-id-type="pmid">21603125</pub-id></mixed-citation>
              </ref>
              <ref id="pone.0052737-Miller1">
                <label>53</label>
                <mixed-citation publication-type="other">Miller G (1955) Note on the bias of information estimates. In: Quastler H, editor. Information theory in psychology II-B. Glencoe, IL: Free Press. pp. 95–100.</mixed-citation>
              </ref>
              <ref id="pone.0052737-Hausser1">
                <label>54</label>
                <mixed-citation publication-type="journal"><name><surname>Hausser</surname><given-names>J</given-names></name>, <name><surname>Strimmer</surname><given-names>K</given-names></name> (<year>2009</year>) <article-title>Entropy inference and the James-Stein estimator, with application to nonlinear gene association networks</article-title>. <source>J Mach Learn Res</source>
<volume>10</volume>: <fpage>1469</fpage>–<lpage>1484</lpage>.</mixed-citation>
              </ref>
              <ref id="pone.0052737-Cohen1">
                <label>55</label>
                <mixed-citation publication-type="other">Cohen J (1988) Statistical power analysis for the behavioral sciences (2<sup>nd</sup> ed.). Hillsdale, NJ: L. Erlbaum.</mixed-citation>
              </ref>
              <ref id="pone.0052737-Velichkovsky1">
                <label>56</label>
                <mixed-citation publication-type="other">Velichkovsky B, Sprenger A, Pomplun M (1997) Auf dem Weg zur Blickmaus: Die Beeinflussung der Fixationsdauer durch kognitive und kommunikative Aufgaben. Gemeinsame Fachtagung des German Chapter of the ACM, der Gesellschaft für Informatik (GI) und der Technischen Universität Dresden, 317–327. Stuttgart: Teubner.</mixed-citation>
              </ref>
              <ref id="pone.0052737-Frey1">
                <label>57</label>
                <mixed-citation publication-type="journal"><name><surname>Frey</surname><given-names>HP</given-names></name>, <name><surname>Honey</surname><given-names>C</given-names></name>, <name><surname>König</surname><given-names>P</given-names></name> (<year>2008</year>) <article-title>What's color got to do with it? the influence of color on visual attention in different categories</article-title>. <source>J Vision</source>
<volume>8</volume>: <fpage>1</fpage>–<lpage>17</lpage>.</mixed-citation>
              </ref>
              <ref id="pone.0052737-Bradley3">
                <label>58</label>
                <mixed-citation publication-type="other">Bradley M, Lang PJ (2007) The international affective picture systems (IAPS) in the study of emotion and attention. In: Coan JA, Allen JJB, editors. Handbook of emotion elicitation and assessment. Oxford: Oxford University Press. pp. 29–46.</mixed-citation>
              </ref>
              <ref id="pone.0052737-VanMerle1">
                <label>59</label>
                <mixed-citation publication-type="journal"><name><surname>Van Merle</surname><given-names>HJF</given-names></name>, <name><surname>Hermans</surname><given-names>EJ</given-names></name>, <name><surname>Qin</surname><given-names>S</given-names></name>, <name><surname>Fernández</surname><given-names>G</given-names></name> (<year>2009</year>) <article-title>From specificity to sensitivity: how acute stress affects amygdala processing of biologically salient stimuli</article-title>. <source>Biol Psychiat</source>
<volume>66</volume>: <fpage>649</fpage>–<lpage>655</lpage>.<pub-id pub-id-type="pmid">19596123</pub-id></mixed-citation>
              </ref>
              <ref id="pone.0052737-Ito1">
                <label>60</label>
                <mixed-citation publication-type="journal"><name><surname>Ito</surname><given-names>TA</given-names></name>, <name><surname>Larsen</surname><given-names>JT</given-names></name>, <name><surname>Smith</surname><given-names>NK</given-names></name>, <name><surname>Cacioppo</surname><given-names>JT</given-names></name> (<year>1998</year>) <article-title>Negative information weights more heavily on the brain: the negativity bias in evacuative categorizations</article-title>. <source>J Pers Soc Psychol</source>
<volume>75</volume>: <fpage>887</fpage>–<lpage>900</lpage>.<pub-id pub-id-type="pmid">9825526</pub-id></mixed-citation>
              </ref>
              <ref id="pone.0052737-Segerstrom1">
                <label>61</label>
                <mixed-citation publication-type="journal"><name><surname>Segerstrom</surname><given-names>AC</given-names></name> (<year>2001</year>) <article-title>Optimism and attentional bias for negative and positive stimuli</article-title>. <source>Pers Soc Psychol B</source>
<volume>27</volume>: <fpage>1334</fpage>–<lpage>1343</lpage>.</mixed-citation>
              </ref>
              <ref id="pone.0052737-Isaacowitz1">
                <label>62</label>
                <mixed-citation publication-type="journal"><name><surname>Isaacowitz</surname><given-names>DM</given-names></name>, <name><surname>Wadlinger</surname><given-names>HA</given-names></name>, <name><surname>Goren</surname><given-names>D</given-names></name>, <name><surname>Wilson</surname><given-names>HR</given-names></name> (<year>2006</year>) <article-title>Is there an age-related positivity effect in visual attention? a comparison of two methodologies</article-title>. <source>Emotion</source>
<volume>6</volume>: <fpage>511</fpage>–<lpage>516</lpage>.<pub-id pub-id-type="pmid">16938091</pub-id></mixed-citation>
              </ref>
              <ref id="pone.0052737-Kaspar4">
                <label>63</label>
                <mixed-citation publication-type="other">Kaspar K, Stelz H (submitted) The paradoxical effect of praise and blame: age-related differences.</mixed-citation>
              </ref>
              <ref id="pone.0052737-Mather1">
                <label>64</label>
                <mixed-citation publication-type="journal"><name><surname>Mather</surname><given-names>M</given-names></name>, <name><surname>Sutherland</surname><given-names>MR</given-names></name> (<year>2011</year>) <article-title>Arousal-biased competition in perception and memory</article-title>. <source>Perspect Psychol Sci</source>
<volume>6</volume>: <fpage>114</fpage>–<lpage>133</lpage>.<pub-id pub-id-type="pmid">21660127</pub-id></mixed-citation>
              </ref>
              <ref id="pone.0052737-Kaspar5">
                <label>65</label>
                <mixed-citation publication-type="journal"><name><surname>Kaspar</surname><given-names>K</given-names></name>, <name><surname>König</surname><given-names>P</given-names></name> (<year>2012</year>) <article-title>Emotions and personality traits as high-level factors in visual attention: a review</article-title>. <source>Front Hum Neurosci</source>
<volume>6</volume>: <fpage>321</fpage>.<pub-id pub-id-type="pmid">23226124</pub-id></mixed-citation>
              </ref>
              <ref id="pone.0052737-Kensinger1">
                <label>66</label>
                <mixed-citation publication-type="journal"><name><surname>Kensinger</surname><given-names>EA</given-names></name> (<year>2004</year>) <article-title>Remembering emotional experiences: the contribution of valence and arousal</article-title>. <source>Rev Neurosci</source>
<volume>15</volume>: <fpage>241</fpage>–<lpage>251</lpage>.<pub-id pub-id-type="pmid">15526549</pub-id></mixed-citation>
              </ref>
              <ref id="pone.0052737-Lykins1">
                <label>67</label>
                <mixed-citation publication-type="journal"><name><surname>Lykins</surname><given-names>AD</given-names></name>, <name><surname>Meana</surname><given-names>M</given-names></name>, <name><surname>Kambe</surname><given-names>G</given-names></name> (<year>2006</year>) <article-title>Detection of differential viewing patterns to erotic and non-erotic stimuli using eye-tracking methodology</article-title>. <source>Arch Sex Behav</source>
<volume>35</volume>: <fpage>569</fpage>–<lpage>575</lpage>.<pub-id pub-id-type="pmid">17031585</pub-id></mixed-citation>
              </ref>
            </ref-list>
          </back>
        </article>
      </metadata>
    </record>
  </GetRecord>
</OAI-PMH>
