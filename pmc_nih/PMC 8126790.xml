<?xml version="1.0" encoding="UTF-8"?>
<OAI-PMH xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/ http://www.openarchives.org/OAI/2.0/OAI-PMH.xsd">
  <responseDate>2024-05-03T03:35:37Z</responseDate>
  <request verb="GetRecord" identifier="oai:pubmedcentral.nih.gov:8126790" metadataPrefix="pmc">https:/www.ncbi.nlm.nih.gov/pmc/oai/oai.cgi</request>
  <GetRecord>
    <record>
      <header>
        <identifier>oai:pubmedcentral.nih.gov:8126790</identifier>
        <datestamp>2021-05-21</datestamp>
        <setSpec>pnas</setSpec>
        <setSpec>pmc-open</setSpec>
      </header>
      <metadata>
        <article xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" xsi:schemaLocation="https://jats.nlm.nih.gov/ns/archiving/1.3/ https://jats.nlm.nih.gov/archiving/1.3/xsd/JATS-archivearticle1-3.xsd" article-type="research-article">
          <front>
            <journal-meta>
              <journal-id journal-id-type="nlm-ta">Proc Natl Acad Sci U S A</journal-id>
              <journal-id journal-id-type="iso-abbrev">Proc Natl Acad Sci U S A</journal-id>
              <journal-id journal-id-type="hwp">pnas</journal-id>
              <journal-id journal-id-type="pmc">pnas</journal-id>
              <journal-id journal-id-type="publisher-id">PNAS</journal-id>
              <journal-title-group>
                <journal-title>Proceedings of the National Academy of Sciences of the United States of America</journal-title>
              </journal-title-group>
              <issn pub-type="ppub">0027-8424</issn>
              <issn pub-type="epub">1091-6490</issn>
              <publisher>
                <publisher-name>National Academy of Sciences</publisher-name>
              </publisher>
            </journal-meta>
            <article-meta>
              <article-id pub-id-type="accession">PMC8126790</article-id>
              <article-id pub-id-type="pmcid">PMC8126790</article-id>
              <article-id pub-id-type="pmc-uid">8126790</article-id>
              <article-id pub-id-type="pmid">33893178</article-id>
              <article-id pub-id-type="pmid">33893178</article-id>
              <article-id pub-id-type="publisher-id">202026610</article-id>
              <article-id pub-id-type="doi">10.1073/pnas.2026610118</article-id>
              <article-categories>
                <subj-group subj-group-type="hwp-journal-coll">
                  <subject>416</subject>
                  <subject>530</subject>
                </subj-group>
                <subj-group subj-group-type="heading">
                  <subject>Physical Sciences</subject>
                  <subj-group>
                    <subject>Engineering</subject>
                  </subj-group>
                </subj-group>
              </article-categories>
              <title-group>
                <article-title>Automated, multiparametric monitoring of respiratory biomarkers and vital signs in clinical and home settings for COVID-19 patients</article-title>
              </title-group>
              <contrib-group>
                <contrib contrib-type="author">
                  <contrib-id contrib-id-type="orcid" authenticated="false">http://orcid.org/0000-0002-1822-1122</contrib-id>
                  <name>
                    <surname>Ni</surname>
                    <given-names>Xiaoyue</given-names>
                  </name>
                  <xref ref-type="aff" rid="aff1">
                    <sup>a</sup>
                  </xref>
                  <xref ref-type="aff" rid="aff2">
                    <sup>b</sup>
                  </xref>
                  <xref ref-type="author-notes" rid="fn1">
                    <sup>1</sup>
                  </xref>
                </contrib>
                <contrib contrib-type="author">
                  <name>
                    <surname>Ouyang</surname>
                    <given-names>Wei</given-names>
                  </name>
                  <xref ref-type="aff" rid="aff1">
                    <sup>a</sup>
                  </xref>
                  <xref ref-type="author-notes" rid="fn1">
                    <sup>1</sup>
                  </xref>
                </contrib>
                <contrib contrib-type="author">
                  <contrib-id contrib-id-type="orcid" authenticated="false">http://orcid.org/0000-0002-1808-7824</contrib-id>
                  <name>
                    <surname>Jeong</surname>
                    <given-names>Hyoyoung</given-names>
                  </name>
                  <xref ref-type="aff" rid="aff1">
                    <sup>a</sup>
                  </xref>
                  <xref ref-type="author-notes" rid="fn1">
                    <sup>1</sup>
                  </xref>
                </contrib>
                <contrib contrib-type="author">
                  <contrib-id contrib-id-type="orcid" authenticated="false">http://orcid.org/0000-0001-9933-1931</contrib-id>
                  <name>
                    <surname>Kim</surname>
                    <given-names>Jin-Tae</given-names>
                  </name>
                  <xref ref-type="aff" rid="aff1">
                    <sup>a</sup>
                  </xref>
                </contrib>
                <contrib contrib-type="author">
                  <contrib-id contrib-id-type="orcid" authenticated="false">http://orcid.org/0000-0002-0750-5007</contrib-id>
                  <name>
                    <surname>Tzaveils</surname>
                    <given-names>Andreas</given-names>
                  </name>
                  <xref ref-type="aff" rid="aff1">
                    <sup>a</sup>
                  </xref>
                  <xref ref-type="aff" rid="aff3">
                    <sup>c</sup>
                  </xref>
                  <xref ref-type="aff" rid="aff4">
                    <sup>d</sup>
                  </xref>
                </contrib>
                <contrib contrib-type="author">
                  <contrib-id contrib-id-type="orcid" authenticated="false">http://orcid.org/0000-0001-8865-3038</contrib-id>
                  <name>
                    <surname>Mirzazadeh</surname>
                    <given-names>Ali</given-names>
                  </name>
                  <xref ref-type="aff" rid="aff5">
                    <sup>e</sup>
                  </xref>
                </contrib>
                <contrib contrib-type="author">
                  <name>
                    <surname>Wu</surname>
                    <given-names>Changsheng</given-names>
                  </name>
                  <xref ref-type="aff" rid="aff1">
                    <sup>a</sup>
                  </xref>
                </contrib>
                <contrib contrib-type="author">
                  <contrib-id contrib-id-type="orcid" authenticated="false">http://orcid.org/0000-0003-1626-7669</contrib-id>
                  <name>
                    <surname>Lee</surname>
                    <given-names>Jong Yoon</given-names>
                  </name>
                  <xref ref-type="aff" rid="aff6">
                    <sup>f</sup>
                  </xref>
                </contrib>
                <contrib contrib-type="author">
                  <name>
                    <surname>Keller</surname>
                    <given-names>Matthew</given-names>
                  </name>
                  <xref ref-type="aff" rid="aff7">
                    <sup>g</sup>
                  </xref>
                </contrib>
                <contrib contrib-type="author">
                  <contrib-id contrib-id-type="orcid" authenticated="false">http://orcid.org/0000-0001-7695-6381</contrib-id>
                  <name>
                    <surname>Mummidisetty</surname>
                    <given-names>Chaithanya K.</given-names>
                  </name>
                  <xref ref-type="aff" rid="aff8">
                    <sup>h</sup>
                  </xref>
                </contrib>
                <contrib contrib-type="author">
                  <contrib-id contrib-id-type="orcid" authenticated="false">http://orcid.org/0000-0003-3835-9601</contrib-id>
                  <name>
                    <surname>Patel</surname>
                    <given-names>Manish</given-names>
                  </name>
                  <xref ref-type="aff" rid="aff1">
                    <sup>a</sup>
                  </xref>
                  <xref ref-type="aff" rid="aff9">
                    <sup>i</sup>
                  </xref>
                </contrib>
                <contrib contrib-type="author">
                  <name>
                    <surname>Shawen</surname>
                    <given-names>Nicholas</given-names>
                  </name>
                  <xref ref-type="aff" rid="aff8">
                    <sup>h</sup>
                  </xref>
                </contrib>
                <contrib contrib-type="author">
                  <name>
                    <surname>Huang</surname>
                    <given-names>Joy</given-names>
                  </name>
                  <xref ref-type="aff" rid="aff10">
                    <sup>j</sup>
                  </xref>
                </contrib>
                <contrib contrib-type="author">
                  <name>
                    <surname>Chen</surname>
                    <given-names>Hope</given-names>
                  </name>
                  <xref ref-type="aff" rid="aff10">
                    <sup>j</sup>
                  </xref>
                </contrib>
                <contrib contrib-type="author">
                  <contrib-id contrib-id-type="orcid" authenticated="false">http://orcid.org/0000-0002-0370-3014</contrib-id>
                  <name>
                    <surname>Ravi</surname>
                    <given-names>Sowmya</given-names>
                  </name>
                  <xref ref-type="aff" rid="aff11">
                    <sup>k</sup>
                  </xref>
                </contrib>
                <contrib contrib-type="author">
                  <name>
                    <surname>Chang</surname>
                    <given-names>Jan-Kai</given-names>
                  </name>
                  <xref ref-type="aff" rid="aff1">
                    <sup>a</sup>
                  </xref>
                  <xref ref-type="aff" rid="aff12">
                    <sup>l</sup>
                  </xref>
                </contrib>
                <contrib contrib-type="author">
                  <contrib-id contrib-id-type="orcid" authenticated="false">http://orcid.org/0000-0002-9765-1541</contrib-id>
                  <name>
                    <surname>Lee</surname>
                    <given-names>KunHyuck</given-names>
                  </name>
                  <xref ref-type="aff" rid="aff1">
                    <sup>a</sup>
                  </xref>
                  <xref ref-type="aff" rid="aff13">
                    <sup>m</sup>
                  </xref>
                </contrib>
                <contrib contrib-type="author">
                  <name>
                    <surname>Wu</surname>
                    <given-names>Yixin</given-names>
                  </name>
                  <xref ref-type="aff" rid="aff1">
                    <sup>a</sup>
                  </xref>
                  <xref ref-type="aff" rid="aff13">
                    <sup>m</sup>
                  </xref>
                </contrib>
                <contrib contrib-type="author">
                  <name>
                    <surname>Lie</surname>
                    <given-names>Ferrona</given-names>
                  </name>
                  <xref ref-type="aff" rid="aff1">
                    <sup>a</sup>
                  </xref>
                </contrib>
                <contrib contrib-type="author">
                  <contrib-id contrib-id-type="orcid" authenticated="false">http://orcid.org/0000-0002-8433-5515</contrib-id>
                  <name>
                    <surname>Kang</surname>
                    <given-names>Youn J.</given-names>
                  </name>
                  <xref ref-type="aff" rid="aff1">
                    <sup>a</sup>
                  </xref>
                </contrib>
                <contrib contrib-type="author">
                  <name>
                    <surname>Kim</surname>
                    <given-names>Jong Uk</given-names>
                  </name>
                  <xref ref-type="aff" rid="aff14">
                    <sup>n</sup>
                  </xref>
                </contrib>
                <contrib contrib-type="author">
                  <contrib-id contrib-id-type="orcid" authenticated="false">http://orcid.org/0000-0002-5199-424X</contrib-id>
                  <name>
                    <surname>Chamorro</surname>
                    <given-names>Leonardo P.</given-names>
                  </name>
                  <xref ref-type="aff" rid="aff15">
                    <sup>o</sup>
                  </xref>
                </contrib>
                <contrib contrib-type="author">
                  <contrib-id contrib-id-type="orcid" authenticated="false">http://orcid.org/0000-0002-8331-5440</contrib-id>
                  <name>
                    <surname>Banks</surname>
                    <given-names>Anthony R.</given-names>
                  </name>
                  <xref ref-type="aff" rid="aff1">
                    <sup>a</sup>
                  </xref>
                </contrib>
                <contrib contrib-type="author">
                  <name>
                    <surname>Bharat</surname>
                    <given-names>Ankit</given-names>
                  </name>
                  <xref ref-type="aff" rid="aff16">
                    <sup>p</sup>
                  </xref>
                </contrib>
                <contrib contrib-type="author">
                  <name>
                    <surname>Jayaraman</surname>
                    <given-names>Arun</given-names>
                  </name>
                  <xref ref-type="aff" rid="aff8">
                    <sup>h</sup>
                  </xref>
                </contrib>
                <contrib contrib-type="author">
                  <name>
                    <surname>Xu</surname>
                    <given-names>Shuai</given-names>
                  </name>
                  <xref ref-type="aff" rid="aff1">
                    <sup>a</sup>
                  </xref>
                  <xref ref-type="aff" rid="aff17">
                    <sup>q</sup>
                  </xref>
                  <xref ref-type="corresp" rid="cor1">
                    <sup>2</sup>
                  </xref>
                </contrib>
                <contrib contrib-type="author">
                  <name>
                    <surname>Rogers</surname>
                    <given-names>John A.</given-names>
                  </name>
                  <xref ref-type="aff" rid="aff1">
                    <sup>a</sup>
                  </xref>
                  <xref ref-type="aff" rid="aff3">
                    <sup>c</sup>
                  </xref>
                  <xref ref-type="aff" rid="aff13">
                    <sup>m</sup>
                  </xref>
                  <xref ref-type="aff" rid="aff18">
                    <sup>r</sup>
                  </xref>
                  <xref ref-type="aff" rid="aff19">
                    <sup>s</sup>
                  </xref>
                  <xref ref-type="aff" rid="aff20">
                    <sup>t</sup>
                  </xref>
                  <xref ref-type="aff" rid="aff21">
                    <sup>u</sup>
                  </xref>
                  <xref ref-type="corresp" rid="cor1">
                    <sup>2</sup>
                  </xref>
                </contrib>
                <aff id="aff1"><sup>a</sup>Querrey Simpson Institute for Bioelectronics, <institution>Northwestern University</institution>, Evanston, <addr-line>IL</addr-line> 60208;</aff>
                <aff id="aff2"><sup>b</sup>Department of Mechanical Engineering and Materials Science, <institution>Duke University</institution>, Durham, <addr-line>NC</addr-line> 27708;</aff>
                <aff id="aff3"><sup>c</sup>Department of Biomedical Engineering, <institution>Northwestern University</institution>, Evanston, <addr-line>IL</addr-line> 60208;</aff>
                <aff id="aff4"><sup>d</sup>Medical Scientist Training Program, Feinberg School of Medicine, <institution>Northwestern University</institution>, Chicago, <addr-line>IL</addr-line> 60611;</aff>
                <aff id="aff5"><sup>e</sup>College of Computing, <institution>Georgia Institute of Technology</institution>, Atlanta, <addr-line>GA</addr-line> 30332;</aff>
                <aff id="aff6"><sup>f</sup><institution>Sibel Inc.</institution>, Niles, <addr-line>IL</addr-line> 60714;</aff>
                <aff id="aff7"><sup>g</sup><institution>Sonica Health</institution>, Niles, <addr-line>IL</addr-line> 60714;</aff>
                <aff id="aff8"><sup>h</sup>Max Nader Lab for Rehabilitation Technologies and Outcomes Research, Center for Bionic Medicine, <institution>Shirley Ryan AbilityLab</institution>, Chicago, <addr-line>IL</addr-line> 60611;</aff>
                <aff id="aff9"><sup>i</sup>College of Medicine, <institution>University of Illinois at Chicago</institution>, Chicago, <addr-line>IL</addr-line> 60612;</aff>
                <aff id="aff10"><sup>j</sup>Feinberg School of Medicine, <institution>Northwestern University</institution>, Chicago, <addr-line>IL</addr-line> 60611;</aff>
                <aff id="aff11"><sup>k</sup>Division of Thoracic Surgery, Feinberg School of Medicine, <institution>Northwestern University</institution>, Chicago, <addr-line>IL</addr-line> 60611;</aff>
                <aff id="aff12"><sup>l</sup><institution>Wearifi Inc.</institution>, Evanston, <addr-line>IL</addr-line> 60201;</aff>
                <aff id="aff13"><sup>m</sup>Department of Materials Science and Engineering, <institution>Northwestern University</institution>, Evanston, <addr-line>IL</addr-line> 60208;</aff>
                <aff id="aff14"><sup>n</sup>School of Chemical Engineering, <institution>Sungkyunkwan University</institution>, Suwon, 16419, <country>Republic of Korea</country>;</aff>
                <aff id="aff15"><sup>o</sup>Department of Mechanical Science and Engineering, <institution>University of Illinois at Urbana</institution>–<institution>Champaign</institution>, Champaign, <addr-line>IL</addr-line> 61801;</aff>
                <aff id="aff16"><sup>p</sup>Department of Surgery, Feinberg School of Medicine, <institution>Northwestern University</institution>, Chicago, <addr-line>IL</addr-line> 60611;</aff>
                <aff id="aff17"><sup>q</sup>Department of Dermatology, Feinberg School of Medicine, <institution>Northwestern University</institution>, Chicago, <addr-line>IL</addr-line> 60611;</aff>
                <aff id="aff18"><sup>r</sup>Department of Mechanical Engineering, <institution>Northwestern University</institution>, Evanston, <addr-line>IL</addr-line> 60208;</aff>
                <aff id="aff19"><sup>s</sup>Department of Chemistry, <institution>Northwestern University</institution>, Evanston, <addr-line>IL</addr-line> 60208;</aff>
                <aff id="aff20"><sup>t</sup>Department of Electrical Engineering and Computer Science, <institution>Northwestern University</institution>, Evanston, <addr-line>IL</addr-line> 60208;</aff>
                <aff id="aff21"><sup>u</sup>Department of Neurological Surgery, <institution>Northwestern University</institution>, Evanston, <addr-line>IL</addr-line> 60208</aff>
              </contrib-group>
              <author-notes>
                <corresp id="cor1"><sup>2</sup>To whom correspondence may be addressed. Email: <email>stevexu@northwestern.edu</email> or <email>jrogers@northwestern.edu</email>.</corresp>
                <fn fn-type="edited-by">
                  <p>Contributed by John A. Rogers, March 22, 2021 (sent for review January 11, 2021; reviewed by Metin Akay and Jun Chen)</p>
                </fn>
                <fn fn-type="con">
                  <p>Author contributions: X.N., W.O., H.J., J.-T.K., L.P.C., A.R.B., A.B., A.J., S.X., and J.A.R. designed research; X.N., W.O., H.J., J.-T.K., A.T., A.M., C.W., J.Y.L., M.K., C.K.M., M.P., N.S., J.-K.C., K.L., J.U.K., A.R.B, A.B., A.J., S.X., and J.A.R. performed research; X.N., W.O., J.-T.K., A.T., A.M., C.W., J.H., H.C., S.R., Y.W., F.L., Y.J.K., and J.A.R. analyzed data; and X.N., W.O., H.J., J.-T.K., S.X., and J.A.R. wrote the paper.</p>
                </fn>
                <fn fn-type="con">
                  <p>Reviewers: M.A., University of Houston; and J.C., University of California, Los Angeles.</p>
                </fn>
                <fn fn-type="equal" id="fn1">
                  <p><sup>1</sup>X.N., W.O., and H.J. contributed equally to this work.</p>
                </fn>
              </author-notes>
              <pub-date pub-type="ppub">
                <day>11</day>
                <month>5</month>
                <year>2021</year>
              </pub-date>
              <pub-date pub-type="epub">
                <day>23</day>
                <month>4</month>
                <year>2021</year>
              </pub-date>
              <pub-date pub-type="pmc-release">
                <day>23</day>
                <month>4</month>
                <year>2021</year>
              </pub-date>
              <!-- PMC Release delay is 0 months and 0 days and was based on the <pub-date pub-type="epub"/>. -->
              <volume>118</volume>
              <issue>19</issue>
              <elocation-id>e2026610118</elocation-id>
              <permissions>
                <copyright-statement>Copyright © 2021 the Author(s). Published by PNAS.</copyright-statement>
                <copyright-year>2021</copyright-year>
                <license>
                  <ali:license_ref specific-use="textmining" content-type="ccbylicense">https://creativecommons.org/licenses/by/4.0/</ali:license_ref>
                  <license-p>This open access article is distributed under <ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution License 4.0 (CC BY)</ext-link>.</license-p>
                </license>
              </permissions>
              <self-uri xlink:title="pdf" xlink:href="pnas.202026610.pdf"/>
              <abstract abstract-type="executive-summary">
                <title>Significance</title>
                <p>Continuous measurements of health status can be used to guide the care of patients and to manage the spread of infectious diseases. Conventional monitoring systems cannot be deployed outside of hospital settings, and existing wearables cannot capture key respiratory biomarkers. This paper describes an automated wireless device and a data analysis approach that overcome these limitations, tailored for COVID-19 patients, frontline health care workers, and others at high risk. Vital signs and respiratory activity such as cough can reveal early signs of infection and quantitate responses to therapeutics. Long-term trials on COVID-19 patients in clinical and home settings demonstrate the translational value of this technology.</p>
              </abstract>
              <abstract>
                <p>Capabilities in continuous monitoring of key physiological parameters of disease have never been more important than in the context of the global COVID-19 pandemic. Soft, skin-mounted electronics that incorporate high-bandwidth, miniaturized motion sensors enable digital, wireless measurements of mechanoacoustic (MA) signatures of both core vital signs (heart rate, respiratory rate, and temperature) and underexplored biomarkers (coughing count) with high fidelity and immunity to ambient noises. This paper summarizes an effort that integrates such MA sensors with a cloud data infrastructure and a set of analytics approaches based on digital filtering and convolutional neural networks for monitoring of COVID-19 infections in sick and healthy individuals in the hospital and the home. Unique features are in quantitative measurements of coughing and other vocal events, as indicators of both disease and infectiousness. Systematic imaging studies demonstrate correlations between the time and intensity of coughing, speaking, and laughing and the total droplet production, as an approximate indicator of the probability for disease spread. The sensors, deployed on COVID-19 patients along with healthy controls in both inpatient and home settings, record coughing frequency and intensity continuously, along with a collection of other biometrics. The results indicate a decaying trend of coughing frequency and intensity through the course of disease recovery, but with wide variations across patient populations. The methodology creates opportunities to study patterns in biometrics across individuals and among different demographic groups.</p>
              </abstract>
              <kwd-group>
                <kwd>wearable electronics</kwd>
                <kwd>digital health</kwd>
                <kwd>biomarkers</kwd>
                <kwd>respiratory disease</kwd>
                <kwd>COVID-19</kwd>
              </kwd-group>
              <funding-group>
                <award-group id="gs1">
                  <funding-source id="sp1">National Science Foundation (NSF)<named-content content-type="funder-id">100000001</named-content></funding-source>
                  <award-id rid="sp1">RAPID program</award-id>
                  <principal-award-recipient>John A. Rogers</principal-award-recipient>
                </award-group>
                <award-group id="gs2">
                  <funding-source id="sp2">Biomedical Advanced Research and Development</funding-source>
                  <award-id rid="sp2">75A50119C00043</award-id>
                  <principal-award-recipient>Shuai Xu</principal-award-recipient>
                  <principal-award-recipient>John A. Rogers</principal-award-recipient>
                </award-group>
                <award-group id="gs3">
                  <funding-source id="sp3">HHS | National Institutes of Health (NIH)<named-content content-type="funder-id">100000002</named-content></funding-source>
                  <award-id rid="sp3">R41AG062023</award-id>
                  <principal-award-recipient>Shuai Xu</principal-award-recipient>
                  <principal-award-recipient>John A. Rogers</principal-award-recipient>
                </award-group>
                <award-group id="gs4">
                  <funding-source id="sp4">HHS | National Institutes of Health (NIH)<named-content content-type="funder-id">100000002</named-content></funding-source>
                  <award-id rid="sp4">R43AG060812</award-id>
                  <principal-award-recipient>Shuai Xu</principal-award-recipient>
                  <principal-award-recipient>John A. Rogers</principal-award-recipient>
                </award-group>
                <award-group id="gs5">
                  <funding-source id="sp5">HHS | National Institutes of Health (NIH)<named-content content-type="funder-id">100000002</named-content></funding-source>
                  <award-id rid="sp5">R41AG062023-02S1</award-id>
                  <principal-award-recipient>Shuai Xu</principal-award-recipient>
                  <principal-award-recipient>John A. Rogers</principal-award-recipient>
                </award-group>
                <award-group id="gs6">
                  <funding-source id="sp6">Michael J. Fox Foundation for Parkinsonʾs Research (MJFF)<named-content content-type="funder-id">100000864</named-content></funding-source>
                  <award-id rid="sp6">17777</award-id>
                  <principal-award-recipient>Shuai Xu</principal-award-recipient>
                  <principal-award-recipient>John A. Rogers</principal-award-recipient>
                </award-group>
              </funding-group>
              <counts>
                <page-count count="12"/>
              </counts>
              <custom-meta-group>
                <custom-meta>
                  <meta-name>access-type</meta-name>
                  <meta-value>free</meta-value>
                </custom-meta>
              </custom-meta-group>
            </article-meta>
          </front>
          <body>
            <p content-type="flushleft">As of December 26, The Centers for Disease Control and Prevention (CDC) tabulations indicate over 18 million recorded cases of COVID-19 and more than 329,592 in deaths in the United States (<xref rid="r1" ref-type="bibr">1</xref>). Accurate and widespread testing is a key component of the response to this pandemic (<xref rid="r2" ref-type="bibr">2</xref>). Although the capacity and availability of COVID-19 molecular diagnostics continues to increase, shortcomings follow from variabilities in the accuracy of the tests, constraints in materials and supplies, long turnaround times associated with certain tests, inadequate access to testing sites, and a lack of human resources (<xref rid="r3" ref-type="bibr">3</xref>). An additional challenge is in limited prognostic tools to assess the trajectory of infection and the eventual need for hospitalization or mechanical ventilation. The CDC confirms that COVID-19 can be contracted via airborne transmission along with contact and droplet transmission—features that underscore the need to improve capabilities in risk stratification of exposures via contact tracing and to ensure sufficient quarantining for recovering individuals.</p>
            <p>To address some of these needs, a range of digital health tools, from mobile applications for collecting self-reported symptoms to consumer wearable devices and clinical-grade medical sensors for tracking physiological status, are under development and in initial stages of deployment (<xref rid="r4" ref-type="bibr">4</xref>). Researchers at FitBit report the ability to identify infection with COVID-19 via four previous days of data collected from their wrist-worn devices to yield overnight heart rate, respiratory rate, and heart rate variability (<xref rid="r5" ref-type="bibr">5</xref>). Others claim similar detection capabilities with alternative wrist-based devices (<xref rid="r6" ref-type="bibr">6</xref>). Several ongoing large-scale trials aim to evaluate these wearables for early detection of COVID-19 infection, from smart rings (Oura Ring) to skin-interfaced patches [VitalConnect (<xref rid="r7" ref-type="bibr">7</xref>), Philips (<xref rid="r8" ref-type="bibr">8</xref>), Sonica (<xref rid="r9" ref-type="bibr">9</xref>)], to other smart watches [e.g., Empatica (<xref rid="r10" ref-type="bibr">10</xref>)] with support from various federal agencies. Devices that mount on the finger or wrist can monitor some subset of conventional vital signs (<xref rid="r11" ref-type="bibr">11</xref><xref rid="r12" ref-type="bibr"/><xref rid="r13" ref-type="bibr"/><xref rid="r14" ref-type="bibr"/>–<xref rid="r15" ref-type="bibr">15</xref>), such as heart rate. Loose interfaces at these body locations, however, limit the range of detectable physiological activities, particularly respiratory signals (<xref rid="r16" ref-type="bibr">16</xref>, <xref rid="r17" ref-type="bibr">17</xref>). The inability to capture complex health information reduces the potential for precise and reliable analysis (<xref rid="r18" ref-type="bibr">18</xref>). Development of robust metrics for early detection and disease tracking requires multiparametric operation across different digital biomarkers and unconventional metrics relevant to the disease of interest. Challenges remain in addressing these requirements simultaneously while maintaining simplicity and ease of use of the sensing system, as is necessary for practical deployment at scale in remote, continuous monitoring settings (<xref rid="r19" ref-type="bibr">19</xref>).</p>
            <p>As COVID-19 is a respiratory disease, cough and other sounds from the thoracic cavity, trachea, and esophagus are examples of highly relevant biometrics. Laboratory-scale studies demonstrate cough-based diagnoses of diverse respiratory diseases through measurements of frequency (<xref rid="r20" ref-type="bibr">20</xref>), intensity (<xref rid="r21" ref-type="bibr">21</xref>), persistency (<xref rid="r22" ref-type="bibr">22</xref>), and unique audio features (<xref rid="r23" ref-type="bibr">23</xref>). Investigations on audio recording data show differences between COVID-19 positive and negative subjects’ vocalizing patterns including phonation of speech (<xref rid="r24" ref-type="bibr">24</xref>, <xref rid="r25" ref-type="bibr">25</xref>), breathing, and coughing sounds (<xref rid="r26" ref-type="bibr">26</xref><xref rid="r27" ref-type="bibr"/><xref rid="r28" ref-type="bibr"/>–<xref rid="r29" ref-type="bibr">29</xref>). The results may suggest possibilities for disease monitoring in asymptomatic patients. Recent work applies voice profiling and computer audition to track cough, speech, respiratory, and other sounds for risk assessment and diagnosis of COVID-19 (<xref rid="r30" ref-type="bibr">30</xref>, <xref rid="r31" ref-type="bibr">31</xref>). Monitoring cough and other vocal events (speaking, laughing, etc.) not only provides a signature of disease but also has potential in generating metrics of infectiousness, as these mechanisms yield aerosols/droplets that contribute to virus transmission (<xref rid="r32" ref-type="bibr">32</xref><xref rid="r33" ref-type="bibr"/>–<xref rid="r34" ref-type="bibr">34</xref>). Previous studies show that the total volume of aerosols correlate with the loudness and duration of vocal events. Measurements of the timing and intensity of sounds may, therefore, serve as reliable means to quantify one aspect associated with risks of spreading the disease (<xref rid="r35" ref-type="bibr">35</xref>).</p>
            <p>Point-of-care or semicontinuous methods for quantifying coughing or other vocal activities rely on electromyography, respiratory inductive plethysmography, accelerometry, or auditory recordings captured with one or several sensors, sometimes with other exploratory approaches (e.g., the nasal thermistor or the electrocardiography) (<xref rid="r36" ref-type="bibr">36</xref><xref rid="r37" ref-type="bibr"/><xref rid="r38" ref-type="bibr"/><xref rid="r39" ref-type="bibr"/><xref rid="r40" ref-type="bibr"/>–<xref rid="r41" ref-type="bibr">41</xref>). Digital signal processing followed by machine learning algorithms often serves as the basis for classification (<xref rid="r42" ref-type="bibr">42</xref><xref rid="r43" ref-type="bibr"/><xref rid="r44" ref-type="bibr"/><xref rid="r45" ref-type="bibr"/><xref rid="r46" ref-type="bibr"/><xref rid="r47" ref-type="bibr"/><xref rid="r48" ref-type="bibr"/><xref rid="r49" ref-type="bibr"/><xref rid="r50" ref-type="bibr"/><xref rid="r51" ref-type="bibr"/><xref rid="r52" ref-type="bibr"/>–<xref rid="r53" ref-type="bibr">53</xref>). Microphone-based methods prevail due to their widespread availability and their alignment with large crowd-sourced datasets (e.g., COUGHVID, HealthMode, DetectNow, VoiceMed). A key challenge is that background sounds and/or environmental noises frustrate robust and accurate measurements. Measurements of loudness can be unreliable because they depend on the separation between the device and the subject. Most importantly, audio recordings raise privacy and legal issues, thereby limiting the scale of application.</p>
            <p>The results presented here bypass these disadvantages, to allow continuous assessments of respiratory biomarkers correlative to health status and droplet/aerosol production, with additional information on a range of traditional vital signs. Here, a simple, wireless monitoring device (<xref rid="r54" ref-type="bibr">54</xref>) combines with a cloud interface and a data analytics approach to allow continuous monitoring of a breadth of conventional (e.g., heart rate, respiratory rate, physical activity, body orientation, and temperature) and unconventional (e.g., coughing, speaking) physiological parameters of direct relevance to COVID-19. The results serve as a quantitative basis for 1) detecting early signs of symptoms in health care workers and other high-risk populations, 2) monitoring symptomatic progression of infected individuals, and 3) tracking responses to therapeutics in clinical settings. In addition, systematic studies presented here indicate that coughing, speaking, and laughing events measured with these devices correlate to the total amount of droplet production. This link offers an opportunity to quantify the infectiousness of individuals, as critical information in caring for patients and for improved risk stratification in the context of contact tracing and individual quarantines.</p>
            <p>Pilot studies on COVID-19 patients at an academic medical center (Northwestern Memorial Hospital) and a rehabilitation hospital (Shirley Ryan AbilityLab) include 3,111 h of data spanning a total of 363 d from 37 patients (20 females, 17 males), in an overall implementation that supports automated operation, with minimal user burden. Long-term monitoring reveals trends in various parameters, including coughing frequency, following the test-positive date for eight patients (four females, four males) over more than 7 d. Evaluations across 27 patients (15 females, 12 males) with ages between 21 and 75 y reveal diverse coughing patterns across individuals and consistent trends during the recovery process.</p>
            <sec sec-type="results" id="s1">
              <title>Results</title>
              <sec id="s2">
                <title>Sensor Designs, System Configurations, and Wireless, Cloud-Enabled Modes of Operation.</title>
                <p><xref ref-type="fig" rid="fig01">Fig. 1<italic>A</italic></xref> presents a schematic illustration of the system. The circuit architecture represents an advanced version of the soft, skin-interfaced mechanoacoustic (MA) device reported previously (<xref rid="r54" ref-type="bibr">54</xref>). Briefly, a flexible printed circuit board (fPCB; 25-<inline-formula><mml:math id="i1"><mml:mi mathvariant="normal">μ</mml:mi></mml:math></inline-formula>m-thick middle polyimide with double-sided 12-<inline-formula><mml:math id="i2"><mml:mi mathvariant="normal">μ</mml:mi></mml:math></inline-formula>m-thick rolled, annealed copper, AP7164R, DuPont) with serpentine conductive traces supports collections of chip-scale components including a high-bandwidth, inertial measurement unit (IMU) with a triaxial accelerometer (LSMDSL, STMicroelectronics) as the key sensing element, a Bluetooth Low Energy (BLE) system-on-a-chip (SoC) for control and wireless connectivity, an on-board memory module for data storage, and a wireless unit for recharging a compact battery. A thin, soft elastomer membrane (Ecoflex, 00-30, smooth on, 300 <inline-formula><mml:math id="i3"><mml:mi mathvariant="normal">μ</mml:mi></mml:math></inline-formula>m) completely encapsulates the device as a compliant, nonirritating interface to the suprasternal notch (SN), supported by a thin, double-sided biomedical adhesive. The design of the system for the studies reported here includes an automated user interface that minimizes manual operations, where the wireless charging platform serves as a hub to switch modes from recording to data transfer. Specifically, the device remains in data acquisition mode when not on the charger. During charging, the device automatically stops recording and starts transmitting data to a BLE-enabled device such as a phone or a tablet with internet connectivity to a Health Insurance Portability and Accountability Act (HIPPA) compliant cloud server. Algorithms operating on the server deliver results to a graphical dashboard for feedback to health workers and/or patients.</p>
                <fig id="fig01" fig-type="featured" orientation="portrait" position="float">
                  <label>Fig. 1.</label>
                  <caption>
                    <p>The health monitoring system incorporating an MA sensor, Bluetooth and cloud-based data transmission, automated data processing platform, and a user interface with a minimum need for manual operation. (<italic>A</italic>) Schematic illustration of the operational flow of the system, which consists of a device, cloud, and data processing platforms. (<italic>B</italic>) Sample three-axis acceleration raw data acquired continuously over 48 h on a COVID-19 patient. Dashed lines indicate occurrences of various representative body processes of interest, shown in (<italic>C</italic>) zoomed-in 2-min windows.</p>
                  </caption>
                  <graphic id="gra1" xlink:href="pnas.2026610118fig01"/>
                </fig>
                <p>When interfaced to the SN, the device captures subtle vibrations of the skin as signatures of a wide range of physiological processes (<xref rid="r54" ref-type="bibr">54</xref>). <xref ref-type="fig" rid="fig01">Fig. 1<italic>B</italic></xref> shows an example of three-axis acceleration data recorded from an inpatient (female, age 53 y) wearing the device for 48 h. The sampling rate for motions perpendicular to the surface of the skin (<italic>z</italic> axis) is 1,666 Hz; the rates for the <italic>x</italic> axis (perpendicular to the axis of the neck) and <italic>y</italic> axis (along the neck) are 416 Hz. <xref ref-type="fig" rid="fig01">Fig. 1<italic>C</italic></xref> shows time series representations of sample events in 2-min windows. Features associated with coughing and speaking include high-frequency components with significant amplitudes (<inline-formula><mml:math id="i4"><mml:mo>∼</mml:mo><mml:mn>1</mml:mn><mml:msup><mml:mrow><mml:mn>0</mml:mn></mml:mrow><mml:mrow><mml:mo>○</mml:mo></mml:mrow></mml:msup><mml:mo> </mml:mo><mml:mi>g</mml:mi></mml:math></inline-formula>) along the <italic>z</italic> and <italic>y</italic> axis but small amplitudes (<inline-formula><mml:math id="i5"><mml:mo>∼</mml:mo><mml:mn>1</mml:mn><mml:msup><mml:mrow><mml:mn>0</mml:mn></mml:mrow><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:mo> </mml:mo><mml:mi>g</mml:mi></mml:math></inline-formula>) along the <italic>x</italic> axis. Physical activity induces comparatively large accelerations (<inline-formula><mml:math id="i6"><mml:mo>∼</mml:mo><mml:mn>1</mml:mn><mml:msup><mml:mrow><mml:mn>0</mml:mn></mml:mrow><mml:mrow><mml:mo>○</mml:mo></mml:mrow></mml:msup><mml:mo> </mml:mo><mml:mi>g</mml:mi></mml:math></inline-formula>) along all axes. During the periods without such activities, subtle vital signals from respiratory and cardiac cycles are readily apparent. Recordings during sleep can also yield body orientations and snoring events, including those that are scarcely audible.</p>
              </sec>
              <sec id="s3">
                <title>Algorithm Development.</title>
                <p>The focus here is on extraction of different vocal and respiratory events from these raw data. Methods for determining other important parameters, such as overall activity levels, heart rate, and respiration rate, can be found elsewhere (<xref rid="r54" ref-type="bibr">54</xref>). In the context of COVID-19, a particular interest is in identifying and tracking coughing events, in the presence of other MA signals. <xref ref-type="fig" rid="fig02">Fig. 2</xref> presents a scheme for data preprocessing that exploits time–frequency features to differentiate coughing from other common daily activities. Algorithm development uses recordings captured from 10 healthy normal subjects in controlled experiments with a protocol (see <xref ref-type="sec" rid="s7"><italic>Materials and Methods</italic></xref> for details) that generates a large number of events of interest in various body postures. <xref ref-type="fig" rid="fig02">Fig. 2<italic>A</italic></xref> shows typical <italic>z</italic> axis data from a representative experimental session. Each testing sequence begins and ends with three taps of the fingers on the device as time stamp markers. In between are consecutive 10 forced coughs, 10 laughing events, 10 throat clearing events, 30 s of walking, 10 cycles of breathing, and more than 20 s of speaking. <xref ref-type="fig" rid="fig02">Fig. 2<italic>B</italic></xref> shows time series and spectrogram representations of such events, the latter of which uses short-time Fourier transform and a Hanning window with a width <inline-formula><mml:math id="i7"><mml:mi mathvariant="normal">Δ</mml:mi><mml:mi>t</mml:mi></mml:math></inline-formula> = 0.4 s moving in time steps of <inline-formula><mml:math id="i8"><mml:mi>δ</mml:mi><mml:mi>t</mml:mi></mml:math></inline-formula> = 0.01 s. The algorithm considers each set of windowed data independently in the process of cough determination. The coughing signals feature a broad-bandwidth impulse-like response, followed usually by a high-frequency chirp (&gt;200 Hz). Speaking signals also have high-frequency components, but usually with distinct harmonic features. An algorithm based on such harmonics can screen the data for prominent speaking periods (<xref ref-type="fig" rid="fig02">Fig. 2<italic>C</italic></xref>). After excluding speaking events, a minimum amplitude threshold <inline-formula><mml:math id="i9"><mml:msub><mml:mrow><mml:mi>P</mml:mi></mml:mrow><mml:mrow><mml:mstyle mathvariant="italic"><mml:mi>t</mml:mi><mml:mi>h</mml:mi><mml:mi>r</mml:mi><mml:mi>s</mml:mi></mml:mstyle></mml:mrow></mml:msub></mml:math></inline-formula> = <inline-formula><mml:math id="i10"><mml:mo>−</mml:mo></mml:math></inline-formula>10,000 detects peaks of the logarithm of spectral power integrated across the high-frequency band (&gt;10 Hz) (<inline-formula><mml:math id="i11"><mml:msub><mml:mrow><mml:mi>P</mml:mi></mml:mrow><mml:mrow><mml:mi>M</mml:mi><mml:mi>A</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula>) and labels them as cough-like events, with a minimum time interval between peak events of 0.4 s (<xref ref-type="fig" rid="fig02">Fig. 2<italic>D</italic></xref>). Here, cough-like events include laughing, throat clearing, and also some speaking periods that exhibit unclear harmonics. <xref ref-type="fig" rid="fig02">Fig. 2<italic>E</italic></xref> shows the data processing flow, which begins with raw <italic>z</italic> axis data and returns the time stamps for speaking and cough-like events, as well as their associated integrated logarithm power. Such an analysis applied to the testing data detects 26.4 s of speaking with clear harmonics features, and identifies 10 coughing, 20 laughing, 12 throat clearing, 36 speaking, and 6 tapping instances as cough-like (<xref ref-type="fig" rid="fig02">Fig. 2<italic>A</italic></xref>).</p>
                <fig id="fig02" fig-type="figure" orientation="portrait" position="float">
                  <label>Fig. 2.</label>
                  <caption>
                    <p>The signal preprocessing steps that identify broadband events of interest from quiet and speaking times from MA measurements. (<italic>A</italic>) The raw <italic>z</italic> axis data generated from controlled experiments on healthy normal subjects, with all of the events of interest repeated in sequence following a designed protocol (see <xref ref-type="sec" rid="s7"><italic>Materials and Methods</italic></xref> for details). (<italic>B</italic>) Example 400-ms clips of the raw <italic>z</italic> axis data and their corresponding spectrogram features. (<italic>C</italic>) Speaking signals distinct with a clear presence of harmonics (<inline-formula><mml:math id="i12"><mml:mi>P</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi>f</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="i13"><mml:mi>P</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi>f</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:math></inline-formula> of fundamental frequencies <inline-formula><mml:math id="i14"><mml:msub><mml:mrow><mml:mi>f</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:math></inline-formula> in the spectrogram analysis <inline-formula><mml:math id="i15"><mml:mi>P</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>f</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:math></inline-formula>, where <inline-formula><mml:math id="i16"><mml:mn>2</mml:mn><mml:msub><mml:mrow><mml:mi>f</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>≈</mml:mo><mml:mi>f</mml:mi><mml:mn>2</mml:mn></mml:math></inline-formula>; see ref. <xref rid="r54" ref-type="bibr">54</xref> for details). Detected speaking periods are shaded in blue in the spectrogram. (<italic>D</italic>) After excluding speaking time, the detection of the high-frequency (<inline-formula><mml:math id="i17"><mml:mi>f</mml:mi><mml:mo>&gt;</mml:mo><mml:mn>10</mml:mn></mml:math></inline-formula> Hz) MA power peaks with a minimum time interval of 0.4 s and a threshold of <inline-formula><mml:math id="i18"><mml:mo>−</mml:mo></mml:math></inline-formula>10,000 yields time stamps for cough-like events that feature the impulse-like broadband acoustics. (<italic>E</italic>) A flow diagram summarizing the preprocessing steps that take in the raw <italic>z</italic> axis data and output the time stamps for cough-like and speaking events, along with their MA power, <inline-formula><mml:math id="i19"><mml:msub><mml:mrow><mml:mi>P</mml:mi></mml:mrow><mml:mrow><mml:mi>M</mml:mi><mml:mi>A</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula>.</p>
                  </caption>
                  <graphic id="gra2" xlink:href="pnas.2026610118fig02"/>
                </fig>
                <p>Distinguishing actual coughs from the pool of cough-like events demands further classification by machine learning. A convolutional neural network (CNN) uses as inputs Morlet wavelet transforms of 0.4-s raw <italic>z</italic> axis data (shaped by the Hanning window) of these events (<xref ref-type="fig" rid="fig03">Fig. 3<italic>A</italic></xref>). The wavelet transform offers advantages compared to the short-time Fourier transform because of its favorable resolution in characterizing nonstationary signals, which improves the accuracy of classification. <xref ref-type="fig" rid="fig03">Fig. 3<italic>B</italic></xref> shows scalograms of cough-like events, including tapping (one type of motion artifact), coughing, laughing, throat clearing, and speaking events. These scalograms, with shapes of <inline-formula><mml:math id="i20"><mml:mn>60</mml:mn><mml:mo>×</mml:mo><mml:mn>666</mml:mn><mml:mo>×</mml:mo><mml:mn>1</mml:mn></mml:math></inline-formula>, serve as inputs to the CNN model. As shown in <xref ref-type="fig" rid="fig03">Fig. 3<italic>C</italic></xref>, the CNN starts with a three-channel convolutional layer with a kernel size of <inline-formula><mml:math id="i21"><mml:mn>3</mml:mn><mml:mo>×</mml:mo><mml:mn>3</mml:mn></mml:math></inline-formula>, followed by a standard 50-layer residual neural network (ResNet), a CNN architecture for image classification (<xref rid="r55" ref-type="bibr">55</xref>). The output of the ResNet flattens to a layer of 86,106 neurons, followed by two fully connected layers with rectified linear unit activation and two dropout layers (<inline-formula><mml:math id="i22"><mml:mi>p</mml:mi><mml:mo>=</mml:mo><mml:mn>0.5</mml:mn></mml:math></inline-formula>) alternately. The final fully connected layer of the CNN model has five neurons with Softmax activation, which corresponds to probabilities associated with the five types of events of interest: coughing, speaking, throat clearing, laughing, and motion artifact, where most of the motion artifacts are those events arising from physical contact on or around the device.</p>
                <fig id="fig03" fig-type="figure" orientation="portrait" position="float">
                  <label>Fig. 3.</label>
                  <caption>
                    <p>The machine learning algorithm for the classification of cough-like events extracted by the preprocessing algorithm. (<italic>A</italic>) Steps of feature scalogram generation from raw data. (<italic>B</italic>) Representative scalograms of events of interest. (<italic>C</italic>) The architecture of a CNN that takes in a feature scalogram and outputs its probabilities of classes. (<italic>D</italic>) The averaged confusion matrix from the iterated 20 leave-one-out testings. (<italic>E</italic>) The overall testing accuracy on each left-out subject using a model trained on the other 19 subjects. (<italic>F</italic>) The macroaveraged ROC curves of each left-out subject using a model trained on the other 19 subjects and the corresponding AUC. a.u., arbitrary unit.</p>
                  </caption>
                  <graphic id="gra3" xlink:href="pnas.2026610118fig03"/>
                </fig>
                <p>Data collected from 10 healthy volunteers yield labeled time windows consisting of 1,379 coughing, 1,441 speaking, 1,313 laughing, 1,423 throat clearing, and 2,890 motion artifact events. Because sample events generated in controlled experiments can differ from those that occur naturally in uncontrolled settings, the training of the CNN model uses not only scalograms of labeled events from 10 healthy volunteers (subjects 1 to 10) but also 10 COVID-19 patients during natural daily behaviors (subjects 11 to 20). Determinations of ground truth from the patient data involve listening to soundtracks created from the accelerometer data and then manually labeling the data (see <xref ref-type="sec" rid="s7"><italic>Materials and Methods</italic></xref> for code availability). Most of the events associated with coughing, speaking, and motion artifacts can be determined unambiguously in this manner. Difficulties arise in distinguishing between laughing, throat clearing, and certain periods of speaking, thereby leading to some level of uncertainty. Such manual analysis of data collected from 10 COVID-19 patients generates a total of 1,405 coughing, 1,449 speaking, 193 laughing, 210 throat clearing, and 2,905 motion artifact events. <ext-link ext-link-type="uri" xlink:href="https://www.pnas.org/lookup/suppl/doi:10.1073/pnas.2026610118/-/DCSupplemental" xlink:show="new"><italic>SI Appendix</italic>, Table S1</ext-link> includes detailed demographic and data collection information for all of the training subjects.</p>
                <p>The generalization performance of the CNN model can be determined using a leave-one-out strategy, where one leaves a subject out of the training set (19 subjects for training) and then tests the trained model on this subject. Iterations apply this approach to each of the 20 subjects. Each training set consists of a random collection of 80% of the labeled events from the 19 subjects, with the remaining 20% used for validation. The training uses an Adam optimization algorithm. <xref ref-type="fig" rid="fig03">Fig. 3<italic>D</italic></xref> shows the averaged confusion matrix of 20 leave-one-out testing cycles. The model achieves accuracies of <inline-formula><mml:math id="i23"><mml:mn>0.90</mml:mn><mml:mo>±</mml:mo><mml:mn>0.08</mml:mn></mml:math></inline-formula> for coughing, <inline-formula><mml:math id="i24"><mml:mn>0.88</mml:mn><mml:mo>±</mml:mo><mml:mn>0.1</mml:mn></mml:math></inline-formula> for speaking, <inline-formula><mml:math id="i25"><mml:mn>0.79</mml:mn><mml:mo>±</mml:mo><mml:mn>0.14</mml:mn></mml:math></inline-formula> for throat clearing, <inline-formula><mml:math id="i26"><mml:mn>0.81</mml:mn><mml:mo>±</mml:mo><mml:mn>0.14</mml:mn></mml:math></inline-formula> for laughing, and <inline-formula><mml:math id="i27"><mml:mn>0.98</mml:mn><mml:mo>±</mml:mo><mml:mn>0.02</mml:mn></mml:math></inline-formula> for motion artifact. The classifications for throat clearing and laughing have comparatively lower average accuracies and higher standard deviations, due to their similarity to certain speaking signals, as evidenced by the confusion matrix (<xref ref-type="fig" rid="fig03">Fig. 3<italic>D</italic></xref>). <xref ref-type="fig" rid="fig03">Fig. 3<italic>E</italic></xref> shows the overall five-way classification accuracies on each subject using a model trained on the other 19 subjects. The minimum overall accuracy is 0.85 for all subjects. The receiver operation characteristic (ROC) curve characterizes the trade-off between sensitivity and specificity in binary classification—varying the threshold of the cutoff probability at the final output layer generates ROC curves of each of the five types of events (coughing vs. noncoughing, speaking vs. nonspeaking, etc.). <xref ref-type="fig" rid="fig03">Fig. 3<italic>F</italic></xref> presents the macroaveraged ROC curves for each subject. The high area under the curve (AUC) of &gt;0.97 for all subjects indicates that the model achieves a good balance between sensitivity and specificity (see <ext-link ext-link-type="uri" xlink:href="https://www.pnas.org/lookup/suppl/doi:10.1073/pnas.2026610118/-/DCSupplemental" xlink:show="new"><italic>SI Appendix</italic>, Table S2</ext-link> for detailed information).</p>
              </sec>
              <sec id="s4">
                <title>MA Sensing of Droplet Production.</title>
                <p>Given the transmissibility of many types of viruses through droplets and aerosols, MA measurements that correlate the timing and intensity of activities associated with droplet production may yield reliable metrics of the risks of the population spread of COVID-19. Robust identification of coughing events, along with their frequency, intensity, and, in the future, detailed time dynamics (i.e., effective sounds), has relevance in this context. Other forms of vocalization such as speaking, singing, shouting, etc., are also important. Previous studies show that different types and volumes of vocal or respiratory-related events yield significantly different levels of aerosol production (<xref rid="r35" ref-type="bibr">35</xref>), with direct relevance to evaluating the risks of viral transmission. <xref ref-type="fig" rid="fig04">Fig. 4<italic>A</italic></xref> presents results that calibrate the high-frequency power <inline-formula><mml:math id="i28"><mml:msub><mml:mrow><mml:mi>P</mml:mi></mml:mrow><mml:mrow><mml:mi>M</mml:mi><mml:mi>A</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> associated with the <italic>z</italic> axis acceleration component of the MA signals to measurements with a decibel meter <inline-formula><mml:math id="i29"><mml:msub><mml:mrow><mml:mi>P</mml:mi></mml:mrow><mml:mrow><mml:mi>d</mml:mi><mml:mi>B</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> in a quiet (background noise of &lt;40 dB) environment for cases of coughing, speaking (repeating words “terminator”), and laughing from a healthy normal subject (male, Asian, age 30 y). The results show a linear correlation <inline-formula><mml:math id="i30"><mml:msub><mml:mrow><mml:mi>P</mml:mi></mml:mrow><mml:mrow><mml:mi>M</mml:mi><mml:mi>A</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mi>p</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:msub><mml:mrow><mml:mi>P</mml:mi></mml:mrow><mml:mrow><mml:mi>d</mml:mi><mml:mi>B</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi>p</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:math></inline-formula> for all three classes in the audible range of 55 dB to 85 dB, with <inline-formula><mml:math id="i31"><mml:msub><mml:mrow><mml:mi>p</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>200</mml:mn><mml:mo>±</mml:mo><mml:mn>20</mml:mn></mml:math></inline-formula> dB<sup>−1</sup>, <inline-formula><mml:math id="i33"><mml:msub><mml:mrow><mml:mi>p</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mo>−</mml:mo><mml:mn>12,000</mml:mn><mml:mo>±</mml:mo><mml:mn>1,700</mml:mn></mml:math></inline-formula> dB<sup>−1</sup> for coughing; <inline-formula><mml:math id="i35"><mml:msub><mml:mrow><mml:mi>p</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>105</mml:mn><mml:mo>±</mml:mo><mml:mn>10</mml:mn></mml:math></inline-formula> dB<sup>−1</sup>, <inline-formula><mml:math id="i37"><mml:msub><mml:mrow><mml:mi>p</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mo>−</mml:mo><mml:mn>7,000</mml:mn><mml:mo>±</mml:mo><mml:mn>700</mml:mn></mml:math></inline-formula> dB<sup>−1</sup> for speaking; and <inline-formula><mml:math id="i39"><mml:msub><mml:mrow><mml:mi>p</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>114</mml:mn><mml:mo>±</mml:mo><mml:mn>30</mml:mn></mml:math></inline-formula> dB<sup>−1</sup>, <inline-formula><mml:math id="i41"><mml:msub><mml:mrow><mml:mi>p</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mo>−</mml:mo><mml:mn>5,800</mml:mn><mml:mo>±</mml:mo><mml:mn>1,200</mml:mn></mml:math></inline-formula> dB<sup>−1</sup> for laughing (<ext-link ext-link-type="uri" xlink:href="https://www.pnas.org/lookup/suppl/doi:10.1073/pnas.2026610118/-/DCSupplemental" xlink:show="new"><italic>SI Appendix</italic>, Fig. S1</ext-link>).</p>
                <fig id="fig04" fig-type="figure" orientation="portrait" position="float">
                  <label>Fig. 4.</label>
                  <caption>
                    <p>MA sensing to quantify the transmission of droplets. (<italic>A</italic>) MA power vs. decibel meter measurement for coughing, speaking, and laughing. (<italic>B</italic>) Experimental setup for optical imaging of droplets. (<italic>C</italic>) Sample image of coughing. (<italic>D</italic>–<italic>F</italic>) Time series of MA <italic>z</italic> axis acceleration (Z<sub>ACC</sub>) in sync with the analysis of MA power and the imaging detection of the number of the particles. (<italic>G</italic>–<italic>I</italic>) Instantaneous images of coughing, talking, and laughing at the peak of corresponding marked boxes in <italic>D</italic>–<italic>F</italic>. (<italic>J</italic>–<italic>L</italic>) Detected particles with sizes indicated by the diameters of the gray circular symbols, overlapped with velocity contour fields at the corresponding instances in <italic>G</italic>–<italic>I</italic>; the color denotes stream-wise velocity in the horizontal (<italic>x</italic> axis) direction. (<italic>M</italic>–<italic>O</italic>) Box and whisker plots showing the number of particles with mean, median, and interquartile range (IQR) for all measured cycles of coughing, speaking, and laughing, respectively. See <xref ref-type="sec" rid="s7"><italic>Materials and Methods</italic></xref> for full description.</p>
                  </caption>
                  <graphic id="gra4" xlink:href="pnas.2026610118fig04"/>
                </fig>
                <p><xref ref-type="fig" rid="fig04">Fig. 4 <italic>B</italic> and <italic>C</italic></xref> shows the experimental setup of quantitative imaging studies (see <xref ref-type="sec" rid="s7"><italic>Materials and Methods</italic></xref> for details) that examine correlations between MA data and droplet production, with a focus on relationships between the total number of droplets and the intensities of coughing, speaking, and laughing. The measurements include droplet dynamics captured via particle tracking velocimetry (PTV; see <xref ref-type="sec" rid="s7"><italic>Materials and Methods</italic></xref> for details), power levels from the MA data (<inline-formula><mml:math id="i43"><mml:msub><mml:mrow><mml:mi>P</mml:mi></mml:mrow><mml:mrow><mml:mi>M</mml:mi><mml:mi>A</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula>), and audio levels from a decibel meter (<inline-formula><mml:math id="i44"><mml:msub><mml:mrow><mml:mi>P</mml:mi></mml:mrow><mml:mrow><mml:mi>d</mml:mi><mml:mi>B</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula>). <xref ref-type="fig" rid="fig04">Fig. 4 <italic>D</italic>–<italic>F</italic></xref> shows a sequence of results from the MA sensor and the PTV analysis for coughing, speaking, and laughing, respectively, where markers indicate events correctly identified and classified by the automated algorithm. <xref ref-type="fig" rid="fig04">Fig. 4 <italic>G</italic>–<italic>I</italic></xref> are images of coughing, talking, and laughing at the peak of corresponding marked boxes in <xref ref-type="fig" rid="fig04">Fig. 4 <italic>D</italic>–<italic>F</italic></xref>. The PTV method tracks individual particles in the Lagrangian frame of ref. <xref rid="r59" ref-type="bibr">59</xref>. <xref ref-type="fig" rid="fig04">Fig. 4 <italic>J</italic>–<italic>L</italic></xref> shows the detected particles, with sizes indicated by the diameters of the gray circular symbols. As expected, the findings indicate that a larger number of droplets (determined across the investigation area of <inline-formula><mml:math id="i45"><mml:mo>∼</mml:mo><mml:mn>34</mml:mn><mml:mo>×</mml:mo><mml:mo>∼</mml:mo><mml:mn>17</mml:mn></mml:math></inline-formula> cm<sup>2</sup>, and with radius R &gt; 50 <inline-formula><mml:math id="i47"><mml:mi mathvariant="normal">μ</mml:mi></mml:math></inline-formula>m in the detectable range) results from coughing (200 to 800 droplets) than from speaking or laughing (10 to 200 droplets) at comparable decibel levels and time durations. More than 60% of droplets are smaller than 150 <inline-formula><mml:math id="i48"><mml:mi mathvariant="normal">μ</mml:mi></mml:math></inline-formula>m in radius for all measured respiratory activities (<ext-link ext-link-type="uri" xlink:href="https://www.pnas.org/lookup/suppl/doi:10.1073/pnas.2026610118/-/DCSupplemental" xlink:show="new"><italic>SI Appendix</italic>, Fig. S2</ext-link>).</p>
                <p>Interpolated horizontal velocity (u) contours from droplet trajectories indicate a large swirling motion for coughing, with positive velocity near the mouth and negative velocity in the bottom of the investigated area (<xref ref-type="fig" rid="fig04">Fig. 4<italic>J</italic></xref>). Droplets show ballistic behavior for speaking and dispersive behavior for laughing (<xref ref-type="fig" rid="fig04">Fig. 4 <italic>K</italic> and <italic>L</italic></xref>). The ballistic behavior of droplets results from enhanced jet-like transport of the expelled airflow induced by plosive sounds (<xref rid="r56" ref-type="bibr">56</xref>). Drastically different inertial particle dynamics occur depending on the size of droplets, even within the same cycle. Specifically, small droplets linger in the air and respond to ambient flows. Large droplets travel at high velocities and are minimally influenced by flows, within a range investigated. Statistical analyses of the total number of droplets (<inline-formula><mml:math id="i49"><mml:msub><mml:mrow><mml:mi>N</mml:mi></mml:mrow><mml:mrow><mml:mi>d</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula>) of all measured respiratory activities at various audio levels appear in <xref ref-type="fig" rid="fig04">Fig. 4 <italic>M</italic> and <italic>O</italic></xref>. The number of droplets exhibits some correlation to the audio decibel level and the power intensity of the MA data, for all activities. <ext-link ext-link-type="uri" xlink:href="https://www.pnas.org/lookup/suppl/doi:10.1073/pnas.2026610118/-/DCSupplemental" xlink:show="new"><italic>SI Appendix</italic>, Fig. S3</ext-link> and <ext-link ext-link-type="uri" xlink:href="https://www.pnas.org/lookup/suppl/doi:10.1073/pnas.2026610118/-/DCSupplemental" xlink:show="new">Movie S1</ext-link> include additional results from the imaging analysis of droplet dynamics.</p>
              </sec>
              <sec id="s5">
                <title>Multiparametric Monitoring from a Cohort of COVID-19 Patients.</title>
                <p>Scaled deployment of the MA device and the machine learning algorithm on COVID-19 patients in a clinical setting demonstrates practical utility and patient compliance without user or physician burden. The studies involve continuous, long-term (&gt;7 d) monitoring of parameters relevant to patient status, not only coughing dynamics but also other forms of vocalization, along with heart rate, respiration rate, body orientation, and overall activity. These pilot studies correspond to 3,111 h of data from 37 patients (20 females, 17 males; see <ext-link ext-link-type="uri" xlink:href="https://www.pnas.org/lookup/suppl/doi:10.1073/pnas.2026610118/-/DCSupplemental" xlink:show="new"><italic>SI Appendix</italic></ext-link> for detailed demographic information) with 27,651 detected coughs. <xref ref-type="fig" rid="fig05">Fig. 5<italic>A</italic></xref> shows data and analysis results for a representative 1-h session with a female patient. The CNN model, trained using a process that is blind to any of the patients described in this section, returns predicted classes for each cough-like event detected by the preprocessing step. A manual labeling process based on audio files provides reference labels for comparison. Statistical analysis, on a total of 10,258 randomly sampled events from 10 patients (6 females, 4 males; patient IDs listed in <ext-link ext-link-type="uri" xlink:href="https://www.pnas.org/lookup/suppl/doi:10.1073/pnas.2026610118/-/DCSupplemental" xlink:show="new"><italic>SI Appendix</italic>, Table S1</ext-link>) with manual labels shows macroaveraged sensitivity (i.e. recall) of <inline-formula><mml:math id="i50"><mml:mo>≥</mml:mo><mml:mn>0.87</mml:mn></mml:math></inline-formula>, specificity of <inline-formula><mml:math id="i51"><mml:mo>≥</mml:mo><mml:mn>0.96</mml:mn></mml:math></inline-formula>, and <inline-formula><mml:math id="i52"><mml:mo>≥</mml:mo><mml:mn>0.85</mml:mn></mml:math></inline-formula> precision for coughing (<italic>N</italic> = 2,785) and artifacts detection (<inline-formula><mml:math id="i53"><mml:mi>N</mml:mi><mml:mo>=</mml:mo><mml:mn>2,768</mml:mn></mml:math></inline-formula>) (<xref ref-type="fig" rid="fig05">Fig. 5<italic>B</italic></xref> and <ext-link ext-link-type="uri" xlink:href="https://www.pnas.org/lookup/suppl/doi:10.1073/pnas.2026610118/-/DCSupplemental" xlink:show="new"><italic>SI Appendix</italic>, Table S2</ext-link>). The sensitivity and precision for speaking (<inline-formula><mml:math id="i54"><mml:mi>N</mml:mi><mml:mo>=</mml:mo><mml:mn>2,758</mml:mn></mml:math></inline-formula>), throat clearing (<inline-formula><mml:math id="i55"><mml:mi>N</mml:mi><mml:mo>=</mml:mo><mml:mn>1,212</mml:mn></mml:math></inline-formula>), and laughing (<inline-formula><mml:math id="i56"><mml:mi>N</mml:mi><mml:mo>=</mml:mo><mml:mn>735</mml:mn></mml:math></inline-formula>) are as low as 0.58, likely due, in part, to the ambiguities in ground truth labeling. <ext-link ext-link-type="uri" xlink:href="https://www.pnas.org/lookup/suppl/doi:10.1073/pnas.2026610118/-/DCSupplemental" xlink:show="new"><italic>SI Appendix</italic>, Table S2</ext-link> includes additional details on statistical analyses with subject-specific information. <xref ref-type="fig" rid="fig05">Fig. 5<italic>C</italic></xref> presents results of coughing counts per 5 min in bars and the associated coughing effort (i.e., <inline-formula><mml:math id="i57"><mml:msub><mml:mrow><mml:mi>P</mml:mi></mml:mrow><mml:mrow><mml:mi>M</mml:mi><mml:mi>A</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula>) in color. In general, the coughing frequency and intensity peak in the morning, and distribute evenly throughout the day. <xref ref-type="fig" rid="fig05">Fig. 5<italic>D</italic></xref> presents a similar analysis of speaking, with uniformly distributed speaking time and loudness (i.e., <inline-formula><mml:math id="i58"><mml:msub><mml:mrow><mml:mi>P</mml:mi></mml:mrow><mml:mrow><mml:mi>M</mml:mi><mml:mi>A</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula>) during daytime.</p>
                <fig id="fig05" fig-type="figure" orientation="portrait" position="float">
                  <label>Fig. 5.</label>
                  <caption>
                    <p>Deployment of MA devices on to the COVID-19 patients in clinical settings. (<italic>A</italic>) Representative <italic>z</italic> axis acceleration data measured from a female patient. The automated algorithm detects cough-like events and outputs five-way classification for the events to coughing (0), speaking (1), throat clearing (2), laughing (3), and motion artifacts (4). (<italic>B</italic>) The macroaveraged testing performance (sensitivity/recall, specificity, and precision) of each type of event on the 10 patients with manual labels, which include 10,258 randomly sampled events in total. (<italic>C</italic> and <italic>D</italic>) Example results for the detected coughing and talking frequency and intensity (color-coded) in 5-min windows from continuous 48-h monitoring of the same patient (raw acceleration data are shown in <xref ref-type="fig" rid="fig01">Fig. 1 <italic>B</italic> and <italic>C</italic></xref>). (<italic>E</italic>–<italic>G</italic>) The vital signs information includes heart rate (HR) in a unit of beats per minute (BtPM) and respiration rate (RR) in a unit of breaths per minute (BrPM), and physical activity (PA), extracted from the same measurement, with their amplitude information color coded. a.u., arbitrary unit.</p>
                  </caption>
                  <graphic id="gra5" xlink:href="pnas.2026610118fig05"/>
                </fig>
                <p>Previously reported algorithms applied to these same MA data streams yield other important parameters (<xref rid="r54" ref-type="bibr">54</xref>). For example, <xref ref-type="fig" rid="fig05">Fig. 5 <italic>E</italic>–<italic>G</italic></xref> summarizes heart rate, respiration rate, and physical activities, where the color-coded intensity values correspond to peak amplitudes of cardiac signals in the frequency band 20 Hz to 55 Hz and root-mean-square values for low-passed respiration cycles in the band 0.1 Hz to 1 Hz. <xref ref-type="fig" rid="fig06">Fig. 6 <italic>A</italic>–<italic>E</italic></xref> presents this collective information (coughing counts, speaking time, heart rate, respiration rate, and physical activity, and their associated intensity or amplitude) for the same patient over 1 mo. Gray shaded areas indicate periods when the patient is not wearing the device. The same analysis has been applied to a total of 27 patients (15 females, 12 males) whose data are not used in building the CNN model. <ext-link ext-link-type="uri" xlink:href="https://www.pnas.org/lookup/suppl/doi:10.1073/pnas.2026610118/-/DCSupplemental" xlink:show="new"><italic>SI Appendix</italic>, Figs. S4</ext-link>–<ext-link ext-link-type="uri" xlink:href="https://www.pnas.org/lookup/suppl/doi:10.1073/pnas.2026610118/-/DCSupplemental" xlink:show="new">S20</ext-link> shows the results for an additional 17 patients (9 females, 8 males; patient IDs listed in <ext-link ext-link-type="uri" xlink:href="https://www.pnas.org/lookup/suppl/doi:10.1073/pnas.2026610118/-/DCSupplemental" xlink:show="new"><italic>SI Appendix</italic>, Table S1</ext-link>) with a minimum of 7 d of enrollment.</p>
                <fig id="fig06" fig-type="figure" orientation="portrait" position="float">
                  <label>Fig. 6.</label>
                  <caption>
                    <p>Long-term monitoring of coughing and other biometrics of COVID-19 patients. Long-term MA sensing of (<italic>A</italic>) cough frequency per hour, (<italic>B</italic>) talk time per hour, (<italic>C</italic>) heart rate, (<italic>D</italic>) respiration rate, and (<italic>E</italic>) physical activity for the same patient shown in <xref ref-type="fig" rid="fig05">Fig. 5 <italic>A</italic></xref> and <italic>C</italic>–<italic>G</italic>, with the intensity or amplitude information of the associated events color coded in each time bin. (<italic>F</italic>) The time series plot of coughing counts organized in days post the test-positive date from eight COVID-19 patients. (<italic>G</italic>) The age distribution of the 27 patients whose data are not used to build the machine learning model. (<italic>H</italic>) The histogram of coughing frequency of the 27 patients. Ages for 3 females and 2 males are not reported (NR). (<italic>I</italic>) The cough intensity versus cough frequency analyzed for each hour of data, clustered by four demographic groups. a.u., arbitrary unit; BrPM, breaths per minute; BtPM, beats per minute.</p>
                  </caption>
                  <graphic id="gra6" xlink:href="pnas.2026610118fig06"/>
                </fig>
                <p><xref ref-type="fig" rid="fig06">Fig. 6<italic>F</italic></xref> presents a time series plot for eight patients (four females, four males; patient IDs listed in <ext-link ext-link-type="uri" xlink:href="https://www.pnas.org/lookup/suppl/doi:10.1073/pnas.2026610118/-/DCSupplemental" xlink:show="new"><italic>SI Appendix</italic>, Table S1</ext-link>) with the date of a positive PCR test for COVID-19, where the event of interest is coughing count organized by days after the test. The results suggest a correlation between coughing frequency and the gradual process of recovery, as might be expected. The significant variation in decay rates, however, indicates individual-specific recovery and aerosolization potential. <xref ref-type="fig" rid="fig06">Fig. 6<italic>G</italic></xref> summarizes the age distribution for the total of 27 testing patients. <xref ref-type="fig" rid="fig06">Fig. 6<italic>H</italic></xref> compares the histogram of coughing frequency of these individuals, to reveal the diverse regularity of coughing across time. <xref ref-type="fig" rid="fig06">Fig. 6<italic>I</italic></xref> shows the coughing frequency versus the average coughing intensity for all hourly measurements, clustered into four demographic groups (males of age <inline-formula><mml:math id="i59"><mml:mo>&lt;</mml:mo></mml:math></inline-formula>55 y, males of age <inline-formula><mml:math id="i60"><mml:mo>≥</mml:mo></mml:math></inline-formula>55 y, females of age <inline-formula><mml:math id="i61"><mml:mo>&lt;</mml:mo></mml:math></inline-formula>55 y, females of age <inline-formula><mml:math id="i62"><mml:mo>≥</mml:mo></mml:math></inline-formula>55 y). The available results suggest that females tend to cough more than males. <ext-link ext-link-type="uri" xlink:href="https://www.pnas.org/lookup/suppl/doi:10.1073/pnas.2026610118/-/DCSupplemental" xlink:show="new"><italic>SI Appendix</italic>, Table S1</ext-link> includes detailed demographic and data collection information for all of the testing patients. The statistics may provide insights for creating guidelines for disease management and containment. Further studies on an expanded patient population with detailed demographic information are, however, necessary to enable big-data–based studies of the demographic dependence and/or individual variance of relevant biometrics.</p>
              </sec>
            </sec>
            <sec sec-type="discussion" id="s6">
              <title>Discussion</title>
              <p content-type="flushleft">This paper introduces an automated hardware–software solution for sensing of diverse health information relevant to patient status, with a focus on underexplored respiratory biomarkers such as cough and their changes with COVID-19 disease state. Scaled studies indicate applicability to COVID-19 patients in both clinical and home settings. The approach relies on a soft, wireless sensing device placed on the SN, to capture data that can be processed through a combination of digital filtering and machine learning techniques to separate and quantify different body processes. In addition to patient status, these data show promise in tracking droplet/aerosol production and, therefore, disease transmission related to cough and other expiratory events. The results have implications for early detection, patient care, and disease management, with specific relevance to COVID-19.</p>
              <p>These systems allow for multiparametric monitoring with minimal burden, through a range of conventional and unconventional signatures of health status. Cough is an example of a potentially important biomarker that can yield insights to complement those from analysis of traditional vital signals. Extensions of the approaches reported here can be considered in strategies that extract additional information from specific forms of speech (e.g., plosive consonants), advanced assessments of coughing and respiratory sounds, and correlations between body positions and these activities, as well as coupled responses and timing intervals between different events. MA sensing of distinctive features in respiratory biomarkers and physiological characteristics between COVID-19 patients and healthy subjects suggests a versatile platform for disease monitoring and management. The addition of optical sensors will enable measurements of blood oxygenation, without affecting the ability to simultaneously capture MA signals. The results offer many possibilities in data fusion for precision healthcare, including but not constrained to COVID-19 (<xref rid="r19" ref-type="bibr">19</xref>, <xref rid="r57" ref-type="bibr">57</xref>, <xref rid="r58" ref-type="bibr">58</xref>). Scaled deployment will yield large amounts of accessible biometric data, as the potential basis for predictive disease models, cost-effective care of patients, and containment of disease transmission.</p>
            </sec>
            <sec sec-type="materials|methods" id="s7">
              <title>Materials and Methods</title>
              <sec id="s8">
                <title>Device Design and Components.</title>
                <p>The fPCB schematic diagram and board layout were designed using AUTODESK EAGLE (version 9.6.0) for a stretchable and bendable MA device. Serpentine-shaped outlines connect three separated islands (main body, sensor, and charging coil). A summary of the bill of materials for the device includes 0201 and 0402 inch footprint (imperial code) passive components (resistors, capacitors, and inductors), four turns of wireless charging coil pattern (resonance frequency: 13.56 MHz), full-bridge rectifier, power management integrated circuits (IC) (Bq25120a, Texas Instruments), 3.0-V step-down power converter (TPS62740, Texas Instruments), 3.7-V lithium polymer battery (75 mAh), voltage and current protection IC for Li-Polymer battery (BQ2970, Texas Instruments), BLE SoC (nRF52840, Nordic Semiconductor), flash memory (MT29F4G, Micron), and IMU (LSM6DSL, STMicroelectronics).</p>
              </sec>
              <sec id="s9">
                <title>Device Fabrication and Encapsulation.</title>
                <p>Panels of fPCB were manufactured, and surface-mount device processes were performed by an International Organization for Standardization 9001-compliant manufacturer. Customized firmware was downloaded by Segger Embedded Studio, followed by an fPCB folding and battery soldering process. Each aluminum mold for top and bottom layers was prepared with a freeform prototyping machine (Roland MDX 540), and the devices were encapsulated using precured top and bottom layers (Silbione-4420, each 300 <inline-formula><mml:math id="i63"><mml:mi mathvariant="normal">μ</mml:mi></mml:math></inline-formula>m thick) after filling with silicone elastomer (Eco-Flex 0030, 1:1 ratio) in the cavity in which the device was positioned. After fixing and pressing top/bottom molds using clamps, the mold was placed into an oven that holds a temperature of 95 °C for 20 min to cure the silicone elastomer. The mold was then taken out of the oven and placed in a room temperature area for 20 min to cool down. After cooling down, the clamps were removed, the encapsulated device was placed on a cutting surface, and excess enclosure material was removed using a prefabricated hand-held die cutter. A CO<sub>2</sub> laser formed the shape of the double-sided adhesives and yielded a smooth and clean contour cut.</p>
              </sec>
              <sec id="s10">
                <title>Data Collection.</title>
                <p>All of the participants provided written/verbal consent prior to their participation in this research study (see <ext-link ext-link-type="uri" xlink:href="https://www.pnas.org/lookup/suppl/doi:10.1073/pnas.2026610118/-/DCSupplemental" xlink:show="new"><italic>SI Appendix</italic>, Table S1</ext-link> for demographic information of all individuals studied). Study procedures were approved by the Northwestern University Institutional Review Board (STU00202449 and STU00212522) and were registered on ClinicalTrials.gov (NCT02865070 and NCT04393558). All study-related procedures were carried out in accordance with the standards listed in the Declaration of Helsinki, 1964. During the study, participants wore an MA device at SN (<xref ref-type="fig" rid="fig01">Fig. 1<italic>A</italic></xref>). In the case of patients, a clinician/research staff assisted in placing the sensor.</p>
                <p>Healthy controls were asked to perform 18 repetitions of the following sequence of activities with some variability in the intensity of each of the activities over a 2- to 4-h period: three taps on the sensor, 10 coughs, 10 laughs, 10 throat clearings, 30 s of walking, 10 cycles of breathing (inhale and exhale), more than 20 s of speaking, and three taps on the sensor. Of these repetitions, sedentary activities in five sets were performed while sitting, five sets during standing, and eight sets while lying down (two in supine, two in prone, two in left recumbent, and two in right recumbent) positions. In the case of patients, a reduced set of activities were used at the beginning of each test, which included three taps on the sensor, five coughs, five cycles of deep breathing, and three taps on the sensor.</p>
              </sec>
              <sec id="s11">
                <title>Sterilization Process.</title>
                <p>After each use, the MA sensor was thoroughly disinfected/cleaned with isopropyl alcohol (70% or above) or Oxivir TB wipes (0.5% hydrogen peroxide) and left to dry at room temperature, and the same process was repeated twice.</p>
              </sec>
              <sec id="s12">
                <title>Convolutional Neural Network.</title>
                <p>The CNN starts with a convolution with a kernel size of 3 × 3 and three different kernels, followed by a standard 50-layer ResNet as described in detail in ref. <xref rid="r55" ref-type="bibr">55</xref>. At the output of the ResNet, a flattening layer of 86,106 neurons follows. Finally, three fully interconnected layers with 512, 128, and 5 neurons, respectively, and two dropout layers with <italic>P</italic> = 0.5 follow alternately. The CNN uses an Adam optimizer for training. The training process follows a leave-one-out strategy, where one leaves a subject out of the training set (19 remaining subjects for training) and then tests the trained model on this subject. Each training set applies a fivefold cross-validation procedure. This approach iterates through each of the 20 subjects. <ext-link ext-link-type="uri" xlink:href="https://www.pnas.org/lookup/suppl/doi:10.1073/pnas.2026610118/-/DCSupplemental" xlink:show="new"><italic>SI Appendix</italic>, Table S2</ext-link> includes detailed information on the cross-validation results for each subject.</p>
              </sec>
              <sec id="s13">
                <title>Data Analytics.</title>
                <p>All analysis used Python 3.0 with SciPy, PyWavelets, and TensorFlow packages.</p>
              </sec>
              <sec id="s14">
                <title>Code Availability.</title>
                <p>The codes used for audio soundtrack conversion and manual labeling processes are available on GitHub at <ext-link ext-link-type="uri" xlink:href="https://github.com/nixiaoyue/MA-cough" xlink:show="new">https://github.com/nixiaoyue/MA-cough</ext-link>. The analysis codes used in this study are available from the authors upon request.</p>
              </sec>
              <sec id="s15">
                <title>Droplet Dynamics via PTV.</title>
                <p>Droplet dynamics of coughing, speaking, and laughing were quantified by PTV. Coughing, speaking (the word “terminator” was used), and laughing were repeated 14, 26, and 15 times, respectively, at various decibel levels. More data samples for speaking were collected to cover a wider range of decibels up to 100 dB. Each respiratory activity was performed in the customized box made of acrylic glass with an inner dimension of <inline-formula><mml:math id="i65"><mml:mn>45</mml:mn><mml:mo>×</mml:mo><mml:mn>30</mml:mn><mml:mo>×</mml:mo><mml:mn>30</mml:mn></mml:math></inline-formula> cm<sup>3</sup> (L<inline-formula><mml:math id="i67"><mml:mo> </mml:mo><mml:mo>×</mml:mo><mml:mo> </mml:mo></mml:math></inline-formula>W<inline-formula><mml:math id="i68"><mml:mo> </mml:mo><mml:mo>×</mml:mo><mml:mo> </mml:mo></mml:math></inline-formula>H). The investigation area for tracking droplets was <inline-formula><mml:math id="i69"><mml:mo>∼</mml:mo><mml:mn>34</mml:mn><mml:mo>×</mml:mo><mml:mo>∼</mml:mo><mml:mn>17</mml:mn></mml:math></inline-formula> cm<sup>2</sup> illuminated by 16 arrays for 600 lumen LED light bars. PTV experiments were recorded by a 2,048 <inline-formula><mml:math id="i71"><mml:mo>×</mml:mo></mml:math></inline-formula> 1,088 Emergent HT-2000M with 50-mm F1.4 manual focus Kowa lens at the frame rate of 338 frames per second. To achieve continuous and simultaneous measurements with MA sensor and audio meter (Decibel X, calibrated by SD-4023 sound level meter and R8090 Sound Level Calibrator), approximately 10,000 frames were recorded for each respiratory activity. Preprocessing, calibration, tracking, and postprocessing are performed by a previously developed PTV code (<xref rid="r59" ref-type="bibr">59</xref>). Image sequences were preprocessed by subtracting the background noise and enhancing the contract. Droplets are detected at the subpixel level with the area estimation. The scattering cross-section of a detected droplet and refractive index of droplet as well as the surrounding medium, air, and wavelength of the light source were used to calculate the actual radius of detected droplets based on the Mie scattering theory (<xref rid="r60" ref-type="bibr">60</xref>, <xref rid="r61" ref-type="bibr">61</xref>). The minimum radius of droplets measured in this work is <inline-formula><mml:math id="i72"><mml:mo>∼</mml:mo><mml:mn>60</mml:mn></mml:math></inline-formula>
<inline-formula><mml:math id="i73"><mml:mi mathvariant="normal">μ</mml:mi></mml:math></inline-formula>m. Detected droplets were tracked using the Hungarian algorithm and linked by performing a five-frame gap closing to produce longer trajectories. Velocity and Lagrangian acceleration were filtered and computed using fourth-order B splines. Vector contour fields were obtained by interpolating scattered Lagrangian flow particles at each frame based on the natural neighbor interpolation method.</p>
              </sec>
            </sec>
            <sec sec-type="supplementary-material">
              <title>Supplementary Material</title>
              <supplementary-material content-type="local-data">
                <label>Supplementary File</label>
                <media xlink:href="pnas.2026610118.sapp.pdf"/>
              </supplementary-material>
              <supplementary-material content-type="local-data">
                <label>Supplementary File</label>
                <media xlink:href="pnas.2026610118.sm01.mp4"/>
              </supplementary-material>
            </sec>
          </body>
          <back>
            <ack>
              <p>J.A.R. acknowledges support from the NSF Grants for Rapid Response Research (RAPID) program for development of the data analysis algorithms reported here. S.X. and J.A.R. recognize support from Contract 75A50119C00043 awarded by the Biomedical Advanced Research and Development Authority, Contract R41AG062023 by the NIH, Contract R43AG060812 by the NIH, Contract R41AG062023-02S1 by the NIH, and Grant 17777 from the Michael J. Fox Foundation. The work was also supported by the Querrey-Simpson Institute for Bioelectronics at Northwestern University.</p>
            </ack>
            <fn-group>
              <fn fn-type="COI-statement">
                <p>Competing interest statement: X.N., H.J., J.Y.L., K.L., A.J., S.X., and J.A.R. report inventorships and potential royalties in patents assigned to Northwestern University. M.K. and J.Y.L. are employees of a small private company with a commercial interest in the technology. A.R.B., S.X., and J.A.R. report equity ownership in a small private company with a commercial interest in the technology.</p>
              </fn>
              <fn fn-type="supplementary-material">
                <p>This article contains supporting information online at <ext-link ext-link-type="uri" xlink:href="https://www.pnas.org/lookup/suppl/doi:10.1073/pnas.2026610118/-/DCSupplemental" xlink:show="new">https://www.pnas.org/lookup/suppl/doi:10.1073/pnas.2026610118/-/DCSupplemental</ext-link>.</p>
              </fn>
            </fn-group>
            <sec id="s16" sec-type="data-availability">
              <title>Data Availability</title>
              <p>All relevant data are included in the article and <ext-link ext-link-type="uri" xlink:href="https://www.pnas.org/lookup/suppl/doi:10.1073/pnas.2026610118/-/DCSupplemental" xlink:show="new"><italic>SI Appendix</italic></ext-link>. Additional supporting data are available from the corresponding authors on request. All request for raw and analyzed data and materials will be reviewed by the corresponding authors to verify whether the request is subject to any intellectual property or confidentiality obligations. Patient related data not included in the paper were generated as part of clinical trials and may be subject to patient confidentiality.</p>
            </sec>
            <ref-list>
              <ref id="r1">
                <label>1</label>
                <mixed-citation publication-type="other"><person-group person-group-type="author"><collab>Centers for Disease Control and Prevention</collab></person-group>, <article-title>Coronavirus disease 2019 (COVID-19). Cases in the U.S. New cases by day</article-title>. <ext-link ext-link-type="uri" xlink:href="https://covid.cdc.gov/covid-data-tracker/" xlink:show="new">https://covid.cdc.gov/covid-data-tracker/</ext-link>. <comment>Accessed 26 December 2020</comment>.</mixed-citation>
              </ref>
              <ref id="r2">
                <label>2</label>
                <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>C. P.</given-names><surname>West</surname></string-name>, <string-name><given-names>V. M.</given-names><surname>Montori</surname></string-name>, <string-name><given-names>P.</given-names><surname>Sampathkumar</surname></string-name></person-group>, <article-title>COVID-19 testing: The threat of false-negative results</article-title>. <source>Mayo Clinic Proc.</source><volume>95</volume>, <fpage>1127</fpage>–<lpage>1129</lpage> (<year>2020</year>).</mixed-citation>
              </ref>
              <ref id="r3">
                <label>3</label>
                <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>S. D.</given-names><surname>Pettit</surname></string-name><etal/></person-group>, <article-title>‘All in’: A pragmatic framework for COVID-19 testing and action on a global scale</article-title>. <source>EMBO Mol. Med.</source><volume>12</volume>, <fpage>e12634</fpage> (<year>2020</year>).<pub-id pub-id-type="pmid">32375201</pub-id></mixed-citation>
              </ref>
              <ref id="r4">
                <label>4</label>
                <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>C.</given-names><surname>Menni</surname></string-name><etal/></person-group>, <article-title>Real-time tracking of self-reported symptoms to predict potential COVID-19</article-title>. <source>Nat. Med.</source><volume>26</volume>, <fpage>1037</fpage>–<lpage>1040</lpage> (<year>2020</year>).<pub-id pub-id-type="pmid">32393804</pub-id></mixed-citation>
              </ref>
              <ref id="r5">
                <label>5</label>
                <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>A.</given-names><surname>Natarajan</surname></string-name>, <string-name><given-names>H. W.</given-names><surname>Su</surname></string-name>, <string-name><given-names>C.</given-names><surname>Heneghan</surname></string-name></person-group>, <article-title>Assessment of physiological signs associated with COVID-19 measured using wearable devices</article-title>. <source>NPJ Digit. Med.</source><volume>3</volume>, <fpage>156</fpage> (<year>2020</year>).<pub-id pub-id-type="pmid">33299095</pub-id></mixed-citation>
              </ref>
              <ref id="r6">
                <label>6</label>
                <mixed-citation publication-type="other"><person-group person-group-type="author"><string-name><given-names>D.</given-names><surname>Miller</surname></string-name><etal/></person-group>, <article-title>Analyzing changes in respiratory rate to predict the risk of COVID-19 infection</article-title>. medRxiv [Preprint] (<year>2020</year>). <pub-id pub-id-type="doi">10.1101/2020.06.18.20131417</pub-id> (Accessed 26 December 2020).</mixed-citation>
              </ref>
              <ref id="r7">
                <label>7</label>
                <mixed-citation publication-type="other"><person-group person-group-type="author"><collab>US Department of Health and Human Services</collab></person-group>, <article-title>BARDA and VitalConnect partner to monitor nursing home and COVID-19 patients for early indication of patient deterioration</article-title>. <ext-link ext-link-type="uri" xlink:href="https://www.medicalcountermeasures.gov/newsroom/2020/vitalconnect/" xlink:show="new">https://www.medicalcountermeasures.gov/newsroom/2020/vitalconnect/</ext-link>. <comment>Accessed 26 December 2020</comment>.</mixed-citation>
              </ref>
              <ref id="r8">
                <label>8</label>
                <mixed-citation publication-type="other"><person-group person-group-type="author"><collab>Philips</collab></person-group>, <article-title>Philips launches next generation wearable biosensor for early patient deterioration detection, including clinical surveillance for COVID-19</article-title>. <ext-link ext-link-type="uri" xlink:href="https://www.philips.com/a-w/about/news/archive/standard/news/press/2020/20200526-philips-launches-next-generation-wearable-biosensor-for-early-patient-deterioration-detection-including-clinical-surveillance-for-covid-19.html" xlink:show="new">https://www.philips.com/a-w/about/news/archive/standard/news/press/2020/20200526-philips-launches-next-generation-wearable-biosensor-for-early-patient-deterioration-detection-including-clinical-surveillance-for-covid-19.html</ext-link>. <comment>Accessed 26 December 2020</comment>.</mixed-citation>
              </ref>
              <ref id="r9">
                <label>9</label>
                <mixed-citation publication-type="other"><person-group person-group-type="author"><collab>US Department of Health and Human Services</collab></person-group>, <article-title>BARDA and Sonica Health expand partnership to develop wearable patch as an early monitoring platform for COVID-19 infection</article-title>. <comment><ext-link ext-link-type="uri" xlink:href="https://www.medicalcountermeasures.gov/newsroom/2020/sonica/" xlink:show="new">https://www.medicalcountermeasures.gov/newsroom/2020/sonica/</ext-link></comment>. <comment>Accessed 26 December 2020</comment>.</mixed-citation>
              </ref>
              <ref id="r10">
                <label>10</label>
                <mixed-citation publication-type="other"><person-group person-group-type="author"><collab>US Department of Health and Human Services</collab></person-group>, <article-title>BARDA and Empatica forge new partnership to develop an early monitoring platform to identify COVID-19 infection</article-title>. <ext-link ext-link-type="uri" xlink:href="https://www.medicalcountermeasures.gov/newsroom/2020/empatica/" xlink:show="new">https://www.medicalcountermeasures.gov/newsroom/2020/empatica/</ext-link>. <comment>Accessed 26 December 2020</comment>.</mixed-citation>
              </ref>
              <ref id="r11">
                <label>11</label>
                <mixed-citation publication-type="other"><person-group person-group-type="author"><string-name><given-names>T.</given-names><surname>Mishra</surname></string-name><etal/></person-group>, <article-title>Early detection of COVID-19 using a smartwatch</article-title>. medRxiv [Preprint] (<year>2020</year>). <pub-id pub-id-type="doi">10.1101/2020.07.06.20147512</pub-id> (Accessed 26 December 2020).</mixed-citation>
              </ref>
              <ref id="r12">
                <label>12</label>
                <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>T.</given-names><surname>Greenhalgh</surname></string-name>, <string-name><given-names>G. C. H.</given-names><surname>Koh</surname></string-name>, <string-name><given-names>J.</given-names><surname>Car</surname></string-name></person-group>, <article-title>COVID-19: A remote assessment in primary care</article-title>. <source>BMJ</source><volume>368</volume>, <fpage>m1182</fpage> (<year>2020</year>).<pub-id pub-id-type="pmid">32213507</pub-id></mixed-citation>
              </ref>
              <ref id="r13">
                <label>13</label>
                <mixed-citation publication-type="other"><person-group person-group-type="author"><string-name><given-names>S.</given-names><surname>Hassantabar</surname></string-name><etal/></person-group>, <article-title>CovidDeep: SARS-CoV-2/COVID-19 test based on wearable medical sensors and efficient neural networks</article-title>. arXiv [Preprint] (<year>2020</year>). <ext-link ext-link-type="uri" xlink:href="https://arxiv.org/abs/2007.10497" xlink:show="new">https://arxiv.org/abs/2007.10497</ext-link> (Accessed 26 December 2020).</mixed-citation>
              </ref>
              <ref id="r14">
                <label>14</label>
                <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>N. M.</given-names><surname>Hemphill</surname></string-name>, <string-name><given-names>M. T. Y.</given-names><surname>Kuan</surname></string-name>, <string-name><given-names>K. C.</given-names><surname>Harris</surname></string-name></person-group>, <article-title>Reduced physical activity during COVID-19 pandemic in children with congenital heart disease</article-title>. <source>Can. J. Cardiol.</source><volume>36</volume>, <fpage>1130</fpage>–<lpage>1134</lpage> (<year>2020</year>).<pub-id pub-id-type="pmid">32387502</pub-id></mixed-citation>
              </ref>
              <ref id="r15">
                <label>15</label>
                <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>K.</given-names><surname>Meng</surname></string-name><etal/></person-group>, <article-title>A wireless textile-based sensor system for self-powered personalized health care</article-title>. <source>Matter</source><volume>2</volume>, <fpage>896</fpage>–<lpage>907</lpage> (<year>2020</year>).</mixed-citation>
              </ref>
              <ref id="r16">
                <label>16</label>
                <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>H.</given-names><surname>Liu</surname></string-name><etal/></person-group>, <article-title>Comparison of different modulations of photoplethysmography in extracting respiratory rate: From a physiological perspective</article-title>. <source>Physiol. Meas.</source><volume>41</volume>, <fpage>41</fpage> (<year>2020</year>).</mixed-citation>
              </ref>
              <ref id="r17">
                <label>17</label>
                <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>W.</given-names><surname>Karlen</surname></string-name>, <string-name><given-names>S.</given-names><surname>Raman</surname></string-name>, <string-name><given-names>J. M.</given-names><surname>Ansermino</surname></string-name>, <string-name><given-names>G. A.</given-names><surname>Dumont</surname></string-name></person-group>, <article-title>Multiparameter respiratory rate estimation from the photoplethysmogram</article-title>. <source>IEEE Trans. Biomed. Eng.</source><volume>60</volume>, <fpage>1946</fpage>–<lpage>1953</lpage> (<year>2013</year>).<pub-id pub-id-type="pmid">23399950</pub-id></mixed-citation>
              </ref>
              <ref id="r18">
                <label>18</label>
                <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>Hyoyoung.</given-names><surname>Jeong</surname></string-name>, <string-name><given-names>John. A.</given-names><surname>Rogers</surname></string-name>, <string-name><given-names>Shuai.</given-names><surname>Xu</surname></string-name></person-group>, <article-title>Continuous on-body sensing for the COVID-19 pandemic: Gaps and opportunities</article-title>. <source>Sci. Adv.</source><volume>6</volume>, <fpage>eabd4794</fpage> (<year>2020</year>).<pub-id pub-id-type="pmid">32917604</pub-id></mixed-citation>
              </ref>
              <ref id="r19">
                <label>19</label>
                <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>R.</given-names><surname>Gravina</surname></string-name>, <string-name><given-names>P.</given-names><surname>Alinia</surname></string-name>, <string-name><given-names>H.</given-names><surname>Ghasemzadeh</surname></string-name>, <string-name><given-names>G.</given-names><surname>Fortino</surname></string-name></person-group>, <article-title>Multi-sensor fusion in body sensor networks: State-of-the-art and research challenges</article-title>. <source>Inf. Fusion</source><volume>35</volume>, <fpage>1339</fpage>–<lpage>1351</lpage> (<year>2017</year>).</mixed-citation>
              </ref>
              <ref id="r20">
                <label>20</label>
                <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>R. G.</given-names><surname>Loudon</surname></string-name>, <string-name><given-names>L. C.</given-names><surname>Brown</surname></string-name></person-group>, <article-title>Cough frequency in patients with respiratory disease</article-title>. <source>Am. Rev. Respir. Dis.</source><volume>96</volume>, <fpage>1137</fpage>–<lpage>1143</lpage> (<year>1967</year>).<pub-id pub-id-type="pmid">6057616</pub-id></mixed-citation>
              </ref>
              <ref id="r21">
                <label>21</label>
                <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>L.</given-names><surname>Pavesi</surname></string-name>, <string-name><given-names>S.</given-names><surname>Subburaj</surname></string-name>, <string-name><given-names>K.</given-names><surname>Porter-Shaw</surname></string-name></person-group>, <article-title>Application and validation of a computerized cough acquisition system for objective monitoring of acute cough: A meta-analysis</article-title>. <source>Chest</source><volume>120</volume>, <fpage>1121</fpage>–<lpage>1128</lpage> (<year>2001</year>).<pub-id pub-id-type="pmid">11591548</pub-id></mixed-citation>
              </ref>
              <ref id="r22">
                <label>22</label>
                <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>M. M.</given-names><surname>Cloutier</surname></string-name>, <string-name><given-names>G. M.</given-names><surname>Loughlin</surname></string-name></person-group>, <article-title>Chronic cough in children: A manifestation of airway hyperreactivity</article-title>. <source>Pediatrics</source><volume>67</volume>, <fpage>6</fpage>–<lpage>12</lpage> (<year>1981</year>).<pub-id pub-id-type="pmid">7243436</pub-id></mixed-citation>
              </ref>
              <ref id="r23">
                <label>23</label>
                <mixed-citation publication-type="other"><person-group person-group-type="author"><string-name><given-names>C.</given-names><surname>Bales</surname></string-name><etal/></person-group>, <article-title>“Can machine learning be used to recognize and diagnose coughs?</article-title>” in <source>International Conference on e-Health and Bioengineering</source>
<comment>(EHB, Iasi, Romania, 2020), pp. 1–4</comment>. <year>2020</year>.</mixed-citation>
              </ref>
              <ref id="r24">
                <label>24</label>
                <mixed-citation publication-type="other"><person-group person-group-type="author"><string-name><given-names>M. A.</given-names><surname>Ismail</surname></string-name>, <string-name><given-names>S.</given-names><surname>Deshmukh</surname></string-name>, <string-name><given-names>R.</given-names><surname>Singh</surname></string-name></person-group>. <article-title>Detection of COVID-19 through the analysis of vocal fold oscillations</article-title>. arXiv [Preprint] (<year>2020</year>). <ext-link ext-link-type="uri" xlink:href="https://arxiv.org/abs/2010.10707" xlink:show="new">https://arxiv.org/abs/2010.10707</ext-link> (Accessed 5 March 2021).</mixed-citation>
              </ref>
              <ref id="r25">
                <label>25</label>
                <mixed-citation publication-type="other"><person-group person-group-type="author"><string-name><given-names>S.</given-names><surname>Deshmukh</surname></string-name>, <string-name><given-names>M. A.</given-names><surname>Ismail</surname></string-name>, <string-name><given-names>R.</given-names><surname>Singh</surname></string-name></person-group>, <article-title>Interpreting glottal flow dynamics for detecting COVID-19 from voice</article-title>. arXiv [Preprint] (<year>2020</year>). <ext-link ext-link-type="uri" xlink:href="https://arxiv.org/abs/2010.16318" xlink:show="new">https://arxiv.org/abs/2010.16318</ext-link> (Accessed 5 March 2021).</mixed-citation>
              </ref>
              <ref id="r26">
                <label>26</label>
                <mixed-citation publication-type="other"><person-group person-group-type="author"><string-name><given-names>B. W.</given-names><surname>Schuller</surname></string-name>, <string-name><given-names>H.</given-names><surname>Coppock</surname></string-name>, <string-name><given-names>A.</given-names><surname>Gaskell</surname></string-name></person-group>, <article-title>Detecting COVID-19 from breathing and coughing sounds using deep neural networks</article-title>. arXiv [Preprint] (<year>2020</year>). <ext-link ext-link-type="uri" xlink:href="https://arxiv.org/abs/2012.14553" xlink:show="new">https://arxiv.org/abs/2012.14553</ext-link> (Accessed 5 March 2021).</mixed-citation>
              </ref>
              <ref id="r27">
                <label>27</label>
                <mixed-citation publication-type="other"><person-group person-group-type="author"><string-name><given-names>Y.</given-names><surname>Huang</surname></string-name><etal/></person-group>, <article-title>The respiratory sound features of COVID-19 patients fill gaps between clinical data and screening methods</article-title>. medRxiv [Preprint] (<year>2020</year>). <pub-id pub-id-type="doi">10.1101/2020.04.07.20051060</pub-id> (Accessed 5 March 2021).</mixed-citation>
              </ref>
              <ref id="r28">
                <label>28</label>
                <mixed-citation publication-type="other"><person-group person-group-type="author"><string-name><given-names>P.</given-names><surname>Bagad</surname></string-name><etal/></person-group>, <article-title>Cough against COVID: Evidence of COVID-19 signature in cough sounds</article-title>. arXiv [Preprint] (<year>2020</year>). <ext-link ext-link-type="uri" xlink:href="https://arxiv.org/abs/2009.08790" xlink:show="new">https://arxiv.org/abs/2009.08790</ext-link> (Accessed 5 March 2021).</mixed-citation>
              </ref>
              <ref id="r29">
                <label>29</label>
                <mixed-citation publication-type="other"><person-group person-group-type="author"><string-name><given-names>A.</given-names><surname>Pal</surname></string-name>, <string-name><given-names>M.</given-names><surname>Sankarasubbu</surname></string-name></person-group>, <article-title>Pay attention to the cough: Early diagnosis of COVID-19 using interpretable symptoms embeddings with cough sound signal processing</article-title>. arXiv [Preprint] (<year>2020</year>). <ext-link ext-link-type="uri" xlink:href="https://arxiv.org/abs/2010.02417" xlink:show="new">https://arxiv.org/abs/2010.02417</ext-link> (Accessed 5 March 2021).</mixed-citation>
              </ref>
              <ref id="r30">
                <label>30</label>
                <mixed-citation publication-type="other"><person-group person-group-type="author"><string-name><given-names>S.</given-names><surname>Agrawal</surname></string-name><etal/></person-group>, <article-title>BuildForCOVID19</article-title>. <ext-link ext-link-type="uri" xlink:href="https://buildforcovid19.io/detect-now/" xlink:show="new">https://buildforcovid19.io/detect-now/</ext-link>. <comment>Accessed 26 December 2020</comment>.</mixed-citation>
              </ref>
              <ref id="r31">
                <label>31</label>
                <mixed-citation publication-type="other"><person-group person-group-type="author"><string-name><given-names>C.</given-names><surname>Mascolo</surname></string-name><etal/></person-group>, <article-title>COVID-19 sounds app</article-title> (<year>2020</year>). <ext-link ext-link-type="uri" xlink:href="https://www.covid-19-sounds.org/en/" xlink:show="new">https://www.covid-19-sounds.org/en/</ext-link>. <comment>Accessed 26 December 2020</comment>.</mixed-citation>
              </ref>
              <ref id="r32">
                <label>32</label>
                <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>G.</given-names><surname>Zayas</surname></string-name><etal/></person-group>, <article-title>Cough aerosol in healthy participants: Fundamental knowledge to optimize droplet-spread infectious respiratory disease management</article-title>. <source>BMC Pulm. Med.</source><volume>12</volume>, <fpage>11</fpage> (<year>2012</year>).<pub-id pub-id-type="pmid">22436202</pub-id></mixed-citation>
              </ref>
              <ref id="r33">
                <label>33</label>
                <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>R.</given-names><surname>Mittal</surname></string-name>, <string-name><given-names>R.</given-names><surname>Ni</surname></string-name>, <string-name><given-names>J. H.</given-names><surname>Seo</surname></string-name></person-group>, <article-title>The flow physics of COVID-19</article-title>. <source>J. Fluid Mech.</source><volume>894</volume>, <fpage>F2</fpage> (<year>2020</year>).</mixed-citation>
              </ref>
              <ref id="r34">
                <label>34</label>
                <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>T.</given-names><surname>Dbouk</surname></string-name>, <string-name><given-names>D.</given-names><surname>Drikakis</surname></string-name></person-group>, <article-title>On coughing and airborne droplet transmission to humans</article-title>. <source>Phys. Fluids</source><volume>32</volume>, <fpage>053310</fpage> (<year>2020</year>).</mixed-citation>
              </ref>
              <ref id="r35">
                <label>35</label>
                <mixed-citation publication-type="other"><person-group person-group-type="author"><string-name><given-names>F. K. A.</given-names><surname>Gregson</surname></string-name><etal/></person-group>, <article-title>Comparing the respirable aerosol concentrations and particle size distributions generated by singing , speaking and breathing</article-title>. ChemRxiv [Preprint] (<year>2020</year>). <pub-id pub-id-type="doi">10.26434/chemrxiv.12789221.v1</pub-id> (Accessed 26 December 2020).</mixed-citation>
              </ref>
              <ref id="r36">
                <label>36</label>
                <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>G. A.</given-names><surname>Fontana</surname></string-name>, <string-name><given-names>T.</given-names><surname>Pantaleo</surname></string-name>, <string-name><given-names>F.</given-names><surname>Lavorini</surname></string-name>, <string-name><given-names>V.</given-names><surname>Boddi</surname></string-name>, <string-name><given-names>P.</given-names><surname>Panuccio</surname></string-name></person-group>, <article-title>A noninvasive electromyographic study on threshold and intensity of cough in humans</article-title>. <source>Eur. Respir. J.</source><volume>10</volume>, <fpage>983</fpage>–<lpage>989</lpage> (<year>1997</year>).<pub-id pub-id-type="pmid">9163635</pub-id></mixed-citation>
              </ref>
              <ref id="r37">
                <label>37</label>
                <mixed-citation publication-type="book"><person-group person-group-type="author"><string-name><given-names>T.</given-names><surname>Drugman</surname></string-name><etal/></person-group>, “<article-title>Audio and contact microphones for cough detection</article-title>” in <source>13th Annual Conference of the International Speech Communication Association 2012, INTERSPEECH 2012</source> (<publisher-name>International Speech Communication Association</publisher-name>, <year>2012</year>), <comment>vol. 2</comment>, pp. <fpage>1302</fpage>–<lpage>1305</lpage>.</mixed-citation>
              </ref>
              <ref id="r38">
                <label>38</label>
                <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>T.</given-names><surname>Drugman</surname></string-name><etal/></person-group>, <article-title>Objective study of sensor relevance for automatic cough detection</article-title>. <source>IEEE J. Biomed. Health Inform.</source><volume>17</volume>, <fpage>699</fpage>–<lpage>707</lpage> (<year>2013</year>).<pub-id pub-id-type="pmid">24592470</pub-id></mixed-citation>
              </ref>
              <ref id="r39">
                <label>39</label>
                <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>A.</given-names><surname>Jasmine</surname></string-name>, <string-name><given-names>A. K.</given-names><surname>Jayanthy</surname></string-name></person-group>, <article-title>Sensor-based system for automatic cough detection and classification</article-title>. <source>Test Eng. Manag.</source><volume>83</volume>, <fpage>13826</fpage>–<lpage>13834</lpage> (<year>2020</year>).</mixed-citation>
              </ref>
              <ref id="r40">
                <label>40</label>
                <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>A.</given-names><surname>Bush</surname></string-name></person-group>, <article-title>Diagnostic and therapeutic methods—A new device for ambulatory cough recording</article-title>. <source>Heart Lung</source><volume>186</volume>, <fpage>178</fpage>–<lpage>186</lpage> (<year>1994</year>).</mixed-citation>
              </ref>
              <ref id="r41">
                <label>41</label>
                <mixed-citation publication-type="book"><person-group person-group-type="author"><string-name><given-names>T.</given-names><surname>Elfaramawy</surname></string-name>, <string-name><given-names>C. L.</given-names><surname>Fall</surname></string-name>, <string-name><given-names>M.</given-names><surname>Morissette</surname></string-name>, <string-name><given-names>F.</given-names><surname>Lellouche</surname></string-name>, <string-name><given-names>B.</given-names><surname>Gosselin</surname></string-name></person-group>, “<article-title>Wireless respiratory monitoring and coughing detection using a wearable patch sensor network</article-title>” in <source>Proceedings - 2017 IEEE 15th International New Circuits and Systems Conference, NEWCAS 2017</source> (<publisher-name>Institute of Electrical and Electronics Engineers</publisher-name>, <year>2017</year>), pp. <fpage>197</fpage>–<lpage>200</lpage>.</mixed-citation>
              </ref>
              <ref id="r42">
                <label>42</label>
                <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>J.</given-names><surname>Amoh</surname></string-name>, <string-name><given-names>K.</given-names><surname>Odame</surname></string-name></person-group>, <article-title>Deep neural networks for identifying cough sounds</article-title>. <source>IEEE Trans Biomed. Circ. Syst.</source><volume>10</volume>, <fpage>1003</fpage>–<lpage>1011</lpage> (<year>2016</year>).</mixed-citation>
              </ref>
              <ref id="r43">
                <label>43</label>
                <mixed-citation publication-type="other"><person-group person-group-type="author"><string-name><given-names>R. X. A.</given-names><surname>Pramono</surname></string-name>, <string-name><given-names>S. A.</given-names><surname>Imtiaz</surname></string-name>, <string-name><given-names>E.</given-names><surname>Rodriguez-Villegas</surname></string-name></person-group>, “<article-title>Automatic cough detection in acoustic signal using spectral features</article-title>” in <source>Proceedings of the Annual International Conference of the IEEE Engineering in Medicine and Biology Society</source> (Engineering in Medicine and Biology Society, <year>2019</year>), pp. <fpage>7153</fpage>–<lpage>7156</lpage>.</mixed-citation>
              </ref>
              <ref id="r44">
                <label>44</label>
                <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>K.</given-names><surname>Kosasih</surname></string-name>, <string-name><given-names>U. R.</given-names><surname>Abeyratne</surname></string-name>, <string-name><given-names>V.</given-names><surname>Swarnkar</surname></string-name>, <string-name><given-names>R.</given-names><surname>Triasih</surname></string-name></person-group>, <article-title>Wavelet augmented cough analysis for rapid childhood pneumonia diagnosis</article-title>. <source>IEEE Trans. Biomed. Eng.</source><volume>62</volume>, <fpage>1185</fpage>–<lpage>1194</lpage> (<year>2015</year>).<pub-id pub-id-type="pmid">25532164</pub-id></mixed-citation>
              </ref>
              <ref id="r45">
                <label>45</label>
                <mixed-citation publication-type="other"><person-group person-group-type="author"><string-name><given-names>L</given-names><surname>Di Perna</surname></string-name><etal/></person-group>, “<article-title>An automated and unobtrusive system for cough detection</article-title>” in <source>2017 IEEE Life Sciences Conference, LSC 2017</source> (Institute of Electrical and Electronics Engineers, <year>2018</year>), pp. <fpage>190</fpage>–<lpage>193</lpage>.</mixed-citation>
              </ref>
              <ref id="r46">
                <label>46</label>
                <mixed-citation publication-type="book"><person-group person-group-type="author"><string-name><given-names>I. D. S.</given-names><surname>Miranda</surname></string-name>, <string-name><given-names>A. H.</given-names><surname>Diacon</surname></string-name>, <string-name><given-names>T. R.</given-names><surname>Niesler</surname></string-name></person-group>, “<article-title>A comparative study of features for acoustic cough detection using deep architectures</article-title>” in <source>Proceedings of the Annual International Conference of the IEEE Engineering in Medicine and Biology Society</source>(<publisher-name>Engineering in Medicine and Biology Society</publisher-name>, <year>2019</year>), pp. <fpage>2601</fpage>–<lpage>2605</lpage>.</mixed-citation>
              </ref>
              <ref id="r47">
                <label>47</label>
                <mixed-citation publication-type="other"><person-group person-group-type="author"><string-name><given-names>H. H.</given-names><surname>Wang</surname></string-name>, <string-name><given-names>J. M.</given-names><surname>Liu</surname></string-name>, <string-name><given-names>M.</given-names><surname>You</surname></string-name>, <string-name><given-names>G. Z.</given-names><surname>Li</surname></string-name></person-group>, “<article-title>Audio signals encoding for cough classification using convolutional neural networks: A comparative study</article-title>” in <source>Proceedings - 2015 IEEE International Conference on Bioinformatics and Biomedicine, BIBM 2015</source> (Institute of Electrical and Electronics Engineers, <year>2015</year>), pp. <fpage>442</fpage>–<lpage>445</lpage>.</mixed-citation>
              </ref>
              <ref id="r48">
                <label>48</label>
                <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>Y. A.</given-names><surname>Amrulloh</surname></string-name>, <string-name><given-names>U. R.</given-names><surname>Abeyratne</surname></string-name>, <string-name><given-names>V.</given-names><surname>Swarnkar</surname></string-name>, <string-name><given-names>R.</given-names><surname>Triasih</surname></string-name>, <string-name><given-names>A.</given-names><surname>Setyati</surname></string-name></person-group>, <article-title>Automatic cough segmentation from non-contact sound recordings in pediatric wards</article-title>. <source>Biomed. Signal Process Contr.</source><volume>21</volume>, <fpage>126</fpage>–<lpage>136</lpage> (<year>2015</year>).</mixed-citation>
              </ref>
              <ref id="r49">
                <label>49</label>
                <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>J.</given-names><surname>Monge-Alvarez</surname></string-name>, <string-name><given-names>C.</given-names><surname>Hoyos-Barcelo</surname></string-name>, <string-name><given-names>P.</given-names><surname>Lesso</surname></string-name>, <string-name><given-names>P.</given-names><surname>Casaseca-De-La-Higuera</surname></string-name></person-group>, <article-title>Robust detection of audio-cough events using local Hu moments</article-title>. <source>IEEE J. Biomed. Health Inform.</source><volume>23</volume>, <fpage>184</fpage>–<lpage>196</lpage> (<year>2019</year>).<pub-id pub-id-type="pmid">29994432</pub-id></mixed-citation>
              </ref>
              <ref id="r50">
                <label>50</label>
                <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>C.</given-names><surname>Hoyos-Barcelo</surname></string-name>, <string-name><given-names>J.</given-names><surname>Monge-Alvarez</surname></string-name>, <string-name><given-names>M. Z.</given-names><surname>Shakir</surname></string-name>, <string-name><given-names>J. M.</given-names><surname>Alcaraz-Calero</surname></string-name>, <string-name><given-names>P.</given-names><surname>Casaseca-De-La-Higuera</surname></string-name></person-group>, <article-title>Efficient k-NN implementation for real-time detection of cough events in smartphones</article-title>. <source>IEEE J. Biomed. Health Inform.</source><volume>22</volume>, <fpage>1662</fpage>–<lpage>1671</lpage> (<year>2018</year>).<pub-id pub-id-type="pmid">29990219</pub-id></mixed-citation>
              </ref>
              <ref id="r51">
                <label>51</label>
                <mixed-citation publication-type="book"><person-group person-group-type="author"><string-name><given-names>A.</given-names><surname>Teyhouee</surname></string-name>, <string-name><given-names>N. D.</given-names><surname>Osgood</surname></string-name></person-group>, “<article-title>Cough detection using hidden markov models</article-title>” in <source>International Conference on Social Computing, Behavioral-Cultural Modeling and Prediction and Behavior Representation in Modeling and Simulation</source> (<publisher-name>Springer</publisher-name>, <year>2019</year>), pp. <fpage>266</fpage>–<lpage>276</lpage>.</mixed-citation>
              </ref>
              <ref id="r52">
                <label>52</label>
                <mixed-citation publication-type="book"><person-group person-group-type="author"><string-name><given-names>E. C.</given-names><surname>Larson</surname></string-name>, <string-name><given-names>T. J.</given-names><surname>Lee</surname></string-name>, <string-name><given-names>S.</given-names><surname>Liu</surname></string-name>, <string-name><given-names>M.</given-names><surname>Rosenfeld</surname></string-name>, <string-name><given-names>S. N.</given-names><surname>Patel</surname></string-name></person-group>, “<article-title>Accurate and privacy preserving cough sensing using a low-cost microphone</article-title>” in <source>UbiComp’11 - Proceedings of the 2011 ACM Conference on Ubiquitous Computing</source> (<publisher-name>Association for Computing Machinery</publisher-name>, <year>2011</year>), pp. <fpage>375</fpage>–<lpage>384</lpage>.</mixed-citation>
              </ref>
              <ref id="r53">
                <label>53</label>
                <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>Z.</given-names><surname>Zhou</surname></string-name><etal/></person-group>, <article-title>Sign-to-speech translation using machine-learning-assisted stretchable sensor arrays</article-title>. <source>Nat. Electron.</source><volume>3</volume>, <fpage>571</fpage>–<lpage>578</lpage> (<year>2020</year>).</mixed-citation>
              </ref>
              <ref id="r54">
                <label>54</label>
                <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>K. H.</given-names><surname>Lee</surname></string-name><etal/></person-group>, <article-title>Mechano-acoustic sensing of physiological processes and body motions via a soft wireless device placed at the suprasternal notch</article-title>. <source>Nat. Biomed. Eng.</source><volume>4</volume>, <fpage>148</fpage>–<lpage>158</lpage> (<year>2020</year>).<pub-id pub-id-type="pmid">31768002</pub-id></mixed-citation>
              </ref>
              <ref id="r55">
                <label>55</label>
                <mixed-citation publication-type="other"><person-group person-group-type="author"><string-name><given-names>K.</given-names><surname>He</surname></string-name>, <string-name><given-names>X.</given-names><surname>Zhang</surname></string-name>, <string-name><given-names>S.</given-names><surname>Ren</surname></string-name>, <string-name><given-names>J.</given-names><surname>Sun</surname></string-name></person-group>, “<article-title>Deep residual learning for image recognition</article-title>” in <source>Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition</source> (Institute of Electrical and Electronics Engineers, <year>2016</year>), pp. <fpage>770</fpage>–<lpage>778</lpage>.</mixed-citation>
              </ref>
              <ref id="r56">
                <label>56</label>
                <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>M.</given-names><surname>Abkarian</surname></string-name>, <string-name><given-names>S.</given-names><surname>Mendez</surname></string-name>, <string-name><given-names>N.</given-names><surname>Xue</surname></string-name>, <string-name><given-names>F.</given-names><surname>Yang</surname></string-name>, <string-name><given-names>H. A.</given-names><surname>Stone</surname></string-name></person-group>, <article-title>Speech can produce jet-like transport relevant to asymptomatic spreading of virus</article-title>. <source>Proc. Natl. Acad. Sci. U.S.A.</source><volume>117</volume>, <fpage>25237</fpage>–<lpage>25245</lpage> (<year>2020</year>).<pub-id pub-id-type="pmid">32978297</pub-id></mixed-citation>
              </ref>
              <ref id="r57">
                <label>57</label>
                <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>D.</given-names><surname>Lahat</surname></string-name>, <string-name><given-names>T.</given-names><surname>Adali</surname></string-name>, <string-name><given-names>C.</given-names><surname>Jutten</surname></string-name></person-group>, <article-title>Multimodal data fusion: An overview of methods, challenges, and prospects</article-title>. <source>Proc. IEEE</source><volume>103</volume>, <fpage>1449</fpage>–<lpage>1477</lpage> (<year>2015</year>).</mixed-citation>
              </ref>
              <ref id="r58">
                <label>58</label>
                <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>P.</given-names><surname>Kumari</surname></string-name>, <string-name><given-names>L.</given-names><surname>Mathew</surname></string-name>, <string-name><given-names>P.</given-names><surname>Syal</surname></string-name></person-group>, <article-title>Increasing trend of wearables and multimodal interface for human activity monitoring: A review</article-title>. <source>Biosens. Bioelectron.</source><volume>90</volume>, <fpage>298</fpage>–<lpage>307</lpage> (<year>2017</year>).<pub-id pub-id-type="pmid">27931004</pub-id></mixed-citation>
              </ref>
              <ref id="r59">
                <label>59</label>
                <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>J. T.</given-names><surname>Kim</surname></string-name>, <string-name><given-names>J.</given-names><surname>Nam</surname></string-name>, <string-name><given-names>S.</given-names><surname>Shen</surname></string-name>, <string-name><given-names>C.</given-names><surname>Lee</surname></string-name>, <string-name><given-names>L. P.</given-names><surname>Chamorro</surname></string-name></person-group>, <article-title>On the dynamics of air bubbles in Rayleigh–Bénard convection</article-title>. <source>J. Fluid Mech.</source><volume>891</volume>, <fpage>A7</fpage> (<year>2020</year>).</mixed-citation>
              </ref>
              <ref id="r60">
                <label>60</label>
                <mixed-citation publication-type="book"><person-group person-group-type="author"><string-name><given-names>C. F.</given-names><surname>Bohren</surname></string-name></person-group>, <source>Absorption and Scattering of Light by Small Particles</source> (<publisher-name>John Wiley</publisher-name>, <year>1983</year>).</mixed-citation>
              </ref>
              <ref id="r61">
                <label>61</label>
                <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>J.</given-names><surname>Schäfer</surname></string-name>, <string-name><given-names>S.-C.</given-names><surname>Lee</surname></string-name>, <string-name><given-names>A.</given-names><surname>Kienle</surname></string-name></person-group>, <article-title>Calculation of the near fields for the scattering of electromagnetic waves by multiple infinite cylinders at perpendicular incidence</article-title>. <source>J. Quant. Spectrosc. Radiat. Transf.</source><volume>11</volume>, <fpage>2113</fpage>–<lpage>2123</lpage> (<year>2012</year>).</mixed-citation>
              </ref>
            </ref-list>
          </back>
        </article>
      </metadata>
    </record>
  </GetRecord>
</OAI-PMH>
