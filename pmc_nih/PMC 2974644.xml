<?xml version="1.0" encoding="UTF-8"?>
<OAI-PMH xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/ http://www.openarchives.org/OAI/2.0/OAI-PMH.xsd">
  <responseDate>2024-05-03T06:51:56Z</responseDate>
  <request verb="GetRecord" identifier="oai:pubmedcentral.nih.gov:2974644" metadataPrefix="pmc">https:/www.ncbi.nlm.nih.gov/pmc/oai/oai.cgi</request>
  <GetRecord>
    <record>
      <header>
        <identifier>oai:pubmedcentral.nih.gov:2974644</identifier>
        <datestamp>2010-11-15</datestamp>
        <setSpec>plosone</setSpec>
        <setSpec>pmc-open</setSpec>
      </header>
      <metadata>
        <article xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" xsi:schemaLocation="https://jats.nlm.nih.gov/ns/archiving/1.3/ https://jats.nlm.nih.gov/archiving/1.3/xsd/JATS-archivearticle1-3.xsd" article-type="research-article">
          <front>
            <journal-meta>
              <journal-id journal-id-type="nlm-ta">PLoS One</journal-id>
              <journal-id journal-id-type="iso-abbrev">PLoS ONE</journal-id>
              <journal-id journal-id-type="publisher-id">plos</journal-id>
              <journal-id journal-id-type="pmc">plosone</journal-id>
              <journal-title-group>
                <journal-title>PLoS ONE</journal-title>
              </journal-title-group>
              <issn pub-type="epub">1932-6203</issn>
              <publisher>
                <publisher-name>Public Library of Science</publisher-name>
                <publisher-loc>San Francisco, USA</publisher-loc>
              </publisher>
            </journal-meta>
            <article-meta>
              <article-id pub-id-type="accession">PMC2974644</article-id>
              <article-id pub-id-type="pmcid">PMC2974644</article-id>
              <article-id pub-id-type="pmc-uid">2974644</article-id>
              <article-id pub-id-type="pmid">21079773</article-id>
              <article-id pub-id-type="pmid">21079773</article-id>
              <article-id pub-id-type="publisher-id">10-PONE-RA-21370R1</article-id>
              <article-id pub-id-type="doi">10.1371/journal.pone.0013860</article-id>
              <article-categories>
                <subj-group subj-group-type="heading">
                  <subject>Research Article</subject>
                </subj-group>
                <subj-group subj-group-type="Discipline">
                  <subject>Mental Health/Psychology</subject>
                  <subject>Neuroscience/Psychology</subject>
                  <subject>Neuroscience/Experimental Psychology</subject>
                </subj-group>
              </article-categories>
              <title-group>
                <article-title>Emotion Modulation of Visual Attention: Categorical and Temporal Characteristics</article-title>
                <alt-title alt-title-type="running-head">Emotion and Attention</alt-title>
              </title-group>
              <contrib-group>
                <contrib contrib-type="author">
                  <name>
                    <surname>Ciesielski</surname>
                    <given-names>Bethany G.</given-names>
                  </name>
                  <xref ref-type="aff" rid="aff1"/>
                </contrib>
                <contrib contrib-type="author">
                  <name>
                    <surname>Armstrong</surname>
                    <given-names>Thomas</given-names>
                  </name>
                  <xref ref-type="aff" rid="aff1"/>
                </contrib>
                <contrib contrib-type="author">
                  <name>
                    <surname>Zald</surname>
                    <given-names>David H.</given-names>
                  </name>
                  <xref ref-type="aff" rid="aff1"/>
                </contrib>
                <contrib contrib-type="author">
                  <name>
                    <surname>Olatunji</surname>
                    <given-names>Bunmi O.</given-names>
                  </name>
                  <xref ref-type="aff" rid="aff1"/>
                  <xref ref-type="corresp" rid="cor1">
                    <sup>*</sup>
                  </xref>
                </contrib>
              </contrib-group>
              <aff id="aff1">
                <addr-line>Department of Psychology, Vanderbilt University, Nashville, Tennessee, United States of America</addr-line>
              </aff>
              <contrib-group>
                <contrib contrib-type="editor">
                  <name>
                    <surname>García</surname>
                    <given-names>Antonio Verdejo</given-names>
                  </name>
                  <role>Editor</role>
                  <xref ref-type="aff" rid="edit1"/>
                </contrib>
              </contrib-group>
              <aff id="edit1">University of Granada, Spain</aff>
              <author-notes>
                <corresp id="cor1">* E-mail: <email>olabunmi.o.olatunji@vanderbilt.edu</email></corresp>
                <fn fn-type="con">
                  <p>Conceived and designed the experiments: BC DZ BO. Performed the experiments: BC. Analyzed the data: BC TA BO. Wrote the paper: BC BO. Analyzed and interpreted the data: TA. Provided critical revisions important for intellectual content: TA DZ.</p>
                </fn>
              </author-notes>
              <pub-date pub-type="collection">
                <year>2010</year>
              </pub-date>
              <pub-date pub-type="epub">
                <day>5</day>
                <month>11</month>
                <year>2010</year>
              </pub-date>
              <volume>5</volume>
              <issue>11</issue>
              <elocation-id>e13860</elocation-id>
              <history>
                <date date-type="received">
                  <day>20</day>
                  <month>7</month>
                  <year>2010</year>
                </date>
                <date date-type="accepted">
                  <day>15</day>
                  <month>10</month>
                  <year>2010</year>
                </date>
              </history>
              <permissions>
                <copyright-statement>Ciesielski et al.</copyright-statement>
                <copyright-year>2010</copyright-year>
                <license xlink:href="http://creativecommons.org/licenses/by/4.0/">
                  <license-p>This is an open-access article distributed under the terms of the Creative Commons Attribution License, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are properly credited.</license-p>
                </license>
              </permissions>
              <abstract>
                <sec>
                  <title>Background</title>
                  <p>Experimental research has shown that emotional stimuli can either enhance or impair attentional performance. However, the relative effects of specific emotional stimuli and the specific time course of these differential effects are unclear.</p>
                </sec>
                <sec>
                  <title>Methodology/Principal Findings</title>
                  <p>In the present study, participants (<italic>n</italic> = 50) searched for a single target within a rapid serial visual presentation of images. Irrelevant fear, disgust, erotic or neutral images preceded the target by two, four, six, or eight items. At lag 2, erotic images induced the greatest deficits in subsequent target processing compared to other images, consistent with a large emotional attentional blink. Fear and disgust images also produced a larger attentional blinks at lag 2 than neutral images. Erotic, fear, and disgust images continued to induce greater deficits than neutral images at lag 4 and 6. However, target processing deficits induced by erotic, fear, and disgust images at intermediate lags (lag 4 and 6) did not consistently differ from each other. In contrast to performance at lag 2, 4, and 6, enhancement in target processing for emotional stimuli was observed in comparison to neutral stimuli at lag 8.</p>
                </sec>
                <sec>
                  <title>Conclusions/Significance</title>
                  <p>These findings suggest that task-irrelevant emotion information, particularly erotica, impairs intentional allocation of attention at early temporal stages, but at later temporal stages, emotional stimuli can have an enhancing effect on directed attention. These data suggest that the effects of emotional stimuli on attention can be both positive and negative depending upon temporal factors.</p>
                </sec>
              </abstract>
              <counts>
                <page-count count="6"/>
              </counts>
            </article-meta>
          </front>
          <body>
            <sec id="s1">
              <title>Introduction</title>
              <p>Attention's inextricable link to emotion is implied, but not necessarily stated in cognitive theories of emotion <xref rid="pone.0013860-Oatley1" ref-type="bibr">[1]</xref>. However, a full understanding of this linkage requires increased specification of the properties of this linkage as attention is multidimensional, and can involve both intentional (top-down) directed or selective attention commensurate with goal pursuits, as well as bottom-up involuntary effects in which attention is captured by salient stimuli.</p>
              <p>Research on emotion influences on attention has focused mainly on the prioritization of emotional stimuli over neutral stimuli using paradigms that utilize a single or restricted temporal window to examine effects. However, recent studies examining emotional influences over a longer timescale suggest that the adaptive effects of emotion on attention are more dynamic and complex than typically appreciated. For example, Bocanegra and Zeelenberg <xref rid="pone.0013860-Bocanegra1" ref-type="bibr">[2]</xref> manipulated the temporal distance between emotional cues (negative words) and a subsequent neutral target by varying cue-target inter-stimulus intervals (ISIs). Emotional cues impaired target identification at short ISIs (50 and 500 ms) but improved target identification at longer ISIs (1000 ms). A subsequent study using emotional auditory stimuli further supports that emotional stimuli may have a twofold effect on attention (e.g.; <xref rid="pone.0013860-Zeelenberg1" ref-type="bibr">[3]</xref>). Whereas emotional stimuli may initially capture and hold attention, thereby impairing processing of contiguous or temporally proximal neutral stimuli, processing of neutral stimuli may be enhanced once attention is released from emotional stimuli. For example, in studies presenting a fearful face followed by a pause sufficient to allow disengagement of attention, improvements in perception (i.e. contrast sensitivity; <xref rid="pone.0013860-Phelps1" ref-type="bibr">[4]</xref>) and search efficiency <xref rid="pone.0013860-Becker1" ref-type="bibr">[5]</xref> have been observed. These findings follow an evolutionary logic, as ‘emotion induced blindness’ may be adaptive in forcing us to register important stimuli, but would quickly become a liability if attention could not be reallocated towards other relevant information necessary for the execution of an appropriate response.</p>
              <p>Although time course appears to dictate the extent to which emotion improves or impairs directed attention, it is not clear if the twofold effect is observed for emotion in general or if this effect varies as a function of the specific type of emotional information. For example, the twofold effects of emotion on attention have been almost exclusively reported in paradigms using either arousing, and mostly negatively valenced words or fearful facial expressions. Although other types of emotional stimuli (such as emotionally valenced pictures) have clearly been observed to impact performance on tasks requiring directed attention, it is not clear if these stimuli also produce this two-stage capture followed by enhancement effect. Also, it is unclear how different aversive contents may vary in their effects on attention across time. It has been suggested that different negative emotional contents have distinct effects on attention (e.g., <xref rid="pone.0013860-Whalen1" ref-type="bibr">[6]</xref>), and the few extant studies testing this hypothesis suggest that differential patterns exist. For example, a recent study found that while difficulty disengaging attention is observed during exposure to fear and disgust stimuli, this effect is greater for disgust stimuli compared to fear stimuli <xref rid="pone.0013860-Cisler1" ref-type="bibr">[7]</xref>. Vermeulen, Godefroid, and Mermillod <xref rid="pone.0013860-Vermeulen1" ref-type="bibr">[8]</xref> also found that processing fear exerts greater inhibitory responses on distractors relative to processing disgust. This may have functional significance in that observation of fear in a peer warrants quick identification, but also a rapid shift away from them to identify the source of the fear. In contrast, disgust information, relaying the likelihood of contamination, may not necessitate rapid shifting of attention, but may rather facilitate further processing or extension of the hold component of deployed attentional resources in order to fully characterize the risk. Such a model appears consistent with recent research showing that the perception of fear is gated by selective attention at early latencies during exposure, whereas the perception of disgust appears to be modulated by attention allocation at later latencies <xref rid="pone.0013860-Santos1" ref-type="bibr">[9]</xref>.</p>
              <p>Similarly, it is necessary to determine if these effects are valence or arousal specific. In a number of paradigms, erotic stimuli produce as strong or stronger effects than aversive stimuli on task performance <xref rid="pone.0013860-Anderson1" ref-type="bibr">[10]</xref>, <xref rid="pone.0013860-Most1" ref-type="bibr">[11]</xref>. Such findings strongly argue that many emotional effects are driven by arousal rather than valence. However, to date evidence regarding whether such positive arousing stimuli have a two-stage impact on attention is lacking.</p>
              <p>The present study employs a rapid serial visual presentation (RSVP) paradigm to assess emotion's modulation of attention at different time intervals. Specifically, the study examines differences in attentional capture and subsequent attentional enhancement between fear, disgust, erotic and neutral images. In order to examine the two-fold effect which suggests that emotion impairs attention at shorter time intervals but improves it at longer time intervals <xref rid="pone.0013860-Bocanegra1" ref-type="bibr">[2]</xref>, the intervals between distracter and target images were varied at 200 ms, 400 ms, 600 ms, and 800 ms lags. Examination of performance at these 4 lags represents a novel extension of prior research examining the time course of the emotional attentional blink, which has traditionally been examined at just 2 lags. It was predicted that emotional images would produce large deficits in target identification compared to neutral images. It was also predicted that deficits in target identification would be more pronounced when the irrelevant emotional images preceded the target at shorter lags relative to longer lags. With regards to time course, it was predicted that attentional enhancement effects of emotion would be observed at longer lags relative to the shorter lags of the RSVP.</p>
            </sec>
            <sec sec-type="methods" id="s2">
              <title>Methods</title>
              <sec id="s2a">
                <title>Participants</title>
                <p>The study was conducted with approval of the ethic committee, the Institutional Review Board of Vanderbilt University, Nashville, TN, USA. Fifty undergraduate students, each of whom gave informed written consent prior to beginning the study (76% female; 72% Caucasian) with a mean age of 19.54 (SD = 1.13) participated in exchange for research credit.</p>
              </sec>
              <sec id="s2b">
                <title>Stimuli</title>
                <p>The visual stimuli were images standardized to 320 by 240 pixels consisting of four categories of emotional distracters: 42 disgusting images, 42 erotic images, 42 fear evoking images, 42 neutral images, 252 upright landscapes/architectural that were used as ‘background stimuli, and 80 target images consisting of landscape/architectural photos, 40 rotated 90° degrees to the left and 40 rotated 90° to the right. Fear, disgust, and neutral pictures were partially drawn from the International Affective Picture System (IAPS; <xref rid="pone.0013860-Lang1" ref-type="bibr">[12]</xref>) and were supplemented with similar pictures found from publicly available sources. Our partitioning of the images to the specific emotional categories was guided by prior research that has addressed this issue with the IAPS. In cases where images were supplemented with pictures found from publicly available sources, effort was made to employ only images that were representative of the specific emotional category as revealed by prior research. Fear pictures included animals bearing teeth in a threatening manner, humans brandishing weapons, and explosions. Disgust pictures were of contaminated or diseased items including roaches, feces, diseased flesh, and maggot ridden food products. Neutral pictures were scenic in style and included both animals and humans. The erotic images were of nude male-female couples engaging in sexual scenarios and were drawn from the image set used by Most and colleagues <xref rid="pone.0013860-Most1" ref-type="bibr">[11]</xref>.</p>
              </sec>
              <sec id="s2c">
                <title>Procedure</title>
                <p>On each trial of the RSVP task, 17 images on a white background were presented for 100 ms each and one of the images was a distracter and one of the images, the target, was rotated 90° to the left or right (see <xref ref-type="fig" rid="pone-0013860-g001">Figure 1</xref>) using E-prime Software (version 2.0, Psychology Software Tools, Inc.). The refresh rate for the monitors on which the experiment was conducted was 60 Hz, refreshed every 16.6 ms. Stimulus presentation was randomized and each trial consisted of a disgust, fear, erotic, or neutral distracter image that appeared equally in the stream at positions 4, 6, or 8; further, the distracters appeared at varying time intervals, 200 ms (lag 2), 400 ms (lag 4), 600 ms (lag 6), or 800 ms (lag 8) before the rotated image. Participants completed 6 blocks with 28 trials per block. Of the total 168 trials, each distracter type was presented 42 times with 2 trials per distracter type containing no target. The 2 no-target trials per emotion category was included as a check to ensure participants were not simply guessing at chance levels ‘yes’ and ‘no’ for seeing a target, thus if they are answering that they did see targets when none were present one might assume they are just guessing, but accuracy for disgust, erotic, fear and neutral are well above chance for target absent trials (80.28, 96.11, 91.11, &amp; 91.39 percent correct respectively). The 4 lags were equally distributed for 40 trials with targets present per distracter type. Participants were instructed to indicate if they saw a rotated (yes, no; <italic>detection</italic>) image and then asked if they saw a rotated image to report which direction it was turned (right, left; <italic>accuracy</italic>).</p>
                <fig id="pone-0013860-g001" position="float">
                  <object-id pub-id-type="doi">10.1371/journal.pone.0013860.g001</object-id>
                  <label>Figure 1</label>
                  <caption>
                    <title>The trial procedure for the emotional attentional-blink paradigm.</title>
                    <p>Note that the distractors were varied in four categories, disgust, erotic, fear, and neutral at 200, 400, 600, and 800 ms time lags.</p>
                  </caption>
                  <graphic xlink:href="pone.0013860.g001"/>
                </fig>
              </sec>
            </sec>
            <sec id="s3">
              <title>Results</title>
              <sec id="s3a">
                <title>Emotion Content and Target Accuracy</title>
                <p>A 4 (Emotion; disgust, fear, erotic, neutral) X 4 (Lag; 2, 4, 6, 8) ANOVA on accuracy revealed a significant main effect of Emotion category [<italic>F</italic> (3, 147) = 78.16, <italic>p</italic>&lt;.001, partial η<sup>2</sup> = .615]. We were initially interested in examining differences in frequency of false alarms as a function of emotional distractor. However, no such differences were observed and the overall findings for detection and accuracy were essentially identical. Analyses for accuracy, rather than detection, are presented as they reflect more precise performance on the RSVP. Bonferroni corrected pairwise comparisons revealed that the main effect of emotion was significant for erotic, disgust, and fear relative to neutral distracters (<italic>p</italic>s&lt;.001). Furthermore, participants were significantly less accurate for trials of erotic distracters in comparison to both fear and disgust distracter trials (<italic>p</italic>s<italic>&lt;</italic>.001), while fear and disgust did not differ from each other (<italic>p</italic> = 1.00).</p>
              </sec>
              <sec id="s3b">
                <title>Time Course and Target Detection</title>
                <p>The 4 (Emotion; disgust, fear, erotic, neutral) X 4 (Lag; 2, 4, 6, 8) ANOVA on percent accuracy also revealed a significant main effect of Lag [<italic>F</italic> (3, 147) = 276.80, <italic>p</italic>&lt;.001, partial η<sup>2</sup> = .850]. Bonferroni corrected pairwise comparisons showed participants to be significantly less accurate in identifying the direction of a target at Lag 2 in comparison to all other lags (<italic>p</italic>s&lt;.001). Further comparisons revealed that Lag 4 was significantly worse in comparison to percent accurate for lags 6 and 8 (<italic>p</italic>s&lt;.001); while Lag 6 did not differ significantly from Lag 8 for percent accuracy (<italic>p</italic> = 1.00).</p>
              </sec>
              <sec id="s3c">
                <title>Emotion Content, Time Course, and Target Detection</title>
                <p>The 4 (Emotion; disgust, fear, erotic, neutral) X 4 (Lag; 2, 4, 6, 8) ANOVA on accuracy also revealed a significant Emotion X Lag interaction [<italic>F</italic> (9, 441) = 55.97, <italic>p</italic>&lt;.001, partial η<sup>2</sup> = .533]. To examine this interaction, an ANOVA on emotion at each lag was conducted. At each lag a main effect of emotion was observed (Lag 2: [<italic>F</italic> (3,147) = 175.285, <italic>p</italic>&lt;.001, partial η2 = .782], Lag 4: [<italic>F</italic> (3,147) = 11.085, <italic>p</italic>&lt;.001, partial η2 = .184], Lag 6: [<italic>F</italic> (3,147) = 12.392, <italic>p</italic>&lt;.001, partial η2 = .202], Lag 8: [<italic>F</italic> (3,147) = 9.780, <italic>p</italic>&lt;.001, partial η2 = .166]). <xref ref-type="fig" rid="pone-0013860-g002">Figure 2</xref> shows that relative to neutral distracter trials of the same lag, participants were significantly <italic>less</italic> accurate in identifying the direction of the target at 200 ms, 400 ms, and 600 ms lags for all emotional distracters (<italic>p</italic>s&lt;.001, <italic>p</italic>s&lt;.01, <italic>p</italic>s&lt;.001, respectively). Whereas, at lags of 800 ms relative to neutral trials, participants were significantly <italic>more</italic> accurate in identifying the target's direction for erotic, fear, and disgust distracter trials (<italic>p</italic>&lt;.05; <italic>p</italic>&lt;.001; <italic>p</italic>&lt;.001 respectively). See <xref ref-type="table" rid="pone-0013860-t001">Table 1</xref> for the comparisons between each emotional distractor at each lag.</p>
                <fig id="pone-0013860-g002" position="float">
                  <object-id pub-id-type="doi">10.1371/journal.pone.0013860.g002</object-id>
                  <label>Figure 2</label>
                  <caption>
                    <title>Accuracy scores for each emotion by time lag.</title>
                    <p>Error bars represent standard error of the mean.</p>
                  </caption>
                  <graphic xlink:href="pone.0013860.g002"/>
                </fig>
                <table-wrap id="pone-0013860-t001" position="float">
                  <object-id pub-id-type="doi">10.1371/journal.pone.0013860.t001</object-id>
                  <label>Table 1</label>
                  <caption>
                    <title>Raw means and standard deviations of percent correct (accuracy) for the target for each emotional distracter category by lag interval.</title>
                  </caption>
                  <alternatives>
                    <graphic id="pone-0013860-t001-1" xlink:href="pone.0013860.t001"/>
                    <table frame="hsides" rules="groups">
                      <colgroup span="1">
                        <col align="left" span="1"/>
                        <col align="center" span="1"/>
                        <col align="center" span="1"/>
                        <col align="center" span="1"/>
                        <col align="center" span="1"/>
                      </colgroup>
                      <thead>
                        <tr>
                          <td align="left" rowspan="1" colspan="1"/>
                          <td colspan="4" align="left" rowspan="1">Percent Accuracy</td>
                        </tr>
                        <tr>
                          <td align="left" rowspan="1" colspan="1"/>
                          <td align="left" rowspan="1" colspan="1">Disgust</td>
                          <td align="left" rowspan="1" colspan="1">Erotic</td>
                          <td align="left" rowspan="1" colspan="1">Fear</td>
                          <td align="left" rowspan="1" colspan="1">Neutral</td>
                        </tr>
                        <tr>
                          <td align="left" rowspan="1" colspan="1"/>
                          <td align="left" rowspan="1" colspan="1">M (SD)</td>
                          <td align="left" rowspan="1" colspan="1">M (SD)</td>
                          <td align="left" rowspan="1" colspan="1">M (SD)</td>
                          <td align="left" rowspan="1" colspan="1">M (SD)</td>
                        </tr>
                      </thead>
                      <tbody>
                        <tr>
                          <td align="left" rowspan="1" colspan="1">Lag 2</td>
                          <td align="left" rowspan="1" colspan="1">47.33 (19.8)<sup>c</sup>
</td>
                          <td align="left" rowspan="1" colspan="1">20.22 (19.14)<sup>d</sup>
</td>
                          <td align="left" rowspan="1" colspan="1">52.44 (17.25)<sup>b</sup>
</td>
                          <td align="left" rowspan="1" colspan="1">84.67 (15.37)<sup>a</sup>
</td>
                        </tr>
                        <tr>
                          <td align="left" rowspan="1" colspan="1">Lag 4</td>
                          <td align="left" rowspan="1" colspan="1">69.33 (18.04)<sup>b</sup>
</td>
                          <td align="left" rowspan="1" colspan="1">61.33 (18.41)<sup>c</sup>
</td>
                          <td align="left" rowspan="1" colspan="1">65.56 (15.60)<sup>bc</sup>
</td>
                          <td align="left" rowspan="1" colspan="1">77.78 (14.89)<sup>a</sup>
</td>
                        </tr>
                        <tr>
                          <td align="left" rowspan="1" colspan="1">Lag 6</td>
                          <td align="left" rowspan="1" colspan="1">84.67 (13.82)<sup>b</sup>
</td>
                          <td align="left" rowspan="1" colspan="1">82.45 (14.39)<sup>b</sup>
</td>
                          <td align="left" rowspan="1" colspan="1">83.34 (10.82)<sup>b</sup>
</td>
                          <td align="left" rowspan="1" colspan="1">94.00 (7.52)<sup>a</sup>
</td>
                        </tr>
                        <tr>
                          <td align="left" rowspan="1" colspan="1">Lag 8</td>
                          <td align="left" rowspan="1" colspan="1">88.22 (9.37)<sup>a</sup>
</td>
                          <td align="left" rowspan="1" colspan="1">84.22 (13.48)<sup>b</sup>
</td>
                          <td align="left" rowspan="1" colspan="1">88.45 (10.52)<sup>ab</sup>
</td>
                          <td align="left" rowspan="1" colspan="1">79.34 (8.98)<sup>c</sup>
</td>
                        </tr>
                      </tbody>
                    </table>
                  </alternatives>
                  <table-wrap-foot>
                    <fn id="nt101">
                      <label/>
                      <p><italic>Note</italic>: Means for each emotion category in the same row (same lag) with different superscripts are significantly different (all <italic>p'</italic>s&lt;.05; a&gt;b&gt;c&gt;d).</p>
                    </fn>
                  </table-wrap-foot>
                </table-wrap>
              </sec>
              <sec id="s3d">
                <title>Time Course of Emotional Blink Magnitude</title>
                <p>To further quantify the attentional enhancement effects of emotional, relative to neutral images, that was observed at 800 ms, an emotional blink magnitude (EBM) score was determined by subtracting the percent correct for the emotion trials at a particular lag from the neutral trials at the same lag. A positive score is reflective of a detrimental effect of emotional information on attention and a negative score is reflective of a beneficial effect on attention. A 3 (Emotion: disgust, fear, erotic) X 4 (Lag: 2, 4, 6, 8) ANOVA on the EMS revealed a significant main effect of Emotion category [<italic>F</italic> (2, 98) = 32.80, <italic>p</italic>&lt;.001, partial η<sup>2</sup> = .401], Lag [<italic>F</italic> (3, 147) = 129.20, <italic>p</italic>&lt;.001, partial η<sup>2</sup> = .725], and a Emotion X Lag interaction [<italic>F</italic> (6, 294) = 20.89, <italic>p</italic>&lt;.001, partial η<sup>2</sup> = .299]. For each emotion the comparison of the EBM between lags (i.e. Disgust Lag 2 vs. Disgust Lag 4) yielded significant differences (<italic>p</italic>s&lt;.001) for all lag comparisons with the exception of lag 4 to lag 6 comparisons, which did not differ for disgust, erotic, or fear (<italic>p</italic> = .81; <italic>p</italic> = .23; <italic>p</italic> = .69; respectively). <xref ref-type="table" rid="pone-0013860-t002">Table 2</xref> presents the <italic>t</italic>-values for the EMB comparisons at each lag. <xref ref-type="fig" rid="pone-0013860-g003">Figure 3</xref> shows that while the EBM at Lags, 2, 4, and 6 reflects <italic>impairment</italic> in the subsequent identification of a neutral target, the EBM at lag 8 reflects significant <italic>enhancement</italic> of the subsequent identification of a neutral target.</p>
                <fig id="pone-0013860-g003" position="float">
                  <object-id pub-id-type="doi">10.1371/journal.pone.0013860.g003</object-id>
                  <label>Figure 3</label>
                  <caption>
                    <title>Attentional blink magnitude scores for each emotion category by lag, error bars represent standard error of the mean.</title>
                  </caption>
                  <graphic xlink:href="pone.0013860.g003"/>
                </fig>
                <table-wrap id="pone-0013860-t002" position="float">
                  <object-id pub-id-type="doi">10.1371/journal.pone.0013860.t002</object-id>
                  <label>Table 2</label>
                  <caption>
                    <title>Comparison between emotional blink magnitude scores at each lag.</title>
                  </caption>
                  <alternatives>
                    <graphic id="pone-0013860-t002-2" xlink:href="pone.0013860.t002"/>
                    <table frame="hsides" rules="groups">
                      <colgroup span="1">
                        <col align="left" span="1"/>
                        <col align="center" span="1"/>
                        <col align="center" span="1"/>
                        <col align="center" span="1"/>
                        <col align="center" span="1"/>
                        <col align="center" span="1"/>
                        <col align="center" span="1"/>
                        <col align="center" span="1"/>
                        <col align="center" span="1"/>
                        <col align="center" span="1"/>
                      </colgroup>
                      <thead>
                        <tr>
                          <td align="left" rowspan="1" colspan="1"/>
                          <td colspan="3" align="left" rowspan="1">Disgust<italic>t</italic>-value</td>
                          <td colspan="3" align="left" rowspan="1">Erotic<italic>t</italic>-value</td>
                          <td colspan="3" align="left" rowspan="1">Fear<italic>t</italic>-value</td>
                        </tr>
                        <tr>
                          <td align="left" rowspan="1" colspan="1"/>
                          <td align="left" rowspan="1" colspan="1">Lag 2</td>
                          <td align="left" rowspan="1" colspan="1">Lag 4</td>
                          <td align="left" rowspan="1" colspan="1">Lag 6</td>
                          <td align="left" rowspan="1" colspan="1">Lag 2</td>
                          <td align="left" rowspan="1" colspan="1">Lag 4</td>
                          <td align="left" rowspan="1" colspan="1">Lag 6</td>
                          <td align="left" rowspan="1" colspan="1">Lag 2</td>
                          <td align="left" rowspan="1" colspan="1">Lag 4</td>
                          <td align="left" rowspan="1" colspan="1">Lag 6</td>
                        </tr>
                      </thead>
                      <tbody>
                        <tr>
                          <td align="left" rowspan="1" colspan="1">Lag 4</td>
                          <td align="left" rowspan="1" colspan="1">9.87<xref ref-type="table-fn" rid="nt103">*</xref>
</td>
                          <td align="left" rowspan="1" colspan="1">-</td>
                          <td align="left" rowspan="1" colspan="1">-</td>
                          <td align="left" rowspan="1" colspan="1">12.85<xref ref-type="table-fn" rid="nt103">*</xref>
</td>
                          <td align="left" rowspan="1" colspan="1">-</td>
                          <td align="left" rowspan="1" colspan="1">-</td>
                          <td align="left" rowspan="1" colspan="1">5.83<xref ref-type="table-fn" rid="nt103">*</xref>
</td>
                          <td align="left" rowspan="1" colspan="1">-</td>
                          <td align="left" rowspan="1" colspan="1">-</td>
                        </tr>
                        <tr>
                          <td align="left" rowspan="1" colspan="1">Lag 6</td>
                          <td align="left" rowspan="1" colspan="1">8.11<xref ref-type="table-fn" rid="nt103">*</xref>
</td>
                          <td align="left" rowspan="1" colspan="1">−0.24</td>
                          <td align="left" rowspan="1" colspan="1">-</td>
                          <td align="left" rowspan="1" colspan="1">14.05<xref ref-type="table-fn" rid="nt103">*</xref>
</td>
                          <td align="left" rowspan="1" colspan="1">1.21</td>
                          <td align="left" rowspan="1" colspan="1">-</td>
                          <td align="left" rowspan="1" colspan="1">6.65<xref ref-type="table-fn" rid="nt103">*</xref>
</td>
                          <td align="left" rowspan="1" colspan="1">0.40</td>
                          <td align="left" rowspan="1" colspan="1">-</td>
                        </tr>
                        <tr>
                          <td align="left" rowspan="1" colspan="1">Lag 8</td>
                          <td align="left" rowspan="1" colspan="1">16.45<xref ref-type="table-fn" rid="nt103">*</xref>
</td>
                          <td align="left" rowspan="1" colspan="1">5.88<xref ref-type="table-fn" rid="nt103">*</xref>
</td>
                          <td align="left" rowspan="1" colspan="1">6.27<xref ref-type="table-fn" rid="nt103">*</xref>
</td>
                          <td align="left" rowspan="1" colspan="1">18.69<xref ref-type="table-fn" rid="nt103">*</xref>
</td>
                          <td align="left" rowspan="1" colspan="1">7.38<xref ref-type="table-fn" rid="nt103">*</xref>
</td>
                          <td align="left" rowspan="1" colspan="1">6.14<xref ref-type="table-fn" rid="nt103">*</xref>
</td>
                          <td align="left" rowspan="1" colspan="1">11.88<xref ref-type="table-fn" rid="nt103">*</xref>
</td>
                          <td align="left" rowspan="1" colspan="1">6.76<xref ref-type="table-fn" rid="nt103">*</xref>
</td>
                          <td align="left" rowspan="1" colspan="1">7.04<xref ref-type="table-fn" rid="nt103">*</xref>
</td>
                        </tr>
                      </tbody>
                    </table>
                  </alternatives>
                  <table-wrap-foot>
                    <fn id="nt102">
                      <label/>
                      <p>Note:</p>
                    </fn>
                    <fn id="nt103">
                      <label/>
                      <p>*<italic>p</italic>&lt;.001.</p>
                    </fn>
                  </table-wrap-foot>
                </table-wrap>
                <p>A close inspection of <xref ref-type="table" rid="pone-0013860-t001">Table 1</xref> and <xref ref-type="fig" rid="pone-0013860-g002">Figure 2</xref> suggests that at the principal contrast for emotional enhancement (Lag 8), accuracy for the neutral condition appears to worsen, which suggests that the emotional enhancement effect may be an artifact of the decline in accuracy when presented with neutral distractors rather than an increase in emotional accuracy per se. Indeed, an ANOVA of accuracy for neutral distractors only revealed a main effect of Lag [<italic>F</italic> (3, 147) = 26.26, <italic>p</italic>&lt;.001, partial η<sup>2</sup> = .349]. Pairwise comparisons revealed that percent accuracy at each of the 4 lags significantly differed from each other (<italic>p</italic>s&lt;.02), with the exception of Lag 2 and Lag 4 which did not differ from each other (<italic>p</italic> = .41). Thus, a mean neutral accuracy score across the four different Lags was computed (<italic>M</italic> = 83.94, <italic>SD</italic> = 8.49) and employed as the baseline for the contrasts with the emotional categories across the four Lags. Percent accuracy for disgust [<italic>t</italic> (49) = 14.72], erotic [<italic>t</italic> (49) = 23.73], and fear [<italic>t</italic> (49) = 12.87] at Lag 2 was significantly lower than the composite neutral accuracy score (<italic>p</italic>s&lt;.001). Percent accuracy for disgust [<italic>t</italic> (49) = 6.12], erotic [<italic>t</italic> (49) = 8.34], and fear [<italic>t</italic> (49) = 8.55] at Lag 4 was also significantly lower than the composite neutral accuracy score (<italic>p</italic>s&lt;.001). Although percent accuracy for the three emotional categories at Lag 6 did not significantly differ from the composite neutral accuracy score (<italic>p</italic>s&gt;.37), percent accuracy for disgust [<italic>t</italic> (49) = −3.06] and fear [<italic>t</italic> (49) = −2.67] at Lag 8 was significantly higher than the composite neutral accuracy score (<italic>p</italic>s&lt;.02). Percent accuracy for erotic [<italic>t</italic> (49) = 8.34] at Lag 8 did not significantly differ from the composite neutral accuracy score (<italic>p</italic> = .88).</p>
              </sec>
              <sec id="s3e">
                <title>Are the Attentional Effects of Erotic Images Due to Valence and Arousal?</title>
                <p>The image sets were chosen on the basis of forming coherent non-overlapping categories. In order to rule out the possibility that the unique attentional effects of erotic images were not due to differences in valence and arousal, image ratings on valence and arousal were obtained from an independent sample (<italic>n</italic> = 23; 65.2% female; 65.2% Caucasian) with a mean age of 20.35 (SD = 2.57). Participants rated each Disgust (valence  = −24.69, SD = 7.29; arousal  = 46.26, SD = 14.65), Erotic (valence  = 4.45, SD = 15.59; arousal  = 41.77, SD = 20.42), Fear (valence  = −15.83, SD = 7.17; arousal  = 31.98, SD  = 10.36), and Neutral (valence  = 4.87, SD = 3.66; arousal  = 6.18, SD = 5.05) image for valence (−50 =  extremely negative, +50 =  extremely positive, 0 = being no positive or negative valence/neutral) and arousal (0 =  none to 100 =  extremely/most imaginable). A significant difference for valence ratings between disgust images and all other categories was found such that disgust images were rated the most negative (<italic>p</italic>'s&lt;.001). Fear images were rated as significantly more negative than erotic and neutral images (<italic>p</italic>'s&lt;.001). However, the valence of erotic and neutral images did not significantly differ from each other (<italic>p</italic>&gt;.90) such that both were rated on average mildly positive (above a zero score of neither positive nor negative) (although it may be noted that erotic images show a greater variance of valence ratings than any of the other stimuli with ratings). Neutral images were rated significantly less arousing than all other images (<italic>p'</italic>s&lt;.001). Fear images were significantly less arousing than disgust images (<italic>p</italic>&lt;.001), but not erotic images (<italic>p</italic>&gt;.05). Lastly, arousal ratings for disgust and erotic images did not significantly differ from each other (<italic>p</italic>&gt;.05). Given that erotic images were not uniquely characterized by differences in arousal and valance in an independent sample, it is unlikely that the attentional effects of erotic images are entirely due to unique valence and/or arousal characteristics.</p>
              </sec>
            </sec>
            <sec id="s4">
              <title>Discussion</title>
              <p>The present findings revealed that at early time points emotional distracters produced deficits in target identification compared to neutral trials, reflecting involuntary capture of attention by emotional stimuli. This is in agreement with prior research demonstrating that emotionally laden stimuli redirect attentional resources towards emotionally salient <xref rid="pone.0013860-Most1" ref-type="bibr">[11]</xref>, <xref rid="pone.0013860-Most2" ref-type="bibr">[13]</xref> and survival-relevant information <xref rid="pone.0013860-Ohman1" ref-type="bibr">[14]</xref>. Although this emotion induced ‘attentional blink’ was present for as long as 600 ms following all emotion picture types, there was a clear graded decline in the extent of the attentional blink from shorter to longer lags. This suggests processing resources captured by the emotional stimuli become increasingly available for the identification of targets over time, consistent with limited-capacity accounts of the attentional blink informed by interference models (see <xref rid="pone.0013860-Dux1" ref-type="bibr">[15]</xref> for review). Such models suggest that the capture of attention on an item interferes with the processing of subsequent items. Accordingly, attention dwells on the initial object to inform the guidance of behavior <xref rid="pone.0013860-Duncan1" ref-type="bibr">[16]</xref>, but tapers off once the potentially meaningful stimulus has been processed.</p>
              <p>The current study highlights the importance of evaluating time course in delineating the modulation of attention by emotion. This could be seen in the attenuation of attentional blink effects at intermediate lags (400 ms and 600 ms) even for stimuli that showed initially robust emotion induced blindness. Remarkably, by 800 ms, there was no evidence of attentional blinks, but rather there was evidence of enhanced target detection following emotional stimuli relative to neutral stimuli. When accounting for variation in accuracy after neutral distractors across the four Lags, the enhancement effect was robust for fear and disgust but not erotic images. These data converge with the research of Bocanegra and Zeelenberg <xref rid="pone.0013860-Bocanegra1" ref-type="bibr">[2]</xref>, who observed emotion-induced blindness in response to emotional words at short and intermediate ISIs, but emotion-induced “hypervision” (enhanced performance) with a longer (1000 ms) ISIs. Such enhancements at 800 or 1000 ms are in line with other recent research suggesting that negative emotion information aids successive processing of non-emotional objects under certain conditions <xref rid="pone.0013860-Becker1" ref-type="bibr">[5]</xref>.</p>
              <p>The distinct mechanisms underlying the relative beneficial and detrimental effects of emotional information on attention over time remain unclear. It is not known whether the enhancements reflect a compensatory mechanism (engaged in response to attentional capture) or an independent process that relies on a distinct circuitry that acts on a slower time scale than attentional capture. The compensatory mechanism hypothesis would be bolstered if the extent of hypervision were proportional to the extent of the attentional capture for a stimulus group. However, the three emotional categories showed equivalent hypervision effects despite the far greater initial attentional capture by erotica (discussed below). The alternative hypothesis, that there are two distinct processes acting at different timescales, may be a better fit to the data. To the extent that distinct mechanisms account for the beneficial and detrimental carryover effects of emotion on attention, such mechanisms may take effect under a specified time course in which attentional capture predominates or even masks beneficial effects at early and intermediate time points. As such, performance at intermediate time points may not simply reflect the length of emotion induced blindness, but reflect a weighted combination of the two competing processes (especially when aggregated over individuals who likely possess differences in the strength and time course of each process).</p>
              <p>It should be noted that the present data provides a fuller picture of the time course of emotion induced blindness than the time course of emotion induced hypervision. 800 ms is probably close to the minimum time lag for observing emotion induced hypervision, and these effects may intensify at greater delays. Indeed, the effect observed at 800 ms is relative modest relative to the robust attentional capture effects observed at briefer delays. Future research should examine the full time course of these hypervision effects.</p>
              <p>The three emotional stimulus categories produced different levels of impact on attention depending upon the time-point examined. The greatest target identification deficits observed in the present study were for erotic stimuli at short lags. Considering their lack of harm-relevance, the strength and magnitude of the attentional blink produced by these high arousal, but overall pleasant images, is striking. This finding complements the earlier findings of Arnell and colleagues <xref rid="pone.0013860-Arnell1" ref-type="bibr">[17]</xref> and Most et al. <xref rid="pone.0013860-Most1" ref-type="bibr">[11]</xref>, which showed that sexual content greatly diminishes RSVP task accuracy through involuntarily capture of attention. Ratings of the words used by Arnell et al. <xref rid="pone.0013860-Arnell1" ref-type="bibr">[17]</xref> suggest that arousal level, not valence, may partially account for the attentional capture by erotic stimuli. Indeed, arousal-level has also been implicated in the capture of attention by taboo words in an Emotional Stroop task <xref rid="pone.0013860-MacKay1" ref-type="bibr">[18]</xref>. However, the erotic images used in the present study were not found to be significantly more arousing than the disgust images by an independent sample. Although it is difficult to draw definitive conclusions regarding the effects of arousal and valence on the findings observed in the present study, it may be the case that arousal cannot fully explain the extent of the large attentional blink observed for erotic stimuli. Erotic images may uniquely capture attention in large part due to their “shock value.” Independent of the taboo content, erotic images may be viewed as consisting of distinct lower level physical characteristics compared to other image categories. However, a study employing similar erotic images found that scrambling erotic images, such that the basic physical properties are preserved but all meaning is lost, causes the attentional blink from erotic images to disappear (see <xref rid="pone.0013860-Most3" ref-type="bibr">[19]</xref>). Thus emotion content rather than low level physical characteristics of the erotic images are likely to account for erotic stimuli's advantage in capturing attentional resources at Lag 2.</p>
              <p>Examination of the intermediate time intervals revealed that erotic stimuli's relative advantage in capturing and maintaining attentional resources diminished rapidly. Task performance observed at the 400 ms interval showed that erotic stimuli no longer caused greater target processing deficits compared to fear stimuli, and at 600 ms, erotic stimuli showed no difference from either fear or disgust stimuli. This suggests that erotic stimuli may not differ from fear or disgust with regard to the amount of time necessary for processing, or the so called hold component, which others have proposed leads to the relatively larger deficits observed following highly arousing stimuli <xref rid="pone.0013860-Arnell1" ref-type="bibr">[17]</xref>. Rather, the robust effect of erotic stimuli at lag 2 may be due to modulation of the capture component, which would show pronounced initial disruption but not necessarily prolonged ‘blindness’ for subsequent stimuli. Accordingly, erotic stimuli may initially consume the vast majority of available resources while other emotional information may initially consume only part of the available resources allowing greater processing of subsequently occurring items. Assuming that the rate of release of resources or the replenishment of resources is relatively rapid, these initial differences in capture may be largely resolved by intermediate time lags.</p>
              <p>The present findings extend previous work by comparing the effects of different negative emotional content on attention. Prior research suggests that fear and disgust serve different functions <xref rid="pone.0013860-Susskind1" ref-type="bibr">[20]</xref>, and such differences may also be revealed at the level of attention <xref rid="pone.0013860-Cisler1" ref-type="bibr">[7]</xref>. Comparisons between fear and disgust in the present investigation revealed slight differences at lag 2. However, these data do not appear to support a consistent difference in attentional capture by fear and disgust content over time. Alternatively, the overlapping harm appraisals associated with fear and disgust <xref rid="pone.0013860-Woody1" ref-type="bibr">[21]</xref> may make it difficult to reliably detect attentional differences over time between these emotions. It is also possible that fear and disgust only differ in the capture of <italic>spatial</italic> attention. A cognitive task that places greater emphasis on orienting attention to identify objects might reveal distinctions between fear and disgust that were not observed in the present study.</p>
            </sec>
          </body>
          <back>
            <fn-group>
              <fn fn-type="COI-statement">
                <p><bold>Competing Interests: </bold>The authors have declared that no competing interests exist.</p>
              </fn>
              <fn fn-type="financial-disclosure">
                <p><bold>Funding: </bold>This research was supported, in part, by an RO3MH082210-01A1 grant from the National Institute of Mental Health awarded to BOO. The funding agency had no role in study design, data collection and analysis, decision to publish, or preparation of the manuscript.</p>
              </fn>
            </fn-group>
            <ref-list>
              <title>References</title>
              <ref id="pone.0013860-Oatley1">
                <label>1</label>
                <element-citation publication-type="journal">
                  <person-group person-group-type="author">
                    <name>
                      <surname>Oatley</surname>
                      <given-names>K</given-names>
                    </name>
                    <name>
                      <surname>Johnson-Laird</surname>
                      <given-names>PN</given-names>
                    </name>
                  </person-group>
                  <year>1987</year>
                  <article-title>Towards a Cognitive Theory of Emotions.</article-title>
                  <source>Cognition &amp; Emotion</source>
                  <volume>1</volume>
                  <fpage>29</fpage>
                  <lpage>50</lpage>
                </element-citation>
              </ref>
              <ref id="pone.0013860-Bocanegra1">
                <label>2</label>
                <element-citation publication-type="journal">
                  <person-group person-group-type="author">
                    <name>
                      <surname>Bocanegra</surname>
                      <given-names>BR</given-names>
                    </name>
                    <name>
                      <surname>Zeelenberg</surname>
                      <given-names>R</given-names>
                    </name>
                  </person-group>
                  <year>2009</year>
                  <article-title>Dissociating emotion-induced blindness and hypervision.</article-title>
                  <source>Emotion</source>
                  <volume>9</volume>
                  <fpage>865</fpage>
                  <lpage>873</lpage>
                  <pub-id pub-id-type="pmid">20001129</pub-id>
                </element-citation>
              </ref>
              <ref id="pone.0013860-Zeelenberg1">
                <label>3</label>
                <element-citation publication-type="journal">
                  <article-title>Zeelenberg R, Bocanegra BR Auditory emotional cues enhance visual perception.</article-title>
                  <source>Cognition</source>
                  <volume>115</volume>
                  <fpage>202</fpage>
                  <lpage>206</lpage>
                  <pub-id pub-id-type="pmid">20096407</pub-id>
                </element-citation>
              </ref>
              <ref id="pone.0013860-Phelps1">
                <label>4</label>
                <element-citation publication-type="journal">
                  <person-group person-group-type="author">
                    <name>
                      <surname>Phelps</surname>
                      <given-names>EA</given-names>
                    </name>
                    <name>
                      <surname>Ling</surname>
                      <given-names>S</given-names>
                    </name>
                    <name>
                      <surname>Carrasco</surname>
                      <given-names>M</given-names>
                    </name>
                  </person-group>
                  <year>2006</year>
                  <article-title>Emotion facilitates perception and potentiates the perceptual benefits of attention.</article-title>
                  <source>Psychol Sci</source>
                  <volume>17</volume>
                  <fpage>292</fpage>
                  <lpage>299</lpage>
                  <pub-id pub-id-type="pmid">16623685</pub-id>
                </element-citation>
              </ref>
              <ref id="pone.0013860-Becker1">
                <label>5</label>
                <element-citation publication-type="journal">
                  <person-group person-group-type="author">
                    <name>
                      <surname>Becker</surname>
                      <given-names>MW</given-names>
                    </name>
                  </person-group>
                  <year>2009</year>
                  <article-title>Panic search: fear produces efficient visual search for nonthreatening objects.</article-title>
                  <source>Psychol Sci</source>
                  <volume>20</volume>
                  <fpage>435</fpage>
                  <lpage>437</lpage>
                  <pub-id pub-id-type="pmid">19309466</pub-id>
                </element-citation>
              </ref>
              <ref id="pone.0013860-Whalen1">
                <label>6</label>
                <element-citation publication-type="journal">
                  <person-group person-group-type="author">
                    <name>
                      <surname>Whalen</surname>
                      <given-names>PJ</given-names>
                    </name>
                    <name>
                      <surname>Shin</surname>
                      <given-names>LM</given-names>
                    </name>
                    <name>
                      <surname>McInerney</surname>
                      <given-names>SC</given-names>
                    </name>
                    <name>
                      <surname>Fischer</surname>
                      <given-names>H</given-names>
                    </name>
                    <name>
                      <surname>Wright</surname>
                      <given-names>CI</given-names>
                    </name>
                    <etal/>
                  </person-group>
                  <year>2001</year>
                  <article-title>A functional MRI study of human amygdala responses to facial expressions of fear versus anger.</article-title>
                  <source>Emotion</source>
                  <volume>1</volume>
                  <fpage>70</fpage>
                  <lpage>83</lpage>
                  <pub-id pub-id-type="pmid">12894812</pub-id>
                </element-citation>
              </ref>
              <ref id="pone.0013860-Cisler1">
                <label>7</label>
                <element-citation publication-type="journal">
                  <person-group person-group-type="author">
                    <name>
                      <surname>Cisler</surname>
                      <given-names>JM</given-names>
                    </name>
                    <name>
                      <surname>Olatunji</surname>
                      <given-names>BO</given-names>
                    </name>
                    <name>
                      <surname>Lohr</surname>
                      <given-names>JM</given-names>
                    </name>
                    <name>
                      <surname>Williams</surname>
                      <given-names>NL</given-names>
                    </name>
                  </person-group>
                  <year>2009</year>
                  <article-title>Attentional Bias Differences between Fear and Disgust: Implications for the Role of Disgust in Disgust-Related Anxiety Disorders.</article-title>
                  <source>Cogn Emot</source>
                  <volume>23</volume>
                  <fpage>675</fpage>
                  <lpage>687</lpage>
                  <pub-id pub-id-type="pmid">20589224</pub-id>
                </element-citation>
              </ref>
              <ref id="pone.0013860-Vermeulen1">
                <label>8</label>
                <element-citation publication-type="journal">
                  <person-group person-group-type="author">
                    <name>
                      <surname>Vermeulen</surname>
                      <given-names>N</given-names>
                    </name>
                    <name>
                      <surname>Godefroid</surname>
                      <given-names>J</given-names>
                    </name>
                    <name>
                      <surname>Mermillod</surname>
                      <given-names>M</given-names>
                    </name>
                  </person-group>
                  <year>2009</year>
                  <article-title>Emotional modulation of attention: fear increases but disgust reduces the attentional blink.</article-title>
                  <source>PLoS One</source>
                  <volume>4</volume>
                  <fpage>e7924</fpage>
                  <pub-id pub-id-type="pmid">19936235</pub-id>
                </element-citation>
              </ref>
              <ref id="pone.0013860-Santos1">
                <label>9</label>
                <element-citation publication-type="journal">
                  <person-group person-group-type="author">
                    <name>
                      <surname>Santos</surname>
                      <given-names>IM</given-names>
                    </name>
                    <name>
                      <surname>Iglesias</surname>
                      <given-names>J</given-names>
                    </name>
                    <name>
                      <surname>Olivares</surname>
                      <given-names>EI</given-names>
                    </name>
                    <name>
                      <surname>Young</surname>
                      <given-names>AW</given-names>
                    </name>
                  </person-group>
                  <year>2008</year>
                  <article-title>Differential effects of object-based attention on evoked potentials to fearful and disgusted faces.</article-title>
                  <source>Neuropsychologia</source>
                  <volume>46</volume>
                  <fpage>1468</fpage>
                  <lpage>1479</lpage>
                  <pub-id pub-id-type="pmid">18295286</pub-id>
                </element-citation>
              </ref>
              <ref id="pone.0013860-Anderson1">
                <label>10</label>
                <element-citation publication-type="journal">
                  <person-group person-group-type="author">
                    <name>
                      <surname>Anderson</surname>
                      <given-names>AK</given-names>
                    </name>
                  </person-group>
                  <year>2005</year>
                  <article-title>Affective influences on the attentional dynamics supporting awareness.</article-title>
                  <source>J Exp Psychol Gen</source>
                  <volume>134</volume>
                  <fpage>258</fpage>
                  <lpage>281</lpage>
                  <pub-id pub-id-type="pmid">15869349</pub-id>
                </element-citation>
              </ref>
              <ref id="pone.0013860-Most1">
                <label>11</label>
                <element-citation publication-type="journal">
                  <person-group person-group-type="author">
                    <name>
                      <surname>Most</surname>
                      <given-names>SB</given-names>
                    </name>
                    <name>
                      <surname>Smith</surname>
                      <given-names>SD</given-names>
                    </name>
                    <name>
                      <surname>Cooter</surname>
                      <given-names>AB</given-names>
                    </name>
                    <name>
                      <surname>Levy</surname>
                      <given-names>BN</given-names>
                    </name>
                    <name>
                      <surname>Zald</surname>
                      <given-names>DH</given-names>
                    </name>
                  </person-group>
                  <year>2007</year>
                  <article-title>The naked truth: Positive, arousing distractors impair rapid target perception.</article-title>
                  <source>Cognition &amp; Emotion</source>
                  <volume>21</volume>
                  <fpage>964</fpage>
                  <lpage>981</lpage>
                </element-citation>
              </ref>
              <ref id="pone.0013860-Lang1">
                <label>12</label>
                <element-citation publication-type="book">
                  <person-group person-group-type="author">
                    <name>
                      <surname>Lang</surname>
                      <given-names>PJ</given-names>
                    </name>
                    <name>
                      <surname>Bradley</surname>
                      <given-names>MM</given-names>
                    </name>
                    <name>
                      <surname>Cuthbert</surname>
                      <given-names>BN</given-names>
                    </name>
                  </person-group>
                  <year>1999</year>
                  <article-title>International affective picture system (IAPS): Technical manual and affective ratings.</article-title>
                  <publisher-loc>Gainesville</publisher-loc>
                  <publisher-name>University of Florida, Center for Research in Psychophysiology</publisher-name>
                </element-citation>
              </ref>
              <ref id="pone.0013860-Most2">
                <label>13</label>
                <element-citation publication-type="journal">
                  <person-group person-group-type="author">
                    <name>
                      <surname>Most</surname>
                      <given-names>SB</given-names>
                    </name>
                    <name>
                      <surname>Chun</surname>
                      <given-names>MM</given-names>
                    </name>
                    <name>
                      <surname>Widders</surname>
                      <given-names>DM</given-names>
                    </name>
                    <name>
                      <surname>Zald</surname>
                      <given-names>DH</given-names>
                    </name>
                  </person-group>
                  <year>2005</year>
                  <article-title>Attentional rubbernecking: cognitive control and personality in emotion-induced blindness.</article-title>
                  <source>Psychon Bull Rev</source>
                  <volume>12</volume>
                  <fpage>654</fpage>
                  <lpage>661</lpage>
                  <pub-id pub-id-type="pmid">16447378</pub-id>
                </element-citation>
              </ref>
              <ref id="pone.0013860-Ohman1">
                <label>14</label>
                <element-citation publication-type="journal">
                  <person-group person-group-type="author">
                    <name>
                      <surname>Ohman</surname>
                      <given-names>A</given-names>
                    </name>
                    <name>
                      <surname>Flykt</surname>
                      <given-names>A</given-names>
                    </name>
                    <name>
                      <surname>Esteves</surname>
                      <given-names>F</given-names>
                    </name>
                  </person-group>
                  <year>2001</year>
                  <article-title>Emotion drives attention: detecting the snake in the grass.</article-title>
                  <source>J Exp Psychol Gen</source>
                  <volume>130</volume>
                  <fpage>466</fpage>
                  <lpage>478</lpage>
                  <pub-id pub-id-type="pmid">11561921</pub-id>
                </element-citation>
              </ref>
              <ref id="pone.0013860-Dux1">
                <label>15</label>
                <element-citation publication-type="journal">
                  <person-group person-group-type="author">
                    <name>
                      <surname>Dux</surname>
                      <given-names>PE</given-names>
                    </name>
                    <name>
                      <surname>Marois</surname>
                      <given-names>R</given-names>
                    </name>
                  </person-group>
                  <year>2009</year>
                  <article-title>The attentional blink: a review of data and theory.</article-title>
                  <source>Atten Percept Psychophys</source>
                  <volume>71</volume>
                  <fpage>1683</fpage>
                  <lpage>1700</lpage>
                  <pub-id pub-id-type="pmid">19933555</pub-id>
                </element-citation>
              </ref>
              <ref id="pone.0013860-Duncan1">
                <label>16</label>
                <element-citation publication-type="journal">
                  <person-group person-group-type="author">
                    <name>
                      <surname>Duncan</surname>
                      <given-names>J</given-names>
                    </name>
                    <name>
                      <surname>Ward</surname>
                      <given-names>R</given-names>
                    </name>
                    <name>
                      <surname>Shapiro</surname>
                      <given-names>K</given-names>
                    </name>
                  </person-group>
                  <year>1994</year>
                  <article-title>Direct measurement of attentional dwell time in human vision.</article-title>
                  <source>Nature</source>
                  <volume>369</volume>
                  <fpage>313</fpage>
                  <lpage>315</lpage>
                  <pub-id pub-id-type="pmid">8183369</pub-id>
                </element-citation>
              </ref>
              <ref id="pone.0013860-Arnell1">
                <label>17</label>
                <element-citation publication-type="journal">
                  <person-group person-group-type="author">
                    <name>
                      <surname>Arnell</surname>
                      <given-names>KM</given-names>
                    </name>
                    <name>
                      <surname>Killman</surname>
                      <given-names>KV</given-names>
                    </name>
                    <name>
                      <surname>Fijavz</surname>
                      <given-names>D</given-names>
                    </name>
                  </person-group>
                  <year>2007</year>
                  <article-title>Blinded by emotion: target misses follow attention capture by arousing distractors in RSVP.</article-title>
                  <source>Emotion</source>
                  <volume>7</volume>
                  <fpage>465</fpage>
                  <lpage>477</lpage>
                  <pub-id pub-id-type="pmid">17683203</pub-id>
                </element-citation>
              </ref>
              <ref id="pone.0013860-MacKay1">
                <label>18</label>
                <element-citation publication-type="journal">
                  <person-group person-group-type="author">
                    <name>
                      <surname>MacKay</surname>
                      <given-names>DG</given-names>
                    </name>
                    <name>
                      <surname>Shafto</surname>
                      <given-names>M</given-names>
                    </name>
                    <name>
                      <surname>Taylor</surname>
                      <given-names>JK</given-names>
                    </name>
                    <name>
                      <surname>Marian</surname>
                      <given-names>DE</given-names>
                    </name>
                    <name>
                      <surname>Abrams</surname>
                      <given-names>L</given-names>
                    </name>
                    <etal/>
                  </person-group>
                  <year>2004</year>
                  <article-title>Relations between emotion, memory, and attention: evidence from taboo stroop, lexical decision, and immediate memory tasks.</article-title>
                  <source>Mem Cognit</source>
                  <volume>32</volume>
                  <fpage>474</fpage>
                  <lpage>488</lpage>
                </element-citation>
              </ref>
              <ref id="pone.0013860-Most3">
                <label>19</label>
                <element-citation publication-type="journal">
                  <person-group person-group-type="author">
                    <name>
                      <surname>Most</surname>
                      <given-names>SB</given-names>
                    </name>
                    <name>
                      <surname>Junge</surname>
                      <given-names>JA</given-names>
                    </name>
                  </person-group>
                  <year>2008</year>
                  <article-title>Don't look back: Retroactive, dynamic costs and benefits of emotional capture.</article-title>
                  <source>Visual Cognition</source>
                  <volume>16</volume>
                  <fpage>262</fpage>
                  <lpage>278</lpage>
                </element-citation>
              </ref>
              <ref id="pone.0013860-Susskind1">
                <label>20</label>
                <element-citation publication-type="journal">
                  <person-group person-group-type="author">
                    <name>
                      <surname>Susskind</surname>
                      <given-names>JM</given-names>
                    </name>
                    <name>
                      <surname>Lee</surname>
                      <given-names>DH</given-names>
                    </name>
                    <name>
                      <surname>Cusi</surname>
                      <given-names>A</given-names>
                    </name>
                    <name>
                      <surname>Feiman</surname>
                      <given-names>R</given-names>
                    </name>
                    <name>
                      <surname>Grabski</surname>
                      <given-names>W</given-names>
                    </name>
                    <etal/>
                  </person-group>
                  <year>2008</year>
                  <article-title>Expressing fear enhances sensory acquisition.</article-title>
                  <source>Nat Neurosci</source>
                  <volume>11</volume>
                  <fpage>843</fpage>
                  <lpage>850</lpage>
                  <pub-id pub-id-type="pmid">18552843</pub-id>
                </element-citation>
              </ref>
              <ref id="pone.0013860-Woody1">
                <label>21</label>
                <element-citation publication-type="journal">
                  <person-group person-group-type="author">
                    <name>
                      <surname>Woody</surname>
                      <given-names>SR</given-names>
                    </name>
                    <name>
                      <surname>Teachman</surname>
                      <given-names>BA</given-names>
                    </name>
                  </person-group>
                  <year>2000</year>
                  <article-title>Intersection of disgust and fear: Normative and pathological views.</article-title>
                  <source>Clinical Psychology: Science and Practice</source>
                  <volume>7</volume>
                  <fpage>291</fpage>
                  <lpage>311</lpage>
                </element-citation>
              </ref>
            </ref-list>
          </back>
        </article>
      </metadata>
    </record>
  </GetRecord>
</OAI-PMH>
