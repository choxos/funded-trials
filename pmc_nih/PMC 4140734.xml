<?xml version="1.0" encoding="UTF-8"?>
<OAI-PMH xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/ http://www.openarchives.org/OAI/2.0/OAI-PMH.xsd">
  <responseDate>2024-05-03T06:48:19Z</responseDate>
  <request verb="GetRecord" identifier="oai:pubmedcentral.nih.gov:4140734" metadataPrefix="pmc">https:/www.ncbi.nlm.nih.gov/pmc/oai/oai.cgi</request>
  <GetRecord>
    <record>
      <header>
        <identifier>oai:pubmedcentral.nih.gov:4140734</identifier>
        <datestamp>2014-08-25</datestamp>
        <setSpec>plosone</setSpec>
        <setSpec>pmc-open</setSpec>
      </header>
      <metadata>
        <article xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" xsi:schemaLocation="https://jats.nlm.nih.gov/ns/archiving/1.3/ https://jats.nlm.nih.gov/archiving/1.3/xsd/JATS-archivearticle1-3.xsd" article-type="research-article">
          <front>
            <journal-meta>
              <journal-id journal-id-type="nlm-ta">PLoS One</journal-id>
              <journal-id journal-id-type="iso-abbrev">PLoS ONE</journal-id>
              <journal-id journal-id-type="publisher-id">plos</journal-id>
              <journal-id journal-id-type="pmc">plosone</journal-id>
              <journal-title-group>
                <journal-title>PLoS ONE</journal-title>
              </journal-title-group>
              <issn pub-type="epub">1932-6203</issn>
              <publisher>
                <publisher-name>Public Library of Science</publisher-name>
                <publisher-loc>San Francisco, USA</publisher-loc>
              </publisher>
            </journal-meta>
            <article-meta>
              <article-id pub-id-type="accession">PMC4140734</article-id>
              <article-id pub-id-type="pmcid">PMC4140734</article-id>
              <article-id pub-id-type="pmc-uid">4140734</article-id>
              <article-id pub-id-type="pmid">25144200</article-id>
              <article-id pub-id-type="pmid">25144200</article-id>
              <article-id pub-id-type="publisher-id">PONE-D-14-16409</article-id>
              <article-id pub-id-type="doi">10.1371/journal.pone.0105144</article-id>
              <article-categories>
                <subj-group subj-group-type="heading">
                  <subject>Research Article</subject>
                </subj-group>
                <subj-group subj-group-type="Discipline-v2">
                  <subject>Biology and Life Sciences</subject>
                  <subj-group>
                    <subject>Neuroscience</subject>
                    <subj-group>
                      <subject>Cognitive Science</subject>
                      <subj-group>
                        <subject>Cognitive Psychology</subject>
                        <subj-group>
                          <subject>Creativity</subject>
                          <subject>Music Cognition</subject>
                        </subj-group>
                      </subj-group>
                    </subj-group>
                  </subj-group>
                  <subj-group>
                    <subject>Psychology</subject>
                    <subj-group>
                      <subject>Emotions</subject>
                    </subj-group>
                  </subj-group>
                </subj-group>
                <subj-group subj-group-type="Discipline-v2">
                  <subject>Social Sciences</subject>
                </subj-group>
              </article-categories>
              <title-group>
                <article-title>The Role of Emotion in Musical Improvisation: An Analysis of Structural Features</article-title>
                <alt-title alt-title-type="running-head">Emotion and Musical Improvisation: Structural Features</alt-title>
              </title-group>
              <contrib-group>
                <contrib contrib-type="author">
                  <name>
                    <surname>McPherson</surname>
                    <given-names>Malinda J.</given-names>
                  </name>
                  <xref ref-type="aff" rid="aff1">
                    <sup>1</sup>
                  </xref>
                  <xref ref-type="corresp" rid="cor1">
                    <sup>*</sup>
                  </xref>
                </contrib>
                <contrib contrib-type="author">
                  <name>
                    <surname>Lopez-Gonzalez</surname>
                    <given-names>Monica</given-names>
                  </name>
                  <xref ref-type="aff" rid="aff1">
                    <sup>1</sup>
                  </xref>
                  <xref ref-type="aff" rid="aff2">
                    <sup>2</sup>
                  </xref>
                </contrib>
                <contrib contrib-type="author">
                  <name>
                    <surname>Rankin</surname>
                    <given-names>Summer K.</given-names>
                  </name>
                  <xref ref-type="aff" rid="aff1">
                    <sup>1</sup>
                  </xref>
                </contrib>
                <contrib contrib-type="author">
                  <name>
                    <surname>Limb</surname>
                    <given-names>Charles J.</given-names>
                  </name>
                  <xref ref-type="aff" rid="aff1">
                    <sup>1</sup>
                  </xref>
                  <xref ref-type="aff" rid="aff2">
                    <sup>2</sup>
                  </xref>
                </contrib>
              </contrib-group>
              <aff id="aff1">
                <label>1</label>
                <addr-line>Department of Otolaryngology-Head and Neck Surgery, Johns Hopkins University School of Medicine, Baltimore, Maryland, United States of America</addr-line>
              </aff>
              <aff id="aff2">
                <label>2</label>
                <addr-line>Peabody Conservatory of The Johns Hopkins University, Baltimore, Maryland, United States of America</addr-line>
              </aff>
              <contrib-group>
                <contrib contrib-type="editor">
                  <name>
                    <surname>Nusbaum</surname>
                    <given-names>Howard</given-names>
                  </name>
                  <role>Editor</role>
                  <xref ref-type="aff" rid="edit1"/>
                </contrib>
              </contrib-group>
              <aff id="edit1">
                <addr-line>The University of Chicago, United States of America</addr-line>
              </aff>
              <author-notes>
                <corresp id="cor1">* E-mail: <email>mmcpherson@jhu.edu</email></corresp>
                <fn fn-type="COI-statement">
                  <p><bold>Competing Interests: </bold>The authors have declared that no competing interests exist.</p>
                </fn>
                <fn fn-type="con">
                  <p>Conceived and designed the experiments: MJM MLG CJL. Performed the experiments: MJM MLG. Analyzed the data: MJM SKR. Contributed to the writing of the manuscript: MJM SKR CJL.</p>
                </fn>
              </author-notes>
              <pub-date pub-type="collection">
                <year>2014</year>
              </pub-date>
              <pub-date pub-type="epub">
                <day>21</day>
                <month>8</month>
                <year>2014</year>
              </pub-date>
              <volume>9</volume>
              <issue>8</issue>
              <elocation-id>e105144</elocation-id>
              <history>
                <date date-type="received">
                  <day>14</day>
                  <month>4</month>
                  <year>2014</year>
                </date>
                <date date-type="accepted">
                  <day>18</day>
                  <month>7</month>
                  <year>2014</year>
                </date>
              </history>
              <permissions>
                <copyright-statement>Â© 2014 McPherson et al</copyright-statement>
                <copyright-year>2014</copyright-year>
                <copyright-holder>McPherson et al</copyright-holder>
                <license xlink:href="http://creativecommons.org/licenses/by/4.0/">
                  <license-p>This is an open-access article distributed under the terms of the Creative Commons Attribution License, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are properly credited.</license-p>
                </license>
              </permissions>
              <abstract>
                <p>One of the primary functions of music is to convey emotion, yet how music accomplishes this task remains unclear. For example, simple correlations between mode (major vs. minor) and emotion (happy vs. sad) do not adequately explain the enormous range, subtlety or complexity of musically induced emotions. In this study, we examined the structural features of unconstrained musical improvisations generated by jazz pianists in response to emotional cues. We hypothesized that musicians would not utilize any universal rules to convey emotions, but would instead combine heterogeneous musical elements together in order to depict positive and negative emotions. Our findings demonstrate a lack of simple correspondence between emotions and musical features of spontaneous musical improvisation. While improvisations in response to positive emotional cues were more likely to be in major keys, have faster tempos, faster key press velocities and more staccato notes when compared to negative improvisations, there was a wide distribution for each emotion with components that directly violated these primary associations. The finding that musicians often combine disparate features together in order to convey emotion during improvisation suggests that structural diversity may be an essential feature of the ability of music to express a wide range of emotion.</p>
              </abstract>
              <funding-group>
                <funding-statement>This project was funded by the Dana Foundation and the Brain Science Institute of Johns Hopkins University School of Medicine, as well as a Training Grant (T32-DC000023) from the Department of Biomedical Engineering, Johns Hopkins University. The funders had no role in study design, data collection and analysis, decision to publish, or preparation of the manuscript.</funding-statement>
              </funding-group>
              <counts>
                <page-count count="11"/>
              </counts>
              <custom-meta-group>
                <custom-meta id="data-availability">
                  <meta-name>Data Availability</meta-name>
                  <meta-value>The authors confirm that all data underlying the findings are fully available without restriction. All relevant data are within the paper and its Supporting Information files.</meta-value>
                </custom-meta>
              </custom-meta-group>
            </article-meta>
            <notes>
              <title>Data Availability</title>
              <p>The authors confirm that all data underlying the findings are fully available without restriction. All relevant data are within the paper and its Supporting Information files.</p>
            </notes>
          </front>
          <body>
            <sec id="s1">
              <title>Introduction</title>
              <p>Music has been described as the âlanguage of emotionsâ <xref rid="pone.0105144-Eerola1" ref-type="bibr">[1]</xref>, <xref rid="pone.0105144-Spencer1" ref-type="bibr">[2]</xref>, yet how specific features of music are able to both express and elicit emotions remains poorly understood. While each feature of music (e.g. key, mode, tempo, etc.) contributes to the ability of music to convey emotion, no single element sufficiently accounts for the vast emotional range of music. Complicating this issue is the fact that emotional experiences can often defy simple definition or specification because of their subjective nature and varying intensity.</p>
              <p>Thus far, no unified model exists that clearly defines the relationship between musical structure and emotion <xref rid="pone.0105144-Eerola1" ref-type="bibr">[1]</xref>. Several efforts have focused on identifying a set of universal basic emotions that are expressed through music, generally including happiness, sadness, fear, anger and surprise <xref rid="pone.0105144-Ekman1" ref-type="bibr">[3]</xref>, <xref rid="pone.0105144-Panksepp1" ref-type="bibr">[4]</xref>. Other research has proposed tension and relaxation within music as the basis for a music-specific model of emotion <xref rid="pone.0105144-Krumhansl1" ref-type="bibr">[5]</xref>, <xref rid="pone.0105144-McAdams1" ref-type="bibr">[6]</xref>. One of the most commonly used and broadly applicable models defines emotions using the parameters of valence (pleasant vs. unpleasant) and arousal (intensity) <xref rid="pone.0105144-Russell1" ref-type="bibr">[7]</xref>.</p>
              <p>The bulk of knowledge about the relationship between music and emotions comes from studies that examine the perception, rather than the production, of music <xref rid="pone.0105144-Eerola1" ref-type="bibr">[1]</xref>. Many studies have examined one of the most basic emotional distinctions, that of happiness vs. sadness (sometimes referred to as âpositiveâ vs. ânegativeâ), and have found that there are tonal, rhythmic and articulatory differences between âhappyâ and âsadâ emotions. For example, a general correlation exists between perception of happiness and major keys, and sadness and minor keys <xref rid="pone.0105144-Costa1" ref-type="bibr">[8]</xref>â<xref rid="pone.0105144-Kastner1" ref-type="bibr">[13]</xref>. It has also been claimed that specific keys may better express different emotional and aesthetic qualities <xref rid="pone.0105144-Denckla1" ref-type="bibr">[14]</xref>, <xref rid="pone.0105144-Steblin1" ref-type="bibr">[15]</xref>. The pitch range of compositions has been shown to have direct effects on emotion perception, for example, lower pitches are generally perceived as sadder than higher pitches <xref rid="pone.0105144-Costanzo1" ref-type="bibr">[16]</xref>, <xref rid="pone.0105144-Huron1" ref-type="bibr">[17]</xref>, although very high pitches can be associated with extreme sadness or grief <xref rid="pone.0105144-Paul1" ref-type="bibr">[18]</xref>, <xref rid="pone.0105144-Scherer1" ref-type="bibr">[19]</xref>. Additionally, tempo and volume generally increase during happy music and decrease during sad music <xref rid="pone.0105144-DallaBella1" ref-type="bibr">[9]</xref>, <xref rid="pone.0105144-Gagnon1" ref-type="bibr">[10]</xref>, <xref rid="pone.0105144-Juslin1" ref-type="bibr">[20]</xref>â<xref rid="pone.0105144-Rigg1" ref-type="bibr">[24]</xref>. There are also articulation differences between happy and sad compositions; staccato articulations (short notes separated by silence) are generally perceived as happier than legato articulations (no silence between notes and smooth note transitions <xref rid="pone.0105144-Juslin1" ref-type="bibr">[20]</xref>, <xref rid="pone.0105144-Livingstone1" ref-type="bibr">[25]</xref>.</p>
              <p>Studies focusing on the perception of emotion in music alone, however, have limitations that include the potential biases associated with the selection of music chosen by the experimenter and the difficulty of standardizing subjective reports of emotion. More importantly, these approaches minimize the crucial role played by the composer and performer in conveying musical emotion. In over a century of empirical research about the relationship between emotion and music, very few studies have specifically addressed the production of emotional instrumental music by providing musicians with explicit emotional cues and then analyzing their musical output in order to see how musicians accomplish emotionally-motivated musical tasks <xref rid="pone.0105144-Juslin1" ref-type="bibr">[20]</xref>, <xref rid="pone.0105144-Laukka1" ref-type="bibr">[23]</xref>, <xref rid="pone.0105144-Baraldi1" ref-type="bibr">[26]</xref>â<xref rid="pone.0105144-Timmers1" ref-type="bibr">[29]</xref>. These previous production studies were extremely important in indicating the enormous complexity of the relationship between emotion and musical production, yet these studies included several experimental constraints that we attempted to address in the current study. Most of these production studies <xref rid="pone.0105144-Juslin1" ref-type="bibr">[20]</xref>, <xref rid="pone.0105144-Laukka1" ref-type="bibr">[23]</xref>, <xref rid="pone.0105144-Gabrielsson1" ref-type="bibr">[27]</xref>â<xref rid="pone.0105144-Laukka2" ref-type="bibr">[30]</xref> required musicians to alter pre-determined melodies or rhythms to express a specific emotion. Because of this, their analysis was limited to tempo, articulation, volume and timbre, leaving out such features as key and note range (among others) and also biased the musicians by providing them with an essentially arbitrary template upon which to base their responses. In one of the more recent of these studies <xref rid="pone.0105144-Baraldi1" ref-type="bibr">[26]</xref>, pianists were asked to express emotions using improvisations on a single note. This also limited the musiciansâ means of conveying each emotion to modulations in volume, tempo and articulation. Consequently, these experimental paradigms suffered from being musically impoverished and somewhat unnatural.</p>
              <p>A more subtle but equally important potential confounding element within these previous experiments was the use of verbal or language-based cues to direct the musiciansâ performances. There is evidence that language can influence peopleâs perception of emotional stimuli, and it is possible that linguistic labels for emotions could influence musicians to depict them in stereotypic fashion <xref rid="pone.0105144-Barrett1" ref-type="bibr">[31]</xref>, <xref rid="pone.0105144-Mesquita1" ref-type="bibr">[32]</xref>. Such labels may poorly represent the often transient and subjective nature of emotional content in music, where multiple emotions can be implied by a single musical passage or through a single musical feature <xref rid="pone.0105144-Cross1" ref-type="bibr">[33]</xref>, <xref rid="pone.0105144-Huron2" ref-type="bibr">[34]</xref>.</p>
              <p>Here we present the first ecologically valid examination of the production of novel emotional music. In this study, we asked professional jazz pianists to improvise short musical pieces (1 min) based on visually presented emotional cues (photographs and cartoons) without any other musical constraints. By using improvisation and visual emotional cues, we sought to develop a more natural account of musical elements that correspond to positive and negative emotional categories. We hypothesized that a broad range of musical features would characterize improvisations to positive and negative emotional targets, rather than a simplistic (driven by one or two key elements) or predictable relationship between emotional target and musical structure. In order to address this hypothesis, we developed novel visual-emotional cues for this study, and assessed the emotional valence of each cue through behavioral testing. We also tested whether naÃ¯ve listeners perceived any emotional differences between the musical improvisations created by the musicians during the study. Our results demonstrate that musicians employ a diverse range of musical approaches to convey specific emotions in response to emotional cues during unconstrained improvisation. Thus, we argue that musical representations of emotions cannot be sufficiently explained by simplistic correlations (e.g. minor keyâ=âsad, major keyâ=âhappy) between musical features and target emotions. Instead, a broader approach to the diversity of factors that impact emotion in music is crucial to understanding the remarkable ability of music to provide a vast range of deeply emotional experiences.</p>
            </sec>
            <sec sec-type="methods" id="s2">
              <title>Methods</title>
              <sec id="s2a">
                <title>Stimuli Testing</title>
                <sec id="s2a1">
                  <title>Subjects</title>
                  <p>Twenty volunteers, 11 males and 9 females (mean ageâ=â32Â±17 s.d. years, minimum ageâ=â18 years), from the Johns Hopkins University community participated in the stimuli testing. Informed consent was obtained in writing for all subjects and they did not receive monetary compensation for participating. All experimental procedures were approved by the Johns Hopkins University School of Medicine Institutional Review Board.</p>
                </sec>
                <sec id="s2a2">
                  <title>Procedure</title>
                  <p>We developed a set of cartoons and photographs that represented three basic emotional categories (Positive, Ambiguous and Negative). The actresses pictured in this manuscript have both given written informed consent (as outlined in the PLOS consent form) to publish their photographs. The stimuli consisted of 1 min movies showing either a photograph or cartoon (<xref ref-type="fig" rid="pone-0105144-g001">Figure 1</xref>). Each stimulus contained a green dot denoting the beginning, followed by 10 s of a blank screen, followed by 50 s of the emotional cue, and a red dot to denote the end. For the purposes of this study, we used James Russellâs Circumplex model of emotion to define âAmbiguousâ as a neither positive nor negative rating in valence and arousal <xref rid="pone.0105144-Russell1" ref-type="bibr">[7]</xref>, <xref rid="pone.0105144-Huron1" ref-type="bibr">[17]</xref>, <xref rid="pone.0105144-Chapin1" ref-type="bibr">[35]</xref>. Subjects were asked to rate the emotion they perceived in each stimulus by marking an âXâ on an emoticon Visual Analog Scale (VAS) (<xref ref-type="fig" rid="pone-0105144-g002">Figure 2</xref>). The order of the stimuli was randomized for each subject. Subjects were only allowed to view and respond to each image once, and were given an unlimited time to respond.</p>
                  <fig id="pone-0105144-g001" orientation="portrait" position="float">
                    <object-id pub-id-type="doi">10.1371/journal.pone.0105144.g001</object-id>
                    <label>Figure 1</label>
                    <caption>
                      <title>Photographs and Cartoons Used as Visual Stimuli.</title>
                      <p>Cartoon faces representing each of the three emotions were created using Microsoft PowerPoint. The photographs were shot indoors in black and white with a 50 mm lens at f16 using a Nikon D700 digital SLR camera.</p>
                    </caption>
                    <graphic xlink:href="pone.0105144.g001"/>
                  </fig>
                  <fig id="pone-0105144-g002" orientation="portrait" position="float">
                    <object-id pub-id-type="doi">10.1371/journal.pone.0105144.g002</object-id>
                    <label>Figure 2</label>
                    <caption>
                      <title>Visual Analog Scale (VAS) with nine point coding rubric below.</title>
                    </caption>
                    <graphic xlink:href="pone.0105144.g002"/>
                  </fig>
                </sec>
              </sec>
              <sec id="s2b">
                <title>Piano Performance</title>
                <sec id="s2b1">
                  <title>Subjects</title>
                  <p>Fourteen healthy, normal hearing male (11) and female (3) musicians (mean ageâ=â42Â±15 s.d. years) were recruited from the Peabody Institute of The Johns Hopkins University community. All were full-time jazz piano students or professional musicians who had at least five years of professional jazz piano performance experience (mean years of professional experienceâ=â21Â±12 s.d., mean years playing pianoâ=â31Â±14 s.d.). Informed consent was obtained in writing for all subjects, and all subjects received compensation. All experimental procedures were approved by the Johns Hopkins University School of Medicine Institutional Review Board.</p>
                </sec>
                <sec id="s2b2">
                  <title>Procedure</title>
                  <p>The pianists were seated at a 73-key weighted Korg SV-1 piano keyboard, routed through two Mackie MR5mk2 studio reference monitor speakers. Sound levels were kept constant through the entire session and between subjects. All stimuli (photographs and cartoons) were presented on a 21.5 in iMac (OS X 10.6.8) using Max6 (Cycling 74, Walnut, CA). MIDI (Musical Instrument Digital Interface) information from the piano keyboard was recorded using GarageBand (Apple Inc., Cupertino, CA). The data were analyzed in MATLAB (MathWorks, Inc., Natick, MA) using MIDI Toolbox <xref rid="pone.0105144-Eerola2" ref-type="bibr">[36]</xref> and custom scripts. The authors verified the mode and key analyses by visual inspection of the scores.</p>
                  <p>Half of the pianists saw the photographs of Actress A, and the other half saw only the photographs of Actress B. Control blocks (blank white screens) were intermixed with the stimuli. The blank screen control blocks contained a green dot denoting the beginning and a red dot denoting the end. The order of the stimuli was randomized for each subject. Pianists were instructed to look at the monitor, and not their hands, for the duration of the experiment.</p>
                  <p>The experiment was divided into four parts. During the first part, pianists were familiarized with the six emotional stimuli (three cartoons and a subset of three pictures) by viewing each full video clip. During the second portion of the experiment, pianists viewed each stimulus again, and were instructed to improvise a novel composition using both hands and the full range of the keyboard. During the third part of the experiment, pianists were asked to view the same stimuli and improvise a monophonic piece (one note at a time) using their right hand. They were restricted to using a 2.5 octave range (C3 to B-flat 5). The fourth part of the experiment was an exact repetition of part two. Pianists were asked to improvise compositions that matched the emotions expressed in the images (See <xref ref-type="supplementary-material" rid="pone.0105144.s001">File S1</xref> for full instructions). For the blank screen control conditions, which were intended to have no emotional connotations, pianists were instructed to improvise freely. Examples of the stimuli and responses are available online (<ext-link ext-link-type="uri" xlink:href="https://www.youtube.com/user/LimbMusicLab">https://www.youtube.com/user/LimbMusicLab</ext-link>).</p>
                </sec>
              </sec>
              <sec id="s2c">
                <title>Listening Survey</title>
                <sec id="s2c1">
                  <title>Subjects</title>
                  <p>Twenty healthy subjects (mean ageâ=â24Â±5 s.d. years), including ten musicians (4 males, 6 females, with mean years of musical trainingâ=â10.7Â±1.82 s.d.) and ten non-musicians (5 males and 5 females), were recruited from Johns Hopkins University and the greater Baltimore area. Informed consent was obtained in writing for all subjects and they did not receive monetary compensation for participating. All experimental procedures were approved by the Johns Hopkins University School of Medicine Institutional Review Board.</p>
                </sec>
                <sec id="s2c2">
                  <title>Procedure</title>
                  <p>Each subject heard a random sample of improvisations from the piano performance portion of the study. This random sample included four improvisations from each emotional category (Positive, Negative and Ambiguous), with two one-handed and two two-handed improvisations for each emotion. There were a total of ten randomizations â one non-musician and one musician heard each randomization. The subjects listened to the last 50 s of each improvisation through headphones.</p>
                  <p>Subjects were asked to rate the emotion that they believed the improvisation was expressing by marking an emoticon Visual Analog Scale (<xref ref-type="fig" rid="pone-0105144-g002">Figure 2</xref>) with an âXâ. Subjects were allowed to listen to each improvisation once, and were given an unlimited time to respond.</p>
                </sec>
              </sec>
            </sec>
            <sec id="s3">
              <title>Results</title>
              <p>The stimulus testing was conducted to confirm that our visual stimuli were appropriate emotional cues for the piano performance testing. Results were coded using a nine point scale, with 0â=âthe most negative, 4.5â=âambiguous, 9â=âthe most positive. Due to the orthogonal nature of the data, a two-way ANOVA on the ratings with within-subject factors Emotion (Negative, Ambiguous, Positive) by Type (Cartoon, Actress A, Actress B) was calculated to compare the ratings between conditions <xref rid="pone.0105144-Dexter1" ref-type="bibr">[37]</xref>. Significant main effects of Emotion, [<italic>F</italic>(1, 2)â=â564.65, <italic>p</italic>&lt;.001] and Type, [<italic>F</italic>(1,2)â=â15.54, <italic>p</italic>&lt;.001] were observed and their interaction was significant [<italic>F</italic>(1, 4)â=â26.72, <italic>p</italic>&lt;.001].</p>
              <p>Mean ratings for the Negative stimuli: Cartoonâ=â0.5Â±0.94 (s.d.); Actress Aâ=â3.7Â±0.92 (s.d.); Actress Bâ=â2.85Â±1.37 (s.d.). Mean ratings for the Ambiguous stimuli: Cartoonâ=â4.43Â±0.59 (s.d.); Actress Aâ=â4.75Â±0.75 (s.d.); Actress Bâ=â3.95Â±.74 (s.d.). Mean ratings for the Positive Stimuli: Cartoonâ=â8.35Â±0.33 (s.d.); Actress Aâ=â7.65Â±0.80 (s.d.); Actress Bâ=â7.2Â±0.99 (s.d.).</p>
              <sec id="s3a">
                <title>Piano Performance</title>
                <p>The following are multiple analyses that were performed on the data from the improvisations. We analyzed the final 50 s of each trial (during the first 10 s of each trial the pianists were presented with a blank screen). For the measures Note Density, Note Range and Key Press Velocity, we ran separate one-way ANOVAs with factor Trial (Trial 1 and Trial Three, the two-handed trial) to test for an effect of trial order. Because no significant (<italic>p</italic>&gt;.05) effect of Trial (trial order) was found, the two-handed trials were analyzed together. For all analyses except key and note transitions (overlaps and silences) analyses were run separately for one and two-handed improvisations.</p>
                <sec id="s3a1">
                  <title>Note Density</title>
                  <p>Note density is a measure of average notes per second (<xref ref-type="fig" rid="pone-0105144-g003">Figure 3</xref>). Note density can be used as a strong indicator of tempo in monophonic improvisations and a weaker indicator of tempo in polyphonic improvisations, as chords or ornaments such as trills can increase the number of notes per second even if the absolute tempo does not increase. We calculated a two-way ANOVA on note density with within-subject factors Emotion (Negative, Ambiguous, Positive) and Type (Cartoon, Actress). Because no significant (<italic>p&gt;</italic>.05) effects of Type were found, we collapsed the data by Type, and the ANOVA was rerun as a one-way ANOVA with the within-subject factor Emotion (Negative, Ambiguous, Positive). Tukeyâs honestly significant difference criterion was used for post-hoc comparisons. A main effect of Emotion was found for two-handed [<italic>F</italic>(1,3)â=â99.65, <italic>p</italic>&lt;.001] and one-handed trials [F(1,3)â=â53.28, <italic>p</italic>&lt;.001]. For both one- and two-handed trials, a significant difference between Positive and Negative conditions was found (<italic>p</italic>&lt;.001). Note density was significantly different (<italic>p</italic>&lt;.001) between Ambiguous and Positive trials, Positive and Negative trials, and between all emotions and the Control. There was no statistically significant (<italic>p</italic>&gt;.05) difference between the note densities of Ambiguous and Negative trials. Higher note densities were used to express Positive emotions, and lower note densities were used to express Negative and Ambiguous emotions.</p>
                  <fig id="pone-0105144-g003" orientation="portrait" position="float">
                    <object-id pub-id-type="doi">10.1371/journal.pone.0105144.g003</object-id>
                    <label>Figure 3</label>
                    <caption>
                      <title>Average note density of one-handed and two-handed improvisations.</title>
                    </caption>
                    <graphic xlink:href="pone.0105144.g003"/>
                  </fig>
                </sec>
                <sec id="s3a2">
                  <title>Duration Distribution</title>
                  <p>The duration distribution function of the MIDI Toolbox returns the percentage of notes that fall into nine different logarithmically organized bins (note length categories). Length categories are defined as a unit of beats. We set our MIDI tempo so that 1 beatâ=â.5 s (quarter noteâ=â120 Beats Per Minute (BPM)). Therefore, bin 1â=â1/8 s, bin 3â=âÂ¼ s, bin 5â=âÂ½ s, bin 7â=â1 s, and bin 9â=â2 s. The relationship between bin 1 and bin 9 is proportional to the relationship between a sixteenth note and a whole note. Two-sample Kolmogorov-Smirnov tests showed that there were statistical differences (<italic>p</italic>&lt;.05) between corresponding bins of the distributions for Negative, Positive and Ambiguous for both one- and two-handed improvisations. Ambiguous and Control conditions were not statistically different in either condition (<xref ref-type="fig" rid="pone-0105144-g004">Figure 4</xref>).</p>
                  <fig id="pone-0105144-g004" orientation="portrait" position="float">
                    <object-id pub-id-type="doi">10.1371/journal.pone.0105144.g004</object-id>
                    <label>Figure 4</label>
                    <caption>
                      <title>Distributions of note durations for one-handed and two-handed improvisations.</title>
                    </caption>
                    <graphic xlink:href="pone.0105144.g004"/>
                  </fig>
                  <p>During the two-handed control condition, 63.75% of the notes were less than 1 s in duration, which was similar to the 57.5% of notes that were less than 1 s during Ambiguous trials. During Positive improvisations, 24.8% of the notes were â of a second or less, and 73.5% of the notes were less than 1 s. When musicians improvised to the Negative emotion, only 46.94% of the notes were less than 1 s in length.</p>
                </sec>
                <sec id="s3a3">
                  <title>Key Press Velocity</title>
                  <p>Velocity is the measurement of how quickly a key was depressed, and is linearly related to sound pressure level (SPL) <xref rid="pone.0105144-Eerola2" ref-type="bibr">[36]</xref>, <xref rid="pone.0105144-Goebl1" ref-type="bibr">[38]</xref>. Our results show that Positive improvisations tended to be louder than Negative or Ambiguous improvisations (<xref ref-type="fig" rid="pone-0105144-g005">Figure 5</xref>). We calculated a two-way ANOVA on mean key press velocities with factors Emotion (Negative, Ambiguous, Positive) and Type (Cartoon, Actress). Because no significant (<italic>p</italic>&gt;.05) effects of Type were found, we collapsed the data by Type, and the ANOVA was rerun as a one-way ANOVA with the within-subject factor Emotion (Negative, Ambiguous, Positive). Tukeyâs honestly significant difference criterion was used for post-hoc comparisons. A main effect of Emotion was found for two-handed [<italic>F</italic>(1,3)â=â45.69, <italic>p</italic>&lt;.001] and one-handed trials [<italic>F</italic>(1,3)â=â23.51, <italic>p</italic>&lt;.001]. For both and one- and two-handed trials, Positive key press velocities were significantly greater (<italic>p</italic>&lt;.001) than Negative, Ambiguous and Control key press velocities. The difference between the Control improvisations and Negative improvisations for two hands was also significant (<italic>p</italic>&lt;.001).</p>
                  <fig id="pone-0105144-g005" orientation="portrait" position="float">
                    <object-id pub-id-type="doi">10.1371/journal.pone.0105144.g005</object-id>
                    <label>Figure 5</label>
                    <caption>
                      <title>Average key press velocity for one-handed and two-handed improvisations.</title>
                    </caption>
                    <graphic xlink:href="pone.0105144.g005"/>
                  </fig>
                </sec>
                <sec id="s3a4">
                  <title>Note Transitions: Overlaps and Silences</title>
                  <p>Though the pianists were instructed to make their one-handed improvisations completely monophonic, we found that their notes overlapped by fractions of a second when they attempted to create the effect of legato. Conversely, when trying to create the effect of non-legato or staccato, there were silences between the notes. We examined the proportion of overlapping and non-overlapping note transitions for each emotion. There were over twice as many overlapping note transitions in Negative improvisations compared to Positive improvisations (<xref ref-type="fig" rid="pone-0105144-g006">Figure 6</xref>).</p>
                  <fig id="pone-0105144-g006" orientation="portrait" position="float">
                    <object-id pub-id-type="doi">10.1371/journal.pone.0105144.g006</object-id>
                    <label>Figure 6</label>
                    <caption>
                      <title>Overlapping and non-overlapping note transitions during one-handed improvisations.</title>
                    </caption>
                    <graphic xlink:href="pone.0105144.g006"/>
                  </fig>
                </sec>
                <sec id="s3a5">
                  <title>Note Range, Maximum and Minimum</title>
                  <p>We calculated two-way ANOVAs on the note (pitch) minimum, note maximum, and note range (difference between highest and lowest notes during improvisation) using within-subject factors Emotion (Negative, Ambiguous, Positive) and Type (Cartoon, Actress). Because no significant (<italic>p</italic>&gt;.05) effects of Type were found for note range, we collapsed the data by Type, and the ANOVA was rerun as a one-way ANOVA using the within-subject factor Emotion (Negative, Ambiguous, Positive). Tukeyâs honestly significant difference criterion was used for post-hoc comparisons. A main effect of Emotion was found for two-handed [<italic>F</italic>(1,3)â=â30.69, p&lt;.001] and one-handed trials [<italic>F</italic>(1,3)â=â18.34, <italic>p</italic>&lt;.001] (<xref ref-type="fig" rid="pone-0105144-g007">Figure 7</xref>). For both two- and one-handed trials, a significant difference between Positive and Negative conditions was found (<italic>p</italic>&lt;.001). This was primarily accounted for by differences in the note maxima (<italic>p</italic>&lt;.001), not the note minima. There was no statistically significant difference (<italic>p</italic>&gt;.05) in note minima between any of the emotions or the control for one handed improvisations, and for two-handed improvisations, only Positive and Negative improvisations were significantly (<italic>p</italic>&lt;.05) different. There were no significant (<italic>p</italic>&gt;.05) differences between Ambiguous and Negative note ranges. Our results indicate that a wider range in pitch is more highly correlated with the Positive condition, but this is mainly accounted for by differences in note maxima between emotions, not note minima. During improvisation, jazz musicians use higher tones to show happiness, but do not, conversely, use lower tones to show negative emotions.</p>
                  <fig id="pone-0105144-g007" orientation="portrait" position="float">
                    <object-id pub-id-type="doi">10.1371/journal.pone.0105144.g007</object-id>
                    <label>Figure 7</label>
                    <caption>
                      <title>Significant differences between note maximums but not note minimums in both one-handed and two-handed improvisations between all emotions.</title>
                    </caption>
                    <graphic xlink:href="pone.0105144.g007"/>
                  </fig>
                </sec>
                <sec id="s3a6">
                  <title>Mode</title>
                  <p>Key (tonal center) and mode were calculated using the Krumhansl &amp; Schmuckler (KâS) key-finding algorithm, which uses the pitch class distribution of a piece (weighted according to duration) to return a key profile for the piece <xref rid="pone.0105144-Eerola2" ref-type="bibr">[36]</xref>, <xref rid="pone.0105144-Krumhansl2" ref-type="bibr">[39]</xref>â<xref rid="pone.0105144-Krumhansl4" ref-type="bibr">[41]</xref>. We used the KâS key finding algorithm to find the best fit for each entire 50 s improvisation.</p>
                  <p>There was a large amount of variation within each Emotion category (combined across one-handed and two-handed improvisations); 34.52% of the Negative improvisations were in a major key, and conversely, 28.57% of the Positive improvisations were in a minor key. The Ambiguous and Control improvisations showed almost identical proportions of major (58.33% and 61.9%, respectively) to minor (<xref ref-type="fig" rid="pone-0105144-g008">Figure 8</xref>). We conducted a follow-up analysis to determine whether there were any velocity, range or note density differences between major and minor improvisations within any given emotional category or the control. A two-tailed independent t-test was used to compare the ranges, velocities and note densities of major to minor improvisations within each Emotion. There were no significant (<italic>p</italic>&gt;.01) differences between any note densities, key press velocities or ranges of major vs. minor improvisations within any emotion or the Control. This result shows that there was not a significant interaction between mode and other musical variables.</p>
                  <fig id="pone-0105144-g008" orientation="portrait" position="float">
                    <object-id pub-id-type="doi">10.1371/journal.pone.0105144.g008</object-id>
                    <label>Figure 8</label>
                    <caption>
                      <title>Differences in proportion of major to minor keys in Positive, Ambiguous and Negative improvisations.</title>
                    </caption>
                    <graphic xlink:href="pone.0105144.g008"/>
                  </fig>
                </sec>
                <sec id="s3a7">
                  <title>Key</title>
                  <p>With respect to key, the overall tendency was to use A, C, F and G, each in both major and minor, and to use keys with sharps for positive improvisations and keys with flats for negative improvisations (<xref ref-type="fig" rid="pone-0105144-g009">Figure 9</xref>).</p>
                  <fig id="pone-0105144-g009" orientation="portrait" position="float">
                    <object-id pub-id-type="doi">10.1371/journal.pone.0105144.g009</object-id>
                    <label>Figure 9</label>
                    <caption>
                      <title>Histograms of keys used during improvisations, separated by emotion.</title>
                    </caption>
                    <graphic xlink:href="pone.0105144.g009"/>
                  </fig>
                </sec>
              </sec>
              <sec id="s3b">
                <title>Listening Survey</title>
                <p>The listening survey showed that subjects perceived a difference between Positive and Negative improvisations and between Positive and Ambiguous improvisations, however they perceived Ambiguous and Negative improvisations as similar. We found that musical experience did not influence subjectsâ emotional evaluations. Furthermore, improvisations made in response to cartoon and photographs were equally emotionally convincing, and emotional evaluations were unaffected by whether the improvisations were monophonic or polyphonic (performed with one hand or two hands). At least within the realm of piano performance, single melodic lines appear to be as emotionally convincing as polyphonic performances â the musical features present in a monophonic composition appear to be sufficient to effectively convey an emotion.</p>
                <p>Results were coded using a nine point scale rubric, with 0â=âthe most Negative, 4.5â=âAmbiguous, 9â=âthe most positive. We calculated a four-way ANOVA on the ratings with within-subject factors Musical Experience (musician, non-musicians), Emotion (Negative, Ambiguous, Positive), Type (Cartoon, Actress) and Hands (one-handed and two-handed improvisations). Because no significant effects (<italic>p</italic>&gt;.05) of Musical Experience, Type, and Hands were found, the ANOVA was rerun as a one-way ANOVA with the within-subject factor Emotion (Negative, Ambiguous, Positive). A significant main effect of Emotion [<italic>F</italic>(1, 2)â=â45.12, <italic>p</italic>&lt;.001] was observed. The ratings of Positive improvisations were significantly greater (<italic>p</italic>&lt;.001) than ratings for Negative and Ambiguous improvisations. There was no statistical difference (<italic>p</italic>&gt;.05) between Negative and Ambiguous ratings. The mean ratings: Negative improvisationsâ=â3.46Â±1.61 (s.d.); Ambiguous improvisationsâ=â3.51Â±1.71 (s.d.); Positive Improvisationsâ=â5.63Â±1.63 (s.d.). The range of responses for Negative improvisations was 7 points, 6.5 points for Ambiguous improvisations and 7.5 for Positive improvisations (i.e. some Negative improvisations were rated as very positive, and vice versa).</p>
              </sec>
            </sec>
            <sec id="s4">
              <title>Discussion</title>
              <p>Music is viewed as an effective means of expressing emotions, yet there is a large amount of variability in how music can express emotions. Unlike language, where words describing emotions have distinct, agreed upon meanings, the emotional content of music is transient and non-discrete. Multiple emotions can be evoked within a single musical passage. It has been posited that the power of music derives precisely from this fluidity <xref rid="pone.0105144-Cross1" ref-type="bibr">[33]</xref>. While this indeterminacy would make propositional language unfeasible, ambiguity of meaning makes music more powerful by allowing each person to ascribe their own meaning to pieces of music <xref rid="pone.0105144-Cross1" ref-type="bibr">[33]</xref>. The fact that a broad range of musical features can express a given emotion supports the idea that music can express the same emotion in different ways. Individual features of music can be more strongly associated with specific emotional valences, but independently, a single musical feature cannot explain the musical expression of emotions.</p>
              <p>The objective of this study was to explore the range of musical features that jazz pianists use to express emotions while improvising. Our experimental design allowed us to examine emotional music performance in an artistically and ecologically valid setting, and we found that the emotional cue and subsequent emotional intent of the performers greatly influenced all measured musical elements of their performance. Statistical differences were observed in every musical measure between Positive and Negative improvisations. The differences between Ambiguous and Negative improvisations were not as pronounced. There were no statistical differences between Ambiguous and Negative improvisation note densities, ranges, or velocities. Percent of Legato/Staccato notes only differed by approximately 7% between Ambiguous and Negative improvisations. However, almost twice as many Ambiguous improvisations were major compared to Negative improvisations, and the duration distributions for Ambiguous and Negative improvisations were significantly different. Further statistical tests revealed that note density, key press velocity, and note range varied independently of mode - improvisations that did <italic>not</italic> conform to the conventional mode (e.g., a Positive piece played in a minor key), did not show exaggerated emotional effects across other parameters (e.g., faster tempo, higher velocity, more staccatos). Performers did not âcompensateâ for their choice of mode using other musical measures.</p>
              <p>The musical similarities between Ambiguous and Negative improvisations are particularly striking given that the emotional ratings for the Ambiguous and Negative stimuli were significantly different. In an informal post-study survey, four pianists independently stated that the Ambiguous stimuli made them feel âanticipationâ, which some claimed they had expressed through a lower range, monotone textures, and dissonance. Others commented that the Ambiguous faces were more difficult to musically âmatchâ because they were simply ânot emotionalâ. One pianist stated that the âguy with the line mouthâ (referring to the Ambiguous cartoon) âdidnât inspire anythingâ. These statements provide an indication of why Ambiguous and Negative improvisations may have shared certain characteristics. Pianistsâ anticipation, uncertainty or even lack of emotional response to the Ambiguous stimuli contributed to their use of narrow range, slow tempo, and low volume. Ambiguity is, by definition, open to many different interpretations. Perhaps cueing the pianists to improvise something ambiguous caused them to be uncertain of what to do.</p>
              <p>It is important to note that, even when statistically similar to Negative improvisations, the mean values for all Ambiguous improvisation measures (other than the mean one-handed note maximum) fell between the means for Negative and Positive improvisations. Furthermore, pianistsâ choice of mode during Ambiguous improvisations was almost at chance level (41.67% minor, 58.34% major), compared to Negative improvisations, where 65.48% of improvisations were in minor keys. Ambiguous trials were more similar to Positive trials than Negative trials with respect to mode. This may be further indication that gross similarities between certain musical features of Ambiguous and Negative improvisations are not necessarily an indication that the pianistsâ Ambiguous improvisations were tending towards expressing negative emotions. The pianists may have simply been less effortful and expressive during Ambiguous trials. Using less physical effort could have resulted in lower volume, smaller range and fewer notes, but would have had no effect on choice of mode.</p>
              <p>Regardless of what emotion they were trying to convey, the pianists used a wide range of musical features. This may be attributed to the fact that the pianists were spontaneously producing emotionally guided music rather than composing (pre-planning what they would perform). In the post-study interview, all fourteen subjects independently stated that they were using minor keys during Negative improvisation trials and major keys during Positive improvisation trials. These responses are consistent with the Western Classical music convention that major keys are happy and minor keys are sad <xref rid="pone.0105144-Kastner1" ref-type="bibr">[13]</xref>, <xref rid="pone.0105144-Juslin3" ref-type="bibr">[42]</xref>. Upon quantitative analysis, we discovered that this was not fully the case. While a majority of Negative improvisations were in minor keys and the majority of Positive improvisations were in major keys, a large percentage of Negative improvisations were in major (34.52%) and Positive improvisations were in minor keys (28.57%). Therefore, during approximately â of the Positive or Negative improvisations, pianistâs behavior did not match their verbal reports of what they thought they did during the experiment. If pianists had been given more time to plan their improvisations (taken time to write out compositions, for example), their use of musical features may have been less varied, as they might have more closely adhered to specific Western Classical music conventions for expressing emotions.</p>
              <p>We also believe that our use of visual cues impacted the range of musical features used within each emotional category. We decided to use visual cues in order to eliminate all external verbal labels of emotion from our study, as linguistic labels can bias emotion perception and report <xref rid="pone.0105144-Barrett1" ref-type="bibr">[31]</xref>, <xref rid="pone.0105144-Mesquita1" ref-type="bibr">[32]</xref>. We observed a significant main effect of Type (Actress A, Actress B, Cartoon) as well as Emotion (Negative, Ambiguous, Positive) on the emotional ratings of stimuli in the listening survey. Subjects perceived the two Actresses and Cartoons as portraying slightly different emotional valences (though there was still a main effect of Emotion). There was not a similar effect of Type on musical improvisations. If the musicians were trying to precisely âmatchâ the emotion represented in the cues, it is likely that there would have been differences between the improvisations in response to each actress and cartoon. This did not occur. Instead, it seems as if the pianists used the images as more general, rather than specific, emotional cues, resulting in a wider range of musical expression.</p>
              <p>Furthermore, the pianists were instructed to make their improvisation as a whole express the emotion they perceived in the stimuli. Improvisation is the unfolding of multiple events over time, and emotion expressed in music is an emergent property of the entire piece of music. This task left significant room for the pianists to musically and emotionally fluctuate, as long as the overall emotion expressed matches that of the stimuli. Musical emotions may not have a high level of specificity and regularity. While faces can convey a single emotion (âHappyâ, âSadâ, etc.), or compound emotions such as âHappily surprisedâ or âFearfully angryâ <xref rid="pone.0105144-Du1" ref-type="bibr">[43]</xref>, perhaps music primarily expresses multiple, intermixed emotions rather than isolated emotions. This could help account for musicâs universal appeal, and the ability for cross-cultural recognition of musical emotions <xref rid="pone.0105144-Balkwill1" ref-type="bibr">[44]</xref>. It appears that there are general methods to express certain emotional categories, however a large amount of freedom exists within those general approaches.</p>
              <p>The wide distribution of musical features likely accounts for the large range of responses observed in the listening survey. The listening survey revealed that subjects were generally able to discern Positive improvisations from Ambiguous and Negative improvisations, but that the difference between Ambiguous and Negative improvisations was not as clear. Previous studies have found that mode is a particularly strong predictor of emotional perception <xref rid="pone.0105144-Gagnon1" ref-type="bibr">[10]</xref>, <xref rid="pone.0105144-Bowling1" ref-type="bibr">[45]</xref>, yet the mode differences between Ambiguous and Negative improvisations were clearly not sufficient to change peopleâs ratings of the two different emotional categories. This suggests that features such as tempo and articulation may be more important than features such as key when it comes to making emotional judgments.</p>
              <p>Our findings demonstrate that a strict correspondence between emotions and musical features (i.e., Positive-major, Negative-minor) does not explain the diversity of musical expression of emotion. Instead, our results support the hypothesis that there is a high amount of musical variety within each emotional category. Rather than using a simple set of features to express emotions, the pianists used many permutations of features in order to express different emotions. While this high degree of structural variation in music may be particularly pronounced during spontaneous improvisation in comparison to other forms of musical expression, we believe that this enormous variety is directly related to the broad capacity of music to provide compelling, vivid and fluid emotional experiences that are often difficult to describe.</p>
            </sec>
            <sec sec-type="supplementary-material" id="s5">
              <title>Supporting Information</title>
              <supplementary-material content-type="local-data" id="pone.0105144.s001">
                <label>File S1</label>
                <caption>
                  <p>
                    <bold>Instructions for Pianists.</bold>
                  </p>
                  <p>(EPS)</p>
                </caption>
                <media xlink:href="pone.0105144.s001.eps">
                  <caption>
                    <p>Click here for additional data file.</p>
                  </caption>
                </media>
              </supplementary-material>
              <supplementary-material content-type="local-data" id="pone.0105144.s002">
                <label>File S2</label>
                <caption>
                  <p>
                    <bold>Public Date.</bold>
                  </p>
                  <p>(ZIP)</p>
                </caption>
                <media xlink:href="pone.0105144.s002.zip">
                  <caption>
                    <p>Click here for additional data file.</p>
                  </caption>
                </media>
              </supplementary-material>
            </sec>
          </body>
          <back>
            <ref-list>
              <title>References</title>
              <ref id="pone.0105144-Eerola1">
                <label>1</label>
                <mixed-citation publication-type="journal"><name><surname>Eerola</surname><given-names>T</given-names></name>, <name><surname>Vuoskoski</surname><given-names>JK</given-names></name> (<year>2013</year>) <article-title>A Review of Music and Emotion Studies: Approaches, Emotion Models, and Stimuli</article-title>. <source>Music Perception</source>
<volume>30</volume>: <fpage>307</fpage>â<lpage>340</lpage>.</mixed-citation>
              </ref>
              <ref id="pone.0105144-Spencer1">
                <label>2</label>
                <mixed-citation publication-type="other">Spencer H (1928) Essays on education and kindred subjects. New York: E.P. Dutton &amp; Co. xxi, 330 p. p.</mixed-citation>
              </ref>
              <ref id="pone.0105144-Ekman1">
                <label>3</label>
                <mixed-citation publication-type="journal"><name><surname>Ekman</surname><given-names>P</given-names></name> (<year>1992</year>) <article-title>Facial expressions of emotion: an old controversy and new findings</article-title>. <source>Philos Trans</source>
<volume>335</volume>: <fpage>63</fpage>â<lpage>69</lpage>.</mixed-citation>
              </ref>
              <ref id="pone.0105144-Panksepp1">
                <label>4</label>
                <mixed-citation publication-type="other">Panksepp J (1998) Affective neuroscience: the foundations of human and animal emotions. New York: Oxford University Press. xii, 466 p. p.</mixed-citation>
              </ref>
              <ref id="pone.0105144-Krumhansl1">
                <label>5</label>
                <mixed-citation publication-type="journal"><name><surname>Krumhansl</surname><given-names>CL</given-names></name> (<year>1998</year>) <article-title>Topic in music: An empirical study of memorability, openness, and emotion in Mozartâs Quintet in C major and Beethovenâs String Quartet in A minor</article-title>. <source>Music Perception</source>
<volume>16</volume>: <fpage>119</fpage>â<lpage>134</lpage>.</mixed-citation>
              </ref>
              <ref id="pone.0105144-McAdams1">
                <label>6</label>
                <mixed-citation publication-type="journal"><name><surname>McAdams</surname><given-names>S</given-names></name>, <name><surname>Vinew</surname><given-names>B</given-names></name>, <name><surname>Vieillard</surname><given-names>S</given-names></name>, <name><surname>Smith</surname><given-names>B</given-names></name>, <name><surname>Reynolds</surname><given-names>R</given-names></name> (<year>2004</year>) <article-title>Influences of large-scale form on continuous ratings in response to a contemporary piece in a live concert setting</article-title>. <source>Music Perception</source>
<volume>22</volume>: <fpage>265</fpage>â<lpage>296</lpage>.</mixed-citation>
              </ref>
              <ref id="pone.0105144-Russell1">
                <label>7</label>
                <mixed-citation publication-type="journal"><name><surname>Russell</surname><given-names>JA</given-names></name> (<year>1980</year>) <article-title>A Circumplex Model of Affect</article-title>. <source>Journal of Personality and Social Psychology</source>
<volume>39</volume>: <fpage>1161</fpage>â<lpage>1178</lpage>.</mixed-citation>
              </ref>
              <ref id="pone.0105144-Costa1">
                <label>8</label>
                <mixed-citation publication-type="journal"><name><surname>Costa</surname><given-names>M</given-names></name>, <name><surname>Fine</surname><given-names>P</given-names></name>, <name><surname>Bitti</surname><given-names>PER</given-names></name> (<year>2004</year>) <article-title>Interval Distributions, Mode, and Tonal Strength of Melodies as Predictors of Perceived Emotion</article-title>. <source>Music Perception: An Interdisciplinary Journal</source>
<volume>22</volume>: <fpage>1</fpage>â<lpage>14</lpage>.</mixed-citation>
              </ref>
              <ref id="pone.0105144-DallaBella1">
                <label>9</label>
                <mixed-citation publication-type="journal"><name><surname>Dalla Bella</surname><given-names>S</given-names></name>, <name><surname>Peretz</surname><given-names>I</given-names></name>, <name><surname>Rousseau</surname><given-names>L</given-names></name>, <name><surname>Gosselin</surname><given-names>N</given-names></name> (<year>2001</year>) <article-title>A developmental study of the affective value of tempo and mode in music</article-title>. <source>Cognition</source>
<volume>80</volume>: <fpage>B1</fpage>â<lpage>B10</lpage>.<pub-id pub-id-type="pmid">11274986</pub-id></mixed-citation>
              </ref>
              <ref id="pone.0105144-Gagnon1">
                <label>10</label>
                <mixed-citation publication-type="journal"><name><surname>Gagnon</surname><given-names>L</given-names></name>, <name><surname>Peretz</surname><given-names>I</given-names></name> (<year>2003</year>) <article-title>Mode and tempo relative contributions to âhappy-sadâ judgements in equitone melodies</article-title>. <source>Cognition &amp; Emotion</source>
<volume>17</volume>: <fpage>25</fpage>â<lpage>40</lpage>.</mixed-citation>
              </ref>
              <ref id="pone.0105144-Halpern1">
                <label>11</label>
                <mixed-citation publication-type="journal"><name><surname>Halpern</surname><given-names>AR</given-names></name>, <name><surname>Martin</surname><given-names>JS</given-names></name>, <name><surname>Reed</surname><given-names>TD</given-names></name> (<year>2008</year>) <article-title>An ERP Study of Major-Minor Classification in Melodies</article-title>. <source>Music Perception: An Interdisciplinary Journal</source>
<volume>25</volume>: <fpage>181</fpage>â<lpage>191</lpage>.</mixed-citation>
              </ref>
              <ref id="pone.0105144-Hevner1">
                <label>12</label>
                <mixed-citation publication-type="journal"><name><surname>Hevner</surname><given-names>K</given-names></name> (<year>1935</year>) <article-title>The affective character of the major and minor modes in music</article-title>. <source>American Journal of Psychology</source>
<volume>47</volume>: <fpage>103</fpage>â<lpage>118</lpage>.</mixed-citation>
              </ref>
              <ref id="pone.0105144-Kastner1">
                <label>13</label>
                <mixed-citation publication-type="journal"><name><surname>Kastner</surname><given-names>MP</given-names></name>, <name><surname>Crowder</surname><given-names>RG</given-names></name> (<year>1990</year>) <article-title>Perception of the Major/Minor Distinction: IV. Emotional Connotations in Young Children</article-title>. <source>Music Perception: An Interdisciplinary Journal</source>
<volume>8</volume>: <fpage>189</fpage>â<lpage>201</lpage>.</mixed-citation>
              </ref>
              <ref id="pone.0105144-Denckla1">
                <label>14</label>
                <mixed-citation publication-type="other">Denckla BF (1997) Dynamic Intonation for Synthesizer Performance Cambridge: Massachusetts Institute of Technology.</mixed-citation>
              </ref>
              <ref id="pone.0105144-Steblin1">
                <label>15</label>
                <mixed-citation publication-type="other">Steblin R (2002) A history of key characteristics in the eighteenth and early nineteenth centuries. Rochester, NY: University of Rochester Press. xiv, 408 p. p.</mixed-citation>
              </ref>
              <ref id="pone.0105144-Costanzo1">
                <label>16</label>
                <mixed-citation publication-type="journal"><name><surname>Costanzo</surname><given-names>FS</given-names></name>, <name><surname>Markel</surname><given-names>NN</given-names></name>, <name><surname>Constanzo</surname><given-names>PR</given-names></name> (<year>1969</year>) <article-title>Voice quality profile and perceived emotion</article-title>. <source>Journal of Counseling Psychology</source>
<volume>16</volume>: <fpage>267</fpage>â<lpage>270</lpage>.</mixed-citation>
              </ref>
              <ref id="pone.0105144-Huron1">
                <label>17</label>
                <mixed-citation publication-type="other">Huron D, Chordia P, Yim G (2010) The effect of pitch exposure on sadness judgments: An association between sadness and lower-than-normal pitch. ICMPC Conference, Seattle. Washington.</mixed-citation>
              </ref>
              <ref id="pone.0105144-Paul1">
                <label>18</label>
                <mixed-citation publication-type="journal"><name><surname>Paul</surname><given-names>B</given-names></name>, <name><surname>Huron</surname><given-names>D</given-names></name> (<year>2010</year>) <article-title>An Association between Breaking Voice and Grief-related Lyrics in Country Music</article-title>. <source>Empirical Musicology Review</source>
<volume>5</volume>: <fpage>27</fpage>.</mixed-citation>
              </ref>
              <ref id="pone.0105144-Scherer1">
                <label>19</label>
                <mixed-citation publication-type="other">Scherer KR, Scherer U (1981) Speech Behavior and personality. In: J.K. Darby J, editor. Speech Evaluation in Psychiatry. New York: Grune &amp; Stratton. 115â135.</mixed-citation>
              </ref>
              <ref id="pone.0105144-Juslin1">
                <label>20</label>
                <mixed-citation publication-type="journal"><name><surname>Juslin</surname><given-names>PN</given-names></name> (<year>2000</year>) <article-title>Cue utilization in communication of emotion in music performance: Relating performance to perception</article-title>. <source>Journal of Experimental Psychology-Human Perception and Performance</source>
<volume>26</volume>: <fpage>1797</fpage>â<lpage>1812</lpage>.<pub-id pub-id-type="pmid">11129375</pub-id></mixed-citation>
              </ref>
              <ref id="pone.0105144-Juslin2">
                <label>21</label>
                <mixed-citation publication-type="journal"><name><surname>Juslin</surname><given-names>PN</given-names></name>, <name><surname>Laukka</surname><given-names>P</given-names></name> (<year>2003</year>) <article-title>Emotional expression in speech and music - Evidence of cross-modal similarities</article-title>. <source>Emotions inside Out</source>
<volume>1000</volume>: <fpage>279</fpage>â<lpage>282</lpage>.</mixed-citation>
              </ref>
              <ref id="pone.0105144-Kamenetsky1">
                <label>22</label>
                <mixed-citation publication-type="journal"><name><surname>Kamenetsky</surname><given-names>SB</given-names></name>, <name><surname>Hill</surname><given-names>DS</given-names></name>, <name><surname>Trehub</surname><given-names>SE</given-names></name> (<year>1997</year>) <article-title>Effect of Tempo and Dynamics on the Perception of Emotion in Music</article-title>. <source>Psychology of Music</source>
<volume>25</volume>: <fpage>149</fpage>â<lpage>160</lpage>.</mixed-citation>
              </ref>
              <ref id="pone.0105144-Laukka1">
                <label>23</label>
                <mixed-citation publication-type="journal"><name><surname>Laukka</surname><given-names>P</given-names></name>, <name><surname>Gabrielsson</surname><given-names>A</given-names></name>, <name><surname>Juslin</surname><given-names>PN</given-names></name> (<year>2000</year>) <article-title>Impact of intended emotion intensity on cue utilization and decoding accuracy in vocal expression of emotion</article-title>. <source>International Journal of Psychology</source>
<volume>35</volume>: <fpage>288</fpage>â<lpage>288</lpage>.</mixed-citation>
              </ref>
              <ref id="pone.0105144-Rigg1">
                <label>24</label>
                <mixed-citation publication-type="journal"><name><surname>Rigg</surname><given-names>MG</given-names></name> (<year>1940</year>) <article-title>Speed as a determiner of musical mood</article-title>. <source>Journal of Experimental Psychology</source>
<volume>27</volume>: <fpage>566</fpage>â<lpage>571</lpage>.</mixed-citation>
              </ref>
              <ref id="pone.0105144-Livingstone1">
                <label>25</label>
                <mixed-citation publication-type="journal"><name><surname>Livingstone</surname><given-names>SR</given-names></name>, <name><surname>Muhlberger</surname><given-names>R</given-names></name>, <name><surname>Brown</surname><given-names>AR</given-names></name>, <name><surname>Thompson</surname><given-names>WF</given-names></name> (<year>2010</year>) <article-title>Changing Musical Emotion: A Computational Rule System for Modifying Score and Performance</article-title>. <source>Computer Music Journal</source>
<volume>34</volume>: <fpage>41</fpage>â<lpage>64</lpage>.</mixed-citation>
              </ref>
              <ref id="pone.0105144-Baraldi1">
                <label>26</label>
                <mixed-citation publication-type="journal"><name><surname>Baraldi</surname><given-names>FB</given-names></name>, <name><surname>De Poli</surname><given-names>G</given-names></name>, <name><surname>Roda</surname><given-names>A</given-names></name> (<year>2006</year>) <article-title>Communicating expressive intentions with a single piano note</article-title>. <source>Journal of New Music Research</source>
<volume>35</volume>: <fpage>197</fpage>â<lpage>210</lpage>.</mixed-citation>
              </ref>
              <ref id="pone.0105144-Gabrielsson1">
                <label>27</label>
                <mixed-citation publication-type="journal"><name><surname>Gabrielsson</surname><given-names>A</given-names></name>, <name><surname>Juslin</surname><given-names>PN</given-names></name> (<year>1996</year>) <article-title>Emotional Expression in Music Performance: Between the Performerâs Intention and the Listenerâs Experience</article-title>. <source>Psychology of Music</source>
<volume>24</volume>: <fpage>68</fpage>â<lpage>91</lpage>.</mixed-citation>
              </ref>
              <ref id="pone.0105144-Gabrielsson2">
                <label>28</label>
                <mixed-citation publication-type="other">Gabrielsson A, Lindstrom E (1995) Emotional Expression in Synthesizer and Stentograph Performance. Psychomusicology 14.</mixed-citation>
              </ref>
              <ref id="pone.0105144-Timmers1">
                <label>29</label>
                <mixed-citation publication-type="journal"><name><surname>Timmers</surname><given-names>R</given-names></name>, <name><surname>Ashley</surname><given-names>R</given-names></name> (<year>2007</year>) <article-title>Emotional ornamentation in performances of a Handel sonata</article-title>. <source>Music Perception</source>
<volume>25</volume>: <fpage>117</fpage>â<lpage>134</lpage>.</mixed-citation>
              </ref>
              <ref id="pone.0105144-Laukka2">
                <label>30</label>
                <mixed-citation publication-type="journal"><name><surname>Laukka</surname><given-names>P</given-names></name>, <name><surname>Gabrielsson</surname><given-names>A</given-names></name> (<year>2000</year>) <article-title>Emotional Expression in Drumming Performance</article-title>. <source>Psychology of Music</source>
<volume>28</volume>: <fpage>181</fpage>â<lpage>189</lpage>.</mixed-citation>
              </ref>
              <ref id="pone.0105144-Barrett1">
                <label>31</label>
                <mixed-citation publication-type="journal"><name><surname>Barrett</surname><given-names>LF</given-names></name>, <name><surname>Lindquist</surname><given-names>KA</given-names></name>, <name><surname>Gendron</surname><given-names>M</given-names></name> (<year>2007</year>) <article-title>Language as context for the perception of emotion</article-title>. <source>Trends Cogn Sci</source>
<volume>11</volume>: <fpage>327</fpage>â<lpage>332</lpage>.<pub-id pub-id-type="pmid">17625952</pub-id></mixed-citation>
              </ref>
              <ref id="pone.0105144-Mesquita1">
                <label>32</label>
                <mixed-citation publication-type="journal"><name><surname>Mesquita</surname><given-names>B</given-names></name>, <name><surname>Frijda</surname><given-names>NH</given-names></name> (<year>1992</year>) <article-title>Cultural Variations in Emotions: A Review</article-title>. <source>Psychological Bulletin</source>
<volume>112</volume>: <fpage>179</fpage>â<lpage>204</lpage>.<pub-id pub-id-type="pmid">1454891</pub-id></mixed-citation>
              </ref>
              <ref id="pone.0105144-Cross1">
                <label>33</label>
                <mixed-citation publication-type="other">Cross I (2008) Musicality and the human capacity for culture. Musicae Scientiae: 147â167.</mixed-citation>
              </ref>
              <ref id="pone.0105144-Huron2">
                <label>34</label>
                <mixed-citation publication-type="other">Huron D (2006) Sweet Anticipation: Music and the Psychology of Expectation. Cambridge, MA: MIT Press.</mixed-citation>
              </ref>
              <ref id="pone.0105144-Chapin1">
                <label>35</label>
                <mixed-citation publication-type="other">Chapin H, Jantzen K, Kelso JAS, Steinberg F, Large E (2010) Dynamic Emotional and Neural Responses to Music Depend on Performance Expression and Listener Experience. Plos One 5.</mixed-citation>
              </ref>
              <ref id="pone.0105144-Eerola2">
                <label>36</label>
                <mixed-citation publication-type="other">Eerola T, Toiviainen P (2004) MIDI Toolbox: Matlab Tools for Research. KopijyvÃ¤, JyvÃ¤skylÃ¤, Finland: University of JyvÃ¤skylÃ¤.</mixed-citation>
              </ref>
              <ref id="pone.0105144-Dexter1">
                <label>37</label>
                <mixed-citation publication-type="journal"><name><surname>Dexter</surname><given-names>F</given-names></name>, <name><surname>Chestnut</surname><given-names>DH</given-names></name> (<year>1995</year>) <article-title>Analysis of statistical tests to compare visual analog scale measurements among groups</article-title>. <source>Anesthesiology</source>
<volume>82</volume>: <fpage>896</fpage>â<lpage>902</lpage>.<pub-id pub-id-type="pmid">7717561</pub-id></mixed-citation>
              </ref>
              <ref id="pone.0105144-Goebl1">
                <label>38</label>
                <mixed-citation publication-type="journal"><name><surname>Goebl</surname><given-names>W</given-names></name>, <name><surname>Bresin</surname><given-names>R</given-names></name> (<year>2003</year>) <article-title>Measurement and reproduction accuracy of computer-controlled grand pianos</article-title>. <source>Journal of the Acoustical Society of America</source>
<volume>114</volume>: <fpage>2273</fpage>â<lpage>2283</lpage>.<pub-id pub-id-type="pmid">14587624</pub-id></mixed-citation>
              </ref>
              <ref id="pone.0105144-Krumhansl2">
                <label>39</label>
                <mixed-citation publication-type="other">Krumhansl CL, Schmuckler MA (1986) Key-Finding in music: An algorithm based on pattern matching on tonal hierarchies. Mathematical Psychology Meeting.</mixed-citation>
              </ref>
              <ref id="pone.0105144-Krumhansl3">
                <label>40</label>
                <mixed-citation publication-type="journal"><name><surname>Krumhansl</surname><given-names>CL</given-names></name> (<year>1990</year>) <article-title>Tonal Hierarchies and Rare Intervals in Music Cognition</article-title>. <source>Music Perception</source>
<volume>7</volume>: <fpage>309</fpage>â<lpage>324</lpage>.</mixed-citation>
              </ref>
              <ref id="pone.0105144-Krumhansl4">
                <label>41</label>
                <mixed-citation publication-type="journal"><name><surname>Krumhansl</surname><given-names>CL</given-names></name>, <name><surname>Kessler</surname><given-names>EJ</given-names></name> (<year>1982</year>) <article-title>Tracing the Dynamic Changes in Perceived Tonal Organization in a Spatial Representation of Musical Keys</article-title>. <source>Psychological Review</source>
<volume>89</volume>: <fpage>334</fpage>â<lpage>368</lpage>.<pub-id pub-id-type="pmid">7134332</pub-id></mixed-citation>
              </ref>
              <ref id="pone.0105144-Juslin3">
                <label>42</label>
                <mixed-citation publication-type="journal"><name><surname>Juslin</surname><given-names>PN</given-names></name>, <name><surname>Laukka</surname><given-names>P</given-names></name> (<year>2004</year>) <article-title>Expression, perception, and induction of musical emotions: A review and a questionnaire study of everyday listening</article-title>. <source>Journal of New Music Research</source>
<volume>33</volume>: <fpage>217</fpage>â<lpage>238</lpage>.</mixed-citation>
              </ref>
              <ref id="pone.0105144-Du1">
                <label>43</label>
                <mixed-citation publication-type="other">Du S, Tao Y, Martinez AM (2014) Compound facial expressions of emotion. Proceedings of the National Academy of Sciences.</mixed-citation>
              </ref>
              <ref id="pone.0105144-Balkwill1">
                <label>44</label>
                <mixed-citation publication-type="journal"><name><surname>Balkwill</surname><given-names>L</given-names></name>, <name><surname>Thompson</surname><given-names>WF</given-names></name> (<year>1999</year>) <article-title>A Cross-Cultural Investigation of the Perception of Emotion in Music: Pyschophysical and Cultural Cues</article-title>. <source>Music Perception</source>
<volume>17</volume>: <fpage>43</fpage>â<lpage>64</lpage>.</mixed-citation>
              </ref>
              <ref id="pone.0105144-Bowling1">
                <label>45</label>
                <mixed-citation publication-type="journal"><name><surname>Bowling</surname><given-names>DL</given-names></name> (<year>2013</year>) <article-title>A vocal basis for the affective character of musical mode in melody</article-title>. <source>Front Psychol</source>
<volume>4</volume>: <fpage>464</fpage>.<pub-id pub-id-type="pmid">23914179</pub-id></mixed-citation>
              </ref>
            </ref-list>
          </back>
        </article>
      </metadata>
    </record>
  </GetRecord>
</OAI-PMH>
