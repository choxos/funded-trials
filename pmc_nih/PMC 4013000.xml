<?xml version="1.0" encoding="UTF-8"?>
<OAI-PMH xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/ http://www.openarchives.org/OAI/2.0/OAI-PMH.xsd">
  <responseDate>2024-05-03T06:15:07Z</responseDate>
  <request verb="GetRecord" identifier="oai:pubmedcentral.nih.gov:4013000" metadataPrefix="pmc">https:/www.ncbi.nlm.nih.gov/pmc/oai/oai.cgi</request>
  <GetRecord>
    <record>
      <header>
        <identifier>oai:pubmedcentral.nih.gov:4013000</identifier>
        <datestamp>2014-05-09</datestamp>
        <setSpec>plosone</setSpec>
        <setSpec>pmc-open</setSpec>
      </header>
      <metadata>
        <article xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" xsi:schemaLocation="https://jats.nlm.nih.gov/ns/archiving/1.3/ https://jats.nlm.nih.gov/archiving/1.3/xsd/JATS-archivearticle1-3.xsd" article-type="research-article">
          <front>
            <journal-meta>
              <journal-id journal-id-type="nlm-ta">PLoS One</journal-id>
              <journal-id journal-id-type="iso-abbrev">PLoS ONE</journal-id>
              <journal-id journal-id-type="publisher-id">plos</journal-id>
              <journal-id journal-id-type="pmc">plosone</journal-id>
              <journal-title-group>
                <journal-title>PLoS ONE</journal-title>
              </journal-title-group>
              <issn pub-type="epub">1932-6203</issn>
              <publisher>
                <publisher-name>Public Library of Science</publisher-name>
                <publisher-loc>San Francisco, USA</publisher-loc>
              </publisher>
            </journal-meta>
            <article-meta>
              <article-id pub-id-type="accession">PMC4013000</article-id>
              <article-id pub-id-type="pmcid">PMC4013000</article-id>
              <article-id pub-id-type="pmc-uid">4013000</article-id>
              <article-id pub-id-type="pmid">24804788</article-id>
              <article-id pub-id-type="pmid">24804788</article-id>
              <article-id pub-id-type="publisher-id">PONE-D-13-43279</article-id>
              <article-id pub-id-type="doi">10.1371/journal.pone.0096383</article-id>
              <article-categories>
                <subj-group subj-group-type="heading">
                  <subject>Research Article</subject>
                </subj-group>
                <subj-group subj-group-type="Discipline-v2">
                  <subject>Biology and Life Sciences</subject>
                  <subj-group>
                    <subject>Neuroscience</subject>
                    <subj-group>
                      <subject>Cognitive Science</subject>
                      <subj-group>
                        <subject>Cognitive Psychology</subject>
                        <subj-group>
                          <subject>Learning</subject>
                        </subj-group>
                      </subj-group>
                    </subj-group>
                    <subj-group>
                      <subject>Sensory Perception</subject>
                      <subj-group>
                        <subject>Psychophysics</subject>
                      </subj-group>
                    </subj-group>
                    <subj-group>
                      <subject>Sensory Systems</subject>
                      <subj-group>
                        <subject>Visual System</subject>
                      </subj-group>
                    </subj-group>
                  </subj-group>
                  <subj-group>
                    <subject>Psychology</subject>
                    <subj-group>
                      <subject>Behavior</subject>
                      <subj-group>
                        <subject>Human Performance</subject>
                      </subj-group>
                    </subj-group>
                    <subj-group>
                      <subject>Experimental Psychology</subject>
                    </subj-group>
                  </subj-group>
                </subj-group>
                <subj-group subj-group-type="Discipline-v2">
                  <subject>Physical Sciences</subject>
                  <subj-group>
                    <subject>Mathematics</subject>
                    <subj-group>
                      <subject>Statistics (Mathematics)</subject>
                      <subj-group>
                        <subject>Confidence Intervals</subject>
                      </subj-group>
                    </subj-group>
                  </subj-group>
                </subj-group>
              </article-categories>
              <title-group>
                <article-title>Cue-Recruitment for Extrinsic Signals after Training with Low Information Stimuli</article-title>
                <alt-title alt-title-type="running-head">Extrinsic Cue Recruitment with Low-Info Training</alt-title>
              </title-group>
              <contrib-group>
                <contrib contrib-type="author">
                  <name>
                    <surname>Jain</surname>
                    <given-names>Anshul</given-names>
                  </name>
                  <xref ref-type="aff" rid="aff1"/>
                  <xref ref-type="corresp" rid="cor1">
                    <sup>*</sup>
                  </xref>
                </contrib>
                <contrib contrib-type="author">
                  <name>
                    <surname>Fuller</surname>
                    <given-names>Stuart</given-names>
                  </name>
                  <xref ref-type="aff" rid="aff1"/>
                </contrib>
                <contrib contrib-type="author">
                  <name>
                    <surname>Backus</surname>
                    <given-names>Benjamin T.</given-names>
                  </name>
                  <xref ref-type="aff" rid="aff1"/>
                </contrib>
              </contrib-group>
              <aff id="aff1">
                <addr-line>Graduate Center for Vision Research, State University of New York College of Optometry, New York, New York, United States of America</addr-line>
              </aff>
              <contrib-group>
                <contrib contrib-type="editor">
                  <name>
                    <surname>Allen</surname>
                    <given-names>Philip</given-names>
                  </name>
                  <role>Editor</role>
                  <xref ref-type="aff" rid="edit1"/>
                </contrib>
              </contrib-group>
              <aff id="edit1">
                <addr-line>University of Akron, United States of America</addr-line>
              </aff>
              <author-notes>
                <corresp id="cor1">* E-mail: <email>anshuljjain@gmail.com</email></corresp>
                <fn fn-type="COI-statement">
                  <p><bold>Competing Interests: </bold>The authors have declared that no competing interests exist.</p>
                </fn>
                <fn fn-type="con">
                  <p>Conceived and designed the experiments: AJ SF BTB. Performed the experiments: AJ SF. Analyzed the data: AJ SF. Contributed reagents/materials/analysis tools: AJ SF. Wrote the paper: AJ BTB.</p>
                </fn>
              </author-notes>
              <pub-date pub-type="collection">
                <year>2014</year>
              </pub-date>
              <pub-date pub-type="epub">
                <day>7</day>
                <month>5</month>
                <year>2014</year>
              </pub-date>
              <volume>9</volume>
              <issue>5</issue>
              <elocation-id>e96383</elocation-id>
              <history>
                <date date-type="received">
                  <day>18</day>
                  <month>10</month>
                  <year>2013</year>
                </date>
                <date date-type="accepted">
                  <day>7</day>
                  <month>4</month>
                  <year>2014</year>
                </date>
              </history>
              <permissions>
                <copyright-statement>© 2014 Jain et al</copyright-statement>
                <copyright-year>2014</copyright-year>
                <copyright-holder>Jain et al</copyright-holder>
                <license xlink:href="http://creativecommons.org/licenses/by/4.0/">
                  <license-p>This is an open-access article distributed under the terms of the Creative Commons Attribution License, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are properly credited.</license-p>
                </license>
              </permissions>
              <abstract>
                <p>Cue-recruitment occurs when a previously ineffective signal comes to affect the perceptual appearance of a target object, in a manner similar to the trusted cues with which the signal was put into correlation during training <xref rid="pone.0096383-Backus1" ref-type="bibr">[1]</xref>, <xref rid="pone.0096383-Haijiang1" ref-type="bibr">[2]</xref>. Jain, Fuller and Backus <xref rid="pone.0096383-Jain1" ref-type="bibr">[3]</xref> reported that <italic>extrinsic</italic> signals, those not carried by the target object itself, were not recruited even after extensive training. However, recent studies have shown that training using weakened trusted cues can facilitate recruitment of <italic>intrinsic</italic> signals <xref rid="pone.0096383-Harrison1" ref-type="bibr">[4]</xref>–<xref rid="pone.0096383-DiLuca1" ref-type="bibr">[7]</xref>. The current study was designed to examine whether extrinsic signals can be recruited by putting them in correlation with weakened trusted cues. Specifically, we tested whether an extrinsic visual signal, the rotary motion direction of an annulus of random dots, and an extrinsic auditory signal, direction of an auditory pitch glide, can be recruited as cues for the rotation direction of a Necker cube. We found learning, albeit weak, for visual but not for auditory signals. These results extend the generality of the cue-recruitment phenomenon to an extrinsic signal and provide further evidence that the visual system learns to use new signals most quickly when other, long-trusted cues are unavailable or unreliable.</p>
              </abstract>
              <funding-group>
                <funding-statement>This study was supported by the NIH R01-EY013988 (<ext-link ext-link-type="uri" xlink:href="http://www.nih.gov">www.nih.gov</ext-link>) and NSF BCS-0810944 (<ext-link ext-link-type="uri" xlink:href="http://www.nsf.gov">www.nsf.gov</ext-link>) grants to B.T.B. The funders had no role in study design, data collection and analysis, decision to publish, or preparation of the manuscript.</funding-statement>
              </funding-group>
              <counts>
                <page-count count="7"/>
              </counts>
            </article-meta>
          </front>
          <body>
            <sec id="s1">
              <title>Introduction</title>
              <p>The visual system is adaptable. Examples of this ability include improvement in performance over time <xref rid="pone.0096383-Fine1" ref-type="bibr">[8]</xref>, <xref rid="pone.0096383-Seitz1" ref-type="bibr">[9]</xref>, negative adaptation aftereffects (e.g. tilt aftereffect <xref rid="pone.0096383-Gibson1" ref-type="bibr">[10]</xref>) and other modulations of perceptual biases <xref rid="pone.0096383-Jain3" ref-type="bibr">[11]</xref>–<xref rid="pone.0096383-Sinha1" ref-type="bibr">[14]</xref>. Another important form of adaptation is learning to use a sensory signal in a new way, as a cue for constructing appearance. This phenomenon has been called cue-recruitment <xref rid="pone.0096383-Backus1" ref-type="bibr">[1]</xref>, <xref rid="pone.0096383-Haijiang1" ref-type="bibr">[2]</xref>, <xref rid="pone.0096383-Jain2" ref-type="bibr">[6]</xref>. Since 2006 it has been demonstrated that many signals can be recruited as cues using an associative learning paradigm, including location <xref rid="pone.0096383-Haijiang1" ref-type="bibr">[2]</xref>, <xref rid="pone.0096383-Harrison1" ref-type="bibr">[4]</xref>, <xref rid="pone.0096383-Jain2" ref-type="bibr">[6]</xref>, <xref rid="pone.0096383-Jain3" ref-type="bibr">[11]</xref>, <xref rid="pone.0096383-Backus2" ref-type="bibr">[15]</xref>–<xref rid="pone.0096383-Harrison4" ref-type="bibr">[17]</xref>, translation direction <xref rid="pone.0096383-Haijiang1" ref-type="bibr">[2]</xref>, surface-texture <xref rid="pone.0096383-Jain2" ref-type="bibr">[6]</xref>, vertical disparity <xref rid="pone.0096383-DiLuca1" ref-type="bibr">[7]</xref>, color of illumination <xref rid="pone.0096383-Kerrigan1" ref-type="bibr">[18]</xref>, object shape <xref rid="pone.0096383-Harrison2" ref-type="bibr">[5]</xref> and motor actions <xref rid="pone.0096383-AdamsBedford1" ref-type="bibr">[19]</xref>.</p>
              <p>In a common version of the cue recruitment experimental paradigm, participants are trained using trials that contain otherwise ambiguous stimuli, that are disambiguated using long-trusted cues. Critically, the new signal to be learned as a cue is put into statistical correlation with the long-trusted cues during the training, which can result in the new signal acquiring the ability to disambiguate appearance in a manner similar to the long-trusted cues.</p>
              <p>Some signals, such as retinal location or object translation direction, typically show greater learning than other signals such as object shape or vertical disparity, as measured by their ability to bias the perceived rotation direction of an ambiguously rotating 3D object. The strength of learning of the location signal itself has also been shown to vary across experiments with different trained perceptual consequences. For example, the location signal biased the perceived 3D rotation direction of a Necker cube much more strongly than the perceived configuration of stationary 3D shape <xref rid="pone.0096383-Jain2" ref-type="bibr">[6]</xref> or 3D interpretation of self-generated optic flow <xref rid="pone.0096383-Jain3" ref-type="bibr">[11]</xref>.</p>
              <p>It is unlikely that the strength of learning varied across these studies due to uncontrolled stimulus differences across experiments. The differences in learning due to controlled factors, such as the new signal to be learned, whether the session started with ambiguous trials <xref rid="pone.0096383-Harrison1" ref-type="bibr">[4]</xref>, <xref rid="pone.0096383-Harrison4" ref-type="bibr">[17]</xref>, <xref rid="pone.0096383-vanDam1" ref-type="bibr">[20]</xref>, or whether the object was moving, were large; whereas effects are robust to changes in other factors, such as overall size of the display <xref rid="pone.0096383-Harrison1" ref-type="bibr">[4]</xref>, <xref rid="pone.0096383-Harrison4" ref-type="bibr">[17]</xref>, <xref rid="pone.0096383-vanDam1" ref-type="bibr">[20]</xref>. Factors that are intrinsic to the perceptual system play a role. Thus, Jain and Backus <xref rid="pone.0096383-Jain2" ref-type="bibr">[6]</xref> argued that lack of motion was the factor that reduced the strength of learning when location was recruited as cue for stationary 3D shapes.</p>
              <p>It has long been known that animals learn some associations more readily than others. In their classic study, Garcia and Koelling <xref rid="pone.0096383-Garcia1" ref-type="bibr">[21]</xref> showed that rats learned to associate illness with tastes more readily than with auditory or visual signals. On the other hand, rats learned to associate pain more readily with auditory or visual signals than with tastes. Garcia and Koelling argued that the system has prior belief about which signals are predictive about different states of the world; the learning rate is higher for plausible predictors of a given state than for implausible ones.</p>
              <p>Similarly, Jain et al. <xref rid="pone.0096383-Jain1" ref-type="bibr">[3]</xref> found in human perception that extrinsic signals, i.e. signals that are not carried by nor visually connected to the object whose appearance was being trained, were not recruited as cues. These unlearned signals included the rotation direction of an annulus of dots, in the plane of the display, that surrounded the ambiguously rotating cube; the location of a luminous disc, relative to the cube; and auditory signals.</p>
              <p>In these cue-recruitment studies, the learning was under the control of the perceptual system, since all signals were supra-threshold <xref rid="pone.0096383-Haijiang1" ref-type="bibr">[2]</xref>. Thus, whether learning occurs can be interpreted as showing whether the system implicitly believes the signal is likely, a priori, to be informative about the estimated property of the environment <xref rid="pone.0096383-Backus1" ref-type="bibr">[1]</xref>, <xref rid="pone.0096383-Haijiang1" ref-type="bibr">[2]</xref>, <xref rid="pone.0096383-Michel1" ref-type="bibr">[22]</xref>. Nevertheless, learning must be done by specific neural mechanisms, and recent studies on cue-recruitment show that specifics of the training protocol can affect whether learning occurs. For example, the use of low information (i.e. weakly disambiguated) training stimuli can cause stronger learning <xref rid="pone.0096383-Harrison1" ref-type="bibr">[4]</xref>, <xref rid="pone.0096383-Harrison4" ref-type="bibr">[17]</xref>, <xref rid="pone.0096383-vanDam1" ref-type="bibr">[20]</xref>, and can also enable recruitment of signals such as surface texture <xref rid="pone.0096383-Jain2" ref-type="bibr">[6]</xref>, vertical disparity <xref rid="pone.0096383-DiLuca1" ref-type="bibr">[7]</xref> and shape <xref rid="pone.0096383-Harrison2" ref-type="bibr">[5]</xref> that were not recruited when training stimuli were strongly disambiguated. These previous studies employed intrinsic signals, i.e. signals carried by the same display elements as the object itself <xref rid="pone.0096383-Jain1" ref-type="bibr">[3]</xref>. The current study was designed to test whether training with low information stimuli can cause extrinsic signals to be recruited as cues to visual appearance. We considered two extrinsic signals, one visual (unimodal) and one auditory (crossmodal). Specifically, we tested whether an annulus of dots rotating in the plane of the display, or an auditory pitch glide, can be recruited as a cue to bias the perceived rotation direction of a Necker cube rotating about the vertical axis.</p>
            </sec>
            <sec sec-type="materials|methods" id="s2">
              <title>Materials and Methods</title>
              <sec id="s2a">
                <title>Participants</title>
                <p>Forty trainees participated in the experiments, fourteen in Experiment 1, six in Experiment 2A and twenty in Experiment 2B. All participants were naïve to the purpose of the experiments. All participants had normal or corrected-to-normal vision and normal hearing (self reported). We assessed each participant's stereoacuity using the TNO Stereo-acuity test to confirm that stereo-disambiguated training cubes would be seen as specified by disparity; all participants had a stereo acuity of 120 seconds of arc or less.</p>
              </sec>
              <sec id="s2b">
                <title>Ethics Statement</title>
                <p>The experiments were conducted in compliance with the standards set by the Institutional Review Board at SUNY College of Optometry. Participants gave their written informed consent prior to their inclusion in the study and were paid for their participation. All experimental procedures were approved by the Institutional Review Board at SUNY College of Optometry.</p>
              </sec>
              <sec id="s2c">
                <title>Apparatus</title>
                <p>All experiments were implemented on a Dell Precision 3400 computer (Windows platform) using the Python-based virtual reality toolkit Vizard 3.0 (WorldViz LLC, Santa Barbara, CA, USA). Visual stimuli were rear-projected onto a screen using either an Infocus LP350 projector (visual cue recruitment experiment, Experiment 1) or a Cristie Mirage S+ 4K projector (auditory cue recruitment experiment, Experiment 2A and 2B). Auditory stimuli were presented on a Bose® 161 speaker system driven by an AudioSource Stereo Amplifier AMP-100. The speakers were placed on either side of the screen along the horizontal midline. Participants were seated at a distance of 1.3 m from the screen for Experiment 1 and at a distance of 2.0 m from the screen for Experiment 2A and 2B. The visual and auditory cue-recruitment experiments were conducted in different rooms. In Experiment 1 we used an EyeLink I eye tracker (Missisauga, Ontario, Canada) to monitor fixation and record eye position.</p>
              </sec>
              <sec id="s2d">
                <title>Stimuli and Procedure</title>
                <sec id="s2d1">
                  <title>Experiment 1 – Visual Cue-recruitment</title>
                  <p>The visual stimulus consisted of an orthographically projected (i.e. no perspective) wireframe (Necker) cube rotating about the vertical axis (<xref ref-type="fig" rid="pone-0096383-g001">Figure 1A</xref>). Each edge of the cube was a solid parallelepiped with a thickness of 0.6 cm and a length of 15 cm. The cube therefore subtended 12.4 degrees of visual angle. In order to stabilize perception of the cube as a single rigid object, each face of the cube was covered with 25 randomly placed dots. The cube was oriented such that one of the major diagonals was perpendicular to the axis of rotation. The cube was presented in two initial configurations, as “seen-from-above” or as “seen-from-below”. To satisfy these criteria, the yaw, pitch and roll were set to 50, 25 and 25 degrees respectively at the onset for the “seen-from-above” configuration and at 50, −25 and −25 degrees for “seen-from-below” configuration. These two configurations were balanced for each participant to avoid correlating them with cube rotation. The cube's angular velocity about the vertical axis was 72 degrees/second.</p>
                  <fig id="pone-0096383-g001" orientation="portrait" position="float">
                    <object-id pub-id-type="doi">10.1371/journal.pone.0096383.g001</object-id>
                    <label>Figure 1</label>
                    <caption>
                      <title>Design of stimuli used in the study.</title>
                      <p>A) Layout of the stimulus used in Experiment 1. B) Layout of the visual stimulus used in Experiment 2.</p>
                    </caption>
                    <graphic xlink:href="pone.0096383.g001"/>
                  </fig>
                  <p>Left and right eye image segregation, which was necessary to display binocular disparities, was implemented using red-green anaglyphs and matching filter glasses. On training trials, the rotation direction was weakly disambiguated by using transient disparity signals. Disparity had the correct magnitude for the simulated cube, assuming a 6.2 cm interpupillary distance, but was presented only for 150 ms at the beginning of a training trial. After that, the left eye's image was extinguished and there were no disambiguating signals. Under these conditions, the apparent rotation direction established by the transient disparity generally perseverated to the end of the trial. On test trials, only one of the anaglyph images of the cube was presented (to the right eye).</p>
                  <p>A fixation square (2 cm×2 cm) was presented at the center of the screen and the cube was centered 15 cm (7.1 degrees) above the fixation square. The cube's center was simulated to be in the plane of the screen. All stimuli were presented as bright objects on a dark background. Concurrent with the cube stimulus on each trial, we presented a single probe dot (1 cm×1 cm) that translated horizontally in the screen plane through the fixation point, either leftwards or rightwards across a visual angle slightly larger than that of the cube. The dot traveled at approximately the same speed as the closest (or farthest) corner of the cube.</p>
                  <p>The new signal to be trained was a field of randomly placed dots in an annulus surrounding the cube. The annulus rotated within the plane of the display screen at the same angular speed as the cube rotated about its vertical axis, and its rotation direction was perfectly correlated with the cube's rotation direction on training trials. The polarity of the correlation was counterbalanced across participants. The dots had a mean lifetime of 100 ms. The field of dots and the cube were presented simultaneously after the participant confirmed that they had proper fixation (see <xref ref-type="supplementary-material" rid="pone.0096383.s001">Video S1</xref>).</p>
                </sec>
                <sec id="s2d2">
                  <title>General Procedure</title>
                  <p>The experiment consisted of two kinds of trials, Training trials and Test trials. On training trials, the perceived rotation direction of the cube was controlled using a brief pulse of disparity signal as described above to establish the correlation between the cube's rotation direction and the new signal (the rotation direction of the annulus of dots). On test trials the cube was ambiguous (no disparity) and was presented with one or the other value of the new signal. A trial consisted of the presentation of the cube stimulus, the probe dot, and the new signal, and lasted for 1.5 s.</p>
                  <p>The participants' task was to report whether the translation direction (leftward or rightward) of the probe dot was same as the front (closer to the participant) or back (farther away from the participant) side of the cube. Participants pressed the ‘2’ key to report that the front of the cube moved in the same direction as the probe dot and pressed the ‘8’ key to report that the back of the cube moved in the same direction as the probe dot. Because the dot's direction was randomly chosen on each trial, participant responses were decoupled from both perceived rotation direction and the new signal's values. This task was chosen to discourage participants from using complicated cognitive strategies in choosing their response. Post-experiment interviews confirmed that responses were mediated by the apparent rotation of the cube: none of the participants reported having noticed the correlation of training signal and rotation direction, much less having used it to respond. The next trial began 1 s after response. Participants were instructed to fixate on the fixation square throughout the experiment. We monitored fixation during the experiment. The eye-tracker was recalibrated before each of the five blocks.</p>
                  <p>The experiment was conducted over a single session consisting of 480 trials split into five blocks of 96 trials each. The first block contained only training trials, to establish a perceptual history reflecting the correlation of the new signal and cube rotation before beginning to test with ambiguous cubes. The training trials for rightward and leftward rotations were presented equally often but pseudo-randomly sequenced. The sequence was constrained such that participants could not be presented with cubes rotating in the same direction on more than eight consecutive trials. The remaining blocks contained an equal mix of test and training trials presented pseudo-randomly. The sequence was constrained such that participants could never be presented with the same type of trial (test or training) on more than four consecutive trials. <xref ref-type="fig" rid="pone-0096383-g002">Figure 2</xref> shows the structure of typical training and test trials presented in Experiment 1.</p>
                  <fig id="pone-0096383-g002" orientation="portrait" position="float">
                    <object-id pub-id-type="doi">10.1371/journal.pone.0096383.g002</object-id>
                    <label>Figure 2</label>
                    <caption>
                      <title>Structure of typical training and test trials presented in Experiment 1.</title>
                    </caption>
                    <graphic xlink:href="pone.0096383.g002"/>
                  </fig>
                  <p>The 14 participants in Experiment 1 were randomly assigned to two groups: one for which clockwise rotation of the annulus specified leftward rotation of the cube, and one for which it specified rightward rotation of the cube, as a precaution to control for preexisting biases in perception within the population. In fact, both groups showed similar learning, so we did not find evidence of a preexisting bias to use annulus rotation direction as a cue for a particular cube rotation direction (see <xref ref-type="sec" rid="s3">Results and Discussion</xref>). Similarly, in Experiment 2A, the 6 participants were randomly assigned to two groups: one for which upward pitch glide specified leftward rotation of the cube and one for which upward pitch glide specified rightward rotation of the cube.</p>
                </sec>
                <sec id="s2d3">
                  <title>Experiment 2 – Auditory Cue-recruitment</title>
                  <p>The visual stimuli in Experiments 2A and 2B were the same as in Experiment 1 after adjusting the size to account for the larger viewing distance, except that they rotated about the vertical axis at 120 degrees/second and did not contain the annulus of dots (<xref ref-type="fig" rid="pone-0096383-g001">Figure 1B</xref>).</p>
                  <p>In both Experiment 2A and 2B, the new signal to be trained was an auditory composite pitch glide presented simultaneously with visual onset of the stimulus. The pitch glide was formed with five harmonics. For an upward glide (<xref ref-type="supplementary-material" rid="pone.0096383.s003">Audio S1</xref>) frequencies [65.4 130.8 261.6 523.2 1046.4] Hz were increased linearly as a function of time up to [185 370 740 1480 2960] Hz, respectively, over the course of 1.5 s. The direction was reversed for a downward glide (<xref ref-type="supplementary-material" rid="pone.0096383.s004">Audio S2</xref>). The auditory stimulus amplitude was ramped up and down over 20 ms at the start and end of the stimulus, respectively, to avoid audible “clicks” caused by sudden onset and offset. The intensity of the auditory stimulus was set to supra-threshold but comfortable levels (the exact intensity is not critical to the aim of the study).</p>
                  <p>The procedure for Experiment 2A was the same as Experiment 1. The rotation direction on training trials was disambiguated using transient disparity signals presented at the beginning of the trial for 150 ms, just as in Experiment 1. The experiment was conducted over a single session consisting of 480 trials split into five blocks of 96 trials each with the first block containing only training trials. The distribution of test and training trials was same as in Experiment 1.</p>
                  <p>To preview our findings, the results from Experiments 1 and 2A showed that the visual extrinsic signal could be recruited but not the auditory signal. We wondered whether a more sensitive test would reveal recruitment of the sound cue. To this end we devised an experiment in which the disambiguating stereo information on training trials was kept at threshold levels, to further reduce the reliability of the disambiguating information in training trials <xref rid="pone.0096383-Harrison1" ref-type="bibr">[4]</xref>, <xref rid="pone.0096383-DiLuca1" ref-type="bibr">[7]</xref>, <xref rid="pone.0096383-Harrison4" ref-type="bibr">[17]</xref>, <xref rid="pone.0096383-vanDam1" ref-type="bibr">[20]</xref>. Thus, in Experiment 2B, the duration of the disparity signal was computed for each trial using a three-down-one-up staircase procedure so as to maintain 79% correct performance on training trials. The experiment was conducted as a between-groups design. The 20 participants were randomly assigned to either the learning or the control group (2×N = 10). It was conducted in a single session consisting of six blocks of 80 trials. In the learning group the direction of the pitch glide was perfectly correlated with cube's rotation direction as defined by disparity signals while for the control group the direction of the pitch glide was uncorrelated with its rotation direction. In the learning group the association of rotation direction and pitch glide direction were counterbalanced across participants (5 participants in each subgroup). In order to more precisely control the uncertainty level for each participant, and to guarantee that disambiguating cues were not highly informative, we used a staircase procedure that varied the stereo pulse duration so as to maintain a fixed performance level (79% correct). This dynamic adjustment of the reliability of the stereo cue was different from Experiment 1 and Experiment 2A, in which, the stereo pulse had fixed duration. As a result, participants were not expected to have perfect performance on training trials in Experiment 2B; instead, performance on training trials was controlled by the staircase procedure. It could be argued that this may actually deter any learning since the cube is perceived as rotating in the ‘wrong’ direction 21% of the time. However, in a previous study we have shown that learning can occur with even when the percept is not perfectly correlated with the trained signal <xref rid="pone.0096383-Jain3" ref-type="bibr">[11]</xref>.</p>
                  <p>For both groups in Experiment 2B, the stereo-pulse duration was set to 167 ms (20 video frames) at the beginning of the session and was increased or decreased with a step-size of 8.33 ms (1 frame) based on participants' response using a three-down-one-up staircase procedure. The initial duration of the stereo pulse was chosen to ensure that participants would perceive the rotation direction as specified by stereo signals at the beginning of the session. We hypothesized that if the auditory signals were recruited as a cue to rotation direction, then (1) the duration of stereo pulse required to maintain 79% performance level on training trials would be shorter for the group with correlated auditory signals than for the control group, which had uncorrelated auditory signals, and (2) performance on test trials (which were presented only to the right eye and had no stereo) would be correlated with the auditory cue only in the correlated (learning) group. The prediction if the auditory signals are <italic>strongly</italic> recruited is that participants would not require any stereo signals on training trials, while their percepts on the test trials would perfectly determined by the auditory cue. There were 20 test trials interspersed into in the session (See <xref ref-type="supplementary-material" rid="pone.0096383.s002">Video S2</xref>).</p>
                </sec>
              </sec>
            </sec>
            <sec id="s3">
              <title>Results and Discussion</title>
              <p>For statistical analyses, each participant's percent perceived-as-cued on test and training trials in Experiment 1 and Experiment 2A was converted to a z-score measure <xref rid="pone.0096383-Backus3" ref-type="bibr">[23]</xref>, <xref rid="pone.0096383-Dosher1" ref-type="bibr">[24]</xref>. Appearance probability on ambiguous test trials was computed based on the expected response as predicted by the new signal contingency during training. Saturated performances (0% and 100%) were assigned a Z-score of ±2.326. This Z-score corresponds to 2 responses in 200 trials.</p>
              <sec id="s3a">
                <title>Experiment 1 – Visual cue-recruitment</title>
                <p>In Experiment 1 we used the cue recruitment paradigm to examine whether an extrinsic visual signal can be recruited as a cue to appearance of an ambiguous stimulus, specifically whether the perceived rotation direction of an ambiguous Necker cube can be made contingent on the rotation direction of a surrounding annulus of dots. <xref ref-type="fig" rid="pone-0096383-g003">Figure 3A</xref> shows participants' mean performance as a function of number of blocks on disambiguated training and ambiguous test trials. <xref ref-type="fig" rid="pone-0096383-g003">Figure 3B</xref> summarizes the performance for the entire session in terms of Z-scores. The short stereo pulse used to disambiguate the stimuli on training trials was effective in controlling participants' percept of the cubes. Participants perceived the stimulus as specified by stereo 97.53% of the time on training trials (z-score  = 2.05, 95% CI [1.91 2.18], t(13) = 32.48, p&lt;&lt;0.0001). Participants maintained fixation well during the experiment, with breaks in fixation occurring only on 3% of the trials on average for all participants.</p>
                <fig id="pone-0096383-g003" orientation="portrait" position="float">
                  <object-id pub-id-type="doi">10.1371/journal.pone.0096383.g003</object-id>
                  <label>Figure 3</label>
                  <caption>
                    <title>Data show recruitment of extrinsic unimodal signal for construction of visual appearance.</title>
                    <p>A) Participants' (N = 14) mean performance on training (black squares) and test trials (red circles) as function of number of training trials in Experiment 1. The light dotted lines show individual performance for each participant. B) Participants' performance on training (black squares, filled symbol shows mean performance) and test trials (red circles, filled symbol shows mean performance) for the entire session as measured in Z-score units.</p>
                  </caption>
                  <graphic xlink:href="pone.0096383.g003"/>
                </fig>
                <p>Participants' percepts on ambiguous test trials were the dependent measure of interest. Percepts were biased in the direction of training contingency, showing that rotation direction of annulus of dots was recruited as a cue to determine rotation direction of the otherwise ambiguous Necker cube (percent of trials  = 58.04%, z-score  = 0.22, 95% CI = [0.04 0.4], t(13) = 2.58, p = 0.023). It has been shown that perceived motion of an ambiguous stimulus can be influenced by motion of neighboring objects <xref rid="pone.0096383-Eby1" ref-type="bibr">[25]</xref>–<xref rid="pone.0096383-Klink1" ref-type="bibr">[29]</xref>. Therefore, it is possible that a pre-existing bias linking the rotation direction of the annulus of dots to the rotation direction of the cube may have caused the observed effect. If that were true then the strength of learning would have been different for the two groups that were trained with opposite contingency <xref rid="pone.0096383-Jain2" ref-type="bibr">[6]</xref>, <xref rid="pone.0096383-Jain3" ref-type="bibr">[11]</xref>. However, we did not find any difference in participants' performance on training (t(12) = 0.66, p = 0.52) or test trials (t(12) = 1.68, p = 0.12) between the two groups. Further, the effect of learning was in the expected direction for both groups individually, providing further evidence against any preexisting link.</p>
              </sec>
              <sec id="s3b">
                <title>Experiment 2 – Auditory cue-recruitment</title>
                <p>In Experiment 2, we examined whether an extrinsic signal from a different modality (auditory) would be recruited as a cue to visual appearance. Previous attempts to find such an effect have been unsuccessful.</p>
                <p><xref ref-type="fig" rid="pone-0096383-g004">Figure 4A</xref> shows participants' mean performance as a function of number of blocks on disambiguated training and ambiguous test trials. <xref ref-type="fig" rid="pone-0096383-g004">Figure 4B</xref> summarizes the performance for the entire session in terms of Z-scores. The short stereo pulse used to disambiguate the stimuli on training trials was effective in controlling participants' percept of the cubes. Participants perceived the stimulus as specified by stereo 95.1% of the time on training trials (z-score  = 1.81, 95% CI [1.5 2.13], t(5) = 14.57, p&lt;&lt;0.0001). However, participants' percept on ambiguous test trials was unaffected by the training contingency (z-score  = 0.02, 95% CI [−0.04 0.07], t(5) = 0.82, p = 0.45). We compared participants' performance on training and test trials in Experiment 1 to that in Experiment 2A. The performance on training trials was very similar between the two groups (t(18) = 1.51, p = 0.15), however the difference in performance on test trials was marginally significant (t(18) = 1.87, p = 0.07). This result shows that while training with low-information stimuli was successful in causing the recruitment of a unimodal extrinsic signal, it failed to cause a crossmodal extrinsic signal to be recruited. Experiment 1 and 2A were identical in all aspects but for the modality of the signal to be recruited and the rotation speed of the cube. It is unlikely that the rotation speed could have caused a difference in the learning outcome, as cue-recruitment has been demonstrated for a wide range of rotation speeds and it has also been shown that rotation speed does not play a critical role in this type of learning <xref rid="pone.0096383-Harrison2" ref-type="bibr">[5]</xref>.</p>
                <fig id="pone-0096383-g004" orientation="portrait" position="float">
                  <object-id pub-id-type="doi">10.1371/journal.pone.0096383.g004</object-id>
                  <label>Figure 4</label>
                  <caption>
                    <title>Data show lack of learning for crossmodal extrinsic signal in Experiment 2A.</title>
                    <p>A) Participants' (N = 6) mean performance on training (black squares) and test trials (red circles) as function of number of training trials in Experiment 2A. The light dotted lines show individual performance for each participant. B) Participants' performance on training (black squares, filled symbol shows mean performance) and test trials (red circles, filled symbol shows mean performance) for the entire session as measured in Z-score units.</p>
                  </caption>
                  <graphic xlink:href="pone.0096383.g004"/>
                </fig>
                <p>Experiment 2B was conducted as a between-groups experimental design, where we measured the duration of the stereo-pulse required to maintain 79% correct performance for participants in the learning group (auditory signals were perfectly correlated with rotation direction) and for participants in the control group (when auditory signals were uncorrelated with rotation direction).</p>
                <p>We excluded data from participants for whom the duration of the stereo pulse was greater than 200 ms at any time during the session, as it implied that we were unable to sufficiently control their percept on training trials. This threshold was sufficient to control appearance in Experiment 1. There were three participants in the control group and one participant in the learning group who failed to meet the criterion.</p>
                <p><xref ref-type="fig" rid="pone-0096383-g005">Figure 5</xref> shows mean thresholds and individual data for the two groups. The duration thresholds were comparable for both learning and control group (t(14) = 0.8526, p = 0.41), so we did not find evidence that participants learned to use auditory pitch glide direction as a cue to the rotation direction. We also observed no sound-contingent bias on ambiguous test trials (t(8) = 0.31, p = 0.76, mean  = 49.14%, s.e.m = 2.8%).</p>
                <fig id="pone-0096383-g005" orientation="portrait" position="float">
                  <object-id pub-id-type="doi">10.1371/journal.pone.0096383.g005</object-id>
                  <label>Figure 5</label>
                  <caption>
                    <title>Data show lack of learning for crossmodal extrinsic signal in Experiment 2B.</title>
                    <p>Participants' individual and mean stereo-pulse duration thresholds required maintaining 79% correct performance for Control and Learning groups in Experiment 2B. The data do not agree with our prediction that duration thresholds would be smaller in the learning group, so the sound cue appears not to have been recruited.</p>
                  </caption>
                  <graphic xlink:href="pone.0096383.g005"/>
                </fig>
              </sec>
              <sec id="s3c">
                <title>General Discussion</title>
                <p>The results from Experiment 1 are in stark contrast with our previous study in which no learning of extrinsic cues was observed using similar stimuli and experimental design <xref rid="pone.0096383-Jain1" ref-type="bibr">[3]</xref>. The key difference between the two studies is the design of training stimuli. In the current study, the training stimuli were disambiguated using a short stereo pulse, unlike the previous study where stereo signals were present through the entire training trial. In the new study the visual system was forced to resolve the ambiguity using a less reliable stereo signal, and to maintain the percept without the benefit of continued disambiguating information. Previous studies found that low-information stimuli can cause stronger learning <xref rid="pone.0096383-Harrison1" ref-type="bibr">[4]</xref>, <xref rid="pone.0096383-Harrison4" ref-type="bibr">[17]</xref>, <xref rid="pone.0096383-vanDam1" ref-type="bibr">[20]</xref> and cause learning to occur in cases where it did not occur otherwise <xref rid="pone.0096383-Harrison2" ref-type="bibr">[5]</xref>–<xref rid="pone.0096383-DiLuca1" ref-type="bibr">[7]</xref>. The result from Experiment 1 confirms the potency of low-information stimuli in promoting the recruitment of difficult-to-learn cues. The current study did not measure the time course of the learning beyond a single session, particularly whether it lasted over night as it does for other cues <xref rid="pone.0096383-Haijiang1" ref-type="bibr">[2]</xref>.</p>
                <p>Laboratory demonstration of recruitment of a sound cue for visual perception remains an elusive. Experiment 2 constitutes a third failure to find such an effect <xref rid="pone.0096383-Haijiang1" ref-type="bibr">[2]</xref>, <xref rid="pone.0096383-Jain1" ref-type="bibr">[3]</xref>, so we can at this point conclude that the perceptual system is not particularly disposed to learn this particular cross-modal association. However, crossmodal influences do exist, like the McGurk effect <xref rid="pone.0096383-McGurk1" ref-type="bibr">[30]</xref> or the bounce-pass illusion <xref rid="pone.0096383-Sekuler1" ref-type="bibr">[31]</xref>. Moreover, researchers have found strong evidence for cue-combination across modalities using tasks involving localization <xref rid="pone.0096383-Alais1" ref-type="bibr">[32]</xref>, <xref rid="pone.0096383-Jain4" ref-type="bibr">[33]</xref> and motion perception of multimodal stimuli <xref rid="pone.0096383-Jain4" ref-type="bibr">[33]</xref>.</p>
                <p>In a learning study, Michel and Jacobs <xref rid="pone.0096383-Michel2" ref-type="bibr">[34]</xref> showed that judgments of motion direction in a threshold-level random-dot kinematogram task can come to be influenced by auditory signals. However, it is impossible to know where within the system this learning occurred. The sound cues may have influenced only the subjects' final judgments, rather than the visual appearance of the stimulus, because the design did not require the visual system to make a dichotomous decision between alternative perceptual interpretations (i.e. the visual stimuli were not perceptually bistable <xref rid="pone.0096383-Backus3" ref-type="bibr">[23]</xref>). It would not be surprising that subjects can learn to use auditory information to answer a question about a visual stimulus when the question is difficult to answer based on visual information alone. Indeed, the authors of <xref rid="pone.0096383-Michel2" ref-type="bibr">[34]</xref> conjectured that the auditory cue would be learned under the conditions of their study. They also suggest that the processes underlying the learning observed in their study are distinct from the processes involved in contextual dependent learning such as the one examined in the current study.</p>
                <p>The rate and strength of learning for associations between signals and visual appearances lie on a continuum, from fast and readily learned, to unlearnable <xref rid="pone.0096383-Kerrigan1" ref-type="bibr">[18]</xref>. Absence of learning for a particular association means that the perceptual system implicitly believes that the signal cannot be informative, or at least should not be used to inform, about the property of the scene represented by the percept. This behavior by a learning mechanism would prevent spurious learning due to coincidental correlations in the environment, at the cost of missing a new reliable cue should one appear. Learning can occur in cases where the additional cues would be useful for disambiguation in the future, as is the case for the low-information training stimuli we used in Experiment 1. In that case the use of low-information training stimuli overcame a reluctance of the system to recruit an extrinsic cue. But this trick does not always work, as was shown by Experiment 2.</p>
              </sec>
            </sec>
            <sec sec-type="supplementary-material" id="s4">
              <title>Supporting Information</title>
              <supplementary-material content-type="local-data" id="pone.0096383.s001">
                <label>Video S1</label>
                <caption>
                  <p>
                    <bold>Example of the visual stimulus used in Experiment 1.</bold>
                  </p>
                  <p>(MOV)</p>
                </caption>
                <media xlink:href="pone.0096383.s001.mov">
                  <caption>
                    <p>Click here for additional data file.</p>
                  </caption>
                </media>
              </supplementary-material>
              <supplementary-material content-type="local-data" id="pone.0096383.s002">
                <label>Video S2</label>
                <caption>
                  <p>
                    <bold>Example of the visual stimulus used in Experiment 2.</bold>
                  </p>
                  <p>(MOV)</p>
                </caption>
                <media xlink:href="pone.0096383.s002.mov">
                  <caption>
                    <p>Click here for additional data file.</p>
                  </caption>
                </media>
              </supplementary-material>
              <supplementary-material content-type="local-data" id="pone.0096383.s003">
                <label>Audio S1</label>
                <caption>
                  <p>
                    <bold>Upward pitch glide stimulus used in Experiment 2.</bold>
                  </p>
                  <p>(WAV)</p>
                </caption>
                <media xlink:href="pone.0096383.s003.wav">
                  <caption>
                    <p>Click here for additional data file.</p>
                  </caption>
                </media>
              </supplementary-material>
              <supplementary-material content-type="local-data" id="pone.0096383.s004">
                <label>Audio S2</label>
                <caption>
                  <p>
                    <bold>Downward pitch glide stimulus used in Experiment 2.</bold>
                  </p>
                  <p>(WAV)</p>
                </caption>
                <media xlink:href="pone.0096383.s004.wav">
                  <caption>
                    <p>Click here for additional data file.</p>
                  </caption>
                </media>
              </supplementary-material>
            </sec>
          </body>
          <back>
            <ack>
              <p>We thank Benjamin Hunter McFadden for assisting with data collection and Martha Lain for assisting with recruitment of participants.</p>
            </ack>
            <ref-list>
              <title>References</title>
              <ref id="pone.0096383-Backus1">
                <label>1</label>
                <mixed-citation publication-type="other">Backus BT (2011) Recruitment of new visual cues for perceptual appearance. In: Trommershauser J, Körding K, Landy MS, editors. Sensory Cue Integration. Oxford, UK: Oxford University Press.</mixed-citation>
              </ref>
              <ref id="pone.0096383-Haijiang1">
                <label>2</label>
                <mixed-citation publication-type="journal"><name><surname>Haijiang</surname><given-names>Q</given-names></name>, <name><surname>Saunders</surname><given-names>JA</given-names></name>, <name><surname>Stone</surname><given-names>RW</given-names></name>, <name><surname>Backus</surname><given-names>BT</given-names></name> (<year>2006</year>) <article-title>Demonstration of cue recruitment: Change in visual appearance by means of Pavlovian conditioning</article-title>. <source>Proc Natl Acad Sci U S A</source>
<volume>103</volume>: <fpage>483</fpage>–<lpage>488</lpage>.<pub-id pub-id-type="pmid">16387858</pub-id></mixed-citation>
              </ref>
              <ref id="pone.0096383-Jain1">
                <label>3</label>
                <mixed-citation publication-type="journal"><name><surname>Jain</surname><given-names>A</given-names></name>, <name><surname>Fuller</surname><given-names>S</given-names></name>, <name><surname>Backus</surname><given-names>BT</given-names></name> (<year>2010</year>) <article-title>Absence of cue-recruitment for extrinsic signals: sounds, spots, and swirling dots fail to influence perceived 3D rotation direction after training</article-title>. <source>PLoS One</source>
<volume>5</volume>: <fpage>e13295</fpage>.<pub-id pub-id-type="pmid">20949047</pub-id></mixed-citation>
              </ref>
              <ref id="pone.0096383-Harrison1">
                <label>4</label>
                <mixed-citation publication-type="journal"><name><surname>Harrison</surname><given-names>SJ</given-names></name>, <name><surname>Backus</surname><given-names>BT</given-names></name> (<year>2010</year>) <article-title>Uninformative visual experience establishes long term perceptual bias</article-title>. <source>Vision Res</source>
<volume>50</volume>: <fpage>1905</fpage>–<lpage>1911</lpage>.<pub-id pub-id-type="pmid">20600232</pub-id></mixed-citation>
              </ref>
              <ref id="pone.0096383-Harrison2">
                <label>5</label>
                <mixed-citation publication-type="journal">Harrison SJ, Backus BT (2012) Associative learning of shape as a cue to appearance: A new demonstration of cue recruitment. Journal of VIsion <volume>12</volume>..</mixed-citation>
              </ref>
              <ref id="pone.0096383-Jain2">
                <label>6</label>
                <mixed-citation publication-type="journal"><name><surname>Jain</surname><given-names>A</given-names></name>, <name><surname>Backus</surname><given-names>BT</given-names></name> (<year>2013</year>) <article-title>Generalization of cue recruitment to non-moving stimuli: Location and surface-texture contingent biases for 3-D shape perception</article-title>. <source>Vision Research</source>
<volume>82</volume>: <fpage>13</fpage>–<lpage>21</lpage>.<pub-id pub-id-type="pmid">23438583</pub-id></mixed-citation>
              </ref>
              <ref id="pone.0096383-DiLuca1">
                <label>7</label>
                <mixed-citation publication-type="journal"><name><surname>Di Luca</surname><given-names>M</given-names></name>, <name><surname>Ernst</surname><given-names>MO</given-names></name>, <name><surname>Backus</surname><given-names>BT</given-names></name> (<year>2010</year>) <article-title>Learning to use an invisible visual signal for perception</article-title>. <source>Current Biology</source>
<volume>20</volume>: <fpage>1860</fpage>–<lpage>1863</lpage>.<pub-id pub-id-type="pmid">20933421</pub-id></mixed-citation>
              </ref>
              <ref id="pone.0096383-Fine1">
                <label>8</label>
                <mixed-citation publication-type="journal"><name><surname>Fine</surname><given-names>I</given-names></name>, <name><surname>Jacobs</surname><given-names>RA</given-names></name> (<year>2002</year>) <article-title>Comparing perceptual learning tasks: a review</article-title>. <source>J Vis</source>
<volume>2</volume>: <fpage>190</fpage>–<lpage>203</lpage>.<pub-id pub-id-type="pmid">12678592</pub-id></mixed-citation>
              </ref>
              <ref id="pone.0096383-Seitz1">
                <label>9</label>
                <mixed-citation publication-type="journal"><name><surname>Seitz</surname><given-names>AR</given-names></name>, <name><surname>Dinse</surname><given-names>HR</given-names></name> (<year>2007</year>) <article-title>A common framework for perceptual learning</article-title>. <source>Curr Opin Neurobiol</source>
<volume>17</volume>: <fpage>148</fpage>–<lpage>153</lpage>.<pub-id pub-id-type="pmid">17317151</pub-id></mixed-citation>
              </ref>
              <ref id="pone.0096383-Gibson1">
                <label>10</label>
                <mixed-citation publication-type="journal"><name><surname>Gibson</surname><given-names>JJ</given-names></name>, <name><surname>Radner</surname><given-names>M</given-names></name> (<year>1937</year>) <article-title>Adaptation, after-effect and contrast in the perception of tilted lines</article-title>. <source>Journal of Experimental Psychology</source>
<volume>20</volume>: <fpage>453</fpage>–<lpage>467</lpage>.</mixed-citation>
              </ref>
              <ref id="pone.0096383-Jain3">
                <label>11</label>
                <mixed-citation publication-type="journal">Jain A, Backus BT (2010) Experience affects the use of ego-motion signals during 3D shape perception. J Vis <volume>10</volume>..</mixed-citation>
              </ref>
              <ref id="pone.0096383-Adams1">
                <label>12</label>
                <mixed-citation publication-type="journal"><name><surname>Adams</surname><given-names>WJ</given-names></name>, <name><surname>Graf</surname><given-names>EW</given-names></name>, <name><surname>Ernst</surname><given-names>MO</given-names></name> (<year>2004</year>) <article-title>Experience can change the 'light-from-above' prior</article-title>. <source>Nat Neurosci</source>
<volume>7</volume>: <fpage>1057</fpage>–<lpage>1058</lpage>.<pub-id pub-id-type="pmid">15361877</pub-id></mixed-citation>
              </ref>
              <ref id="pone.0096383-Champion1">
                <label>13</label>
                <mixed-citation publication-type="journal">Champion RA, Adams WJ (2007) Modification of the convexity prior but not the light-from-above prior in visual search with shaded objects. J Vis <volume>7</volume>: : 10 11–10.</mixed-citation>
              </ref>
              <ref id="pone.0096383-Sinha1">
                <label>14</label>
                <mixed-citation publication-type="journal"><name><surname>Sinha</surname><given-names>P</given-names></name>, <name><surname>Poggio</surname><given-names>T</given-names></name> (<year>1996</year>) <article-title>Role of learning in three-dimensional form perception</article-title>. <source>Nature</source>
<volume>384</volume>: <fpage>460</fpage>–<lpage>463</lpage>.<pub-id pub-id-type="pmid">8945472</pub-id></mixed-citation>
              </ref>
              <ref id="pone.0096383-Backus2">
                <label>15</label>
                <mixed-citation publication-type="journal">Backus BT, Haijiang Q (2007) Competition between newly recruited and pre-existing visual cues during the construction of visual appearance. Vision Research <volume>47</volume>: 919–924.</mixed-citation>
              </ref>
              <ref id="pone.0096383-Harrison3">
                <label>16</label>
                <mixed-citation publication-type="journal"><name><surname>Harrison</surname><given-names>S</given-names></name>, <name><surname>Backus</surname><given-names>B</given-names></name> (<year>2010</year>) <article-title>Disambiguating Necker cube rotation using a location cue: what types of spatial location signal can the visual system learn?</article-title>
<source>Journal of Vision</source>
<volume>10</volume>: <fpage>23</fpage>.</mixed-citation>
              </ref>
              <ref id="pone.0096383-Harrison4">
                <label>17</label>
                <mixed-citation publication-type="journal"><name><surname>Harrison</surname><given-names>SJ</given-names></name>, <name><surname>Backus</surname><given-names>BT</given-names></name>, <name><surname>Jain</surname><given-names>A</given-names></name> (<year>2011</year>) <article-title>Disambiguation of Necker cube rotation by monocular and binocular depth cues: Relative effectiveness for establishing long-term bias</article-title>. <source>Vision Research</source>
<volume>51</volume>: <fpage>978</fpage>–<lpage>986</lpage>.<pub-id pub-id-type="pmid">21335023</pub-id></mixed-citation>
              </ref>
              <ref id="pone.0096383-Kerrigan1">
                <label>18</label>
                <mixed-citation publication-type="journal"><name><surname>Kerrigan</surname><given-names>IS</given-names></name>, <name><surname>Adams</surname><given-names>WJ</given-names></name> (<year>2013</year>) <article-title>Learning different light prior distributions for different contexts</article-title>. <source>Cognition</source>
<volume>127</volume>: <fpage>99</fpage>–<lpage>104</lpage>.<pub-id pub-id-type="pmid">23376295</pub-id></mixed-citation>
              </ref>
              <ref id="pone.0096383-AdamsBedford1">
                <label>19</label>
                <mixed-citation publication-type="journal">Adams-Bedford J, Wallis G, Backus BT (2013) The impact of intention, action, and learnt contingency on visual perception.. Journal of Vision <volume>13</volume>..</mixed-citation>
              </ref>
              <ref id="pone.0096383-vanDam1">
                <label>20</label>
                <mixed-citation publication-type="journal"><name><surname>van Dam</surname><given-names>LC</given-names></name>, <name><surname>Ernst</surname><given-names>MO</given-names></name> (<year>2010</year>) <article-title>Preexposure disrupts learning of location-contingent perceptual biases for ambiguous stimuli</article-title>. <source>Journal of Vision</source>
<volume>10</volume>: <fpage>15</fpage>.</mixed-citation>
              </ref>
              <ref id="pone.0096383-Garcia1">
                <label>21</label>
                <mixed-citation publication-type="journal"><name><surname>Garcia</surname><given-names>J</given-names></name>, <name><surname>Koelling</surname><given-names>RA</given-names></name> (<year>1966</year>) <article-title>Relation of cue to consequence in avoidance learning</article-title>. <source>Psychonomic Science</source>
<volume>4</volume>: <fpage>123</fpage>–<lpage>124</lpage>.</mixed-citation>
              </ref>
              <ref id="pone.0096383-Michel1">
                <label>22</label>
                <mixed-citation publication-type="journal">Michel MM, Jacobs RA (2008) Learning optimal integration of arbitrary features in a perceptual discrimination task. J Vis <volume>8</volume>: : 3 1–16.</mixed-citation>
              </ref>
              <ref id="pone.0096383-Backus3">
                <label>23</label>
                <mixed-citation publication-type="journal">Backus BT (2009) The Mixture of Bernoulli Experts: a theory to quantify reliance on cues in dichotomous perceptual decisions. J Vis <volume>9</volume>: : 6 1–19.</mixed-citation>
              </ref>
              <ref id="pone.0096383-Dosher1">
                <label>24</label>
                <mixed-citation publication-type="journal"><name><surname>Dosher</surname><given-names>BA</given-names></name>, <name><surname>Sperling</surname><given-names>G</given-names></name>, <name><surname>Wurst</surname><given-names>SA</given-names></name> (<year>1986</year>) <article-title>Tradeoffs between stereopsis and proximity luminance covariance as determinants of perceived 3D structure</article-title>. <source>Vision Res</source>
<volume>26</volume>: <fpage>973</fpage>–<lpage>990</lpage>.<pub-id pub-id-type="pmid">3750879</pub-id></mixed-citation>
              </ref>
              <ref id="pone.0096383-Eby1">
                <label>25</label>
                <mixed-citation publication-type="journal"><name><surname>Eby</surname><given-names>DW</given-names></name>, <name><surname>Loomis</surname><given-names>JM</given-names></name>, <name><surname>Solomon</surname><given-names>EM</given-names></name> (<year>1989</year>) <article-title>Perceptual linkage of multiple objects rotating in depth</article-title>. <source>Perception</source>
<volume>18</volume>: <fpage>427</fpage>–<lpage>444</lpage>.<pub-id pub-id-type="pmid">2813020</pub-id></mixed-citation>
              </ref>
              <ref id="pone.0096383-Gilroy1">
                <label>26</label>
                <mixed-citation publication-type="journal"><name><surname>Gilroy</surname><given-names>LA</given-names></name>, <name><surname>Blake</surname><given-names>R</given-names></name> (<year>2004</year>) <article-title>Physics embedded in visual perception of three-dimensional shape from motion</article-title>. <source>Nat Neurosci</source>
<volume>7</volume>: <fpage>921</fpage>–<lpage>922</lpage>.<pub-id pub-id-type="pmid">15300254</pub-id></mixed-citation>
              </ref>
              <ref id="pone.0096383-Sereno1">
                <label>27</label>
                <mixed-citation publication-type="journal"><name><surname>Sereno</surname><given-names>ME</given-names></name>, <name><surname>Sereno</surname><given-names>MI</given-names></name> (<year>1999</year>) <article-title>2-D center-surround effects on 3-D structure-from-motion</article-title>. <source>J Exp Psychol Hum Percept Perform</source>
<volume>25</volume>: <fpage>1834</fpage>–<lpage>1854</lpage>.<pub-id pub-id-type="pmid">10641318</pub-id></mixed-citation>
              </ref>
              <ref id="pone.0096383-Freeman1">
                <label>28</label>
                <mixed-citation publication-type="journal"><name><surname>Freeman</surname><given-names>ED</given-names></name>, <name><surname>Driver</surname><given-names>J</given-names></name> (<year>2006</year>) <article-title>Subjective appearance of ambiguous structure-from-motion can be driven by objective switches of a separate less ambiguous context</article-title>. <source>Vision Res</source>
<volume>46</volume>: <fpage>4007</fpage>–<lpage>4023</lpage>.<pub-id pub-id-type="pmid">16996558</pub-id></mixed-citation>
              </ref>
              <ref id="pone.0096383-Klink1">
                <label>29</label>
                <mixed-citation publication-type="journal">Klink PC, Noest AJ, Holten V, van den Berg AV, van Wezel RJ (2009) Occlusion-related lateral connections stabilize kinetic depth stimuli through perceptual coupling. J Vis <volume>9</volume>: : 20 21–20.</mixed-citation>
              </ref>
              <ref id="pone.0096383-McGurk1">
                <label>30</label>
                <mixed-citation publication-type="journal"><name><surname>McGurk</surname><given-names>H</given-names></name>, <name><surname>MacDonald</surname><given-names>J</given-names></name> (<year>1976</year>) <article-title>Hearing lips and seeing voices</article-title>. <source>Nature</source>
<volume>264</volume>: <fpage>746</fpage>–<lpage>748</lpage>.<pub-id pub-id-type="pmid">1012311</pub-id></mixed-citation>
              </ref>
              <ref id="pone.0096383-Sekuler1">
                <label>31</label>
                <mixed-citation publication-type="journal"><name><surname>Sekuler</surname><given-names>R</given-names></name>, <name><surname>Sekuler</surname><given-names>AB</given-names></name>, <name><surname>Lau</surname><given-names>R</given-names></name> (<year>1997</year>) <article-title>Sound alters visual motion perception [letter]</article-title>. <source>Nature</source>
<volume>385</volume>: <fpage>308</fpage>.<pub-id pub-id-type="pmid">9002513</pub-id></mixed-citation>
              </ref>
              <ref id="pone.0096383-Alais1">
                <label>32</label>
                <mixed-citation publication-type="journal"><name><surname>Alais</surname><given-names>D</given-names></name>, <name><surname>Burr</surname><given-names>D</given-names></name> (<year>2004</year>) <article-title>The ventriloquist effect results from near-optimal bimodal integration</article-title>. <source>Curr Biol</source>
<volume>14</volume>: <fpage>257</fpage>–<lpage>262</lpage>.<pub-id pub-id-type="pmid">14761661</pub-id></mixed-citation>
              </ref>
              <ref id="pone.0096383-Jain4">
                <label>33</label>
                <mixed-citation publication-type="journal">Jain A, Sally SL, Papathomas TV (2008) Audiovisual short-term influences and aftereffects in motion: examination across three sets of directional pairings. J Vis <volume>8</volume>: : 7 1–13.</mixed-citation>
              </ref>
              <ref id="pone.0096383-Michel2">
                <label>34</label>
                <mixed-citation publication-type="journal"><name><surname>Michel</surname><given-names>MM</given-names></name>, <name><surname>Jacobs</surname><given-names>RA</given-names></name> (<year>2007</year>) <article-title>Parameter learning but not structure learning: a Bayesian network model of constraints on early perceptual learning</article-title>. <source>J Vis</source>
<volume>7</volume>: <fpage>4</fpage>.</mixed-citation>
              </ref>
            </ref-list>
          </back>
        </article>
      </metadata>
    </record>
  </GetRecord>
</OAI-PMH>
