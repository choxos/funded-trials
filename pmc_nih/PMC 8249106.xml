<?xml version="1.0" encoding="UTF-8"?>
<OAI-PMH xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/ http://www.openarchives.org/OAI/2.0/OAI-PMH.xsd">
  <responseDate>2024-05-03T01:42:42Z</responseDate>
  <request verb="GetRecord" identifier="oai:pubmedcentral.nih.gov:8249106" metadataPrefix="pmc">https:/www.ncbi.nlm.nih.gov/pmc/oai/oai.cgi</request>
  <GetRecord>
    <record>
      <header>
        <identifier>oai:pubmedcentral.nih.gov:8249106</identifier>
        <datestamp>2021-07-02</datestamp>
        <setSpec>pheelsevier</setSpec>
        <setSpec>pmc-open</setSpec>
      </header>
      <metadata>
        <article xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" xsi:schemaLocation="https://jats.nlm.nih.gov/ns/archiving/1.3/ https://jats.nlm.nih.gov/archiving/1.3/xsd/JATS-archivearticle1-3.xsd" article-type="research-article" dtd-version="1.3">
          <processing-meta base-tagset="archiving" mathml-version="3.0" table-model="xhtml" tagset-family="jats">
            <restricted-by>pmc</restricted-by>
          </processing-meta>
          <front>
            <journal-meta>
              <journal-id journal-id-type="nlm-ta">Contemp Clin Trials</journal-id>
              <journal-id journal-id-type="iso-abbrev">Contemp Clin Trials</journal-id>
              <journal-title-group>
                <journal-title>Contemporary Clinical Trials</journal-title>
              </journal-title-group>
              <issn pub-type="ppub">1551-7144</issn>
              <issn pub-type="epub">1559-2030</issn>
              <publisher>
                <publisher-name>Elsevier Inc.</publisher-name>
              </publisher>
            </journal-meta>
            <article-meta>
              <article-id pub-id-type="accession">PMC8249106</article-id>
              <article-id pub-id-type="pmcid">PMC8249106</article-id>
              <article-id pub-id-type="pmc-uid">8249106</article-id>
              <article-id pub-id-type="pmid">34217888</article-id>
              <article-id pub-id-type="pii">S1551-7144(21)00236-6</article-id>
              <article-id pub-id-type="doi">10.1016/j.cct.2021.106500</article-id>
              <article-id pub-id-type="publisher-id">106500</article-id>
              <article-categories>
                <subj-group subj-group-type="heading">
                  <subject>Article</subject>
                </subj-group>
              </article-categories>
              <title-group>
                <article-title>An innovative protocol for the artificial speech-directed, contactless administration of laboratory-based comprehensive cognitive assessments: PAAD-2 trial management during the COVID-19 pandemic</article-title>
              </title-group>
              <contrib-group>
                <contrib contrib-type="author" id="au0005">
                  <name>
                    <surname>Park</surname>
                    <given-names>K. Shin</given-names>
                  </name>
                  <xref rid="cr0005" ref-type="corresp">⁎</xref>
                </contrib>
                <contrib contrib-type="author" id="au0010">
                  <name>
                    <surname>Etnier</surname>
                    <given-names>Jennifer L.</given-names>
                  </name>
                </contrib>
                <aff id="af0005">Department of Kinesiology, University of North Carolina at Greensboro, United States of America</aff>
              </contrib-group>
              <author-notes>
                <corresp id="cr0005"><label>⁎</label>Corresponding author.</corresp>
              </author-notes>
              <pub-date pub-type="pmc-release">
                <day>2</day>
                <month>7</month>
                <year>2021</year>
              </pub-date>
              <!-- PMC Release delay is 0 months and 0 days and was based on <pub-date
						pub-type="epub">.-->
              <pub-date pub-type="ppub">
                <month>8</month>
                <year>2021</year>
              </pub-date>
              <pub-date pub-type="epub">
                <day>2</day>
                <month>7</month>
                <year>2021</year>
              </pub-date>
              <volume>107</volume>
              <fpage>106500</fpage>
              <lpage>106500</lpage>
              <history>
                <date date-type="received">
                  <day>14</day>
                  <month>12</month>
                  <year>2020</year>
                </date>
                <date date-type="rev-recd">
                  <day>28</day>
                  <month>6</month>
                  <year>2021</year>
                </date>
                <date date-type="accepted">
                  <day>28</day>
                  <month>6</month>
                  <year>2021</year>
                </date>
              </history>
              <permissions>
                <copyright-statement>© 2021 Elsevier Inc. All rights reserved.</copyright-statement>
                <copyright-year>2021</copyright-year>
                <copyright-holder>Elsevier Inc.</copyright-holder>
                <license>
                  <license-p>Since January 2020 Elsevier has created a COVID-19 resource centre with free information in English and Mandarin on the novel coronavirus COVID-19. The COVID-19 resource centre is hosted on Elsevier Connect, the company's public news and information website. Elsevier hereby grants permission to make all its COVID-19-related research that is available on the COVID-19 resource centre - including this research content - immediately available in PubMed Central and other publicly funded repositories, such as the WHO COVID database with rights for unrestricted research re-use and analyses in any form or by any means with acknowledgement of the original source. These permissions are granted for free by Elsevier for as long as the COVID-19 resource centre remains active.</license-p>
                </license>
              </permissions>
              <abstract id="ab0005">
                <p>The COVID-19 pandemic resulted in suspending in-person human subject research across most institutions in the US. Our extensive cognitive assessment for a phase-2 clinical trial, Physical Activity and Alzheimer's Disease–2 (PAAD-2), was also paused in March 2020. It was important to identify strategies to mitigate the risk of COVID-19 transmission during our testing, which initially required substantial human speech and close person-to-person contact for test directions and instant feedback on paper/pencil tests. Given current understanding of the COVID-19 transmission, we dramatically adjusted the testing protocol to minimize the production of speech droplets and allow social distancing while maintaining the integrity of testing. We adopted state-of-the-art speech synthesis and computerization techniques to create an avatar to speak on behalf of the experimenter for all verbal instructions/feedback, used a document camera to observe the paper/pencil tests from the required distances, and automated the testing sequence and timing. This paper aims 1) to describe an innovative laboratory-based cognitive testing protocol for a completely contact-free, computer-speaking, and semi-automated administration; and 2) to evaluate the integrity of the modified protocol (<italic>n</italic> = 37) compared with the original protocol (<italic>n</italic> = 32). We have successfully operated the modified protocol since July 2020 with no evidence of COVID-19 transmission during testing, and data support that the modified protocol is robust and captures data identical to the original protocol. This transition of data collection methods has been critical during the pandemic and will be useful in future studies to mitigate the risk of contagious disease transmission and standardize laboratory-based psychological tests.</p>
                <p><bold>Trial registration</bold>: ClinicalTrials.gov NCT03876314. Registered March 15, 2019</p>
              </abstract>
              <kwd-group id="ks0005">
                <title>Keywords</title>
                <kwd>Alzheimer's disease</kwd>
                <kwd>Artificial intelligence</kwd>
                <kwd>Executive function</kwd>
                <kwd>Neuropsychological tests</kwd>
                <kwd>Speech synthesis</kwd>
                <kwd>Text-to-speech</kwd>
                <kwd>Physical activity</kwd>
                <kwd>Working memory</kwd>
              </kwd-group>
            </article-meta>
          </front>
          <body>
            <sec id="s0005">
              <label>1</label>
              <title>Introduction</title>
              <p id="p0005">The COVID-19 pandemic resulted in the suspension of in-person human research activities across most institutions in the United States. In response to the serious health threat at global and national levels, universities acted quickly to vacate their campuses by converting teaching to online, sending students, faculty, and staff home to perform their duties remotely, and putting all non-essential research on pause [<xref rid="bb0005" ref-type="bibr">[1]</xref>, <xref rid="bb0010" ref-type="bibr">[2]</xref>, <xref rid="bb0015" ref-type="bibr">[3]</xref>]. Wigginton and colleagues [<xref rid="bb0020" ref-type="bibr">4</xref>] estimated that 80% of on-site research activities at the authors' universities were halted by limiting building access and only permitting studies on animals, patient safety, or COVID-19. Our extensive cognitive testing for a phase-2 clinical trial, Physical Activity and Alzheimer's Disease–2 (PAAD-2) [<xref rid="bb0025" ref-type="bibr">5</xref>], was also paused in March 2020.</p>
              <p id="p0010">It is known that COVID-19 is transmitted from human to human mainly through respiratory droplets that are spread when an infected person coughs, sneeze, or talks particularly when in close contact (within 6 ft) [<xref rid="bb0030" ref-type="bibr">6</xref>]. It was important to identify strategies to mitigate the risk of disease transmission during our four-hour laboratory testing session. Our original protocol required substantial verbal instructions for informed consent and test directions. In many instances, the experimenter was in close contact with a participant to observe cognitive performance on a computer monitor, mobile device, or paper form in order to instantly evaluate and provide necessary feedback. Therefore, our primary goal with protocol modifications was to minimize the possibility of directly and indirectly transmitting aerosolized droplets in our interactions with participants in the laboratory testing.</p>
              <p id="p0015">Mitigation of risk was partially attained by following the university's requirement of wearing face coverings and maintaining social distance for every person involved in testing. We further attempted to minimize the production of small speech droplets, which can cause airborne transmission of COVID-19 in confined environments [<xref rid="bb0035" ref-type="bibr">[7]</xref>, <xref rid="bb0040" ref-type="bibr">[8]</xref>, <xref rid="bb0045" ref-type="bibr">[9]</xref>, <xref rid="bb0050" ref-type="bibr">[10]</xref>]. Moreover, as speaking under face coverings imposes vocal fatigue and discomfort, difficulties in coordinating speech and breathing, and can make speech more difficult to understand [<xref rid="bb0055" ref-type="bibr">11</xref>], our goal was to reduce the need for speaking by the experimenter. As such, we employed state-of-the-art speech synthesis and computer programming techniques to have an avatar speak on behalf of the experimenter for all verbal instructions. We also used a document camera to allow the experimenter to observe participants' performance from the required or even farther distances and employed computer program to automate the testing sequences and timing control.</p>
              <p id="p0020">While the protocol modifications substantially addressed safety concerns relative to COVID-19 transmission, the maintenance of the integrity of testing was critical for the clinical trial. Although the naturalness of synthesized speech has been previously established [<xref rid="bb0060" ref-type="bibr">12</xref>,<xref rid="bb0065" ref-type="bibr">13</xref>], its validity has not been demonstrated for cognitive testing in a laboratory setting. As such, adjustments were made on the protocol in response to pilot testing, and comparisons were made between data collected with the original protocol and the modified protocol. Information regarding the protocol, pilot testing, and these comparisons is intended to assist future investigators to reduce the risk of transmission of contagious diseases and standardize the administration of complex cognitive testing paradigms.</p>
            </sec>
            <sec id="s0010">
              <label>2</label>
              <title>Objectives</title>
              <p id="p0025">The purpose of this paper is 1) to describe the detailed methods used to convert a complex cognitive testing protocol that involved close person-to-person contact, substantial human speech, and manual control of the testing sequence and timing into a completely contact-free, computer-speaking, and semi-automated protocol with the goal of significantly minimizing the production of human speech droplets and ensuring the maintenance of social distancing; and 2) to evaluate the integrity of the administration of the modified protocol (<italic>n</italic> = 37) in comparison with the administration of the original protocol (<italic>n</italic> = 32). This transition in terms of the data collection method has been critical during the COVID-19 pandemic and will be useful in future studies to mitigate the risk of transmitting contagious illnesses and to further standardize and automate the administration of laboratory-based cognitive tests. Our detailed description of the modified laboratory testing is intended to be useful for other researchers to partly or entirely replicate similar protocol adjustments during and after the pandemic while also providing a detailed description of the modification of the PAAD-2 protocol [<xref rid="bb0025" ref-type="bibr">5</xref>] as implemented after July 2020.</p>
            </sec>
            <sec id="s0015">
              <label>3</label>
              <title>Protocol modification</title>
              <sec id="s0020">
                <label>3.1</label>
                <title>Speech synthesis techniques</title>
                <p id="p0030">In 1968, the filmmakers of the epic science movie, “2001: Space Odyssey”, depicted a time 33 years in the future when an artificial intelligence (AI) computer could generate human-like voices to verbally communicate with spaceship crews. This futuristic vision was realized through the use of AI technology about a decade after the imagined year in that machine-generated speech started to become widely available through virtual assistants in smartphones, computers, and other modern devices such as Apple's Siri, Amazon's Alexa, and the Google Assistant [<xref rid="bb0070" ref-type="bibr">14</xref>,<xref rid="bb0075" ref-type="bibr">15</xref>]. Since then, speech synthesis, also known as text-to-speech (TTS) technique, has been rapidly advancing and now the commercial Application Programming Interface (API) platforms<xref rid="fn0005" ref-type="fn">1</xref>
allow people to easily create synthetic speech by converting text input into voice output. Recent improvements in one of the TTS synthesis models, called WaveNet [<xref rid="bb0080" ref-type="bibr">16</xref>,<xref rid="bb0085" ref-type="bibr">17</xref>], and subsequent neural network modeling have enabled substantial enhancements in the naturalness of synthesized voices to the extent of rivaling human speech [<xref rid="bb0060" ref-type="bibr">12</xref>,<xref rid="bb0065" ref-type="bibr">13</xref>]. Not just short-form content at the word, sentence, or paragraph level [<xref rid="bb0090" ref-type="bibr">18</xref>], but synthetic voices reading out a long-form article of more than 900 words were found to be comprehensible and pleasant to listen to for several minutes at a comparable level to human voices [<xref rid="bb0095" ref-type="bibr">19</xref>].</p>
                <p id="p0035">As a necessary modification to the PAAD-2 protocol [<xref rid="bb0025" ref-type="bibr">5</xref>] to allow for data collection during the COVID-19 pandemic, we created and operated synthetic voices that directed the entire 4-h testing session for the informed consent and cognitive assessments allowing the experimenter to maintain the required (6 ft) or even farther distances in the laboratory. Specifically, the summary of the consent form and all verbal instructions/feedback of cognitive tests were written as text or Speech Synthesis Markup Language (SSML) input files in the JavaScript Object Notation (JSON) format. We chose the WaveNet voice (en-US-Wavenet-D) and then set the rate of speaking to 0.89–0.93 and the pitch to −2.8. We used the macOS command line interface to convert the text or SSML files into waveform audio file (WAV) formats on the Google Cloud TTS API. More information on the implementation of the Google Cloud TTS API are proprietary but publicly available in a web document [<xref rid="bb0100" ref-type="bibr">20</xref>]. All WAV files of the synthetic voices were then added as sound components in an open-source programming platform, PsychoPy Experiment Builder [<xref rid="bb0105" ref-type="bibr">21</xref>], which are further described in later sections of this paper.</p>
                <p id="p0040">Consequently, our synthetic voice directs the entire testing session. The voice first explains the COVID-19 safety precautions, briefly describes each paragraph of the informed consent form, provides general instructions for the testing session, gives specific instructions and/or feedback for the Montreal Cognitive Assessment (MoCA) [<xref rid="bb0110" ref-type="bibr">22</xref>], the Test of Premorbid Functioning (TOPF) [<xref rid="bb0115" ref-type="bibr">23</xref>], the Rey-Osterrieth Complex Figure Test (ROCFT) [<xref rid="bb0120" ref-type="bibr">24</xref>,<xref rid="bb0125" ref-type="bibr">25</xref>], the Paced Auditory Serial Addition Test (PASAT) [<xref rid="bb0130" ref-type="bibr">26</xref>], the Rey Auditory Verbal Learning Test (RAVLT) [<xref rid="bb0125" ref-type="bibr">25</xref>], the Trail Making Test (TMT) [<xref rid="bb0135" ref-type="bibr">27</xref>], and the Symbol Digits Modalities Test (SDMT) [<xref rid="bb0140" ref-type="bibr">28</xref>]. To maintain consistency with our original protocol, the synthetic voice asks participants to read and follow the text instructions written on the screen for the tests administered with <italic>E</italic>-Prime 3.0 software [<xref rid="bb0145" ref-type="bibr">29</xref>] on a desktop computer and the NIH Toolbox cognition battery on the iPad [<xref rid="bb0150" ref-type="bibr">30</xref>]. The synthetic voice also directs participants to make appropriate transitions between tasks on the computer with a keyboard or mouse, the iPad, or hard copies of documents, and to take a break for certain durations. See <xref rid="t0005" ref-type="table">Table 1</xref>
for the instruments and response formats of each test.<table-wrap position="float" id="t0005"><label>Table 1</label><caption><p>An overview of the PAAD2 cognitive testing protocol modification in comparison to the original protocol.</p></caption><alt-text id="al0015">Table 1</alt-text><table frame="hsides" rules="groups"><thead><tr><th rowspan="2">Batch</th><th rowspan="2">Testing components</th><th rowspan="2">Operating instrument</th><th rowspan="2">Response format</th><th colspan="4">Comparisons of test duration (min, <italic>M</italic> ± <italic>SE</italic>)<hr/></th></tr><tr><th>Original (<italic>n</italic> = 32)</th><th>Modified (<italic>n</italic> = 37)</th><th>ET (<italic>p</italic>)</th><th><italic>t</italic>-test (<italic>p</italic>)</th></tr></thead><tbody><tr><td rowspan="3">0</td><td>Safety Precautions</td><td>Python<sup>†</sup></td><td>P/P</td><td>–</td><td>5 ± 0.20</td><td>–</td><td>–</td></tr><tr><td>Informed Consent <sup>Pre only</sup></td><td>Python<sup>†</sup></td><td>P/P</td><td>≈ 15</td><td>16 ± 0.58</td><td>&lt; 0.05</td><td>0.09</td></tr><tr><td>MoCA <sup>Δ Pre &amp; Post only</sup></td><td>Python<sup>†</sup></td><td>P/P<sup>Cam</sup>, Verbal<sup>Mic</sup></td><td>≈ 10</td><td>10.2 ± 0.30</td><td>&lt; 0.001</td><td>0.49</td></tr><tr><td rowspan="6">1</td><td>General Instructions</td><td>Python<sup>†</sup></td><td>–</td><td>≈ 3</td><td>2.8 ± 0.24</td><td>&lt; 0.05</td><td>0.48</td></tr><tr><td>Test of Premorbid Functioning <sup>Pre only</sup></td><td>Python<sup>†</sup></td><td>Verbal<sup>Mic</sup></td><td>3.4 ± 0.13</td><td>3.4 ± 0.12</td><td>&lt; 0.01</td><td>0.83</td></tr><tr><td>NIH TB List Sort Working Memory</td><td>iPad</td><td>Verbal</td><td>8.4 ± 0.22</td><td>8.1 ± 0.21</td><td>&lt; 0.05</td><td>0.26</td></tr><tr><td>NIH TB Picture Sequence Memory <sup>Δ</sup></td><td>iPad</td><td>Screen touch</td><td>7.7 ± 0.23</td><td>7.7 ± 0.25</td><td>&lt; 0.01</td><td>0.98</td></tr><tr><td>Mnemonic Similarity Task <sup>Δ</sup></td><td><italic>E</italic>-Prime</td><td>Key press</td><td>10.9 ± 0.10</td><td>11.4 ± 0.24</td><td>0.16</td><td>0.06</td></tr><tr><td>Perceptual Discrimination Task <sup>Δ</sup></td><td>E-Prime</td><td>Key press</td><td>5.78 ± 0.38</td><td>4.64 ± 0.20</td><td>0.75</td><td>&lt; 0.01</td></tr><tr><td rowspan="10">2</td><td>ROCFT – Copy</td><td>Python<sup>†</sup></td><td>P/P</td><td>3.2 ± 0.23</td><td>5.8 ± 0.59</td><td>0.89</td><td>&lt; 0.001</td></tr><tr><td>Break</td><td>Python<sup>†</sup></td><td>–</td><td>3</td><td>3 ± 0.00</td><td>&lt; 0.001</td><td>1</td></tr><tr><td>ROCFT - Immediate Recall</td><td>Python<sup>†</sup></td><td>P/P</td><td>2.7 ± 0.20</td><td>3.87 ± 0.39</td><td>0.39</td><td>&lt; 0.01</td></tr><tr><td>Stroop Color-Word Task</td><td>E-Prime</td><td>Key press</td><td>6.1 ± 0.21</td><td>5.83 ± 0.15</td><td>0.06</td><td>0.37</td></tr><tr><td>PASAT - 3′ and 2’</td><td>Python<sup>† fd</sup></td><td>Verbal<sup>Mic</sup></td><td>≈ 12</td><td>12 ± 0.21</td><td>&lt; 0.01</td><td>0.63</td></tr><tr><td>(Break; for 30 min delay)</td><td>Python<sup>†</sup></td><td>–</td><td>(≈ 5; 30)</td><td>(4.5 ± 0.42; 29.8 ± 0.16)</td><td>&lt; 0.01</td><td>0.21</td></tr><tr><td>ROCFT - Recall</td><td>Python<sup>†</sup></td><td>P/P</td><td>1.8 ± 0.15</td><td>2.5 ± 0.24</td><td>0.33</td><td>&lt; 0.05</td></tr><tr><td>ROCFT - Recognition</td><td>Python<sup>†</sup></td><td>P/P</td><td>≈ 3</td><td>2.8 ± 0.25</td><td>&lt; 0.01</td><td>0.34</td></tr><tr><td>Tower of London - Freiburg version <sup>Δ</sup></td><td>VTS</td><td>Mouse click</td><td>10.5 ± 0.43</td><td>10.2 ± 0.31</td><td>&lt; 0.05</td><td>0.58</td></tr><tr><td>Break</td><td>Python<sup>†</sup></td><td>–</td><td>≈ 7</td><td>7 ± 0.00</td><td>&lt; 0.001</td><td>1</td></tr><tr><td rowspan="8">3</td><td>RAVLT - Learning &amp; Recall <sup>Δ</sup></td><td>Python<sup>†</sup></td><td>Verbal<sup>Mic</sup></td><td>≈ 8</td><td>8.2 ± 0.17</td><td>&lt; 0.01</td><td>0.89</td></tr><tr><td>NIH TB Dimensional Change Card Sort</td><td>iPad</td><td>Screen touch</td><td>5.1 ± 0.05</td><td>5.1 ± 0.06</td><td>&lt; 0.05</td><td>0.25</td></tr><tr><td>NIH TB Flanker Inhibitory Control</td><td>iPad</td><td>Screen touch</td><td>3.3 ± 0.03</td><td>3.3 ± 0.03</td><td>&lt; 0.05</td><td>0.41</td></tr><tr><td>Spatial Working Memory</td><td>E-Prime</td><td>Key press</td><td>11.0 ± 0.31</td><td>10.7 ± 0.19</td><td>&lt; 0.05</td><td>0.46</td></tr><tr><td>Trail Making Test - A &amp; B</td><td>Python<sup>† fd</sup></td><td>P/P<sup>Cam</sup></td><td>≈ 5</td><td>5.0 ± 0.09</td><td>&lt; 0.001</td><td>0.74</td></tr><tr><td>(Break; for 30 min delay)</td><td>Python<sup>†</sup></td><td>–</td><td>(≈ 5; 30)</td><td>(4.7 ± 0.28; 30 ± 0.00)</td><td>&lt; 0.001</td><td>1</td></tr><tr><td>RAVLT - Recall <sup>Δ</sup></td><td>Python<sup>†</sup></td><td>Verbal<sup>Mic</sup></td><td>≈ 1.4</td><td>1.4 ± 0.02</td><td>&lt; 0.05</td><td>0.06</td></tr><tr><td>RAVLT - Recognition <sup>Δ</sup></td><td>Python<sup>†</sup></td><td>P/P</td><td>≈ 3.5</td><td>3.4 ± 0.11</td><td>&lt; 0.01</td><td>0.33</td></tr><tr><td rowspan="7">4</td><td>Paired Associates - Learning &amp; Recall <sup>Δ</sup></td><td>Python<sup>†</sup></td><td>Verbal</td><td>≈ 3</td><td>2.96 ± 0.08</td><td>&lt; 0.01</td><td>0.60</td></tr><tr><td>Matrix Reasoning <sup>Δ</sup></td><td>E-Prime</td><td>Mouse click</td><td>14.1 ± 0.34</td><td>14.1 ± 0.34</td><td>&lt; 0.01</td><td>0.90</td></tr><tr><td>Digits Span - Forward</td><td>E-Prime</td><td>Verbal</td><td>3.1 ± 0.30</td><td>2.8 ± 0.12</td><td>0.08</td><td>0.23</td></tr><tr><td>(Break; for 20 min delay)</td><td>Python<sup>†</sup></td><td>–</td><td>(≈ 2; 20)</td><td>(1.8 ± 0.17; 20 ± 0.00)</td><td>&lt; 0.001</td><td>1</td></tr><tr><td>Paired Associates - Recall <sup>Δ</sup></td><td>Python<sup>†</sup></td><td>Verbal</td><td>≈ 1.5</td><td>1.5 ± 0.08</td><td>&lt; 0.01</td><td>0.60</td></tr><tr><td>Digits Span - Backward</td><td>E-Prime</td><td>Verbal</td><td>2.3 ± 0.16</td><td>2.6 ± 0.25</td><td>&lt; 0.05</td><td>0.34</td></tr><tr><td>Break</td><td>Python<sup>†</sup></td><td>–</td><td>≈ 5</td><td>4.7 ± 0.19</td><td>&lt; 0.01</td><td>0.17</td></tr><tr><td rowspan="6">5</td><td>Logical Memory - Learning &amp; Recall <sup>Δ</sup></td><td>Python<sup>†</sup></td><td>Verbal<sup>Mic</sup></td><td>4.1 ± 0.08</td><td>4.2 ± 0.09</td><td>&lt; 0.05</td><td>0.37</td></tr><tr><td>Spatial Relations <sup>Δ</sup></td><td>E-Prime</td><td>Mouse click</td><td>11.0 ± 0.38</td><td>10.6 ± 0.31</td><td>&lt; 0.05</td><td>0.47</td></tr><tr><td>SDMT Written &amp; Oral Trials</td><td>Python<sup>† fd</sup></td><td>P/P<sup>Cam</sup>, Verbal<sup>Mic</sup></td><td>≈ 7</td><td>6.7 ± 0.09</td><td>0.12</td><td>&lt; 0.01</td></tr><tr><td>SDMT Incidental Learning <sup>Δ</sup></td><td>Python<sup>†</sup></td><td>P/P</td><td>0.82 ± 0.07</td><td>1.1 ± 0.11</td><td>0.12</td><td>0.08</td></tr><tr><td>(Break; for 20 min delay)</td><td>Python<sup>†</sup></td><td>–</td><td>(≈ 1; 20)</td><td>(1.4 ± 0.32; 20.3 ± 0.42)</td><td>&lt; 0.01</td><td>0.32</td></tr><tr><td>Logical Memory - Recall <sup>Δ</sup></td><td>Python<sup>†</sup></td><td>Verbal <sup>Mic</sup></td><td>1.9 ± 0.05</td><td>1.9 ± 0.05</td><td>&lt; 0.05</td><td>0.47</td></tr></tbody></table><table-wrap-foot><fn id="sp0025"><p><italic>Abbreviation</italic>: MoCA, Montreal Cognitive Assessment; PASAT, Paced Auditory Serial Addition Test; P/P, Paper/Pencil; RAVLT, Rey Auditory Verbal Learning Test; ROCFT, Rey-Osterrieth Complex Figure Test; SDMT, Symbol Digits Modalities Test; VTS, Vienna Test System.</p></fn><fn id="sp0030"><p><italic>Note</italic>: One- or two-sample <italic>t</italic>-tests or equivalence tests (ET) were conducted to compare each test duration between two protocols. Results indicate that two protocols are significantly equivalent in terms of its duration. Equivalence margin (Cohen's d, δ) was ±0.7. Some test duration of the original protocol was not measured and thus estimated (≈). Total protocol duration is about 3.5 h at pre-test and 3 h at mid- and pos<italic>t</italic>-test. Synthetic voice is used for all necessary verbal instructions and feedback for informed consent, test directions and transitions, and breaks between tests. <sup>Δ</sup> Different forms are used at pre-, mid-, and post-test. Break duration in parentheses is computed by a custom-developed Python code and instructed by the synthetic voice. † Newly added features in the modified protocol. <sup>fd</sup> Synthetic voice gives feedback in response to a key press if necessary. <sup>Mic</sup> Subjects' verbal responses are recorded on a microphone. <sup>Cam</sup> Subjects' paper works are observed from the required distance using a document camera. Batch 0 without informed consent is combined with Batch 1 at mid- and pos<italic>t</italic>-tests and comes after the biological sampling. Testing sequences of Batch 2 and 4 can be changed by automatic timing control by the Python codes (see <xref rid="f0010" ref-type="fig">Fig. 2</xref> for more information).</p></fn></table-wrap-foot></table-wrap></p>
                <p id="p0045">Before the implementation of our modified protocol, we ensured that the naturalness of synthetic speech was acceptable for an extensive cognitive assessment through pilot testing of the entire protocol. We therefore repeatedly tested whether six pilot subjects (a professor and graduate students in psychology) clearly understand the synthesized instructions for test directions and safety precautions and accordingly designed the new protocol to ensure that the artificial speech and automated sequence are easy to follow and identical to the original protocol, which is further addressed in this paper. In the following paragraphs, we further illustrate how the synthetic voices are presented along with an avatar to direct the entire testing protocol based on precise management of timing.</p>
              </sec>
              <sec id="s0025">
                <label>3.2</label>
                <title>Speaking avatar</title>
                <p id="p0050">We developed an avatar and presented him with his mouth moving along with the synthesized voices given that human speech can be better understood by seeing the face of a talker even when the speech is audible and intact [<xref rid="bb0155" ref-type="bibr">31</xref>]. We first created a series of still images of different facial expressions and compiled the images in short time intervals (less than 50 ms) to create an animation image that included moving eyes, eyebrows, jaws, and lips to imitate facial movements of human speech. We then put a face covering on the avatar as a means of requesting participants to do the same and building rapport with the participant during the testing session. The avatar was added and programmed as a movie component to the PsychoPy Builder [<xref rid="bb0105" ref-type="bibr">21</xref>] in time with the audio files of synthetic voices. Consequently, the synthetic voice was presented in synchrony with the avatar just like he was talking to participants on the computer monitor, so that our participants would better understand verbal instructions and be more engaged with the computer during the testing session.</p>
              </sec>
              <sec id="s0030">
                <label>3.3</label>
                <title>Contact-free testing environment</title>
                <p id="p0055">We configured the hardware by connecting a desktop computer as the central processing unit (CPU) to two monitors, two keyboards, and two mice for an experimenter and a participant (see <xref rid="f0005" ref-type="fig">Fig. 1</xref>
). We set up the dual monitors to display duplicate content, and accordingly, the experimenter was able to operate each test on the computer, observe a participant's performance on the monitor, and observe the behaviors/responses of a participant from farther away than the 6-ft required distance. For a participant to better interact with the computer, we set up a sound speaker and a microphone near the participant's monitor to provide the verbal instructions and audio-record their verbal responses.<fig id="f0005"><label>Fig. 1</label><caption><p>A schematic overview of the avatar-directed contactless cognitive testing environment.</p></caption><alt-text id="al0005">Fig. 1</alt-text><graphic xlink:href="gr1_lrg"/></fig></p>
                <p id="p0060">Some of our paper/pencil tests (i.e., MoCA, TMT, and SDMT) required monitoring participants' hand drawing or writings from a close distance for instant evaluations and/or feedback [<xref rid="bb0110" ref-type="bibr">22</xref>,<xref rid="bb0135" ref-type="bibr">27</xref>,<xref rid="bb0140" ref-type="bibr">28</xref>,<xref rid="bb0160" ref-type="bibr">32</xref>]. We therefore set the paper forms on a clipboard along with a document camera above them, then connected the camera to the computer and developed a custom Python code based on an open-source computer vision package, the OpenCV library [<xref rid="bb0165" ref-type="bibr">33</xref>]. Using this method, the experimenter was able to see the camera view on the monitor to observe participant's performance on the paper forms and provide necessary evaluation/feedback through the synthetic voice by pressing designated keys on the keyboard (see <xref rid="f0005" ref-type="fig">Fig. 1</xref> for a schematic overview).</p>
                <p id="p0065">During the testing, the synthetic voice instructed participants to receive, complete, and submit all paper forms in a contact-free manner for safety management. A file tray along with all test forms in file folders were set up on the right side from the participants, at least 24 h before the testing session. For each test during the testing session, the avatar instructed participants to take out certain testing form(s) from a file folder in a particular color (e.g., yellow, purple, or red) from each shelf of the file tray and complete the necessary paper tests using a pencil or pen. Separate folders were necessary to keep the contents of certain forms of memory tests confidential before the administration (e.g., delayed recognition for the ROCFT and RAVLT). Upon completion of paperwork, participants were instructed to submit the completed forms into the bottom shelf of the file tray.</p>
              </sec>
              <sec id="s0035">
                <label>3.4</label>
                <title>Programming platform</title>
                <p id="p0070">We developed and operated the entire testing protocol using Python programming language [<xref rid="bb0170" ref-type="bibr">34</xref>] along with an open-source Python-based experiment control software, PsychoPy [<xref rid="bb0105" ref-type="bibr">21</xref>,<xref rid="bb0175" ref-type="bibr">35</xref>,<xref rid="bb0180" ref-type="bibr">36</xref>]. PsychoPy (available at <ext-link ext-link-type="uri" xlink:href="http://psychopy.org" id="ir0025">psychopy.org</ext-link>) was developed for designing and editing behavioral experiments based on a graphical user interface (GUI) called “Builder” and/or Python scripts [<xref rid="bb0105" ref-type="bibr">21</xref>,<xref rid="bb0175" ref-type="bibr">35</xref>,<xref rid="bb0180" ref-type="bibr">36</xref>]. PsychoPy Builder allows the researcher to generate a Python script for the developed experiment, which is easily executed as a Python program. PsychoPy allowed us to start and stop the synthetic voice and talking avatar as sound and movie components in synchrony, audio-record verbal responses using the microphone components, measure the duration of test performances, program the sequences of the entire testing procedure, and automatically execute the proper tests using the clock functions and code components based on sub-millisecond precision [<xref rid="bb0185" ref-type="bibr">37</xref>,<xref rid="bb0190" ref-type="bibr">38</xref>]. Detailed information on the components and function are publicly available in the PsychoPy reference manual [<xref rid="bb0195" ref-type="bibr">39</xref>].</p>
              </sec>
              <sec id="s0040">
                <label>3.5</label>
                <title>Batched and automated test timing and sequence</title>
                <p id="p0075">Using the PsychoPy Builder interface, we developed Python programs to enable the computerized administration of the MoCA, TOPF, ROCFT, PASAT, RAVLT, TMT, SDMT, Paired Associates, and Logical Memory based on each test's administration manual. We then compiled all test programs into five (mid- and post-test) or six (pre-test) batches so that the Python programs would keep functioning to measure the timing when participants were working on tests one after another (see <xref rid="t0005" ref-type="table">Table 1</xref> and <xref rid="f0010" ref-type="fig">Fig. 2</xref>
for an overview). By collating the tests into batches, the programs were able to automatically execute the correct test at the exactly desired time, count the duration for a break, and provide instructions for a break of any duration. The Python batches continued its timer function when <italic>E</italic>-Prime tests were operated.<fig id="f0010"><label>Fig. 2</label><caption><p>Flowcharts of the Python batches 2–5.</p><p>Codes are available in the appendix. Batch 0 and 1 have no code components and are excluded in this figure but depicted in <xref rid="t0005" ref-type="table">Table 1</xref>. Abbreviation: DCCS, Dimensional Change Card Sort Test; PASAT; Paced Auditory Serial Addition Test; RAVLT, Rey Auditory Verbal Learning Test; ROCFT, Rey-Osterrieth Complex Figure Test; SDMT, Symbol Digits Modalities Test; TMT, Trail Making Test; TOL-F, Tower of London - Freiburg version; VTS, Vienna Test System.</p></caption><alt-text id="al0010">Fig. 2</alt-text><graphic xlink:href="gr2_lrg"/></fig></p>
                <p id="p0080">While test timing was not important and all directions were provided in a fixed order in Batch 0 and Batch 1, time measurement was critical for the 20- or 30-min delayed recall of the ROCFT, RAVLT, Paired Associates, and Logical Memory tests (hereafter called delayed memory tests) in Batches 2, 3, 4, and 5 to choose and administer the correct test at the exact right time. See <xref rid="f0010" ref-type="fig">Fig. 2</xref> for the sequences and logic of Batch 2, 3, 4, and 5. Immediately after the copy/learning or initial recall trials for the delayed memory tests, a timer was programmed to start keeping track of time. When beginning the break routine, the remaining time to the 20- or 30-min delay was counted, and the avatar instructed the participants to take a break for the measured duration. Using the text components in the PsychoPy Builder, a countdown timer in minutes and seconds was displayed on the computer screen during the break. When the break was over, the avatar provided instructions for the delayed memory tests in each batch. When the time limit was exceeded, rarely but possibly by a few seconds or minutes for slow test takers, no break was offered, and the delayed recall trials started immediately.</p>
              </sec>
              <sec id="s0045">
                <label>3.6</label>
                <title>Automation of program execution</title>
                <p id="p0085">Our modified protocol is further equipped with automatic execution of all computer tests and batch programs with Python or <italic>E</italic>-Prime 3.0 following the manual implementation of the initial program. We enabled the automatic administration by using a customized code from an open-source operating system interface package, OS module, in the Python standard library [<xref rid="bb0200" ref-type="bibr">40</xref>]. We specifically added the function ‘os.startfile (path)’ to the code component in the PsychoPy Builder in order to start a file with its associated application, which acted like double-clicking the designated test files or batch programs. This technique allowed the experimenter to efficiently administer the entire testing protocol by eliminating any chance of wasting time to locate and manually start a test or batch file or making errors by executing a wrong program. Below we further describe how each testing protocol is specifically programmed in the different batches.</p>
              </sec>
              <sec id="s0050">
                <label>3.7</label>
                <title>Informed consent and screening</title>
                <p id="p0090">In Batch 0 at the pre-test, the avatar read out the summary of safety precautions and informed consent. Participants were instructed to press spacebar on the keyboard to move on to the next paragraph when they fully understood the information. They were also encouraged to further read over the hard copies of the consent form or ask any questions to the experimenter. At the end of the consent, the avatar asked the participants to sign the form and submit the signed document into the file tray. The avatar then asked the participant to pick up a pencil and work on the paper form on a clipboard for the first three questions of the MoCA and verbally respond to the other questions. The MoCA was the screening tool of cognitive impairment, so the experimenter, who was trained and certified for the administration of the MoCA, carefully evaluated participants' verbal responses and drawings through the document camera and entered scores for each item on the keyboard. An algorithm was written to score the MoCA responses so that an indication of inclusion or exclusion could be provided. Based upon this, the avatar instructed the cognitively intact individuals to move on to the biological sampling session or people suspected of cognitive impairment to see the experimenter for further instructions. The experimenter discontinued the testing for excluded people and provided appropriate clinical referrals based on the PAAD-2 protocol [<xref rid="bb0025" ref-type="bibr">5</xref>].</p>
              </sec>
              <sec id="s0055">
                <label>3.8</label>
                <title>Operational procedure of testing batches</title>
                <p id="p0095">Batch 1 at the pre-test provided general directions and then asked the participant to pick up the word list card from a colored folder for TOPF. The experimenter evaluated and audio-recorded the verbal responses. After that, the avatar directed the participants to move on to the next two NIH Toolbox tests on the iPad. When the NIH Toolbox tests were finished, the next two tests were sequentially executed with <italic>E</italic>-Prime 3.0. At the mid- and post-test, the avatar started with safety precautions and general instructions. Then without the TOPF, Batch 1 at the mid-test continued the testing with the NIH Toolbox and E-Prime tests in the same order as the pre-test, while Batch 1 at the post-test started with the MoCA and continued to the NIH Toolbox and E-Prime tests in the same order. See <xref rid="t0005" ref-type="table">Table 1</xref> for an overview.</p>
                <p id="p0100">In Batch 2, the avatar instructed the participant to take out paper forms from a colored folder for the ROCFT copy trial. Once finished, the program started a 30-min timer and instructed the participant to take a 3-min break with a countdown timer shown on the screen. After that, participants were asked to take out a paper form for the ROCFT immediate recall. Then, the Stroop Color-Word Task, the same version with a similar clinical trial [<xref rid="bb0205" ref-type="bibr">41</xref>], was executed with <italic>E</italic>-Prime 3.0, and then the PASAT was followed. After that, the avatar instructed a break for the remaining time to 30 min. After the break, the participant was instructed to take out paper forms from colored folders, complete and submit them one after another for the ROCFT delayed recall and recognition. If less than 8 min was left for the PASAT, a break was given and the PASAT was administered after the delayed recognition. Next, the TOL-F [<xref rid="bb0210" ref-type="bibr">42</xref>] was administered, then the avatar gave the half-way break for 7 min. The duration of ROCFT trials was measured by a key press. See <xref rid="t0005" ref-type="table">Table 1</xref> and <xref rid="f0010" ref-type="fig">Fig. 2</xref> for an overview.</p>
                <p id="p0105">After the break, Batch 2 was closed and followed by the Batch 3, in which the RAVLT learning and immediate recalls were administered. Then, the 30-min timer started, and the avatar directed participants to the iPad for the NIH Toolbox tests and then to the computer for the <italic>E</italic>-Prime test. Then, the TMT was followed, for which participants' drawings on the paper forms were observed via the document camera and feedback was given through the synthetic voice by pressing the designated prompt keys by the experimenter, which was programmed based on the TMT protocol [<xref rid="bb0160" ref-type="bibr">32</xref>]. Afterwards, the remaining time was counted, the avatar instructed a break for the measured duration, and a countdown timer was presented. After the break, the avatar gave directions for RAVLT delayed recall and recognition on a paper form, which was obtained from a colored folder and submitted to the file tray. See <xref rid="t0005" ref-type="table">Table 1</xref> and <xref rid="f0010" ref-type="fig">Fig. 2</xref> for an overview.</p>
                <p id="p0110">Without a break, Batch 4 started for the Paired Associates learning and immediate recall. Next, the 20-min timer started, and the avatar gave instructions for the <italic>E</italic>-Prime test. After then, E-Prime tests were executed. The break was verbally instructed by the synthetic voice and provided for the remaining time on the 20-min timer. If insufficient time was left in 20 min delay for a test, the test was skipped, and a break was given for the remaining time. The skipped test was administered after the delayed recall. Then, Batch 4 was closed, and Batch 5 was executed. See <xref rid="t0005" ref-type="table">Table 1</xref> and <xref rid="f0010" ref-type="fig">Fig. 2</xref> for an overview.</p>
                <p id="p0115">Batch 5 started with Logical Memory learning and immediate recall while audio-recording verbal responses, which was followed by a 20-min timer starting with the <italic>E</italic>-Prime test. Subsequently, the SDMT written, oral, and incidental learning trials [<xref rid="bb0140" ref-type="bibr">28</xref>] were administered. For the SDMT practice trials, the experimenter observed the participants' drawings on the paper forms via the document camera and provided appropriate feedback through the synthetic voice by pressing the designated prompt keys. After the SDMT, the program counted the remaining time, the avatar instructed a break for the measured duration with a countdown timer displayed; if no time was left, no break was offered. After the break, the delayed recall followed along with audio-recordings, and upon completion the avatar instructed the end of testing and our appreciation for participants' efforts.</p>
              </sec>
              <sec id="s0060">
                <label>3.9</label>
                <title>Troubleshooting</title>
                <p id="p0120">Batched and automated operation of the test programs are efficient and convenient. Based on our pilot tests, we found that the programs occasionally crashed for an unknown reason, especially when the batch program was continuing its timer during another <italic>E</italic>-Prime 3.0 test. We therefore included in our protocol the use of a manually operated backup timer of 20- and 30-min to ensure the precise administration of the delayed recall trials in the event of a crash. We also placed each program file in the same directory as the batch program, so the experimenter could manually execute a test program at the right time if the automatic execution was not working.</p>
              </sec>
              <sec id="s0065">
                <label>3.10</label>
                <title>Participants and COVID-19 safety precautions</title>
                <p id="p0125">Participants in the PAAD-2 trial are middle-aged (40–65 years) adults with a family history of Alzheimer's disease, who are cognitively normal, healthy enough for exercise, not otherwise clinically impaired, and identified as sedentary based on American College of Sports Medicine (ACSM)’s physical activity guidelines [<xref rid="bb0215" ref-type="bibr">43</xref>]. The inclusion criteria did not specifically include any criteria that put participants into the Centers of Disease Control (CDC)’s high-risk category when originally defined. However, as of June 25, 2020, the CDC removed the specific age threshold of &gt;65 years and replaced that with a statement that risk increases with increasing age [<xref rid="bb0220" ref-type="bibr">44</xref>]. Prior to scheduling participants, we discuss the CDC's risk guidance to ensure that they are aware of their own personal risk category classification. Within 24 h of scheduled visits, the experimenter and participants are required to complete a COVID-19 screening form. This allowed for the reporting of any COVID-19 symptoms or positive diagnosis, any contacts with people having COVID-19 symptoms or positive diagnosis, and/or any travel(s) outside the state in the past 14 days. We also used this screening to identify if participants had additional factors that would put them at increased risk for serious health consequences when contracting COVID-19 [<xref rid="bb0225" ref-type="bibr">45</xref>]. For participants identified as having high risk of serious consequences of a COVID-19 infection [<xref rid="bb0230" ref-type="bibr">46</xref>], we discussed this with the participant prior to scheduling.</p>
                <p id="p0130">We also used additional safety precautions including ensuring that they were the first or only person to complete cognitive testing or that they were scheduled for testing more than one hour following a previous participant. For all participants, when the experimenter or a participant entered the testing room, they were first required to sanitize their hands. After each testing session, all devices on the desks were wiped off. We covered the participant's keyboard with a transparent plastic slip and exchanged it with a new one after each testing session. We wiped off the file tray, the pencils and pen, and the experimenter's and participant's chairs after each testing session, and also switched all of these pieces of equipment with another set of equipment after each participant.</p>
              </sec>
            </sec>
            <sec id="s0070">
              <label>4</label>
              <title>Protocol evaluation</title>
              <sec id="s0075">
                <label>4.1</label>
                <title>Participants</title>
                <p id="p0135">We have safely and efficiently operated the modified protocol since July 2020 for the pre-test (<italic>n</italic> = 37), mid-test (<italic>n</italic> = 7), and post-test (<italic>n</italic> = 15). We compared the pre-test data of the modified protocol with that of the original protocol (<italic>n</italic> = 32) in terms of the test duration (<xref rid="t0005" ref-type="table">Table 1</xref>) and test performance (<xref rid="t0010" ref-type="table">Table 2</xref>
). We describe demographics for participants completing the original and modified protocol in <xref rid="t0010" ref-type="table">Table 2</xref>.<table-wrap position="float" id="t0010"><label>Table 2</label><caption><p>Comparisons of performance between the original and modified PAAD-2 cognitive testing protocol.</p></caption><alt-text id="al0020">Table 2</alt-text><table frame="hsides" rules="groups"><thead><tr><th colspan="2">Participants' characteristics</th><th>Original (<italic>n</italic> = 32)</th><th>Modified (<italic>n</italic> = 37)</th><th>ET <italic>(p</italic>)</th><th><italic>t</italic>- or <italic>z</italic>-test <italic>(p</italic>)</th></tr></thead><tbody><tr><td/><td>Age, mean (SD)</td><td>56.9 ± 1.2</td><td>56.1 ± 0.85</td><td>&lt; 0.05</td><td>0.60</td></tr><tr><td/><td>Female gender (%)</td><td>31 (96.9%)</td><td>31 (83.8%)</td><td>&lt; 0.01</td><td>0.16<sup>†</sup></td></tr><tr><td/><td>Race/ethnicity, non-Hispanic white (%)</td><td>28 (87.5%)</td><td>35 (94.6%)</td><td>&lt; 0.01</td><td>0.54<sup>†</sup></td></tr><tr><td/><td>Years of education, mean (SD)</td><td>16.0 ± 0.33</td><td>16.9 ± 0.36</td><td>0.13</td><td>0.09</td></tr><tr><td colspan="2">Testing batches and components</td><td colspan="2">Comparisons of test scores (%, <italic>M</italic> ± <italic>SE</italic>)</td><td>ET <italic>(p</italic>)</td><td><italic>t</italic>-test <italic>(p</italic>)</td></tr><tr><td>0</td><td>MoCA</td><td>28.2 ± 0.25</td><td>28.4 ± 0.22</td><td>&lt; 0.01</td><td>0.76</td></tr><tr><td rowspan="6">1</td><td>Test of Premorbid Functioning, %</td><td>86.2 ± 1.78</td><td>82.1 ± 1.73</td><td>0.11</td><td>0.11</td></tr><tr><td>NIH TB List Sort Working Memory Score</td><td>53.8 ± 1.33</td><td>54.5 ± 1.25</td><td>&lt; 0.01</td><td>0.73</td></tr><tr><td>NIH TB Picture Sequence Memory Score</td><td>55.1 ± 2.33</td><td>57.0 ± 1.56</td><td>&lt; 0.05</td><td>0.48</td></tr><tr><td>MST - Lure Discrimination Index, %</td><td>12.5 ± 2.30</td><td>21.1 ± 3.22</td><td>0.24</td><td>&lt;0.05</td></tr><tr><td>MST - Old Discrimination Index, %</td><td>84.2 ± 1.90</td><td>81.1 ± 1.50</td><td>0.06</td><td>0.20</td></tr><tr><td>PDT - Perceptual Discrimination Index, %</td><td>83.8 ± 2.01</td><td>83.2 ± 2.39</td><td>&lt; 0.05</td><td>0.83</td></tr><tr><td rowspan="8">2</td><td>ROCFT - Copy, %</td><td>95.7 ± 1.38</td><td>98.2 ± 0.51</td><td>0.13</td><td>0.10</td></tr><tr><td>ROCFT - Immediate Recall, %</td><td>53.4 ± 5.96</td><td>70.5 ± 5.23</td><td>0.23</td><td>&lt; 0.05</td></tr><tr><td>Stroop Effect, %</td><td>21.8 ± 3.17</td><td>23.8 ± 2.42</td><td>&lt; 0.01</td><td>0.62</td></tr><tr><td>PASAT - 3 s, %</td><td>79.2 ± 3.33</td><td>81.6 ± 2.87</td><td>&lt; 0.05</td><td>0.59</td></tr><tr><td>PASAT - 2 s, %</td><td>57.0 ± 2.93</td><td>64.3 ± 2.50</td><td>0.16</td><td>0.06</td></tr><tr><td>ROCFT - Delayed Recall</td><td>51.4 ± 6.20</td><td>67.2 ± 5.81</td><td>0.15</td><td>0.07</td></tr><tr><td>ROCFT - Delayed Recognition</td><td>86.5 ± 0.95</td><td>88.1 ± 1.24</td><td>&lt; 0.05</td><td>0.32</td></tr><tr><td>TOL - Planning Score</td><td>47.5 ± 6.23</td><td>54.3 ± 4.89</td><td>&lt; 0.05</td><td>0.38</td></tr><tr><td rowspan="9">3</td><td>RAVLT - Learning Summary, %</td><td>59.8 ± 2.06</td><td>64.5 ± 1.87</td><td>0.12</td><td>0.09</td></tr><tr><td>RAVLT - Immediate Recall, %</td><td>64.6 ± 3.78</td><td>71.1 ± 3.11</td><td>0.06</td><td>0.19</td></tr><tr><td>NIH TB Dimensional Change Card Sort Score</td><td>55.5 ± 2.31</td><td>55.1 ± 1.83</td><td>&lt; 0.01</td><td>0.90</td></tr><tr><td>NIH TB Flanker Inhibitory Control Score</td><td>42.9 ± 1.46</td><td>42.5 ± 1.62</td><td>&lt; 0.01</td><td>0.86</td></tr><tr><td>Spatial Working Memory Score</td><td>89.0 ± 3.85</td><td>92.7 ± 3.36</td><td>&lt; 0.05</td><td>0.47</td></tr><tr><td>Trail Making - A, <italic>sec</italic></td><td>32.0 ± 1.44</td><td>35.5 ± 1.58</td><td>0.10</td><td>0.11</td></tr><tr><td>Trail Making - B, sec</td><td>52.3 ± 3.00</td><td>52.2 ± 2.48</td><td>&lt; 0.01</td><td>0.98</td></tr><tr><td>RAVLT - Delayed Recall, %</td><td>64.2 ± 4.27</td><td>69.5 ± 3.20</td><td>&lt; 0.05</td><td>0.32</td></tr><tr><td>RAVLT - Delayed Recognition, %</td><td>85.2 ± 2.97</td><td>83.6 ± 2.01</td><td>&lt; 0.01</td><td>0.66</td></tr><tr><td rowspan="5">4</td><td>Paired Associates - Learning Summary, %</td><td>43.1 ± 4.30</td><td>49.1 ± 4.42</td><td>&lt; 0.05</td><td>0.34</td></tr><tr><td>Matrix Reasoning, %</td><td>49.2 ± 3.12</td><td>51.2 ± 2.90</td><td>&lt; 0.01</td><td>0.63</td></tr><tr><td>Digits Span Forward Score, %</td><td>58.7 ± 2.50</td><td>55.6 ± 2.25</td><td>&lt; 0.05</td><td>0.36</td></tr><tr><td>Digits Span Backward Score, %</td><td>38.2 ± 2.60</td><td>36.3 ± 2.79</td><td>&lt; 0.01</td><td>0.63</td></tr><tr><td>Paired Associates - Delayed Recall, %</td><td>32.6 ± 4.51</td><td>36.0 ± 4.67</td><td>&lt; 0.05</td><td>0.60</td></tr><tr><td rowspan="6">5</td><td>Logical Memory - Learning Summary, %</td><td>60.1 ± 2.47</td><td>61.2 ± 1.79</td><td>&lt; 0.01</td><td>0.72</td></tr><tr><td>Spatial Relations, %</td><td>44.4 ± 3.85</td><td>50.4 ± 3.41</td><td>&lt; 0.05</td><td>0.24</td></tr><tr><td>SDMT - Written, %</td><td>48.6 ± 1.52</td><td>47.1 ± 1.03</td><td>&lt; 0.05</td><td>0.43</td></tr><tr><td>SDMT - Oral, %</td><td>55.7 ± 1.84</td><td>55.4 ± 1.67</td><td>&lt; 0.01</td><td>0.89</td></tr><tr><td>SDMT - Incidental Learning, %</td><td>63.1 ± 5.01</td><td>68.3 ± 4.46</td><td>&lt; 0.01</td><td>0.89</td></tr><tr><td>Logical Memory - Delayed Recall, %</td><td>56.2 ± 2.90</td><td>56.2 ± 1.9</td><td>&lt; 0.01</td><td>0.99</td></tr></tbody></table><table-wrap-foot><fn id="sp0040"><p><italic>Abbreviation</italic>: MoCA, Montreal Cognitive Assessment; MST, Mnemonic Similarity Task; PASAT, Paced Auditory Serial Addition Test; PDT, Perceptual Discrimination; RAVLT, Rey Auditory Verbal Learning Test; ROCFT, Rey-Osterrieth Complex Figure Test; SDMT, Symbol Digits Modalities Test.</p></fn><fn id="sp0045"><p><italic>Note</italic>: Two-sample <italic>t</italic>- or <italic>z</italic>-tests or equivalence tests (ET) were conducted to compare the demographics and cognitive performance between two groups of participants of the original and modified protocols. Results indicate that participants of two protocols are significantly equivalent and not significantly different in their ages and 94% of test scores. Equivalence margin (Cohen's d, δ) was ±0.7. <sup>†</sup>Results of Fisher's exact probability tests with Yates continuity correction.</p></fn></table-wrap-foot></table-wrap></p>
              </sec>
              <sec id="s0080">
                <label>4.2</label>
                <title>Data analyses</title>
                <p id="p0140">We conducted a series of one- or two-sample <italic>t</italic>-tests and equivalence tests (ET) to detect significant differences or equivalence between data collected using the original and modified protocols. The goal of ET is to examine whether the null hypothesis that there is significant difference between two parties can be actually rejected, which is exactly opposite to the traditional comparative study (e.g., <italic>t</italic>-test) examining a null hypothesis that there is no meaningful difference between two approaches [<xref rid="bb0235" ref-type="bibr">47</xref>,<xref rid="bb0240" ref-type="bibr">48</xref>]. Significant equivalence is determined with the equivalence margin (δ), the maximum acceptable range of values in which the subtle difference must fall to be considered equivalent [<xref rid="bb0235" ref-type="bibr">47</xref>].</p>
                <p id="p0145">The ET complements the traditional hypothesis testing and vice versa. For example, when the null hypothesis of a traditional <italic>t</italic>-test is accepted, the absence of a true effect is supported but not statistically verified; ET can statistically uphold this case [<xref rid="bb0245" ref-type="bibr">49</xref>]. ET can also identify significantly greater than zero but negligible effect, when it is smaller than the meaningful effect size by falling in the equivalence margin [<xref rid="bb0235" ref-type="bibr">47</xref>]. To examine whether the presence of meaningful differences between two protocols can be rejected, we followed two one-sided tests (TOST) procedure, an established method of ET [<xref rid="bb0240" ref-type="bibr">48</xref>,<xref rid="bb0245" ref-type="bibr">49</xref>], with an upper and lower equivalence margin at ±0.7, which was determined in consideration of the sample size [<xref rid="bb0245" ref-type="bibr">49</xref>]. All statistical analyses were conducted with R 4.0.3 [<xref rid="bb0250" ref-type="bibr">50</xref>].</p>
              </sec>
              <sec id="s0085">
                <label>4.3</label>
                <title>Results</title>
                <p id="p0150">As expected, safety precautions have been effective such that none of the experimenters or participants have contracted COVID-19 in our testing environment. All participants have expressed a clear understanding of the instructions and of the feedback from the synthetic voice. We asked the participants to press a key to repeat the synthesized instructions or request clarification when unclear, yet they rarely pressed the key to replay any instructions or ask clarifying questions (&lt; 1% of the total instructions). Automatic control of the testing sequence and timing functioned well without causing any significant error or delay during the testing. All data including audio recordings of verbal responses, hand-written and -drawn responses on paper forms, and keyboard and mouse responses on the computer have been safely acquired.</p>
                <p id="p0155">Results indicate that the modified protocol is significantly equivalent to the original protocol in terms of its duration (<xref rid="t0005" ref-type="table">Table 1</xref>) and participants' age and performance (<xref rid="t0010" ref-type="table">Table 2</xref>). No significant differences were found in gender (<italic>p</italic> = .16), race/ethnicity (<italic>p</italic> = .54), or years of education (<italic>p</italic> = .09) between the two groups. Although years of education was not significantly equivalent (<italic>p</italic> = .13), gender (<italic>p</italic> = .01) and race/ethnicity (<italic>p</italic> = .01) were significantly equivalent. As can be seen in <xref rid="t0005" ref-type="table">Table 1</xref>, the time control of the modified protocol was robust and the 20- and 30-min time delays were accurately maintained. The only tests for which duration of the modified protocol was significantly different from the original protocol was the ROCFT copy (<italic>p</italic> &lt; .001), immediate recall (<italic>p</italic> &lt; .01), and delayed recall (<italic>p</italic> &lt; .05), for which the task time is completely determined by the participant with no time limit. For all of these, participants in the modified protocol took significantly longer to complete the task than those who used the original protocol. This longer duration of task completion is likely reflected in the marginally higher performance during the modified protocol for copy (<italic>p</italic> = .10), immediate recall (<italic>p</italic> &lt; .05), and delayed recall (<italic>p</italic> = .07). The only other performance difference was found for the MST lure discrimination index (<italic>p</italic> &lt; .05). All other test scores were not significantly different and, in most cases, significantly equivalent between two protocols (see <xref rid="t0010" ref-type="table">Table 2</xref>).</p>
              </sec>
            </sec>
            <sec id="s0090">
              <label>5</label>
              <title>Discussion</title>
              <p id="p0160">In this modified protocol for the PAAD-2 cognitive testing, we describe the specific methods of the implementation of TTS synthesis and computer programming techniques and their benefits for safety and the integrity of cognitive assessment. The adoption of the techniques from AI and computer vision packages enabled us to provide standardized instructions and feedback without human speech and to closely view paper copies of documents from a safe distance (farther than 6 ft) for an extensive cognitive testing protocol during a pandemic. According to the feedback from experimenters and the consistently positive responses from participants, the modified testing procedures have provided a safe and pleasant environment for cognitive assessment for both the experimenter and participants. This is critical for the prevention of COVID-19 but also for the provision of accurate and standardized verbal instructions compared with speaking under a face covering from the required social distance.</p>
              <p id="p0165">We also evaluated the integrity of the modified protocol and substantiated that the modified protocol is robust and generally equivalent to the original protocol in terms of its duration and participants' performance. The automated control of test timing and sequence we developed functioned flawlessly and required less training for the experimenter than traditional human-led administration of cognitive tests. Our interpretation of the marginal differences from the original protocol for the ROCFT test duration is that the experimenter in the original protocol often asked whether the drawing tasks were finished from a close distance, which could have functioned as a prompt to stop the task. By contrast, in the modified protocol, participants self-initiated and finished the drawing task without any prompt and the experimenter was a distance away and not directly observing their behaviors. The longer duration of drawing tasks could be associated with the learning and memory performance.</p>
              <p id="p0170">We acknowledge the limitation of comparing two different groups of participants for the evaluation of protocol legitimacy. Although no significantly different demographic characteristics were detected between the two groups, it is possible that there may be marginal differences in terms of other unmeasured variables between the two groups that could affect cognitive performance. We will carefully consider any marginal differences between the two protocols as the study continues and when analyzing the study outcomes.</p>
              <p id="p0175">Employing speech synthesis technique for neurocognitive testing in the pandemic has the clear advantage of mitigating the risk of the transmission of the virus. Recent studies have revealed that small speech droplets generated by ordinary speaking could remain airborne for extended periods of time and therefore it is highly possible that normal talking causes airborne viral transmission of the COVID-19 virus in confined environments [<xref rid="bb0035" ref-type="bibr">7</xref>,<xref rid="bb0040" ref-type="bibr">8</xref>]. In addition to wearing a face covering, having a computer speak for all necessary instructions and feedback further contributed to eliminating the production of speech droplets and thus substantially decreased the risks for COVID-19 in a confined laboratory environment. We asked about 15 participants to provide either positive or negative feedback on our new testing session and received only positive comments: “Loved the avatar Dr. Shin [Dr. Shin Park] created limiting amount of talking person to person.” and “Everything was extremely safe to the point of over safe. But very much appreciated.”</p>
              <p id="p0180">Moreover, using a synthetic voice facilitates the testing procedure by limiting the extent to which the experimenter must speak while wearing a face covering. Recent evidence indicates that wearing face coverings during professional and essential activities increased the perception of vocal fatigue and discomfort, difficulties in understanding speech, auditory feedback, and difficulties in coordinating speech and breathing [<xref rid="bb0055" ref-type="bibr">11</xref>]. Using synthetic voice eliminates such difficulties and thus the reduces the chance of the experimenter being fatigued and making errors in testing instructions and feedback during an extensive testing session. In this regard, the computerized testing is more rigorous and standardized than the experimenter-led version once properly administered and more easily trainable across administrators at different levels.</p>
              <p id="p0185">Ethical and practical challenges must be considered relative to human research activities during the COVID-19 pandemic [<xref rid="bb0010" ref-type="bibr">2</xref>]. Such challenges include but are not limited to: What level of risk for disease transmission is acceptable to resume in-person human subject research? What safety precautions are mandatory? To address this challenge, researchers developed a risk-benefit framework to prioritize studies in tiers (0,1,2,3) based on a combination of the incremental risk of COVID-19 transmission (high, medium, low, or none) introduced by the research activity and the potential benefits of study participation (1–4) at an individual level [<xref rid="bb0010" ref-type="bibr">2</xref>]. The framework considers contact distance, contact duration, number of contacts per day, personal protective equipment, and participant characteristics (e.g., age, medical condition, risk of contracting COVID-19). Our study would be considered as a tier 2 study with a low risk based on the contact-free administration with the state-of-the-art technologies and safety precautions. Further discussion is needed how to consider our safety precautions into the risk-benefit framework and how to efficiently utilize the modern technology we employed in other research settings.</p>
              <p id="p0190">In recent decades, speech synthesis technology has been widely applied to commercially-available mobile devices and computers [<xref rid="bb0070" ref-type="bibr">14</xref>,<xref rid="bb0075" ref-type="bibr">15</xref>] and also efficiently utilized in the fields of healthcare [<xref rid="bb0255" ref-type="bibr">51</xref>] and education [<xref rid="bb0260" ref-type="bibr">52</xref>]. For example, synthesized speech has been used for an interactive medication reminder and tracking on wrist devices [<xref rid="bb0265" ref-type="bibr">53</xref>], as a clinical assistant for visually impaired people [<xref rid="bb0270" ref-type="bibr">54</xref>], and other assistive device, speech-based healthcare apps, websites, and/or emergency call centers [<xref rid="bb0255" ref-type="bibr">51</xref>]. A comprehensive review on the use of speech technology for healthcare was recently published [<xref rid="bb0255" ref-type="bibr">51</xref>]. Nonetheless, the application of speech technology for behavioral experiments or psychological assessment remains at a rudimentary level. To our knowledge, this innovative protocol for the PAAD-2 is the seminal attempt for synthesized voices to completely replace human speech for informed consent and verbal instructions and feedback for a comprehensive battery of laboratory-based cognitive assessment. This technique can be used for older adults with sensory or cognitive impairments by adjusting the pace of speech to help with their understanding. Other low risk means of behavioral experiments are available such videoconference [<xref rid="bb0275" ref-type="bibr">55</xref>], telephone [<xref rid="bb0280" ref-type="bibr">56</xref>], or web-based software [<xref rid="bb0285" ref-type="bibr">57</xref>]. Such online-based methodologies are beneficial in its mobility and accessibility but limited in its level of precision compared with lab-based systems and have slightly more variability in its measures [<xref rid="bb0285" ref-type="bibr">57</xref>].</p>
              <p id="p0195">We plan to use this methodology in the future even after the pandemic for its benefits for safety management, standardization of test directions, precise control over test timing, automation of test sequences and execution, efficiency of data collection procedures, and the integrity of data obtained. Our employment of the AI-based methods may be informative for other researchers interested in employing safe, rigorous, and automated laboratory tests during and after the pandemic.</p>
            </sec>
            <sec id="s0095">
              <title>Funding</title>
              <p id="p0200">This work has been completed as part of a phase 2 clinical trial (<ext-link ext-link-type="uri" xlink:href="http://ClinicalTrials.gov" id="ir0030">ClinicalTrials.gov</ext-link>
<ext-link ext-link-type="ClinicalTrials.gov" xlink:href="NCT03876314" id="ir0035">NCT03876314</ext-link>), “The Effect of Physical Activity on Cognition Relative to APOE Genotype (PAAD-2)”, which is funded by the <funding-source id="gts0005"><institution-wrap><institution-id institution-id-type="doi">10.13039/100000002</institution-id><institution>National Institutes of Health</institution></institution-wrap></funding-source> (R01AG058919). The content of this manuscript is solely the responsibility of the authors and does not necessarily represent the official views of the <funding-source id="gts0010">NIH</funding-source>.</p>
            </sec>
            <sec id="s0100">
              <title>Ethics approval and consent to participate</title>
              <p id="p0205">Approval for this study was obtained from the Institutional Review Board of the University of North Carolina at Greensboro (IRB number 18–0228). Informed consent was obtained from all individual participants included in the study at the first in-person visit at the pre-test.</p>
            </sec>
            <sec sec-type="data-availability" id="s0105">
              <title>Availability of data and materials</title>
              <p id="p0210">Data sharing not applicable to this article as no datasets were generated or analyzed during the current study.</p>
            </sec>
            <sec id="s0110">
              <title>Consent for publication</title>
              <p id="p0215">Not applicable.</p>
            </sec>
            <sec id="s0115">
              <title>Authors' contributions</title>
              <p id="p0220">KSP initiated the use of speech synthesis techniques, configured the contactless test settings, computerized the entire protocol, operated and tested the protocol for data collection, and wrote the draft and final versions of the manuscript, along with significant contributions from JLE who designed the original protocol, gave input into the conception of the revised protocol, assisted with pilot testing and the provision of protocol modifications, and edited and finalized the manuscript. All authors read and approved the final manuscript and accept personal responsibility for the accuracy and integrity of the presentation of this protocol.</p>
            </sec>
            <sec sec-type="COI-statement">
              <title>Declaration of Competing Interest</title>
              <p id="p0225">The authors declare no commercial, financial or any other conflict of interest in this research.</p>
            </sec>
          </body>
          <back>
            <ref-list id="bi0005">
              <title>References</title>
              <ref id="bb0005">
                <label>1</label>
                <element-citation publication-type="journal" id="rf0005">
                  <person-group person-group-type="author">
                    <name>
                      <surname>Omary</surname>
                      <given-names>M.B.</given-names>
                    </name>
                    <etal/>
                  </person-group>
                  <article-title>The COVID-19 pandemic and research shutdown: staying safe and productive</article-title>
                  <source>J. Clin. Invest.</source>
                  <volume>130</volume>
                  <issue>6</issue>
                  <year>2020</year>
                  <fpage>2745</fpage>
                  <lpage>2748</lpage>
                  <pub-id pub-id-type="pmid">32243259</pub-id>
                </element-citation>
              </ref>
              <ref id="bb0010">
                <label>2</label>
                <element-citation publication-type="journal" id="rf0010">
                  <person-group person-group-type="author">
                    <name>
                      <surname>Lumeng</surname>
                      <given-names>J.C.</given-names>
                    </name>
                    <etal/>
                  </person-group>
                  <article-title>Opinion: a risk–benefit framework for human research during the COVID-19 pandemic</article-title>
                  <source>Proc. Natl. Acad. Sci.</source>
                  <volume>117</volume>
                  <issue>45</issue>
                  <year>2020</year>
                  <fpage>27749</fpage>
                  <lpage>27753</lpage>
                  <pub-id pub-id-type="pmid">33087558</pub-id>
                </element-citation>
              </ref>
              <ref id="bb0015">
                <label>3</label>
                <element-citation publication-type="journal" id="rf0015">
                  <person-group person-group-type="author">
                    <name>
                      <surname>Myers</surname>
                      <given-names>K.R.</given-names>
                    </name>
                    <etal/>
                  </person-group>
                  <article-title>Unequal effects of the COVID-19 pandemic on scientists</article-title>
                  <source>Nat. Hum. Behav.</source>
                  <volume>4</volume>
                  <issue>9</issue>
                  <year>2020</year>
                  <fpage>880</fpage>
                  <lpage>883</lpage>
                  <pub-id pub-id-type="pmid">32669671</pub-id>
                </element-citation>
              </ref>
              <ref id="bb0020">
                <label>4</label>
                <element-citation publication-type="journal" id="rf0020">
                  <person-group person-group-type="author">
                    <name>
                      <surname>Wigginton</surname>
                      <given-names>N.S.</given-names>
                    </name>
                    <etal/>
                  </person-group>
                  <article-title>Moving academic research forward during COVID-19</article-title>
                  <source>Science</source>
                  <volume>368</volume>
                  <issue>6496</issue>
                  <year>2020</year>
                  <fpage>1190</fpage>
                  <lpage>1192</lpage>
                  <pub-id pub-id-type="pmid">32467332</pub-id>
                </element-citation>
              </ref>
              <ref id="bb0025">
                <label>5</label>
                <element-citation publication-type="journal" id="rf0025">
                  <person-group person-group-type="author">
                    <name>
                      <surname>Park</surname>
                      <given-names>K.S.</given-names>
                    </name>
                    <etal/>
                  </person-group>
                  <article-title>The effect of physical activity on cognition relative to APOE genotype (PAAD-2): study protocol for a phase II randomized control trial</article-title>
                  <source>BMC Neurol.</source>
                  <volume>20</volume>
                  <issue>1</issue>
                  <year>2020</year>
                  <fpage>231</fpage>
                  <pub-id pub-id-type="pmid">32503473</pub-id>
                </element-citation>
              </ref>
              <ref id="bb0030">
                <label>6</label>
                <element-citation publication-type="journal" id="rf0030">
                  <person-group person-group-type="author">
                    <name>
                      <surname>Rothe</surname>
                      <given-names>C.</given-names>
                    </name>
                    <etal/>
                  </person-group>
                  <article-title>Transmission of 2019-nCoV infection from an asymptomatic contact in Germany</article-title>
                  <source>N. Engl. J. Med.</source>
                  <volume>382</volume>
                  <issue>10</issue>
                  <year>2020</year>
                  <fpage>970</fpage>
                  <lpage>971</lpage>
                  <pub-id pub-id-type="pmid">32003551</pub-id>
                </element-citation>
              </ref>
              <ref id="bb0035">
                <label>7</label>
                <element-citation publication-type="journal" id="rf0035">
                  <person-group person-group-type="author">
                    <name>
                      <surname>Stadnytskyi</surname>
                      <given-names>V.</given-names>
                    </name>
                    <etal/>
                  </person-group>
                  <article-title>The airborne lifetime of small speech droplets and their potential importance in SARS-CoV-2 transmission</article-title>
                  <source>Proc. Natl. Acad. Sci.</source>
                  <volume>117</volume>
                  <issue>22</issue>
                  <year>2020</year>
                  <fpage>11875</fpage>
                  <lpage>11877</lpage>
                  <pub-id pub-id-type="pmid">32404416</pub-id>
                </element-citation>
              </ref>
              <ref id="bb0040">
                <label>8</label>
                <element-citation publication-type="journal" id="rf0040">
                  <person-group person-group-type="author">
                    <name>
                      <surname>Anfinrud</surname>
                      <given-names>P.</given-names>
                    </name>
                    <etal/>
                  </person-group>
                  <article-title>Visualizing speech-generated oral fluid droplets with laser light scattering</article-title>
                  <source>N. Engl. J. Med.</source>
                  <volume>382</volume>
                  <issue>21</issue>
                  <year>2020</year>
                  <fpage>2061</fpage>
                  <lpage>2063</lpage>
                  <pub-id pub-id-type="pmid">32294341</pub-id>
                </element-citation>
              </ref>
              <ref id="bb0045">
                <label>9</label>
                <element-citation publication-type="journal" id="rf0045">
                  <person-group person-group-type="author">
                    <name>
                      <surname>Morawska</surname>
                      <given-names>L.</given-names>
                    </name>
                    <name>
                      <surname>Cao</surname>
                      <given-names>J.</given-names>
                    </name>
                  </person-group>
                  <article-title>Airborne transmission of SARS-CoV-2: the world should face the reality</article-title>
                  <source>Environ. Int.</source>
                  <volume>139</volume>
                  <year>2020</year>
                  <fpage>105730</fpage>
                  <pub-id pub-id-type="pmid">32294574</pub-id>
                </element-citation>
              </ref>
              <ref id="bb0050">
                <label>10</label>
                <element-citation publication-type="journal" id="rf0050">
                  <person-group person-group-type="author">
                    <name>
                      <surname>Morawska</surname>
                      <given-names>L.</given-names>
                    </name>
                    <name>
                      <surname>Milton</surname>
                      <given-names>D.K.</given-names>
                    </name>
                  </person-group>
                  <article-title>It is time to address airborne transmission of coronavirus disease 2019 (COVID-19)</article-title>
                  <source>Clin. Infect. Dis.</source>
                  <volume>71</volume>
                  <issue>9</issue>
                  <year>2020</year>
                  <fpage>2311</fpage>
                  <lpage>2313</lpage>
                  <pub-id pub-id-type="pmid">32628269</pub-id>
                </element-citation>
              </ref>
              <ref id="bb0055">
                <label>11</label>
                <element-citation publication-type="journal" id="rf0055">
                  <person-group person-group-type="author">
                    <name>
                      <surname>Ribeiro</surname>
                      <given-names>V.V.</given-names>
                    </name>
                    <etal/>
                  </person-group>
                  <article-title>Effect of wearing a face mask on vocal self-perception during a pandemic</article-title>
                  <source>J. Voice</source>
                  <year>2020</year>
                  <comment>In press</comment>
                </element-citation>
              </ref>
              <ref id="bb0060">
                <label>12</label>
                <element-citation publication-type="book" id="rf0060">
                  <person-group person-group-type="author">
                    <name>
                      <surname>Wang</surname>
                      <given-names>Y.</given-names>
                    </name>
                    <etal/>
                  </person-group>
                  <part-title>Tacotron<italic>: towards end-to-end speech synthesis</italic></part-title>
                  <source>INTERSPEECH</source>
                  <year>2017</year>
                </element-citation>
              </ref>
              <ref id="bb0065">
                <label>13</label>
                <element-citation publication-type="book" id="rf0065">
                  <person-group person-group-type="author">
                    <name>
                      <surname>Shen</surname>
                      <given-names>J.</given-names>
                    </name>
                    <etal/>
                  </person-group>
                  <part-title>Natural TTS synthesis by conditioning wavenet on MEL spectrogram predictions</part-title>
                  <source>2018 <italic>IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</italic></source>
                  <year>2018</year>
                </element-citation>
              </ref>
              <ref id="bb0070">
                <label>14</label>
                <element-citation publication-type="other" id="rf0070">
                  <person-group person-group-type="author">
                    <name>
                      <surname>Pew Research Center</surname>
                    </name>
                  </person-group>
                  <article-title><italic>Nearly half of Americans use digital voice assistants, mostly on their smartphones</italic>. 2017 Dec 12th Sep 23, 2020</article-title>
                  <comment>Available from:</comment>
                  <ext-link ext-link-type="uri" xlink:href="http://www.pewrsr.ch/2kquZ8H" id="ir0040">http://www.pewrsr.ch/2kquZ8H</ext-link>
                </element-citation>
              </ref>
              <ref id="bb0075">
                <label>15</label>
                <element-citation publication-type="other" id="rf0075">
                  <person-group person-group-type="author">
                    <name>
                      <surname>Petrock</surname>
                      <given-names>V.</given-names>
                    </name>
                  </person-group>
                  <article-title><italic>Voice Assistant Use Reaches Critical Mass</italic>. 2019 Aug 15th Sep 23, 2020</article-title>
                  <comment>Available from:</comment>
                  <ext-link ext-link-type="uri" xlink:href="https://www.emarketer.com/content/voice-assistant-use-reaches-critical-mass" id="ir0045">https://www.emarketer.com/content/voice-assistant-use-reaches-critical-mass</ext-link>
                </element-citation>
              </ref>
              <ref id="bb0080">
                <label>16</label>
                <element-citation publication-type="other" id="rf0080">
                  <person-group person-group-type="author">
                    <name>
                      <surname>van den Oord</surname>
                      <given-names>A.</given-names>
                    </name>
                    <etal/>
                  </person-group>
                  <article-title><italic>Wavenet: A generative model for raw audio.</italic> arXiv preprint</article-title>
                  <ext-link ext-link-type="uri" xlink:href="https://arxiv.org/abs/1609.03499" id="ir0050">arXiv:1609.03499</ext-link>
                  <year>2016</year>
                </element-citation>
              </ref>
              <ref id="bb0085">
                <label>17</label>
                <element-citation publication-type="book" id="rf0085">
                  <person-group person-group-type="author">
                    <name>
                      <surname>van den Oord</surname>
                      <given-names>A.</given-names>
                    </name>
                    <etal/>
                  </person-group>
                  <part-title>Parallel wavenet: fast high-fidelity speech synthesis</part-title>
                  <person-group person-group-type="editor">
                    <name>
                      <surname>Jennifer</surname>
                      <given-names>D.</given-names>
                    </name>
                    <name>
                      <surname>Andreas</surname>
                      <given-names>K.</given-names>
                    </name>
                  </person-group>
                  <source>Proceedings of the 35th International Conference on Machine Learning</source>
                  <year>2018</year>
                  <fpage>3918</fpage>
                  <lpage>3926</lpage>
                  <comment>Editors. PMLR: Proceedings of Machine Learning Research</comment>
                </element-citation>
              </ref>
              <ref id="bb0090">
                <label>18</label>
                <element-citation publication-type="book" id="rf0090">
                  <person-group person-group-type="author">
                    <name>
                      <surname>Wagner</surname>
                      <given-names>P.</given-names>
                    </name>
                    <etal/>
                  </person-group>
                  <part-title>Speech Synthesis Evaluation - State-of-the-Art Assessment and Suggestion for a Novel Research Program</part-title>
                  <year>2019</year>
                </element-citation>
              </ref>
              <ref id="bb0095">
                <label>19</label>
                <element-citation publication-type="book" id="rf0095">
                  <person-group person-group-type="author">
                    <name>
                      <surname>Cambre</surname>
                      <given-names>J.</given-names>
                    </name>
                    <etal/>
                  </person-group>
                  <part-title>Choice of voices: a large-scale evaluation of text-to-speech voice quality for long-form content</part-title>
                  <source><italic>Proceedings of the</italic> 2020 <italic>CHI Conference on Human Factors in Computing Systems</italic></source>
                  <year>2020</year>
                  <publisher-name>Association for Computing Machinery</publisher-name>
                  <publisher-loc>Honolulu, HI, USA</publisher-loc>
                  <fpage>1</fpage>
                  <lpage>13</lpage>
                </element-citation>
              </ref>
              <ref id="bb0100">
                <label>20</label>
                <element-citation publication-type="other" id="rf0100">
                  <person-group person-group-type="author">
                    <name>
                      <surname>Google Cloud</surname>
                    </name>
                  </person-group>
                  <article-title>Text-to-Speech Documentation</article-title>
                  <comment>Available from:</comment>
                  <ext-link ext-link-type="uri" xlink:href="https://cloud.google.com/text-to-speech/docs" id="ir0055">https://cloud.google.com/text-to-speech/docs</ext-link>
                  <year>2020</year>
                </element-citation>
              </ref>
              <ref id="bb0105">
                <label>21</label>
                <element-citation publication-type="journal" id="rf0105">
                  <person-group person-group-type="author">
                    <name>
                      <surname>Peirce</surname>
                      <given-names>J.</given-names>
                    </name>
                    <etal/>
                  </person-group>
                  <article-title>PsychoPy2: experiments in behavior made easy</article-title>
                  <source>Behav. Res. Methods</source>
                  <volume>51</volume>
                  <issue>1</issue>
                  <year>2019</year>
                  <fpage>195</fpage>
                  <lpage>203</lpage>
                  <pub-id pub-id-type="pmid">30734206</pub-id>
                </element-citation>
              </ref>
              <ref id="bb0110">
                <label>22</label>
                <element-citation publication-type="journal" id="rf0110">
                  <person-group person-group-type="author">
                    <name>
                      <surname>Nasreddine</surname>
                      <given-names>Z.S.</given-names>
                    </name>
                    <etal/>
                  </person-group>
                  <article-title>The Montreal cognitive assessment, MoCA: a brief screening tool for mild cognitive impairment</article-title>
                  <source>J. Am. Geriatr. Soc.</source>
                  <volume>53</volume>
                  <issue>4</issue>
                  <year>2005</year>
                  <fpage>695</fpage>
                  <lpage>699</lpage>
                  <pub-id pub-id-type="pmid">15817019</pub-id>
                </element-citation>
              </ref>
              <ref id="bb0115">
                <label>23</label>
                <element-citation publication-type="book" id="rf0115">
                  <person-group person-group-type="author">
                    <name>
                      <surname>Wechsler</surname>
                      <given-names>D.</given-names>
                    </name>
                  </person-group>
                  <part-title><italic>Test of Premorbid Functioning.</italic> UK Version (TOPF UK)</part-title>
                  <year>2011</year>
                  <publisher-name>Pearson Corporation</publisher-name>
                  <publisher-loc>UK</publisher-loc>
                </element-citation>
              </ref>
              <ref id="bb0120">
                <label>24</label>
                <element-citation publication-type="journal" id="rf0120">
                  <person-group person-group-type="author">
                    <name>
                      <surname>Osterrieth</surname>
                      <given-names>P.A.</given-names>
                    </name>
                  </person-group>
                  <article-title>Le test de copie d'une figure complexe; contribution à l'étude de la perception et de la mémoire. [Test of copying a complex figure; contribution to the study of perception and memory.]</article-title>
                  <source>Arch. Psychol.</source>
                  <volume>30</volume>
                  <year>1944</year>
                  <fpage>206</fpage>
                  <lpage>356</lpage>
                </element-citation>
              </ref>
              <ref id="bb0125">
                <label>25</label>
                <element-citation publication-type="book" id="rf0125">
                  <person-group person-group-type="author">
                    <name>
                      <surname>Rey</surname>
                      <given-names>A.</given-names>
                    </name>
                  </person-group>
                  <part-title>L'examen clinique en Psychologie</part-title>
                  <year>1964</year>
                  <publisher-name>Presses universitaires de France</publisher-name>
                  <publisher-loc>Paris</publisher-loc>
                </element-citation>
              </ref>
              <ref id="bb0130">
                <label>26</label>
                <element-citation publication-type="journal" id="rf0130">
                  <person-group person-group-type="author">
                    <name>
                      <surname>Gronwall</surname>
                      <given-names>D.M.</given-names>
                    </name>
                  </person-group>
                  <article-title>Paced auditory serial-addition task: a measure of recovery from concussion</article-title>
                  <source>Percept. Mot. Skills</source>
                  <volume>44</volume>
                  <issue>2</issue>
                  <year>1977</year>
                  <fpage>367</fpage>
                  <lpage>373</lpage>
                  <pub-id pub-id-type="pmid">866038</pub-id>
                </element-citation>
              </ref>
              <ref id="bb0135">
                <label>27</label>
                <element-citation publication-type="book" id="rf0135">
                  <person-group person-group-type="author">
                    <name>
                      <surname>Reynolds</surname>
                      <given-names>C.R.</given-names>
                    </name>
                  </person-group>
                  <part-title>Comprehensive Trail-Making Test Examiner's Manual</part-title>
                  <year>2002</year>
                  <publisher-name>PRO-ED, Inc.</publisher-name>
                  <publisher-loc>Austin, Texas</publisher-loc>
                </element-citation>
              </ref>
              <ref id="bb0140">
                <label>28</label>
                <element-citation publication-type="book" id="rf0140">
                  <person-group person-group-type="author">
                    <name>
                      <surname>Smith</surname>
                      <given-names>A.</given-names>
                    </name>
                  </person-group>
                  <part-title>Symbol Digit Modalities Test Manual (W-129C)</part-title>
                  <year>2011</year>
                  <publisher-name>Western Psychological Services</publisher-name>
                  <publisher-loc>Torrance, CA</publisher-loc>
                </element-citation>
              </ref>
              <ref id="bb0145">
                <label>29</label>
                <element-citation publication-type="book" id="rf0145">
                  <person-group person-group-type="author">
                    <name>
                      <surname>Psychology Software Tools Inc.</surname>
                    </name>
                  </person-group>
                  <part-title>E-Prime 3.0</part-title>
                  <year>2016</year>
                  <comment>Retrieved from <ext-link ext-link-type="uri" xlink:href="https://www.pstnet.com" id="ir0060">https://www.pstnet.com</ext-link>: Pittsburgh, PA</comment>
                </element-citation>
              </ref>
              <ref id="bb0150">
                <label>30</label>
                <element-citation publication-type="journal" id="rf0150">
                  <person-group person-group-type="author">
                    <name>
                      <surname>Weintraub</surname>
                      <given-names>S.</given-names>
                    </name>
                    <etal/>
                  </person-group>
                  <article-title>Cognition assessment using the NIH Toolbox</article-title>
                  <source>Neurology</source>
                  <volume>80</volume>
                  <issue>11 Suppl 3</issue>
                  <year>2013</year>
                  <fpage>S54</fpage>
                  <lpage>S64</lpage>
                  <pub-id pub-id-type="pmid">23479546</pub-id>
                </element-citation>
              </ref>
              <ref id="bb0155">
                <label>31</label>
                <element-citation publication-type="journal" id="rf0155">
                  <person-group person-group-type="author">
                    <name>
                      <surname>Arnold</surname>
                      <given-names>P.</given-names>
                    </name>
                    <name>
                      <surname>Hill</surname>
                      <given-names>F.</given-names>
                    </name>
                  </person-group>
                  <article-title>Bisensory augmentation: a speechreading advantage when speech is clearly audible and intact</article-title>
                  <source>Br. J. Psychol.</source>
                  <volume>92</volume>
                  <issue>2</issue>
                  <year>2001</year>
                  <fpage>339</fpage>
                  <lpage>355</lpage>
                </element-citation>
              </ref>
              <ref id="bb0160">
                <label>32</label>
                <element-citation publication-type="journal" id="rf0160">
                  <person-group person-group-type="author">
                    <name>
                      <surname>Bowie</surname>
                      <given-names>C.R.</given-names>
                    </name>
                    <name>
                      <surname>Harvey</surname>
                      <given-names>P.D.</given-names>
                    </name>
                  </person-group>
                  <article-title>Administration and interpretation of the Trail Making Test</article-title>
                  <source>Nat. Protoc.</source>
                  <volume>1</volume>
                  <issue>5</issue>
                  <year>2006</year>
                  <fpage>2277</fpage>
                  <lpage>2281</lpage>
                  <pub-id pub-id-type="pmid">17406468</pub-id>
                </element-citation>
              </ref>
              <ref id="bb0165">
                <label>33</label>
                <element-citation publication-type="book" id="rf0165">
                  <person-group person-group-type="author">
                    <name>
                      <surname>Bradski</surname>
                      <given-names>G.</given-names>
                    </name>
                  </person-group>
                  <part-title>The OpenCV Library<italic>.</italic> Dr Dobb's J. Software Tools</part-title>
                  <volume>25</volume>
                  <year>2000</year>
                  <fpage>120</fpage>
                  <lpage>125</lpage>
                </element-citation>
              </ref>
              <ref id="bb0170">
                <label>34</label>
                <element-citation publication-type="book" id="rf0170">
                  <person-group person-group-type="author">
                    <name>
                      <surname>Van Rossum</surname>
                      <given-names>G.</given-names>
                    </name>
                    <name>
                      <surname>Drake</surname>
                      <given-names>F.L.</given-names>
                    </name>
                  </person-group>
                  <part-title>The Python Language Reference Manual</part-title>
                  <year>2011</year>
                  <publisher-name>Network Theory Ltd.</publisher-name>
                </element-citation>
              </ref>
              <ref id="bb0175">
                <label>35</label>
                <element-citation publication-type="journal" id="rf0175">
                  <person-group person-group-type="author">
                    <name>
                      <surname>Peirce</surname>
                      <given-names>J.</given-names>
                    </name>
                  </person-group>
                  <article-title>Generating stimuli for neuroscience using PsychoPy</article-title>
                  <source>Front. Neuroinform.</source>
                  <volume>
                    <bold>2</bold>
                  </volume>
                  <issue>10</issue>
                  <year>2009</year>
                </element-citation>
              </ref>
              <ref id="bb0180">
                <label>36</label>
                <element-citation publication-type="journal" id="rf0180">
                  <person-group person-group-type="author">
                    <name>
                      <surname>Peirce</surname>
                      <given-names>J.</given-names>
                    </name>
                  </person-group>
                  <article-title>PsychoPy—psychophysics software in Python</article-title>
                  <source>J. Neurosci. Methods</source>
                  <volume>162</volume>
                  <issue>1</issue>
                  <year>2007</year>
                  <fpage>8</fpage>
                  <lpage>13</lpage>
                  <pub-id pub-id-type="pmid">17254636</pub-id>
                </element-citation>
              </ref>
              <ref id="bb0185">
                <label>37</label>
                <element-citation publication-type="journal" id="rf0185">
                  <person-group person-group-type="author">
                    <name>
                      <surname>Bridges</surname>
                      <given-names>D.</given-names>
                    </name>
                    <etal/>
                  </person-group>
                  <article-title>The timing mega-study: comparing a range of experiment generators, both lab-based and online</article-title>
                  <source>PeerJ</source>
                  <volume>8</volume>
                  <year>2020</year>
                  <object-id pub-id-type="publisher-id">e9414</object-id>
                </element-citation>
              </ref>
              <ref id="bb0190">
                <label>38</label>
                <element-citation publication-type="journal" id="rf0190">
                  <person-group person-group-type="author">
                    <name>
                      <surname>Garaizar</surname>
                      <given-names>P.</given-names>
                    </name>
                    <name>
                      <surname>Vadillo</surname>
                      <given-names>M.A.</given-names>
                    </name>
                  </person-group>
                  <article-title>Accuracy and precision of visual stimulus timing in PsychoPy: no timing errors in standard usage</article-title>
                  <source>PLoS One</source>
                  <volume>9</volume>
                  <issue>11</issue>
                  <year>2014</year>
                  <object-id pub-id-type="publisher-id">e112033</object-id>
                </element-citation>
              </ref>
              <ref id="bb0195">
                <label>39</label>
                <element-citation publication-type="book" id="rf0195">
                  <person-group person-group-type="author">
                    <name>
                      <surname>Peirce</surname>
                      <given-names>J.</given-names>
                    </name>
                  </person-group>
                  <part-title>PsychoPy - Psychology Software for Python</part-title>
                  <year>2020</year>
                  <fpage>540</fpage>
                </element-citation>
              </ref>
              <ref id="bb0200">
                <label>40</label>
                <element-citation publication-type="book" id="rf0200">
                  <person-group person-group-type="author">
                    <name>
                      <surname>Python Software Foundation</surname>
                    </name>
                  </person-group>
                  <part-title>OS - miscellaneous operating system interfaces</part-title>
                  <source>Python 3.9.0 Documentation - The Python Standard Library</source>
                  <year>2020</year>
                  <publisher-name>Python Software Foundation</publisher-name>
                </element-citation>
              </ref>
              <ref id="bb0205">
                <label>41</label>
                <element-citation publication-type="journal" id="rf0205">
                  <person-group person-group-type="author">
                    <name>
                      <surname>Erickson</surname>
                      <given-names>K.I.</given-names>
                    </name>
                    <etal/>
                  </person-group>
                  <article-title>Investigating gains in neurocognition in an intervention Trial of Exercise (IGNITE): protocol</article-title>
                  <source>Contemp Clin. Trials</source>
                  <volume>85</volume>
                  <year>2019</year>
                  <fpage>105832</fpage>
                  <pub-id pub-id-type="pmid">31465859</pub-id>
                </element-citation>
              </ref>
              <ref id="bb0210">
                <label>42</label>
                <element-citation publication-type="book" id="rf0210">
                  <person-group person-group-type="author">
                    <name>
                      <surname>Kaller</surname>
                      <given-names>C.P.</given-names>
                    </name>
                    <etal/>
                  </person-group>
                  <part-title>Vienna test system manual</part-title>
                  <source>Tower of London - Freiburg Version</source>
                  <year>2011</year>
                  <publisher-name>Schuhfried GmbH</publisher-name>
                  <publisher-loc>Mödling, Austria</publisher-loc>
                </element-citation>
              </ref>
              <ref id="bb0215">
                <label>43</label>
                <element-citation publication-type="book" id="rf0215">
                  <person-group person-group-type="author">
                    <name>
                      <surname>Liguori</surname>
                      <given-names>G.</given-names>
                    </name>
                    <name>
                      <surname>ACSM</surname>
                    </name>
                  </person-group>
                  <person-group person-group-type="editor">
                    <name>
                      <surname>Liguori</surname>
                      <given-names>G.</given-names>
                    </name>
                  </person-group>
                  <source>ACSM's Guidelines for Exercise Testing and Prescription</source>
                  <edition>11th ed</edition>
                  <year>2021</year>
                  <publisher-name>Lippincott Williams and Wilkins</publisher-name>
                  <publisher-loc>Philadelphia, PA</publisher-loc>
                </element-citation>
              </ref>
              <ref id="bb0220">
                <label>44</label>
                <element-citation publication-type="other" id="rf0220">
                  <person-group person-group-type="author">
                    <name>
                      <surname>CDC</surname>
                    </name>
                  </person-group>
                  <article-title><italic>CDC Updates, Expands List of People at Risk of Severe COVID-19 Illness</italic>. 2020 June 25</article-title>
                  <comment>Available from:</comment>
                  <ext-link ext-link-type="uri" xlink:href="https://www.cdc.gov/media/releases/2020/p0625-update-expands-covid-19.html" id="ir0065">https://www.cdc.gov/media/releases/2020/p0625-update-expands-covid-19.html</ext-link>
                  <year>2020</year>
                </element-citation>
              </ref>
              <ref id="bb0225">
                <label>45</label>
                <element-citation publication-type="other" id="rf0225">
                  <person-group person-group-type="author">
                    <name>
                      <surname>CDC</surname>
                    </name>
                  </person-group>
                  <article-title><italic>Coronavirus Disease 2019 (COVID-19)</italic>. People at Increased Risk 2020</article-title>
                  <comment>Available from:</comment>
                  <ext-link ext-link-type="uri" xlink:href="https://www.cdc.gov/coronavirus/2019-ncov/need-extra-precautions/people-at-increased-risk.html" id="ir0070">https://www.cdc.gov/coronavirus/2019-ncov/need-extra-precautions/people-at-increased-risk.html</ext-link>
                </element-citation>
              </ref>
              <ref id="bb0230">
                <label>46</label>
                <element-citation publication-type="other" id="rf0230">
                  <person-group person-group-type="author">
                    <name>
                      <surname>CDC</surname>
                    </name>
                  </person-group>
                  <article-title><italic>People with Certain Medical Conditions</italic>. 2021 Mar. 29</article-title>
                  <comment>Available from:</comment>
                  <ext-link ext-link-type="uri" xlink:href="https://www.cdc.gov/coronavirus/2019-ncov/need-extra-precautions/people-with-medical-conditions.html" id="ir0075">https://www.cdc.gov/coronavirus/2019-ncov/need-extra-precautions/people-with-medical-conditions.html</ext-link>
                  <year>2021</year>
                </element-citation>
              </ref>
              <ref id="bb0235">
                <label>47</label>
                <element-citation publication-type="journal" id="rf0235">
                  <person-group person-group-type="author">
                    <name>
                      <surname>Walker</surname>
                      <given-names>E.</given-names>
                    </name>
                    <name>
                      <surname>Nowacki</surname>
                      <given-names>A.S.</given-names>
                    </name>
                  </person-group>
                  <article-title>Understanding equivalence and noninferiority testing</article-title>
                  <source>J. Gen. Intern. Med.</source>
                  <volume>26</volume>
                  <issue>2</issue>
                  <year>2011</year>
                  <fpage>192</fpage>
                  <lpage>196</lpage>
                  <pub-id pub-id-type="pmid">20857339</pub-id>
                </element-citation>
              </ref>
              <ref id="bb0240">
                <label>48</label>
                <element-citation publication-type="journal" id="rf0240">
                  <person-group person-group-type="author">
                    <name>
                      <surname>Lakens</surname>
                      <given-names>D.</given-names>
                    </name>
                    <name>
                      <surname>Scheel</surname>
                      <given-names>A.M.</given-names>
                    </name>
                    <name>
                      <surname>Isager</surname>
                      <given-names>P.M.</given-names>
                    </name>
                  </person-group>
                  <article-title>Equivalence testing for psychological research: a tutorial</article-title>
                  <source>Adv. Methods Pract. Psychol. Sci.</source>
                  <volume>1</volume>
                  <issue>2</issue>
                  <year>2018</year>
                  <fpage>259</fpage>
                  <lpage>269</lpage>
                </element-citation>
              </ref>
              <ref id="bb0245">
                <label>49</label>
                <element-citation publication-type="journal" id="rf0245">
                  <person-group person-group-type="author">
                    <name>
                      <surname>Lakens</surname>
                      <given-names>D.</given-names>
                    </name>
                  </person-group>
                  <article-title>Equivalence tests</article-title>
                  <source>Soc. Psychol. Personal. Sci.</source>
                  <volume>8</volume>
                  <issue>4</issue>
                  <year>2017</year>
                  <fpage>355</fpage>
                  <lpage>362</lpage>
                  <pub-id pub-id-type="pmid">28736600</pub-id>
                </element-citation>
              </ref>
              <ref id="bb0250">
                <label>50</label>
                <element-citation publication-type="book" id="rf0250">
                  <person-group person-group-type="author">
                    <name>
                      <surname>Core Team</surname>
                      <given-names>R.</given-names>
                    </name>
                  </person-group>
                  <part-title>The R Project for Statistical Computing</part-title>
                  <year>2020</year>
                  <publisher-name>The R Foundation</publisher-name>
                  <publisher-loc>Vienna, Austria</publisher-loc>
                </element-citation>
              </ref>
              <ref id="bb0255">
                <label>51</label>
                <element-citation publication-type="book" id="rf0255">
                  <person-group person-group-type="author">
                    <name>
                      <surname>Latif</surname>
                      <given-names>S.</given-names>
                    </name>
                    <etal/>
                  </person-group>
                  <part-title>
                    <italic>Speech technology for healthcare: opportunities, challenges, and state of the art. IEEE Rev. Biomed. Eng.</italic>
                  </part-title>
                  <volume>14</volume>
                  <year>2020</year>
                  <fpage>342</fpage>
                  <lpage>356</lpage>
                </element-citation>
              </ref>
              <ref id="bb0260">
                <label>52</label>
                <element-citation publication-type="journal" id="rf0260">
                  <person-group person-group-type="author">
                    <name>
                      <surname>Handley</surname>
                      <given-names>Z.</given-names>
                    </name>
                  </person-group>
                  <article-title>Is text-to-speech synthesis ready for use in computer-assisted language learning?</article-title>
                  <source>Speech Comm.</source>
                  <volume>51</volume>
                  <issue>10</issue>
                  <year>2009</year>
                  <fpage>906</fpage>
                  <lpage>919</lpage>
                </element-citation>
              </ref>
              <ref id="bb0265">
                <label>53</label>
                <element-citation publication-type="book" id="rf0265">
                  <person-group person-group-type="author">
                    <name>
                      <surname>Mondol</surname>
                      <given-names>A.S.</given-names>
                    </name>
                    <name>
                      <surname>Emi</surname>
                      <given-names>I.A.</given-names>
                    </name>
                    <name>
                      <surname>Stankovic</surname>
                      <given-names>J.A.</given-names>
                    </name>
                  </person-group>
                  <part-title>MedRem: an interactive medication reminder and tracking system on wrist devices</part-title>
                  <source>2016 IEEE Wireless Health (WH)</source>
                  <year>2016</year>
                </element-citation>
              </ref>
              <ref id="bb0270">
                <label>54</label>
                <element-citation publication-type="book" id="rf0270">
                  <person-group person-group-type="author">
                    <name>
                      <surname>Liu</surname>
                      <given-names>K.-C.</given-names>
                    </name>
                    <etal/>
                  </person-group>
                  <part-title>Voice Helper: a mobile assistive system for visually impaired persons</part-title>
                  <source>2015 IEEE International Conference on Computer and Information Technology</source>
                  <year>2015</year>
                  <publisher-name>IEEE</publisher-name>
                </element-citation>
              </ref>
              <ref id="bb0275">
                <label>55</label>
                <element-citation publication-type="journal" id="rf0275">
                  <person-group person-group-type="author">
                    <name>
                      <surname>Chapman</surname>
                      <given-names>J.E.</given-names>
                    </name>
                    <etal/>
                  </person-group>
                  <article-title>Comparing face-to-face and videoconference completion of the Montreal Cognitive Assessment (MoCA) in community-based survivors of stroke</article-title>
                  <source>J. Telemed. Telecare</source>
                  <year>2019</year>
                  <comment>1357633X1989078</comment>
                </element-citation>
              </ref>
              <ref id="bb0280">
                <label>56</label>
                <element-citation publication-type="journal" id="rf0280">
                  <person-group person-group-type="author">
                    <name>
                      <surname>Katz</surname>
                      <given-names>M.J.</given-names>
                    </name>
                    <etal/>
                  </person-group>
                  <article-title>T-MoCA: A valid phone screen for cognitive impairment in diverse community samples</article-title>
                  <source>Alzheimer's Dementia: Diagn. Assess. Dis. Monitor.</source>
                  <volume>
                    <bold>13</bold>
                  </volume>
                  <issue>1</issue>
                  <year>2021</year>
                </element-citation>
              </ref>
              <ref id="bb0285">
                <label>57</label>
                <element-citation publication-type="journal" id="rf0285">
                  <person-group person-group-type="author">
                    <name>
                      <surname>Bridges</surname>
                      <given-names>D.</given-names>
                    </name>
                    <etal/>
                  </person-group>
                  <article-title>The timing mega-study: comparing a range of experiment generators, both lab-based and online</article-title>
                  <source>PeerJ</source>
                  <volume>8</volume>
                  <year>2020</year>
                  <fpage>e9414</fpage>
                  <pub-id pub-id-type="pmid">33005482</pub-id>
                </element-citation>
              </ref>
            </ref-list>
            <fn-group>
              <fn id="fn0005">
                <label>1</label>
                <p id="np0005">These platforms include Google Cloud (<ext-link ext-link-type="uri" xlink:href="https://cloud.google.com/text-to-speech" id="ir0005">https://cloud.google.com/text-to-speech</ext-link>), IBM Watson (<ext-link ext-link-type="uri" xlink:href="https://www.ibm.com/cloud/watson-text-to-speech" id="ir0010">https://www.ibm.com/cloud/watson-text-to-speech</ext-link>), Amazon Polly (<ext-link ext-link-type="uri" xlink:href="https://aws.amazon.com/polly/" id="ir0015">https://aws.amazon.com/polly/</ext-link>), and Microsoft Azure (<ext-link ext-link-type="uri" xlink:href="https://azure.microsoft.com/en-us/services/cognitive-services/text-to-speech/" id="ir0020">https://azure.microsoft.com/en-us/services/cognitive-services/text-to-speech/</ext-link>) etc.</p>
              </fn>
            </fn-group>
          </back>
        </article>
      </metadata>
    </record>
  </GetRecord>
</OAI-PMH>
