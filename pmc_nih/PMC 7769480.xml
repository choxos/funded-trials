<?xml version="1.0" encoding="UTF-8"?>
<OAI-PMH xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/ http://www.openarchives.org/OAI/2.0/OAI-PMH.xsd">
  <responseDate>2024-05-03T04:11:15Z</responseDate>
  <request verb="GetRecord" identifier="oai:pubmedcentral.nih.gov:7769480" metadataPrefix="pmc">https:/www.ncbi.nlm.nih.gov/pmc/oai/oai.cgi</request>
  <GetRecord>
    <record>
      <header>
        <identifier>oai:pubmedcentral.nih.gov:7769480</identifier>
        <datestamp>2021-01-08</datestamp>
        <setSpec>plosone</setSpec>
        <setSpec>pmc-open</setSpec>
      </header>
      <metadata>
        <article xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" xsi:schemaLocation="https://jats.nlm.nih.gov/ns/archiving/1.3/ https://jats.nlm.nih.gov/archiving/1.3/xsd/JATS-archivearticle1-3.xsd" article-type="research-article">
          <front>
            <journal-meta>
              <journal-id journal-id-type="nlm-ta">PLoS One</journal-id>
              <journal-id journal-id-type="iso-abbrev">PLoS One</journal-id>
              <journal-id journal-id-type="publisher-id">plos</journal-id>
              <journal-id journal-id-type="pmc">plosone</journal-id>
              <journal-title-group>
                <journal-title>PLoS ONE</journal-title>
              </journal-title-group>
              <issn pub-type="epub">1932-6203</issn>
              <publisher>
                <publisher-name>Public Library of Science</publisher-name>
                <publisher-loc>San Francisco, CA USA</publisher-loc>
              </publisher>
            </journal-meta>
            <article-meta>
              <article-id pub-id-type="accession">PMC7769480</article-id>
              <article-id pub-id-type="pmcid">PMC7769480</article-id>
              <article-id pub-id-type="pmc-uid">7769480</article-id>
              <article-id pub-id-type="pmid">33370408</article-id>
              <article-id pub-id-type="doi">10.1371/journal.pone.0244583</article-id>
              <article-id pub-id-type="publisher-id">PONE-D-20-14779</article-id>
              <article-categories>
                <subj-group subj-group-type="heading">
                  <subject>Research Article</subject>
                </subj-group>
                <subj-group subj-group-type="Discipline-v3">
                  <subject>Biology and Life Sciences</subject>
                  <subj-group>
                    <subject>Neuroscience</subject>
                    <subj-group>
                      <subject>Cognitive Science</subject>
                      <subj-group>
                        <subject>Cognitive Psychology</subject>
                        <subj-group>
                          <subject>Perception</subject>
                          <subj-group>
                            <subject>Sensory Perception</subject>
                            <subj-group>
                              <subject>Vision</subject>
                            </subj-group>
                          </subj-group>
                        </subj-group>
                      </subj-group>
                    </subj-group>
                  </subj-group>
                </subj-group>
                <subj-group subj-group-type="Discipline-v3">
                  <subject>Biology and Life Sciences</subject>
                  <subj-group>
                    <subject>Psychology</subject>
                    <subj-group>
                      <subject>Cognitive Psychology</subject>
                      <subj-group>
                        <subject>Perception</subject>
                        <subj-group>
                          <subject>Sensory Perception</subject>
                          <subj-group>
                            <subject>Vision</subject>
                          </subj-group>
                        </subj-group>
                      </subj-group>
                    </subj-group>
                  </subj-group>
                </subj-group>
                <subj-group subj-group-type="Discipline-v3">
                  <subject>Social Sciences</subject>
                  <subj-group>
                    <subject>Psychology</subject>
                    <subj-group>
                      <subject>Cognitive Psychology</subject>
                      <subj-group>
                        <subject>Perception</subject>
                        <subj-group>
                          <subject>Sensory Perception</subject>
                          <subj-group>
                            <subject>Vision</subject>
                          </subj-group>
                        </subj-group>
                      </subj-group>
                    </subj-group>
                  </subj-group>
                </subj-group>
                <subj-group subj-group-type="Discipline-v3">
                  <subject>Biology and Life Sciences</subject>
                  <subj-group>
                    <subject>Neuroscience</subject>
                    <subj-group>
                      <subject>Sensory Perception</subject>
                      <subj-group>
                        <subject>Vision</subject>
                      </subj-group>
                    </subj-group>
                  </subj-group>
                </subj-group>
                <subj-group subj-group-type="Discipline-v3">
                  <subject>Biology and Life Sciences</subject>
                  <subj-group>
                    <subject>Neuroscience</subject>
                    <subj-group>
                      <subject>Cognitive Science</subject>
                      <subj-group>
                        <subject>Cognitive Psychology</subject>
                        <subj-group>
                          <subject>Perception</subject>
                          <subj-group>
                            <subject>Sensory Perception</subject>
                          </subj-group>
                        </subj-group>
                      </subj-group>
                    </subj-group>
                  </subj-group>
                </subj-group>
                <subj-group subj-group-type="Discipline-v3">
                  <subject>Biology and Life Sciences</subject>
                  <subj-group>
                    <subject>Psychology</subject>
                    <subj-group>
                      <subject>Cognitive Psychology</subject>
                      <subj-group>
                        <subject>Perception</subject>
                        <subj-group>
                          <subject>Sensory Perception</subject>
                        </subj-group>
                      </subj-group>
                    </subj-group>
                  </subj-group>
                </subj-group>
                <subj-group subj-group-type="Discipline-v3">
                  <subject>Social Sciences</subject>
                  <subj-group>
                    <subject>Psychology</subject>
                    <subj-group>
                      <subject>Cognitive Psychology</subject>
                      <subj-group>
                        <subject>Perception</subject>
                        <subj-group>
                          <subject>Sensory Perception</subject>
                        </subj-group>
                      </subj-group>
                    </subj-group>
                  </subj-group>
                </subj-group>
                <subj-group subj-group-type="Discipline-v3">
                  <subject>Biology and Life Sciences</subject>
                  <subj-group>
                    <subject>Neuroscience</subject>
                    <subj-group>
                      <subject>Sensory Perception</subject>
                    </subj-group>
                  </subj-group>
                </subj-group>
                <subj-group subj-group-type="Discipline-v3">
                  <subject>Biology and Life Sciences</subject>
                  <subj-group>
                    <subject>Neuroscience</subject>
                    <subj-group>
                      <subject>Cognitive Science</subject>
                      <subj-group>
                        <subject>Cognitive Neuroscience</subject>
                        <subj-group>
                          <subject>Motor Reactions</subject>
                          <subj-group>
                            <subject>Postural Control</subject>
                          </subj-group>
                        </subj-group>
                      </subj-group>
                    </subj-group>
                  </subj-group>
                </subj-group>
                <subj-group subj-group-type="Discipline-v3">
                  <subject>Biology and Life Sciences</subject>
                  <subj-group>
                    <subject>Neuroscience</subject>
                    <subj-group>
                      <subject>Cognitive Neuroscience</subject>
                      <subj-group>
                        <subject>Motor Reactions</subject>
                        <subj-group>
                          <subject>Postural Control</subject>
                        </subj-group>
                      </subj-group>
                    </subj-group>
                  </subj-group>
                </subj-group>
                <subj-group subj-group-type="Discipline-v3">
                  <subject>Biology and Life Sciences</subject>
                  <subj-group>
                    <subject>Neuroscience</subject>
                    <subj-group>
                      <subject>Cognitive Science</subject>
                      <subj-group>
                        <subject>Cognitive Psychology</subject>
                        <subj-group>
                          <subject>Learning</subject>
                        </subj-group>
                      </subj-group>
                    </subj-group>
                  </subj-group>
                </subj-group>
                <subj-group subj-group-type="Discipline-v3">
                  <subject>Biology and Life Sciences</subject>
                  <subj-group>
                    <subject>Psychology</subject>
                    <subj-group>
                      <subject>Cognitive Psychology</subject>
                      <subj-group>
                        <subject>Learning</subject>
                      </subj-group>
                    </subj-group>
                  </subj-group>
                </subj-group>
                <subj-group subj-group-type="Discipline-v3">
                  <subject>Social Sciences</subject>
                  <subj-group>
                    <subject>Psychology</subject>
                    <subj-group>
                      <subject>Cognitive Psychology</subject>
                      <subj-group>
                        <subject>Learning</subject>
                      </subj-group>
                    </subj-group>
                  </subj-group>
                </subj-group>
                <subj-group subj-group-type="Discipline-v3">
                  <subject>Biology and Life Sciences</subject>
                  <subj-group>
                    <subject>Neuroscience</subject>
                    <subj-group>
                      <subject>Learning and Memory</subject>
                      <subj-group>
                        <subject>Learning</subject>
                      </subj-group>
                    </subj-group>
                  </subj-group>
                </subj-group>
                <subj-group subj-group-type="Discipline-v3">
                  <subject>Biology and Life Sciences</subject>
                  <subj-group>
                    <subject>Neuroscience</subject>
                    <subj-group>
                      <subject>Cognitive Science</subject>
                      <subj-group>
                        <subject>Cognitive Psychology</subject>
                        <subj-group>
                          <subject>Learning</subject>
                          <subj-group>
                            <subject>Human Learning</subject>
                          </subj-group>
                        </subj-group>
                      </subj-group>
                    </subj-group>
                  </subj-group>
                </subj-group>
                <subj-group subj-group-type="Discipline-v3">
                  <subject>Biology and Life Sciences</subject>
                  <subj-group>
                    <subject>Psychology</subject>
                    <subj-group>
                      <subject>Cognitive Psychology</subject>
                      <subj-group>
                        <subject>Learning</subject>
                        <subj-group>
                          <subject>Human Learning</subject>
                        </subj-group>
                      </subj-group>
                    </subj-group>
                  </subj-group>
                </subj-group>
                <subj-group subj-group-type="Discipline-v3">
                  <subject>Social Sciences</subject>
                  <subj-group>
                    <subject>Psychology</subject>
                    <subj-group>
                      <subject>Cognitive Psychology</subject>
                      <subj-group>
                        <subject>Learning</subject>
                        <subj-group>
                          <subject>Human Learning</subject>
                        </subj-group>
                      </subj-group>
                    </subj-group>
                  </subj-group>
                </subj-group>
                <subj-group subj-group-type="Discipline-v3">
                  <subject>Biology and Life Sciences</subject>
                  <subj-group>
                    <subject>Neuroscience</subject>
                    <subj-group>
                      <subject>Learning and Memory</subject>
                      <subj-group>
                        <subject>Learning</subject>
                        <subj-group>
                          <subject>Human Learning</subject>
                        </subj-group>
                      </subj-group>
                    </subj-group>
                  </subj-group>
                </subj-group>
                <subj-group subj-group-type="Discipline-v3">
                  <subject>Engineering and Technology</subject>
                  <subj-group>
                    <subject>Signal Processing</subject>
                  </subj-group>
                </subj-group>
                <subj-group subj-group-type="Discipline-v3">
                  <subject>Computer and Information Sciences</subject>
                  <subj-group>
                    <subject>Computer Vision</subject>
                    <subj-group>
                      <subject>Target Detection</subject>
                    </subj-group>
                  </subj-group>
                </subj-group>
                <subj-group subj-group-type="Discipline-v3">
                  <subject>People and Places</subject>
                  <subj-group>
                    <subject>Population Groupings</subject>
                    <subj-group>
                      <subject>Age Groups</subject>
                      <subj-group>
                        <subject>Adults</subject>
                        <subj-group>
                          <subject>Young Adults</subject>
                        </subj-group>
                      </subj-group>
                    </subj-group>
                  </subj-group>
                </subj-group>
              </article-categories>
              <title-group>
                <article-title>Differential effects of visual versus auditory biofeedback training for voluntary postural sway</article-title>
                <alt-title alt-title-type="running-head">Effects of visual versus auditory biofeedback training</alt-title>
              </title-group>
              <contrib-group>
                <contrib contrib-type="author">
                  <contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0002-0791-8469</contrib-id>
                  <name>
                    <surname>Hasegawa</surname>
                    <given-names>Naoya</given-names>
                  </name>
                  <role content-type="https://casrai.org/credit/">Conceptualization</role>
                  <role content-type="https://casrai.org/credit/">Formal analysis</role>
                  <role content-type="https://casrai.org/credit/">Funding acquisition</role>
                  <role content-type="https://casrai.org/credit/">Investigation</role>
                  <role content-type="https://casrai.org/credit/">Methodology</role>
                  <role content-type="https://casrai.org/credit/">Software</role>
                  <role content-type="https://casrai.org/credit/">Visualization</role>
                  <role content-type="https://casrai.org/credit/">Writing – original draft</role>
                  <xref ref-type="aff" rid="aff001">
                    <sup>1</sup>
                  </xref>
                </contrib>
                <contrib contrib-type="author">
                  <name>
                    <surname>Takeda</surname>
                    <given-names>Kenta</given-names>
                  </name>
                  <role content-type="https://casrai.org/credit/">Software</role>
                  <role content-type="https://casrai.org/credit/">Writing – review &amp; editing</role>
                  <xref ref-type="aff" rid="aff002">
                    <sup>2</sup>
                  </xref>
                </contrib>
                <contrib contrib-type="author">
                  <name>
                    <surname>Mancini</surname>
                    <given-names>Martina</given-names>
                  </name>
                  <role content-type="https://casrai.org/credit/">Supervision</role>
                  <role content-type="https://casrai.org/credit/">Writing – review &amp; editing</role>
                  <xref ref-type="aff" rid="aff003">
                    <sup>3</sup>
                  </xref>
                </contrib>
                <contrib contrib-type="author">
                  <name>
                    <surname>King</surname>
                    <given-names>Laurie A.</given-names>
                  </name>
                  <role content-type="https://casrai.org/credit/">Writing – review &amp; editing</role>
                  <xref ref-type="aff" rid="aff003">
                    <sup>3</sup>
                  </xref>
                </contrib>
                <contrib contrib-type="author">
                  <name>
                    <surname>Horak</surname>
                    <given-names>Fay B.</given-names>
                  </name>
                  <role content-type="https://casrai.org/credit/">Funding acquisition</role>
                  <role content-type="https://casrai.org/credit/">Writing – review &amp; editing</role>
                  <xref ref-type="aff" rid="aff003">
                    <sup>3</sup>
                  </xref>
                </contrib>
                <contrib contrib-type="author">
                  <name>
                    <surname>Asaka</surname>
                    <given-names>Tadayoshi</given-names>
                  </name>
                  <role content-type="https://casrai.org/credit/">Conceptualization</role>
                  <role content-type="https://casrai.org/credit/">Funding acquisition</role>
                  <role content-type="https://casrai.org/credit/">Supervision</role>
                  <role content-type="https://casrai.org/credit/">Writing – review &amp; editing</role>
                  <xref ref-type="aff" rid="aff001">
                    <sup>1</sup>
                  </xref>
                  <xref ref-type="corresp" rid="cor001">*</xref>
                </contrib>
              </contrib-group>
              <aff id="aff001">
                <label>1</label>
                <addr-line>Faculty of Health Sciences, Department of Rehabilitation Science, Hokkaido University, Sapporo, Hokkaido, Japan</addr-line>
              </aff>
              <aff id="aff002">
                <label>2</label>
                <addr-line>Department of Rehabilitation for the Movement Functions, Research Institute of National Center for Persons with Disabilities, Tokorozawa, Saitama, Japan</addr-line>
              </aff>
              <aff id="aff003">
                <label>3</label>
                <addr-line>Department of Neurology, Oregon Health &amp; Science University, Portland, Oregon, United States of America</addr-line>
              </aff>
              <contrib-group>
                <contrib contrib-type="editor">
                  <name>
                    <surname>Barbieri</surname>
                    <given-names>Fabio A.</given-names>
                  </name>
                  <role>Editor</role>
                  <xref ref-type="aff" rid="edit1"/>
                </contrib>
              </contrib-group>
              <aff id="edit1">
                <addr-line>São Paulo State University (UNESP), BRAZIL</addr-line>
              </aff>
              <author-notes>
                <fn fn-type="COI-statement" id="coi001">
                  <p><bold>Competing Interests: </bold>The authors have declared that no competing interests exist.</p>
                </fn>
                <corresp id="cor001">* E-mail: <email>ask-chu@hs.hokudai.ac.jp</email></corresp>
              </author-notes>
              <pub-date pub-type="epub">
                <day>28</day>
                <month>12</month>
                <year>2020</year>
              </pub-date>
              <pub-date pub-type="collection">
                <year>2020</year>
              </pub-date>
              <volume>15</volume>
              <issue>12</issue>
              <elocation-id>e0244583</elocation-id>
              <history>
                <date date-type="received">
                  <day>18</day>
                  <month>5</month>
                  <year>2020</year>
                </date>
                <date date-type="accepted">
                  <day>11</day>
                  <month>12</month>
                  <year>2020</year>
                </date>
              </history>
              <permissions>
                <copyright-statement>© 2020 Hasegawa et al</copyright-statement>
                <copyright-year>2020</copyright-year>
                <copyright-holder>Hasegawa et al</copyright-holder>
                <license xlink:href="http://creativecommons.org/licenses/by/4.0/">
                  <license-p>This is an open access article distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution License</ext-link>, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited.</license-p>
                </license>
              </permissions>
              <self-uri content-type="pdf" xlink:href="pone.0244583.pdf"/>
              <abstract>
                <p>Augmented sensory biofeedback training is often used to improve postural control. Our previous study showed that continuous auditory biofeedback was more effective than continuous visual biofeedback to improve postural sway while standing. However, it has also been reported that both discrete visual and auditory biofeedback training, presented intermittently, improves bimanual task performance more than continuous visual biofeedback training. Therefore, this study aimed to investigate the relative effectiveness of discrete visual biofeedback versus discrete auditory biofeedback to improve postural control. Twenty-two healthy young adults were randomly assigned to either a visual or auditory biofeedback group. Participants were asked to shift their center of pressure (COP) by voluntary postural sway forward and backward in line with a hidden target, which moved in a sinusoidal manner and was displayed intermittently. Participants were asked to decrease the diameter of a visual circle (visual biofeedback) or the volume of a sound (auditory biofeedback) based on the distance between the COP and the target in the training session. The feedback and the target were given only when the target reached the inflection points of the sine curves. In addition, the perceptual magnitudes of visual and auditory biofeedback were equalized using Stevens’ power law. Results showed that the mean and standard deviation of the distance between COP and the target were reduced int the test session, removing the augmented sensory biofeedback, in both biofeedback training groups. However, the temporal domain of the performance improved in the test session in the auditory biofeedback training group, but not in the visual biofeedback training group. In conclusion, discrete auditory biofeedback training was more effective for the motor learning of voluntarily postural swaying compared to discrete visual biofeedback training, especially in the temporal domain.</p>
              </abstract>
              <funding-group>
                <award-group id="award001">
                  <funding-source>
                    <institution>Japan Society for the Promotion of Science</institution>
                  </funding-source>
                  <award-id>JSPS20K19371</award-id>
                  <principal-award-recipient>
                    <contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0002-0791-8469</contrib-id>
                    <name>
                      <surname>Hasegawa</surname>
                      <given-names>Naoya</given-names>
                    </name>
                  </principal-award-recipient>
                </award-group>
                <award-group id="award002">
                  <funding-source>
                    <institution>Japan Society for the Promotion of Science</institution>
                  </funding-source>
                  <award-id>JSPS18K10702</award-id>
                </award-group>
                <award-group id="award003">
                  <funding-source>
                    <institution>National Institute of Health</institution>
                  </funding-source>
                  <award-id>R01AG006457</award-id>
                  <principal-award-recipient>
                    <name>
                      <surname>Horak</surname>
                      <given-names>Fay B.</given-names>
                    </name>
                  </principal-award-recipient>
                </award-group>
                <award-group id="award004">
                  <funding-source>
                    <institution-wrap>
                      <institution-id institution-id-type="funder-id">http://dx.doi.org/10.13039/100000738</institution-id>
                      <institution>U.S. Department of Veterans Affairs</institution>
                    </institution-wrap>
                  </funding-source>
                  <award-id>5I01RX001075</award-id>
                  <principal-award-recipient>
                    <name>
                      <surname>Horak</surname>
                      <given-names>Fay B.</given-names>
                    </name>
                  </principal-award-recipient>
                </award-group>
                <funding-statement>This study was supported by Grant-in-Aid for Early-Career Scientists (No. 20K19371, NH) and for Scientific Research (No. 18K10702, TA) from Japan Society for the Promotion of Science (JSPS), the National Institutes of Health under award (No. R01AG006457, PI: FBH), and Department of Veterans Affairs Merit Award (No. 5I01RX001075, PI: FBH). The funders had no role in study design, data collection and analysis, decision to publish, or preparation of the manuscript.</funding-statement>
              </funding-group>
              <counts>
                <fig-count count="6"/>
                <table-count count="2"/>
                <page-count count="16"/>
              </counts>
              <custom-meta-group>
                <custom-meta id="data-availability">
                  <meta-name>Data Availability</meta-name>
                  <meta-value>All relevant data are within the paper.</meta-value>
                </custom-meta>
              </custom-meta-group>
            </article-meta>
            <notes>
              <title>Data Availability</title>
              <p>All relevant data are within the paper.</p>
            </notes>
          </front>
          <body>
            <sec sec-type="intro" id="sec001">
              <title>Introduction</title>
              <p>Augmented sensory biofeedback has been used for decades to train an individual to use his/her own physiological behavior for the purpose of improving performance. The biofeedback systems for postural control aim to provide additional sensory information about postural equilibrium or orientation to the central nervous system [<xref rid="pone.0244583.ref001" ref-type="bibr">1</xref>,<xref rid="pone.0244583.ref002" ref-type="bibr">2</xref>]. Various forms of biofeedback, including visual and auditory, have been suggested to be beneficial for improving postural control in healthy or neurological cohorts [<xref rid="pone.0244583.ref003" ref-type="bibr">3</xref>–<xref rid="pone.0244583.ref005" ref-type="bibr">5</xref>].</p>
              <p>Previous studies have reported that both visual and auditory biofeedback improve postural control during quiet and perturbed stance, as well as gait [<xref rid="pone.0244583.ref006" ref-type="bibr">6</xref>–<xref rid="pone.0244583.ref014" ref-type="bibr">14</xref>]. These results were obtained with continuous biofeedback, where the visual or auditory information was restituted continuously to the user, as opposed to intermittently (discrete). However, the use of continuous biofeedback, particularly visual, seems to result in excessive dependence on the augmented sensory biofeedback, as revealed by performance deterioration upon its removal [<xref rid="pone.0244583.ref002" ref-type="bibr">2</xref>,<xref rid="pone.0244583.ref015" ref-type="bibr">15</xref>,<xref rid="pone.0244583.ref016" ref-type="bibr">16</xref>]. In fact, Lakhani and Mansfield [<xref rid="pone.0244583.ref011" ref-type="bibr">11</xref>] reported that a continuous visual biofeedback, displaying the center of pressure (COP) time-series data on a monitor, successfully reduced postural sway during standing on a foam surface; however, the effects were not maintained when the augmented sensory biofeedback was removed. On the contrary, continuous auditory biofeedback, that provided changing volume and frequency of tones correlated with COP displacements and directions, reduced postural sway during quiet stance even after the augmented sensory biofeedback was removed [<xref rid="pone.0244583.ref006" ref-type="bibr">6</xref>–<xref rid="pone.0244583.ref010" ref-type="bibr">10</xref>]. Although a few studies have reported the effects of visual or auditory biofeedback training on postural control, in our knowledge, only our previous study reported that one modality was better than the other by direct comparison.</p>
              <p>The previous study reported different learning effects resulting from continuous auditory biofeedback training compared to continuous visual biofeedback training during a voluntary postural control task in which subjects aimed to follow a moving target with their body sway [<xref rid="pone.0244583.ref017" ref-type="bibr">17</xref>]. Specifically, the performance, such as timing accuracy relative to the target, was superior after continuous auditory biofeedback training compared to continuous visual biofeedback training, when the augmented sensory biofeedback was removed. In addition, the training effects were retained 48 hours after the biofeedback training, suggesting a learning effect. Recently, Chiou et al. [<xref rid="pone.0244583.ref018" ref-type="bibr">18</xref>] compared the learning effects of continuous or discrete visual biofeedback training and discrete auditory biofeedback training during a bimanual coordination task, such as the 90°-out-of-phase, bimanual coordination pattern. They reported that both discrete visual and auditory biofeedback training resulted in better performance compared to the continuous visual biofeedback training when the augmented sensory biofeedback was removed. However, no significant differences were found between the discrete visual and auditory biofeedback training. The researchers concluded that the different learning effects after biofeedback training was modulated not only by the modalities of biofeedback (visual biofeedback versus auditory biofeedback) but also by the type of information (continuous biofeedback versus discrete biofeedback). However, the study by Chiou et al. [<xref rid="pone.0244583.ref018" ref-type="bibr">18</xref>] investigated the learning effects only in the spatial domain, such as spatial accuracy relative to the target, but not the temporal domain, such as the correlation between actual movements and the ideal movement. Furthermore, it is unknown whether similar learning effects would be achieved using discrete visual biofeedback and discrete auditory biofeedback for postural control.</p>
              <p>The goal of this study was to investigate the learning effects of discrete auditory versus visual biofeedback to improve postural control, using a voluntary postural sway task [<xref rid="pone.0244583.ref017" ref-type="bibr">17</xref>]. A previous study using functional magnetic resonance imaging showed that brain activation increased in sensory-specific areas during visual biofeedback training. In contrast, brain activation gradually decreased over time with auditory biofeedback training [<xref rid="pone.0244583.ref019" ref-type="bibr">19</xref>]. These findings suggest that auditory biofeedback training may suppress reliance on augmented biofeedback during training unlike visual biofeedback training which requires sustained dependence on vision. Moreover, previous studies showed that auditory inputs are processed more quickly, shorter reaction times, compared to visual inputs for motor response [<xref rid="pone.0244583.ref020" ref-type="bibr">20</xref>–<xref rid="pone.0244583.ref022" ref-type="bibr">22</xref>]. Thus, auditory biofeedback would result in faster, more accurate influence on the temporal domain of postural control compared to visual biofeedback. Therefore, we hypothesized that discrete auditory biofeedback training would result in better learning effects than visual biofeedback, especially in the temporal domain for control of voluntary postural sway.</p>
            </sec>
            <sec sec-type="materials|methods" id="sec002">
              <title>Materials and methods</title>
              <sec id="sec003">
                <title>Participants</title>
                <p>Twenty-two healthy young adults (aged 19 to 23) with no known neurological or musculoskeletal disorders participated in this study. The participants were randomly assigned to either auditory biofeedback or visual biofeedback group. Exclusion criteria for both groups were: any neurological or musculoskeletal impairments, or any auditory or visual disabilities that would interfere with balance, or with following instructions. This study was approved by the Hokkaido University Ethics Committee (Project number 16–47). Prior to their inclusion participants were informed about the experimental protocol and gave their written informed consent. All works were conducted in accordance with the declaration of Helsinki (1964).</p>
              </sec>
              <sec id="sec004">
                <title>Equipment</title>
                <p>A force plate (Kistler, Model 9286A, Winterthur, Switzerland) was used to calculate the COP coordinates in the anteroposterior (AP) direction. Force plate data were collected at a sampling frequency of 1000 Hz and filtered with a fourth-order 10-Hz low-pass zero-lag Butterworth filter. Real-time biofeedback was provided on a 19-inch monitor (visual) or by two speakers (auditory) located approximately 1 m from the participant. Biofeedback was programmed using LabVIEW version 2016 (The National Instruments Corp., Austin, TX, USA).</p>
              </sec>
              <sec id="sec005">
                <title>Procedure</title>
                <p>Participants were instructed to stand barefoot with their arms crossed on their chest, and their feet parallel and positioned 1 cm medial to the right or left anterior superior iliac spine [<xref rid="pone.0244583.ref023" ref-type="bibr">23</xref>]. To measure the stability limits in the AP direction, participants were asked to stand still for 5 seconds before they were asked to lean in the forward direction as far as they could, and to hold the maximum COP position for 30 seconds using a visual point on the monitor indicating COP displacement. The same procedure was then repeated in the backward direction. We trained postural control in the AP direction to reduce feedback complexity and allow participants to focus on COP fluctuations along a single axis [<xref rid="pone.0244583.ref013" ref-type="bibr">13</xref>]. The point moved upward on the monitor, located at eye level, as the COP moved forward and vice versa. After measuring the stability limits, participants were asked to perform the test and training sessions with the same stance and position of arms while maintaining attention on the monitor.</p>
              </sec>
              <sec id="sec006">
                <title>Test sessions</title>
                <p>The participant performed 5 test sessions: before and after first training (pre-1 and post-1), before and after second training (pre-2 and post-2), and 48 hours after the second training session (retention) (<xref ref-type="fig" rid="pone.0244583.g001">Fig 1</xref>). Participants were asked to track real-time body COP displacements in line with a moving target. First, the target moved to 80% of the stability limits in the forward direction in each participant, and then, moved back to 70% of the stability limits in the backward direction in each participant. The movements of the target consisted of sine curves at 0.23 Hz [<xref rid="pone.0244583.ref017" ref-type="bibr">17</xref>,<xref rid="pone.0244583.ref024" ref-type="bibr">24</xref>] and repeated seven cycles for 30 seconds in each trial. A red-colored circle became visible at the center of the monitor in synchronization with a beeping sound only when the target reached the sine-wave inflection points (hidden target, see <xref ref-type="fig" rid="pone.0244583.g002">Fig 2</xref>). To calculate the start positions of the target, participants were asked to stand still for 5 seconds, and then they saw a black-colored circle on the monitor with a beeping sound as a start signal of the movements of the target. The start position of the target was averaged from the COP displacements during the first 5 seconds of each trial using the customized program of LabVIEW.</p>
                <fig id="pone.0244583.g001" orientation="portrait" position="float">
                  <object-id pub-id-type="doi">10.1371/journal.pone.0244583.g001</object-id>
                  <label>Fig 1</label>
                  <caption>
                    <title>Study design.</title>
                    <p>Participants were randomized into one of two groups: discrete visual biofeedback or discrete auditory biofeedback. The white boxes indicate five test sessions, and black boxes indicate two training sessions. The participants performed four test sessions: pre-1 and post-1 on the first day (Day 1), pre-2 and post-2 on the second day (Day 2), and retention on the fourth day after training (Day 4) without augmented sensory biofeedback. The training sessions consisted of 8 blocks with augmented sensory biofeedback. One block consisted of 5 trials, and each trial (seven cycles) had a duration of 35 seconds. BF, augmented sensory biofeedback.</p>
                  </caption>
                  <graphic xlink:href="pone.0244583.g001"/>
                </fig>
                <fig id="pone.0244583.g002" orientation="portrait" position="float">
                  <object-id pub-id-type="doi">10.1371/journal.pone.0244583.g002</object-id>
                  <label>Fig 2</label>
                  <caption>
                    <title>Representative example of the target movements and the displacements of center of pressure.</title>
                    <p>The black solid line represents the displacements of center of pressure (COP), and the black dashed line represents the movements of target in anteroposterior directions. Participants received visual and auditory cue, and augmented sensory biofeedback. Augmented sensory biofeedback was presented 75 milliseconds before and after the moving target reached the inflection points of sine curves. The gray-colored areas represent the time intervals of biofeedback. BF, augmented sensory biofeedback.</p>
                  </caption>
                  <graphic xlink:href="pone.0244583.g002"/>
                </fig>
              </sec>
              <sec id="sec007">
                <title>Training sessions</title>
                <p>The participants of both groups performed 80 trials across two consecutive days (8 blocks of 5 trials/day) with a 5-minute rest between the blocks. Each block consisted of 5 trials, and each trial had a duration of 35 seconds. Participants in each group were allowed to first familiarize themselves with the task for 35 seconds. The participants in visual biofeedback group were required to make the diameter of a colored circle smaller by moving their COP. The diameter of the circle changed according to the distance between the real-time COP displacement and the moving target, growing as the COP displacement moved farther from the target and shrinking as the COP displacement moved closer to the target (<xref ref-type="fig" rid="pone.0244583.g003">Fig 3</xref>). Moreover, the color of the circle changed according to the position of the COP displacement to the target; a yellow color indicated the COP displacement shifted from the target in the forward direction (<xref ref-type="fig" rid="pone.0244583.g003">Fig 3A</xref>) and blue indicated the COP displacement shifted from the target in the backward direction (<xref ref-type="fig" rid="pone.0244583.g003">Fig 3B</xref>).</p>
                <fig id="pone.0244583.g003" orientation="portrait" position="float">
                  <object-id pub-id-type="doi">10.1371/journal.pone.0244583.g003</object-id>
                  <label>Fig 3</label>
                  <caption>
                    <title>Augmented sensory biofeedback.</title>
                    <p>For visual biofeedback, the diameter of the colored circle changed according to the distance between the real-time center of pressure (COP) displacement and the moving target in the anteroposterior direction. The larger yellow circle indicated that the COP displacement moved farther from the target and shifted from the target in the forward direction (A), while the larger blue circle indicated that the COP displacement moved farther from the target and shifted from the target in the backward direction (B). For auditory biofeedback, the volume changed according to the distance. The sound generated was higher-pitched (3000 Hz) as COP displacement shifted from the target in the forward direction (A) and lower-pitched (1000 Hz) as COP displacement shifted from the target in the backward direction (B). The visual biofeedback was displayed on the top of the monitor or the auditory biofeedback was sounded from the speaker in front of participants when the moving target reached the inflection point in a forward direction and vice versa. BF, augmented sensory biofeedback.</p>
                  </caption>
                  <graphic xlink:href="pone.0244583.g003"/>
                </fig>
                <p>The participants in the auditory biofeedback group were required to modify the volume of a sound, reducing it as the distance between the COP displacement and the target decreased. In addition, the generated sound was higher-pitched (3000 Hz) as COP displacement shifted from the target in the forward direction (<xref ref-type="fig" rid="pone.0244583.g003">Fig 3A</xref>) and lower-pitched (1000 Hz) as COP displacement shifted from the target in the backward direction (<xref ref-type="fig" rid="pone.0244583.g003">Fig 3B</xref>). Both augmented sensory biofeedbacks were presented 75 milliseconds before and after the moving target reached the sine-wave inflection points (<xref ref-type="fig" rid="pone.0244583.g002">Fig 2</xref>) [<xref rid="pone.0244583.ref018" ref-type="bibr">18</xref>]. To inform the next direction of the moving target to participants, the visual biofeedback was displayed on the top of the monitor or the auditory biofeedback was sounded from the speaker in front of participants when the moving target reached the inflection point in a forward direction and vice versa. The perceptual magnitudes of visual biofeedback and auditory biofeedback were equalized according to Stevens’ power law [<xref rid="pone.0244583.ref025" ref-type="bibr">25</xref>] as follows:
<disp-formula id="pone.0244583.e001"><alternatives><graphic xlink:href="pone.0244583.e001.jpg" id="pone.0244583.e001g" mimetype="image" position="anchor" orientation="portrait"/><mml:math id="M1"><mml:mi>S</mml:mi><mml:mo>=</mml:mo><mml:msup><mml:mrow><mml:mi>D</mml:mi></mml:mrow><mml:mrow><mml:mfrac><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:mfrac></mml:mrow></mml:msup></mml:math></alternatives><label>(1)</label></disp-formula>
where <italic>S</italic> is the perceptual magnitude, <italic>D</italic> is the distance between the COP displacement and the target, and <italic>n</italic> is defined by the sensory modality (visual: 0.9, auditory: 0.3). When the biofeedback was auditory, visual environmental cues were available and when the biofeedback was visual, auditory environmental cues were available.</p>
              </sec>
              <sec id="sec008">
                <title>Outcome measures and statistical analysis</title>
                <p>All signals were processed offline using MATLAB R2018b (The Mathworks Inc., Natick, MA, USA). Although the signals obtained in the test session had seven cycles, only six cycles were analyzed, excluding the first sine curve, in order to ignore the timing error due to the initiation of body sway. To evaluate the effects of motor learning, the mean and standard deviation (SD) of the distance between COP displacement and the target displacement were calculated for the 6 cycles in each trial. Then, the mean (D<sub>mean</sub>) and SD (D<sub>SD</sub>) across 5 trials in each block were calculated. Furthermore, the peak of COP displacement to both forward and backward direction in each cycle was normalized as a percentage for the stability limits toward both directions in each participant. Last, the difference between the peak COP displacement and the peak target displacement was calculated for the 6 cycles in each direction. The mean across 6 cycles in each direction was calculated, and then, the mean across 5 trials in each block was calculated as the value of “mean peak difference”. This variable means a spatial error at the time intervals of biofeedback.</p>
                <p>To evaluate the temporal domain of learning effects, coherence analysis was performed. Coherence is a function of the power spectral density of the COP displacement and the target signal, and the cross-power spectral density of the two signals. Magnitude-squared coherence is estimated as a function of sway frequency, with coherence values indicating the correspondence of the COP displacement signal to the target signal at each frequency bin ranging from 0, absence of any temporal relationship between the signals, to 1, perfect synchrony [<xref rid="pone.0244583.ref026" ref-type="bibr">26</xref>]. The spectral phase revealed the temporal relationship between two signals, expressed in degrees. The absolute synchronization between the two signals was represented by 0-degree phase lag, while the positive and negative values indicated that the COP displacement followed or preceded the target signal, respectively. To assess the temporal accuracy of postural control, we used the absolute value of phase lag [<xref rid="pone.0244583.ref027" ref-type="bibr">27</xref>]. The coherence function determined the magnitude-squared coherence estimate of the two signals using Welch’s method with 6 segments of non-overlapping Hamming windows (frequency resolution = 0.01Hz) to average modified period grams. The peak coherence at 0.23 Hz was estimated on a subject-by-subject basis. The 95% confidence limit for the coherence spectrum was 0.45. The significant value was determined from the total segments per subject as follows:
<disp-formula id="pone.0244583.e002"><alternatives><graphic xlink:href="pone.0244583.e002.jpg" id="pone.0244583.e002g" mimetype="image" position="anchor" orientation="portrait"/><mml:math id="M2"><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mn>0.05</mml:mn></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mfrac bevelled="true"><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>L</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mfrac></mml:mrow></mml:msup></mml:math></alternatives><label>(2)</label></disp-formula>
where <italic>L</italic> is number of the total segments [<xref rid="pone.0244583.ref028" ref-type="bibr">28</xref>].</p>
                <p>Two-way mixed-design ANOVA was used with the factors <italic>Group</italic> (visual biofeedback and auditory biofeedback) and <italic>Test session</italic> (pre-1, post-1, pre-2, post-2 and retention) to analyze possible differences in the above-mentioned parameters. Post-hoc analysis was performed using Bonferroni pairwise comparison. The relationships across the relative values of parameters were calculated using <italic>Pearson’s</italic> correlation coefficient in each group. The relative values were calculated as the values on the retention test divided by those on the pre-1, and then were transformed to their natural logarithms to ensure the normal distribution. Thus, the relative values indicated the amount of learning effects. The statistical analysis for the outcome measure and correlation were processed using SPSS Statistics version 25.0 (IBM, Armonk, NY, USA). The statistical significance was set to <italic>p</italic> &lt; 0.05.</p>
              </sec>
            </sec>
            <sec sec-type="results" id="sec009">
              <title>Results</title>
              <p>No significant differences in participants’ age, sex, height, weight, or foot length were found between the visual biofeedback and auditory biofeedback groups (<xref rid="pone.0244583.t001" ref-type="table">Table 1</xref>).</p>
              <table-wrap id="pone.0244583.t001" orientation="portrait" position="float">
                <object-id pub-id-type="doi">10.1371/journal.pone.0244583.t001</object-id>
                <label>Table 1</label>
                <caption>
                  <title>The characteristics of the participants.</title>
                </caption>
                <alternatives>
                  <graphic id="pone.0244583.t001g" xlink:href="pone.0244583.t001"/>
                  <table frame="hsides" rules="groups">
                    <colgroup span="1">
                      <col align="left" valign="middle" span="1"/>
                      <col align="left" valign="middle" span="1"/>
                      <col align="left" valign="middle" span="1"/>
                      <col align="left" valign="middle" span="1"/>
                    </colgroup>
                    <thead>
                      <tr>
                        <th align="center" style="background-color:#FFFFFF" rowspan="1" colspan="1"/>
                        <th align="center" style="background-color:#FFFFFF" rowspan="1" colspan="1">auditory BF (n = 11)</th>
                        <th align="center" style="background-color:#FFFFFF" rowspan="1" colspan="1">visual BF (n = 11)</th>
                        <th align="center" style="background-color:#FFFFFF" rowspan="1" colspan="1"><italic>p</italic>-value</th>
                      </tr>
                    </thead>
                    <tbody>
                      <tr>
                        <td align="left" style="background-color:#FFFFFF" rowspan="1" colspan="1">Age (years)</td>
                        <td align="center" style="background-color:#FFFFFF" rowspan="1" colspan="1">21.6±1.5</td>
                        <td align="center" style="background-color:#FFFFFF" rowspan="1" colspan="1">21.7±0.6</td>
                        <td align="center" style="background-color:#FFFFFF" rowspan="1" colspan="1">0.856</td>
                      </tr>
                      <tr>
                        <td align="left" style="background-color:#FFFFFF" rowspan="1" colspan="1">Gender (male/female)</td>
                        <td align="center" style="background-color:#FFFFFF" rowspan="1" colspan="1">7 / 4</td>
                        <td align="center" style="background-color:#FFFFFF" rowspan="1" colspan="1">6 / 5</td>
                        <td align="center" style="background-color:#FFFFFF" rowspan="1" colspan="1">0.361<sup>a</sup></td>
                      </tr>
                      <tr>
                        <td align="left" style="background-color:#FFFFFF" rowspan="1" colspan="1">Height (cm)</td>
                        <td align="center" style="background-color:#FFFFFF" rowspan="1" colspan="1">167.9±8.1</td>
                        <td align="center" style="background-color:#FFFFFF" rowspan="1" colspan="1">165.6±10.4</td>
                        <td align="center" style="background-color:#FFFFFF" rowspan="1" colspan="1">0.569</td>
                      </tr>
                      <tr>
                        <td align="left" style="background-color:#FFFFFF" rowspan="1" colspan="1">Weight (kg)</td>
                        <td align="center" style="background-color:#FFFFFF" rowspan="1" colspan="1">61.4±9.4</td>
                        <td align="center" style="background-color:#FFFFFF" rowspan="1" colspan="1">57.7±11.6</td>
                        <td align="center" style="background-color:#FFFFFF" rowspan="1" colspan="1">0.420</td>
                      </tr>
                      <tr>
                        <td align="left" style="background-color:#FFFFFF" rowspan="1" colspan="1">Foot length (right)</td>
                        <td align="center" style="background-color:#FFFFFF" rowspan="1" colspan="1">24.9±1.6</td>
                        <td align="center" style="background-color:#FFFFFF" rowspan="1" colspan="1">24.0±2.1</td>
                        <td align="center" style="background-color:#FFFFFF" rowspan="1" colspan="1">0.251</td>
                      </tr>
                    </tbody>
                  </table>
                </alternatives>
                <table-wrap-foot>
                  <fn id="t001fn001">
                    <p>Groups compared using independent sample t-test or Chi-squared test and significance level of 0.05 (a: Chi-squared test). Mean ± Standard deviation; BF, augmented sensory biofeedback.</p>
                  </fn>
                </table-wrap-foot>
              </table-wrap>
              <sec id="sec010">
                <title>Spatial domain</title>
                <p>A significant reduction of D<sub>mean</sub> and D<sub>SD</sub> was observed due to <italic>Test session</italic> (D<sub>mean</sub>: F<sub>4, 21</sub> = 45.801, <italic>p</italic> &lt; 0.001; D<sub>SD</sub>: F<sub>4, 21</sub> = 25.807, <italic>p</italic> &lt; 0.001; <xref rid="pone.0244583.t002" ref-type="table">Table 2</xref>). In addition, these reductions were similar across the two biofeedback groups (D<sub>mean</sub>: F<sub>1, 21</sub> = 0.185, <italic>p</italic> = 0.671; D<sub>SD</sub>: F<sub>1, 21</sub> = 0.022, <italic>p</italic> = 0.884; <xref rid="pone.0244583.t002" ref-type="table">Table 2</xref>).</p>
                <table-wrap id="pone.0244583.t002" orientation="portrait" position="float">
                  <object-id pub-id-type="doi">10.1371/journal.pone.0244583.t002</object-id>
                  <label>Table 2</label>
                  <caption>
                    <title>Results from two-way mixed-design ANOVA for each outcome measure in the spatial and temporal domain.</title>
                  </caption>
                  <alternatives>
                    <graphic id="pone.0244583.t002g" xlink:href="pone.0244583.t002"/>
                    <table frame="hsides" rules="groups">
                      <colgroup span="1">
                        <col align="left" valign="middle" span="1"/>
                        <col align="left" valign="middle" span="1"/>
                        <col align="left" valign="middle" span="1"/>
                        <col align="left" valign="middle" span="1"/>
                        <col align="left" valign="middle" span="1"/>
                        <col align="left" valign="middle" span="1"/>
                        <col align="left" valign="middle" span="1"/>
                        <col align="left" valign="middle" span="1"/>
                        <col align="left" valign="middle" span="1"/>
                        <col align="left" valign="middle" span="1"/>
                        <col align="left" valign="middle" span="1"/>
                        <col align="left" valign="middle" span="1"/>
                        <col align="left" valign="middle" span="1"/>
                        <col align="left" valign="middle" span="1"/>
                      </colgroup>
                      <thead>
                        <tr>
                          <th align="left" style="background-color:#FFFFFF" rowspan="1" colspan="1"/>
                          <th align="justify" colspan="5" style="background-color:#FFFFFF" rowspan="1">auditory BF, Mean (SD)</th>
                          <th align="justify" colspan="8" style="background-color:#FFFFFF" rowspan="1">visual BF, Mean (SD)</th>
                        </tr>
                        <tr>
                          <th align="justify" style="background-color:#FFFFFF" rowspan="1" colspan="1">Outcomes</th>
                          <th align="justify" style="background-color:#FFFFFF" rowspan="1" colspan="1">Pre-1</th>
                          <th align="justify" style="background-color:#FFFFFF" rowspan="1" colspan="1">Post-1</th>
                          <th align="justify" style="background-color:#FFFFFF" rowspan="1" colspan="1">Pre-2</th>
                          <th align="justify" style="background-color:#FFFFFF" rowspan="1" colspan="1">Post-2</th>
                          <th align="justify" style="background-color:#FFFFFF" rowspan="1" colspan="1">Retention</th>
                          <th align="justify" style="background-color:#FFFFFF" rowspan="1" colspan="1">Pre-1</th>
                          <th align="justify" style="background-color:#FFFFFF" rowspan="1" colspan="1">Post-1</th>
                          <th align="justify" style="background-color:#FFFFFF" rowspan="1" colspan="1">Pre-2</th>
                          <th align="justify" style="background-color:#FFFFFF" rowspan="1" colspan="1">Post-2</th>
                          <th align="justify" style="background-color:#FFFFFF" rowspan="1" colspan="1">Retention</th>
                          <th align="justify" style="background-color:#FFFFFF" rowspan="1" colspan="1">Fixed factor</th>
                          <th align="justify" style="background-color:#FFFFFF" rowspan="1" colspan="1"><italic>F</italic> value</th>
                          <th align="justify" style="background-color:#FFFFFF" rowspan="1" colspan="1"><italic>p</italic>-value</th>
                        </tr>
                      </thead>
                      <tbody>
                        <tr>
                          <td align="left" style="background-color:#FFFFFF" rowspan="1" colspan="1">
                            <bold>Spatial</bold>
                          </td>
                          <td align="left" style="background-color:#FFFFFF" rowspan="1" colspan="1"/>
                          <td align="left" style="background-color:#FFFFFF" rowspan="1" colspan="1"/>
                          <td align="left" style="background-color:#FFFFFF" rowspan="1" colspan="1"/>
                          <td align="left" style="background-color:#FFFFFF" rowspan="1" colspan="1"/>
                          <td align="left" style="background-color:#FFFFFF" rowspan="1" colspan="1"/>
                          <td align="left" style="background-color:#FFFFFF" rowspan="1" colspan="1"/>
                          <td align="left" style="background-color:#FFFFFF" rowspan="1" colspan="1"/>
                          <td align="left" style="background-color:#FFFFFF" rowspan="1" colspan="1"/>
                          <td align="left" style="background-color:#FFFFFF" rowspan="1" colspan="1"/>
                          <td align="left" style="background-color:#FFFFFF" rowspan="1" colspan="1"/>
                          <td align="left" style="background-color:#FFFFFF" rowspan="1" colspan="1"/>
                          <td align="left" style="background-color:#FFFFFF" rowspan="1" colspan="1"/>
                          <td align="left" style="background-color:#FFFFFF" rowspan="1" colspan="1"/>
                        </tr>
                        <tr>
                          <td align="left" style="background-color:#FFFFFF" rowspan="1" colspan="1">D<sub>mean</sub> (mm)</td>
                          <td align="left" style="background-color:#FFFFFF" rowspan="1" colspan="1">28.9</td>
                          <td align="left" style="background-color:#FFFFFF" rowspan="1" colspan="1">18.0</td>
                          <td align="left" style="background-color:#FFFFFF" rowspan="1" colspan="1">20.9</td>
                          <td align="left" style="background-color:#FFFFFF" rowspan="1" colspan="1">17.6</td>
                          <td align="left" style="background-color:#FFFFFF" rowspan="1" colspan="1">20.0</td>
                          <td align="left" style="background-color:#FFFFFF" rowspan="1" colspan="1">25.8</td>
                          <td align="left" style="background-color:#FFFFFF" rowspan="1" colspan="1">17.6</td>
                          <td align="left" style="background-color:#FFFFFF" rowspan="1" colspan="1">21.7</td>
                          <td align="left" style="background-color:#FFFFFF" rowspan="1" colspan="1">16.6</td>
                          <td align="left" style="background-color:#FFFFFF" rowspan="1" colspan="1">20.2</td>
                          <td align="left" style="background-color:#FFFFFF" rowspan="1" colspan="1">
                            <bold>Test</bold>
                          </td>
                          <td align="left" style="background-color:#FFFFFF" rowspan="1" colspan="1">
                            <bold>45.801</bold>
                          </td>
                          <td align="left" style="background-color:#FFFFFF" rowspan="1" colspan="1">
                            <bold>&lt; 0.001</bold>
                          </td>
                        </tr>
                        <tr>
                          <td align="left" style="background-color:#FFFFFF" rowspan="1" colspan="1"/>
                          <td align="left" style="background-color:#FFFFFF" rowspan="1" colspan="1">(3.7)</td>
                          <td align="left" style="background-color:#FFFFFF" rowspan="1" colspan="1">(3.3)</td>
                          <td align="left" style="background-color:#FFFFFF" rowspan="1" colspan="1">(3.1)</td>
                          <td align="left" style="background-color:#FFFFFF" rowspan="1" colspan="1">(3.9)</td>
                          <td align="left" style="background-color:#FFFFFF" rowspan="1" colspan="1">(4.6)</td>
                          <td align="left" style="background-color:#FFFFFF" rowspan="1" colspan="1">(5.4)</td>
                          <td align="left" style="background-color:#FFFFFF" rowspan="1" colspan="1">(5.6)</td>
                          <td align="left" style="background-color:#FFFFFF" rowspan="1" colspan="1">(5.7)</td>
                          <td align="left" style="background-color:#FFFFFF" rowspan="1" colspan="1">(4.4)</td>
                          <td align="left" style="background-color:#FFFFFF" rowspan="1" colspan="1">(4.9)</td>
                          <td align="left" style="background-color:#FFFFFF" rowspan="1" colspan="1">Group</td>
                          <td align="left" style="background-color:#FFFFFF" rowspan="1" colspan="1">0.185</td>
                          <td align="left" style="background-color:#FFFFFF" rowspan="1" colspan="1">0.671</td>
                        </tr>
                        <tr>
                          <td align="left" style="background-color:#FFFFFF" rowspan="1" colspan="1"/>
                          <td align="left" style="background-color:#FFFFFF" rowspan="1" colspan="1"/>
                          <td align="left" style="background-color:#FFFFFF" rowspan="1" colspan="1"/>
                          <td align="left" style="background-color:#FFFFFF" rowspan="1" colspan="1"/>
                          <td align="left" style="background-color:#FFFFFF" rowspan="1" colspan="1"/>
                          <td align="left" style="background-color:#FFFFFF" rowspan="1" colspan="1"/>
                          <td align="left" style="background-color:#FFFFFF" rowspan="1" colspan="1"/>
                          <td align="left" style="background-color:#FFFFFF" rowspan="1" colspan="1"/>
                          <td align="left" style="background-color:#FFFFFF" rowspan="1" colspan="1"/>
                          <td align="left" style="background-color:#FFFFFF" rowspan="1" colspan="1"/>
                          <td align="left" style="background-color:#FFFFFF" rowspan="1" colspan="1"/>
                          <td align="left" style="background-color:#FFFFFF" rowspan="1" colspan="1">Interaction</td>
                          <td align="left" style="background-color:#FFFFFF" rowspan="1" colspan="1">1.538</td>
                          <td align="left" style="background-color:#FFFFFF" rowspan="1" colspan="1">0.199</td>
                        </tr>
                        <tr>
                          <td align="left" style="background-color:#FFFFFF" rowspan="1" colspan="1">D<sub>SD</sub> (mm)</td>
                          <td align="left" style="background-color:#FFFFFF" rowspan="1" colspan="1">29.1</td>
                          <td align="left" style="background-color:#FFFFFF" rowspan="1" colspan="1">18.8</td>
                          <td align="left" style="background-color:#FFFFFF" rowspan="1" colspan="1">22.0</td>
                          <td align="left" style="background-color:#FFFFFF" rowspan="1" colspan="1">18.7</td>
                          <td align="left" style="background-color:#FFFFFF" rowspan="1" colspan="1">21.8</td>
                          <td align="left" style="background-color:#FFFFFF" rowspan="1" colspan="1">26.9</td>
                          <td align="left" style="background-color:#FFFFFF" rowspan="1" colspan="1">18.2</td>
                          <td align="left" style="background-color:#FFFFFF" rowspan="1" colspan="1">23.8</td>
                          <td align="left" style="background-color:#FFFFFF" rowspan="1" colspan="1">18.5</td>
                          <td align="left" style="background-color:#FFFFFF" rowspan="1" colspan="1">21.5</td>
                          <td align="left" style="background-color:#FFFFFF" rowspan="1" colspan="1">
                            <bold>Test</bold>
                          </td>
                          <td align="left" style="background-color:#FFFFFF" rowspan="1" colspan="1">
                            <bold>25.807</bold>
                          </td>
                          <td align="left" style="background-color:#FFFFFF" rowspan="1" colspan="1">
                            <bold>&lt; 0.001</bold>
                          </td>
                        </tr>
                        <tr>
                          <td align="left" style="background-color:#FFFFFF" rowspan="1" colspan="1"/>
                          <td align="left" style="background-color:#FFFFFF" rowspan="1" colspan="1">(4.5)</td>
                          <td align="left" style="background-color:#FFFFFF" rowspan="1" colspan="1">(3.7)</td>
                          <td align="left" style="background-color:#FFFFFF" rowspan="1" colspan="1">(4.2)</td>
                          <td align="left" style="background-color:#FFFFFF" rowspan="1" colspan="1">(3.0)</td>
                          <td align="left" style="background-color:#FFFFFF" rowspan="1" colspan="1">(5.8)</td>
                          <td align="left" style="background-color:#FFFFFF" rowspan="1" colspan="1">(6.5)</td>
                          <td align="left" style="background-color:#FFFFFF" rowspan="1" colspan="1">(7.6)</td>
                          <td align="left" style="background-color:#FFFFFF" rowspan="1" colspan="1">(7.8)</td>
                          <td align="left" style="background-color:#FFFFFF" rowspan="1" colspan="1">(7.7)</td>
                          <td align="left" style="background-color:#FFFFFF" rowspan="1" colspan="1">(6.6)</td>
                          <td align="left" style="background-color:#FFFFFF" rowspan="1" colspan="1">Group</td>
                          <td align="left" style="background-color:#FFFFFF" rowspan="1" colspan="1">0.022</td>
                          <td align="left" style="background-color:#FFFFFF" rowspan="1" colspan="1">0.884</td>
                        </tr>
                        <tr>
                          <td align="left" style="background-color:#FFFFFF" rowspan="1" colspan="1"/>
                          <td align="left" style="background-color:#FFFFFF" rowspan="1" colspan="1"/>
                          <td align="left" style="background-color:#FFFFFF" rowspan="1" colspan="1"/>
                          <td align="left" style="background-color:#FFFFFF" rowspan="1" colspan="1"/>
                          <td align="left" style="background-color:#FFFFFF" rowspan="1" colspan="1"/>
                          <td align="left" style="background-color:#FFFFFF" rowspan="1" colspan="1"/>
                          <td align="left" style="background-color:#FFFFFF" rowspan="1" colspan="1"/>
                          <td align="left" style="background-color:#FFFFFF" rowspan="1" colspan="1"/>
                          <td align="left" style="background-color:#FFFFFF" rowspan="1" colspan="1"/>
                          <td align="left" style="background-color:#FFFFFF" rowspan="1" colspan="1"/>
                          <td align="left" style="background-color:#FFFFFF" rowspan="1" colspan="1"/>
                          <td align="left" style="background-color:#FFFFFF" rowspan="1" colspan="1">Interaction</td>
                          <td align="left" style="background-color:#FFFFFF" rowspan="1" colspan="1">0.856</td>
                          <td align="left" style="background-color:#FFFFFF" rowspan="1" colspan="1">0.494</td>
                        </tr>
                        <tr>
                          <td align="left" style="background-color:#FFFFFF" rowspan="1" colspan="1">Mean peak</td>
                          <td align="left" style="background-color:#FFFFFF" rowspan="1" colspan="1">29.1</td>
                          <td align="left" style="background-color:#FFFFFF" rowspan="1" colspan="1">13.7</td>
                          <td align="left" style="background-color:#FFFFFF" rowspan="1" colspan="1">18.7</td>
                          <td align="left" style="background-color:#FFFFFF" rowspan="1" colspan="1">12.7</td>
                          <td align="left" style="background-color:#FFFFFF" rowspan="1" colspan="1">17.1</td>
                          <td align="left" style="background-color:#FFFFFF" rowspan="1" colspan="1">31.5</td>
                          <td align="left" style="background-color:#FFFFFF" rowspan="1" colspan="1">14.9</td>
                          <td align="left" style="background-color:#FFFFFF" rowspan="1" colspan="1">26.9</td>
                          <td align="left" style="background-color:#FFFFFF" rowspan="1" colspan="1">15.3</td>
                          <td align="left" style="background-color:#FFFFFF" rowspan="1" colspan="1">25.3</td>
                          <td align="left" style="background-color:#FFFFFF" rowspan="1" colspan="1">
                            <bold>Test</bold>
                          </td>
                          <td align="left" style="background-color:#FFFFFF" rowspan="1" colspan="1">
                            <bold>52.563</bold>
                          </td>
                          <td align="left" style="background-color:#FFFFFF" rowspan="1" colspan="1">
                            <bold>&lt; 0.001</bold>
                          </td>
                        </tr>
                        <tr>
                          <td align="left" style="background-color:#FFFFFF" rowspan="1" colspan="1">difference (%)</td>
                          <td align="left" style="background-color:#FFFFFF" rowspan="1" colspan="1">(7.1)</td>
                          <td align="left" style="background-color:#FFFFFF" rowspan="1" colspan="1">(5.9)</td>
                          <td align="left" style="background-color:#FFFFFF" rowspan="1" colspan="1">(7.2)</td>
                          <td align="left" style="background-color:#FFFFFF" rowspan="1" colspan="1">(2.2)</td>
                          <td align="left" style="background-color:#FFFFFF" rowspan="1" colspan="1">(5.6)</td>
                          <td align="left" style="background-color:#FFFFFF" rowspan="1" colspan="1">(8.2)</td>
                          <td align="left" style="background-color:#FFFFFF" rowspan="1" colspan="1">(3.5)</td>
                          <td align="left" style="background-color:#FFFFFF" rowspan="1" colspan="1">(6.8)</td>
                          <td align="left" style="background-color:#FFFFFF" rowspan="1" colspan="1">(2.7)</td>
                          <td align="left" style="background-color:#FFFFFF" rowspan="1" colspan="1">(6.0)</td>
                          <td align="left" style="background-color:#FFFFFF" rowspan="1" colspan="1">
                            <bold>Group</bold>
                          </td>
                          <td align="left" style="background-color:#FFFFFF" rowspan="1" colspan="1">
                            <bold>6.048</bold>
                          </td>
                          <td align="left" style="background-color:#FFFFFF" rowspan="1" colspan="1">
                            <bold>0.023</bold>
                          </td>
                        </tr>
                        <tr>
                          <td align="left" style="background-color:#FFFFFF" rowspan="1" colspan="1"/>
                          <td align="left" style="background-color:#FFFFFF" rowspan="1" colspan="1"/>
                          <td align="left" style="background-color:#FFFFFF" rowspan="1" colspan="1"/>
                          <td align="left" style="background-color:#FFFFFF" rowspan="1" colspan="1"/>
                          <td align="left" style="background-color:#FFFFFF" rowspan="1" colspan="1"/>
                          <td align="left" style="background-color:#FFFFFF" rowspan="1" colspan="1"/>
                          <td align="left" style="background-color:#FFFFFF" rowspan="1" colspan="1"/>
                          <td align="left" style="background-color:#FFFFFF" rowspan="1" colspan="1"/>
                          <td align="left" style="background-color:#FFFFFF" rowspan="1" colspan="1"/>
                          <td align="left" style="background-color:#FFFFFF" rowspan="1" colspan="1"/>
                          <td align="left" style="background-color:#FFFFFF" rowspan="1" colspan="1"/>
                          <td align="left" style="background-color:#FFFFFF" rowspan="1" colspan="1">
                            <bold>Interaction</bold>
                          </td>
                          <td align="left" style="background-color:#FFFFFF" rowspan="1" colspan="1">
                            <bold>3.336</bold>
                          </td>
                          <td align="left" style="background-color:#FFFFFF" rowspan="1" colspan="1">
                            <bold>0.026</bold>
                          </td>
                        </tr>
                        <tr>
                          <td align="left" style="background-color:#FFFFFF" rowspan="1" colspan="1">
                            <bold>Temporal</bold>
                          </td>
                          <td align="left" style="background-color:#FFFFFF" rowspan="1" colspan="1"/>
                          <td align="left" style="background-color:#FFFFFF" rowspan="1" colspan="1"/>
                          <td align="left" style="background-color:#FFFFFF" rowspan="1" colspan="1"/>
                          <td align="left" style="background-color:#FFFFFF" rowspan="1" colspan="1"/>
                          <td align="left" style="background-color:#FFFFFF" rowspan="1" colspan="1"/>
                          <td align="left" style="background-color:#FFFFFF" rowspan="1" colspan="1"/>
                          <td align="left" style="background-color:#FFFFFF" rowspan="1" colspan="1"/>
                          <td align="left" style="background-color:#FFFFFF" rowspan="1" colspan="1"/>
                          <td align="left" style="background-color:#FFFFFF" rowspan="1" colspan="1"/>
                          <td align="left" style="background-color:#FFFFFF" rowspan="1" colspan="1"/>
                          <td align="left" style="background-color:#FFFFFF" rowspan="1" colspan="1"/>
                          <td align="left" style="background-color:#FFFFFF" rowspan="1" colspan="1"/>
                          <td align="left" style="background-color:#FFFFFF" rowspan="1" colspan="1"/>
                        </tr>
                        <tr>
                          <td align="left" style="background-color:#FFFFFF" rowspan="1" colspan="1">Magnitude of</td>
                          <td align="left" style="background-color:#FFFFFF" rowspan="1" colspan="1">0.958</td>
                          <td align="left" style="background-color:#FFFFFF" rowspan="1" colspan="1">0.973</td>
                          <td align="left" style="background-color:#FFFFFF" rowspan="1" colspan="1">0.973</td>
                          <td align="left" style="background-color:#FFFFFF" rowspan="1" colspan="1">0.980</td>
                          <td align="left" style="background-color:#FFFFFF" rowspan="1" colspan="1">0.980</td>
                          <td align="left" style="background-color:#FFFFFF" rowspan="1" colspan="1">0.961</td>
                          <td align="left" style="background-color:#FFFFFF" rowspan="1" colspan="1">0.966</td>
                          <td align="left" style="background-color:#FFFFFF" rowspan="1" colspan="1">0.962</td>
                          <td align="left" style="background-color:#FFFFFF" rowspan="1" colspan="1">0.961</td>
                          <td align="left" style="background-color:#FFFFFF" rowspan="1" colspan="1">0.969</td>
                          <td align="left" style="background-color:#FFFFFF" rowspan="1" colspan="1">
                            <bold>Test</bold>
                          </td>
                          <td align="left" style="background-color:#FFFFFF" rowspan="1" colspan="1">
                            <bold>6.463</bold>
                          </td>
                          <td align="left" style="background-color:#FFFFFF" rowspan="1" colspan="1">
                            <bold>&lt; 0.001</bold>
                          </td>
                        </tr>
                        <tr>
                          <td align="left" style="background-color:#FFFFFF" rowspan="1" colspan="1">coherence</td>
                          <td align="left" style="background-color:#FFFFFF" rowspan="1" colspan="1">(0.009)</td>
                          <td align="left" style="background-color:#FFFFFF" rowspan="1" colspan="1">(0.011)</td>
                          <td align="left" style="background-color:#FFFFFF" rowspan="1" colspan="1">(0.008)</td>
                          <td align="left" style="background-color:#FFFFFF" rowspan="1" colspan="1">(0.004)</td>
                          <td align="left" style="background-color:#FFFFFF" rowspan="1" colspan="1">(0.005)</td>
                          <td align="left" style="background-color:#FFFFFF" rowspan="1" colspan="1">(0.011)</td>
                          <td align="left" style="background-color:#FFFFFF" rowspan="1" colspan="1">(0.018)</td>
                          <td align="left" style="background-color:#FFFFFF" rowspan="1" colspan="1">(0.017)</td>
                          <td align="left" style="background-color:#FFFFFF" rowspan="1" colspan="1">(0.013)</td>
                          <td align="left" style="background-color:#FFFFFF" rowspan="1" colspan="1">(0.011)</td>
                          <td align="left" style="background-color:#FFFFFF" rowspan="1" colspan="1">
                            <bold>Group</bold>
                          </td>
                          <td align="left" style="background-color:#FFFFFF" rowspan="1" colspan="1">
                            <bold>9.676</bold>
                          </td>
                          <td align="left" style="background-color:#FFFFFF" rowspan="1" colspan="1">
                            <bold>0.006</bold>
                          </td>
                        </tr>
                        <tr>
                          <td align="left" style="background-color:#FFFFFF" rowspan="1" colspan="1"/>
                          <td align="left" style="background-color:#FFFFFF" rowspan="1" colspan="1"/>
                          <td align="left" style="background-color:#FFFFFF" rowspan="1" colspan="1"/>
                          <td align="left" style="background-color:#FFFFFF" rowspan="1" colspan="1"/>
                          <td align="left" style="background-color:#FFFFFF" rowspan="1" colspan="1"/>
                          <td align="left" style="background-color:#FFFFFF" rowspan="1" colspan="1"/>
                          <td align="left" style="background-color:#FFFFFF" rowspan="1" colspan="1"/>
                          <td align="left" style="background-color:#FFFFFF" rowspan="1" colspan="1"/>
                          <td align="left" style="background-color:#FFFFFF" rowspan="1" colspan="1"/>
                          <td align="left" style="background-color:#FFFFFF" rowspan="1" colspan="1"/>
                          <td align="left" style="background-color:#FFFFFF" rowspan="1" colspan="1"/>
                          <td align="left" style="background-color:#FFFFFF" rowspan="1" colspan="1">
                            <bold>Interaction</bold>
                          </td>
                          <td align="left" style="background-color:#FFFFFF" rowspan="1" colspan="1">
                            <bold>3.254</bold>
                          </td>
                          <td align="left" style="background-color:#FFFFFF" rowspan="1" colspan="1">
                            <bold>0.016</bold>
                          </td>
                        </tr>
                        <tr>
                          <td align="left" style="background-color:#FFFFFF" rowspan="1" colspan="1">Phase lag</td>
                          <td align="left" style="background-color:#FFFFFF" rowspan="1" colspan="1">16.1</td>
                          <td align="left" style="background-color:#FFFFFF" rowspan="1" colspan="1">11.6</td>
                          <td align="left" style="background-color:#FFFFFF" rowspan="1" colspan="1">11.3</td>
                          <td align="left" style="background-color:#FFFFFF" rowspan="1" colspan="1">9.2</td>
                          <td align="left" style="background-color:#FFFFFF" rowspan="1" colspan="1">8.4</td>
                          <td align="left" style="background-color:#FFFFFF" rowspan="1" colspan="1">17.2</td>
                          <td align="left" style="background-color:#FFFFFF" rowspan="1" colspan="1">16.0</td>
                          <td align="left" style="background-color:#FFFFFF" rowspan="1" colspan="1">17.3</td>
                          <td align="left" style="background-color:#FFFFFF" rowspan="1" colspan="1">15.3</td>
                          <td align="left" style="background-color:#FFFFFF" rowspan="1" colspan="1">19.9</td>
                          <td align="left" style="background-color:#FFFFFF" rowspan="1" colspan="1">
                            <bold>Test</bold>
                          </td>
                          <td align="left" style="background-color:#FFFFFF" rowspan="1" colspan="1">
                            <bold>3.887</bold>
                          </td>
                          <td align="left" style="background-color:#FFFFFF" rowspan="1" colspan="1">
                            <bold>0.006</bold>
                          </td>
                        </tr>
                        <tr>
                          <td align="left" style="background-color:#FFFFFF" rowspan="1" colspan="1">(degrees)</td>
                          <td align="left" style="background-color:#FFFFFF" rowspan="1" colspan="1">(5.4)</td>
                          <td align="left" style="background-color:#FFFFFF" rowspan="1" colspan="1">(5.1)</td>
                          <td align="left" style="background-color:#FFFFFF" rowspan="1" colspan="1">(4.7)</td>
                          <td align="left" style="background-color:#FFFFFF" rowspan="1" colspan="1">(3.2)</td>
                          <td align="left" style="background-color:#FFFFFF" rowspan="1" colspan="1">(3.9)</td>
                          <td align="left" style="background-color:#FFFFFF" rowspan="1" colspan="1">(4.9)</td>
                          <td align="left" style="background-color:#FFFFFF" rowspan="1" colspan="1">(4.3)</td>
                          <td align="left" style="background-color:#FFFFFF" rowspan="1" colspan="1">(9.7)</td>
                          <td align="left" style="background-color:#FFFFFF" rowspan="1" colspan="1">(6.4)</td>
                          <td align="left" style="background-color:#FFFFFF" rowspan="1" colspan="1">(5.8)</td>
                          <td align="left" style="background-color:#FFFFFF" rowspan="1" colspan="1">
                            <bold>Group</bold>
                          </td>
                          <td align="left" style="background-color:#FFFFFF" rowspan="1" colspan="1">
                            <bold>9.249</bold>
                          </td>
                          <td align="left" style="background-color:#FFFFFF" rowspan="1" colspan="1">
                            <bold>0.006</bold>
                          </td>
                        </tr>
                        <tr>
                          <td align="left" style="background-color:#FFFFFF" rowspan="1" colspan="1"/>
                          <td align="left" style="background-color:#FFFFFF" rowspan="1" colspan="1"/>
                          <td align="left" style="background-color:#FFFFFF" rowspan="1" colspan="1"/>
                          <td align="left" style="background-color:#FFFFFF" rowspan="1" colspan="1"/>
                          <td align="left" style="background-color:#FFFFFF" rowspan="1" colspan="1"/>
                          <td align="left" style="background-color:#FFFFFF" rowspan="1" colspan="1"/>
                          <td align="left" style="background-color:#FFFFFF" rowspan="1" colspan="1"/>
                          <td align="left" style="background-color:#FFFFFF" rowspan="1" colspan="1"/>
                          <td align="left" style="background-color:#FFFFFF" rowspan="1" colspan="1"/>
                          <td align="left" style="background-color:#FFFFFF" rowspan="1" colspan="1"/>
                          <td align="left" style="background-color:#FFFFFF" rowspan="1" colspan="1"/>
                          <td align="left" style="background-color:#FFFFFF" rowspan="1" colspan="1">
                            <bold>Interaction</bold>
                          </td>
                          <td align="left" style="background-color:#FFFFFF" rowspan="1" colspan="1">
                            <bold>5.554</bold>
                          </td>
                          <td align="left" style="background-color:#FFFFFF" rowspan="1" colspan="1">
                            <bold>0.002</bold>
                          </td>
                        </tr>
                      </tbody>
                    </table>
                  </alternatives>
                  <table-wrap-foot>
                    <fn id="t002fn001">
                      <p>Bold values indicate significant effects at p &lt; 0.05.</p>
                    </fn>
                    <fn id="t002fn002">
                      <p>BF, augmented sensory biofeedback; D<sub>mean</sub>, the mean distance between the center of pressure (COP) and the moving target; D<sub>SD</sub>, the standard deviation (SD) of the distance between the COP and the moving target.</p>
                    </fn>
                  </table-wrap-foot>
                </table-wrap>
                <p>Post-hoc testing revealed that D<sub>mean</sub> and D<sub>SD</sub> in the retention trials were significantly decreased compared to pre-1 for both biofeedback groups (D<sub>mean</sub>: auditory biofeedback, <italic>p</italic> &lt; 0.001; visual biofeedback, <italic>p</italic> = 0.001; D<sub>SD</sub>: auditory biofeedback, <italic>p</italic> &lt; 0.001; visual biofeedback, <italic>p</italic> = 0.019; <xref ref-type="fig" rid="pone.0244583.g004">Fig 4A and 4B</xref>).</p>
                <fig id="pone.0244583.g004" orientation="portrait" position="float">
                  <object-id pub-id-type="doi">10.1371/journal.pone.0244583.g004</object-id>
                  <label>Fig 4</label>
                  <caption>
                    <title>Learning effects of both augmented sensory biofeedback training on outcomes in spatial domain.</title>
                    <p>Point plots of the mean (A) and standard deviation (SD) (B) of the distances between the center of pressure (COP) and the moving target, and the mean difference of peak movements between COP displacement and the moving target in the forward and backward directions (Mean peak difference) (C). The black circles represent the auditory augmented sensory biofeedback group, and the white squares represent the visual biofeedback group. Error bar shows a SD. * and † indicates a significant difference within auditory and visual biofeedback group, respectively (<italic>p</italic> &lt; 0.05), and § indicates a significant difference between groups (<italic>p</italic> &lt; 0.05). BF, augmented sensory biofeedback.</p>
                  </caption>
                  <graphic xlink:href="pone.0244583.g004"/>
                </fig>
                <p>The mean peak difference between the COP and target displacements significantly decreased with <italic>Test session</italic> (F<sub>4, 21</sub> = 52.563, <italic>p</italic> &lt; 0.001), suggesting that the reduction of spatial error at peak occurred in both biofeedback groups (<xref rid="pone.0244583.t002" ref-type="table">Table 2</xref>). A larger spatial error was observed in the visual biofeedback group compared to the auditory biofeedback group (F<sub>1, 21</sub> = 6.048, <italic>p</italic> = 0.023; <xref rid="pone.0244583.t002" ref-type="table">Table 2</xref>). A significant interaction between <italic>Test session</italic> and <italic>Group</italic> (F<sub>4, 21</sub> = 3.336, <italic>p</italic> = 0.026; <xref rid="pone.0244583.t002" ref-type="table">Table 2</xref>) was found for mean peak difference. Specifically, the post-hoc analysis showed that the decrease in mean peak difference was greater in the auditory biofeedback group compared to the visual biofeedback group at the pre-2 (<italic>p</italic> = 0.012), post-2 (<italic>p</italic> = 0.020), and retention (<italic>p</italic> = 0.003) (<xref ref-type="fig" rid="pone.0244583.g004">Fig 4C</xref>).</p>
              </sec>
              <sec id="sec011">
                <title>Temporal domain</title>
                <p>A significant interaction effect between <italic>Test session</italic> and <italic>Group</italic> was found on the magnitude of coherence and the phase lag (F<sub>4, 21</sub> = 3.254, <italic>p</italic> = 0.016; <xref rid="pone.0244583.t002" ref-type="table">Table 2</xref>). Specifically, the post-hoc analysis revealed that the auditory, but not the visual, biofeedback group showed a significant increase in the magnitude of coherence in the post-1 (<italic>p</italic> = 0.001), pre-2 (<italic>p</italic> = 0.001), post-2 (<italic>p</italic> &lt; 0.001) and retention (<italic>p</italic> &lt; 0.001) compared to baseline pre-1 (<xref ref-type="fig" rid="pone.0244583.g005">Fig 5A</xref>). The higher value of the magnitude of coherence indicated better success in tracking the target of the COP displacements. Moreover, the magnitude of coherence in the post-2 and retention in the auditory biofeedback group was significantly higher than that in the visual biofeedback group pre-1 in the auditory biofeedback group (<italic>p</italic> &lt; 0.001 and <italic>p</italic> = 0.017, respectively). However, the visual biofeedback group showed no significant difference between pre-1 and the other test sessions (<xref ref-type="fig" rid="pone.0244583.g005">Fig 5A</xref>).</p>
                <fig id="pone.0244583.g005" orientation="portrait" position="float">
                  <object-id pub-id-type="doi">10.1371/journal.pone.0244583.g005</object-id>
                  <label>Fig 5</label>
                  <caption>
                    <title>Significant learning effects of auditory augmented sensory biofeedback training on outcomes in temporal domain.</title>
                    <p>Mean and standard deviation (SD) plots of two temporal measures: (A) magnitude of coherence and (B) phase lag. The black circles represent the auditory augmented sensory biofeedback group, and the white squares represent the visual biofeedback group. Error bar shows a standard deviation. *indicate a significant difference within auditory biofeedback group (<italic>p</italic> &lt; 0.05) and § indicates a significant difference between groups (<italic>p</italic> &lt; 0.05). N.S., non-significance; BF, augmented sensory biofeedback.</p>
                  </caption>
                  <graphic xlink:href="pone.0244583.g005"/>
                </fig>
                <p>Further, a reduction of phase lag after auditory, but not visual, biofeedback training was found, revealed by a significant interaction between <italic>Test session</italic> and <italic>Group</italic> (F<sub>4, 21</sub> = 5.554, <italic>p</italic> = 0.001; <xref rid="pone.0244583.t002" ref-type="table">Table 2</xref>). Post-hoc analysis showed that the phase lag was significantly lower in the auditory biofeedback group than in the visual biofeedback group in the pre-1 (<italic>p</italic> = 0.043), post-2 (<italic>p</italic> = 0.010) and retention (<italic>p</italic> &lt; 0.001) (<xref ref-type="fig" rid="pone.0244583.g005">Fig 5B</xref>). A smaller phase lag means a better temporal synchronization between the COP displacements and the target. In addition, the auditory biofeedback group showed significant reduction on the phase lag in the other test sessions post-1, pre-2, post-2, and retention compared to that in the pre-1 (<italic>p</italic> = 0.011, <italic>p</italic> = 0.005, <italic>p</italic> &lt; 0.001, and <italic>p</italic> &lt; 0.001, respectively). On the other hand, no significant difference was shown between pre-1 and the other test sessions in the visual biofeedback group (<xref ref-type="fig" rid="pone.0244583.g005">Fig 5B</xref>).</p>
              </sec>
              <sec id="sec012">
                <title>Correlation</title>
                <p>We found a significant relationship between the relative value of D<sub>mean</sub> and that of D<sub>SD</sub> in both biofeedback groups (auditory biofeedback: <italic>r</italic> = 0.831, <italic>p</italic> = 0.002; visual biofeedback: <italic>r</italic> = 0.751, <italic>p</italic> = 0.008). The relative value of the mean peak difference between COP and target displacement was significantly positively correlated with that of D<sub>mean</sub> in the auditory biofeedback group (<italic>r</italic> = 0.606, <italic>p</italic> = 0.048), but not in the visual biofeedback group (<italic>r</italic> = 0.506, <italic>p</italic> = 0.112) (<xref ref-type="fig" rid="pone.0244583.g006">Fig 6</xref>). No other significant relationships were found across the relative values.</p>
                <fig id="pone.0244583.g006" orientation="portrait" position="float">
                  <object-id pub-id-type="doi">10.1371/journal.pone.0244583.g006</object-id>
                  <label>Fig 6</label>
                  <caption>
                    <title>Significant correlation between the improvements in both spatial errors in auditory augmented sensory biofeedback group.</title>
                    <p>Scatter plots of the relative value of the mean distance between the center of pressure (COP) and the moving target (D<sub>mean</sub>) with the relative value of the mean difference of peak movements between COP displacement and the moving target in the forward and backward directions (Mean peak difference). Black circles represent the auditory augmented sensory biofeedback group, and white squares represent the visual augmented sensory biofeedback group. Transformed values to their natural logarithms are displayed, and <italic>p</italic>-value was calculated by a <italic>Pearson’s</italic> correlation coefficient. BF, augmented sensory biofeedback.</p>
                  </caption>
                  <graphic xlink:href="pone.0244583.g006"/>
                </fig>
              </sec>
            </sec>
            <sec sec-type="conclusions" id="sec013">
              <title>Discussion</title>
              <p>Our findings reveal that discrete auditory biofeedback was more effective than discrete visual biofeedback for motor learning of voluntary postural sway (even after equalizing the perceptual magnitude of each type of biofeedback). The results of this study showed that both discrete biofeedback trainings improved postural control in the spatial domain under the no-feedback condition on the retention test compared to the pre-test (pre-1). However, only the discrete auditory biofeedback training enhanced postural control both in the temporal and spatial domains at the time intervals of biofeedback. Furthermore, the improvements in the spatial error at the time intervals of biofeedback significantly correlated with improvements in the spatial error over the whole trial in the auditory biofeedback group, but not in the visual biofeedback group.</p>
              <p>As hypothesized, the learning effects of discrete auditory biofeedback training on postural control were superior to the discrete visual biofeedback training, particularly in the temporal domain and at the time intervals of biofeedback. One of the mechanisms explaining such differences between visual and auditory biofeedback may be a link between auditory and proprioceptive sensory systems. In fact, several studies demonstrated that auditory biofeedback enhanced multisensory integration and perceptual neural representation [<xref rid="pone.0244583.ref002" ref-type="bibr">2</xref>, <xref rid="pone.0244583.ref029" ref-type="bibr">29</xref>–<xref rid="pone.0244583.ref032" ref-type="bibr">32</xref>]. For example, a study reported that auditory biofeedback training induced a significant enhancement of knee proprioception, shown as a lower knee repositioning error with auditory biofeedback [<xref rid="pone.0244583.ref030" ref-type="bibr">30</xref>,<xref rid="pone.0244583.ref031" ref-type="bibr">31</xref>]. In addition, the enhancement remained during the no-feedback condition immediately or 24-hour after the auditory biofeedback training [<xref rid="pone.0244583.ref031" ref-type="bibr">31</xref>]. These results suggested that, after auditory biofeedback, the participants not only learned to reproduce the movement precisely but also learned a more precise use of proprioceptive information from the knee joint. Likewise, some of the neuroimaging studies also supported the finding that auditory biofeedback can promote coactivation in a broad network response to auditory and proprioceptive information [<xref rid="pone.0244583.ref019" ref-type="bibr">19</xref>,<xref rid="pone.0244583.ref032" ref-type="bibr">32</xref>]. In contrast, visual biofeedback activates only the cortical areas playing a role in visuomotor transformation [<xref rid="pone.0244583.ref019" ref-type="bibr">19</xref>]. Therefore, one possibility why auditory biofeedback was superior to visual biofeedback is that the auditory biofeedback system uses different learning strategies than the visual biofeedback system. In other words, visual biofeedback may promote a visuomotor transformation during augmented sensory biofeedback training, while auditory biofeedback may proceed motor learning by strengthening the intermodal coupling between auditory and proprioceptive information which contributes to the performance without augmented sensory biofeedback. Previous studies showed a stronger cognitive involvement, represented by increased brain activation of prefrontal areas [<xref rid="pone.0244583.ref019" ref-type="bibr">19</xref>] and putamen [<xref rid="pone.0244583.ref033" ref-type="bibr">33</xref>], in performing a sensory-motor task when using auditory biofeedback compared to visual biofeedback. More cognitive involvement may enhance attention to intrinsic sensory information, especially proprioceptive information, and that may explain why the postural performance with auditory biofeedback was better than performance with visual biofeedback, even after the biofeedback was removed. Another explanation for the different learning strategies may be the different temporal accuracy between auditory and visual biofeedback. The stimulus-response reaction times for visual inputs are tens to one hundred milliseconds slower than that for auditory inputs [<xref rid="pone.0244583.ref020" ref-type="bibr">20</xref>,<xref rid="pone.0244583.ref034" ref-type="bibr">34</xref>]. Therefore, auditory biofeedback has an advantage in temporal resolution compared to visual biofeedback, which provides more temporal accuracy and reduced spatial error for auditory, than visual, biofeedback training. The slower visual processing results in delayed postural motor responses as apparent in the coherence values (lower values for the visual–moving the body less coherent with the stimulus) and in the phase (higher values for the visual–moving lagging behind the stimulus) (<xref ref-type="fig" rid="pone.0244583.g005">Fig 5</xref>).</p>
              <p>We also found a significant reduction of spatial error under the no-feedback, retention, condition after either discrete visual or discrete auditory biofeedback training. In contrast, a previous study showed that continuous auditory biofeedback training, but not continuous visual biofeedback training, reduced spatial error for a voluntarily postural control task under the no-feedback condition even immediately after training [<xref rid="pone.0244583.ref017" ref-type="bibr">17</xref>]. This discrepancy could be explained by the type of biofeedback (continuous versus discrete). Consistent with our results, a recent study showed that discrete visual biofeedback training improved bimanual movements under the no-feedback condition after the biofeedback training similarly to discrete auditory biofeedback training, but not continuous visual biofeedback training [<xref rid="pone.0244583.ref018" ref-type="bibr">18</xref>]. Some researchers argue that reduced learning effects by visual biofeedback training are caused by “visual dominance” which is an excessive reliance on visual input with reduced other sensory contributions under the condition with visual biofeedback [<xref rid="pone.0244583.ref035" ref-type="bibr">35</xref>,<xref rid="pone.0244583.ref036" ref-type="bibr">36</xref>]. Therefore, reduced frequency of visual biofeedback during discrete biofeedback training, compared to continuous biofeedback training, may suppress the visual dominance, and then enhance spontaneous motor learning using proprioceptive input that contributes to the performance without biofeedback. This was supported by our results. In fact, the reduced mean peak difference was significantly associated with improvements of postural control in the spatial domain (D<sub>mean</sub>) in the auditory biofeedback group only. D<sub>mean</sub> indicates the average spatial error for one trial, which consists of the area with and without augmented sensory biofeedback in the training session. Therefore, the significant correlation between reduced mean peak difference and improvements of postural control in the spatial domain result suggests that enhanced accuracy when using auditory biofeedback is responsible for the reduced spatial error under the no-feedback condition. On the other hand, no significant correlation between the improvements in mean peak difference and D<sub>mean</sub> was found in the discrete visual biofeedback training. This finding could suggest that reduced the whole spatial error in the discrete visual biofeedback group may be mainly caused by reduced spatial error of the area without augmented sensory biofeedback in the training session. In other words, the discrete visual biofeedback improves voluntary postural sway performance in the spatial domain mainly using spontaneous motor learning, not based on enhanced sensory information.</p>
              <p>There are some limitations to this study. First, this experiment was performed with a small number of young participants. Therefore, we cannot be certain our findings would apply to people with neurologic disorders or older participants. Second, the learning effects by discrete biofeedback training were not directly compared with learning effects by the continuous biofeedback training. Last, neuroimaging should be investigated to understand the motor learning mechanisms underlying the different learning effects of visual biofeedback versus auditory biofeedback training.</p>
            </sec>
            <sec sec-type="conclusions" id="sec014">
              <title>Conclusions</title>
              <p>This randomized trial demonstrated that discrete auditory biofeedback training was more effective than discrete visual biofeedback training for the motor learning of the voluntary postural sway task. Future studies should investigate the learning effects of the different types of visual and auditory biofeedback trainings in elderly persons or in people with sensory disorders. Furthermore, cortical activity and muscle synergies with sensory biofeedback training for postural control should be investigated in future studies.</p>
            </sec>
          </body>
          <back>
            <ack>
              <p>The authors thank our participants for generously donating their time to participate, and Norimasa Kakuya for helping with data collection and helping with study procedures.</p>
            </ack>
            <ref-list>
              <title>References</title>
              <ref id="pone.0244583.ref001">
                <label>1</label>
                <mixed-citation publication-type="journal"><name><surname>Moore</surname><given-names>S</given-names></name>, <name><surname>Woollacott</surname><given-names>M</given-names></name>. <article-title>The use of biofeedback to improve postural stability</article-title>. <source>Phys Ther Practice</source>. <year>1993</year>; <volume>2</volume>: <fpage>1</fpage>–<lpage>19</lpage>.</mixed-citation>
              </ref>
              <ref id="pone.0244583.ref002">
                <label>2</label>
                <mixed-citation publication-type="journal"><name><surname>Sigrist</surname><given-names>R</given-names></name>, <name><surname>Rauter</surname><given-names>G</given-names></name>, <name><surname>Riener</surname><given-names>R</given-names></name>, <name><surname>Wolf</surname><given-names>P</given-names></name>. <article-title>Augmented visual, auditory, haptic, and multimodal feedback in motor learning: a review</article-title>. <source>Psychon Bull Rev</source>. <year>2013</year>; <volume>20</volume>(<issue>1</issue>): <fpage>21</fpage>–<lpage>53</lpage>. <pub-id pub-id-type="doi">10.3758/s13423-012-0333-8</pub-id>
<?supplied-pmid 23132605?><pub-id pub-id-type="pmid">23132605</pub-id></mixed-citation>
              </ref>
              <ref id="pone.0244583.ref003">
                <label>3</label>
                <mixed-citation publication-type="journal"><name><surname>Alhasan</surname><given-names>H</given-names></name>, <name><surname>Hood</surname><given-names>V</given-names></name>, <name><surname>Mainwaring</surname><given-names>F</given-names></name>. <article-title>The effect of visual biofeedback on balance in elderly population: a systematic review</article-title>. <source>Clin Interv Aging</source>. <year>2017</year>; <volume>12</volume>: <fpage>487</fpage>–<lpage>497</lpage>. <pub-id pub-id-type="doi">10.2147/CIA.S127023</pub-id>
<?supplied-pmid 28293105?><pub-id pub-id-type="pmid">28293105</pub-id></mixed-citation>
              </ref>
              <ref id="pone.0244583.ref004">
                <label>4</label>
                <mixed-citation publication-type="journal"><name><surname>Sienko</surname><given-names>KH</given-names></name>, <name><surname>Seidler</surname><given-names>RD</given-names></name>, <name><surname>Carender</surname><given-names>WJ</given-names></name>, <name><surname>Goodworth</surname><given-names>AD</given-names></name>, <name><surname>Whitney</surname><given-names>SL</given-names></name>, <name><surname>Peterka</surname><given-names>RJ</given-names></name>. <article-title>Potential Mechanisms of Sensory Augmentation Systems on Human Balance Control</article-title>. <source>Front Neurol</source>. <year>2018</year>; <volume>9</volume>: <fpage>944</fpage><pub-id pub-id-type="doi">10.3389/fneur.2018.00944</pub-id><?supplied-pmid 30483209?><pub-id pub-id-type="pmid">30483209</pub-id></mixed-citation>
              </ref>
              <ref id="pone.0244583.ref005">
                <label>5</label>
                <mixed-citation publication-type="journal"><name><surname>Zijlstra</surname><given-names>A</given-names></name>, <name><surname>Mancini</surname><given-names>M</given-names></name>, <name><surname>Chiari</surname><given-names>L</given-names></name>, <name><surname>Zijlstra</surname><given-names>W</given-names></name>. <article-title>Biofeedback for training balance and mobility tasks in older populations: a systematic review</article-title>. <source>J Neuroeng Rehabil</source>. <year>2010</year>; <volume>7</volume>: <fpage>58</fpage><pub-id pub-id-type="doi">10.1186/1743-0003-7-58</pub-id><?supplied-pmid 21143921?><pub-id pub-id-type="pmid">21143921</pub-id></mixed-citation>
              </ref>
              <ref id="pone.0244583.ref006">
                <label>6</label>
                <mixed-citation publication-type="journal"><name><surname>Dozza</surname><given-names>M</given-names></name>, <name><surname>Chiari</surname><given-names>L</given-names></name>, <name><surname>Chan</surname><given-names>B</given-names></name>, <name><surname>Rocchi</surname><given-names>L</given-names></name>, <name><surname>Horak</surname><given-names>FB</given-names></name>, <name><surname>Cappello</surname><given-names>A</given-names></name>. <article-title>Influence of a portable audio-biofeedback device on structural properties of postural sway.</article-title><source>J Neuroeng Rehabil</source>. <year>2005</year>; <volume>2</volume>:<fpage>13</fpage><pub-id pub-id-type="doi">10.1186/1743-0003-2-13</pub-id><?supplied-pmid 15927058?><pub-id pub-id-type="pmid">15927058</pub-id></mixed-citation>
              </ref>
              <ref id="pone.0244583.ref007">
                <label>7</label>
                <mixed-citation publication-type="journal"><name><surname>Dozza</surname><given-names>M</given-names></name>, <name><surname>Horak</surname><given-names>FB</given-names></name>, <name><surname>Chiari</surname><given-names>L</given-names></name>. <article-title>Auditory biofeedback substitutes for loss of sensory information in maintaining stance</article-title>. <source>Exp Brain Res</source>. <year>2007</year>; <volume>178</volume>(<issue>1</issue>): <fpage>37</fpage>–<lpage>48</lpage>. <pub-id pub-id-type="doi">10.1007/s00221-006-0709-y</pub-id>
<?supplied-pmid 17021893?><pub-id pub-id-type="pmid">17021893</pub-id></mixed-citation>
              </ref>
              <ref id="pone.0244583.ref008">
                <label>8</label>
                <mixed-citation publication-type="journal"><name><surname>Dozza</surname><given-names>M</given-names></name>, <name><surname>Chiari</surname><given-names>L</given-names></name>, <name><surname>Peterka</surname><given-names>RJ</given-names></name>, <name><surname>Wall</surname><given-names>C</given-names></name>, <name><surname>Horak</surname><given-names>FB</given-names></name>. <article-title>What is the most effective type of audio-biofeedback for postural motor learning?</article-title><source>Gait Posture.</source><year>2011</year>; <volume>34</volume>(<issue>3</issue>): <fpage>313</fpage>–<lpage>319</lpage>. <pub-id pub-id-type="doi">10.1016/j.gaitpost.2011.05.016</pub-id>
<?supplied-pmid 21703858?><pub-id pub-id-type="pmid">21703858</pub-id></mixed-citation>
              </ref>
              <ref id="pone.0244583.ref009">
                <label>9</label>
                <mixed-citation publication-type="journal"><name><surname>Carpinella</surname><given-names>I</given-names></name>, <name><surname>Cattaneo</surname><given-names>D</given-names></name>, <name><surname>Bonora</surname><given-names>G</given-names></name>, <name><surname>Bowman</surname><given-names>T</given-names></name>, <name><surname>Martina</surname><given-names>L</given-names></name>, <name><surname>Montesano</surname><given-names>A</given-names></name>, <etal>et al</etal><article-title>Wearable Sensor-Based Biofeedback Training for Balance and Gait in Parkinson Disease: A Pilot Randomized Controlled Trial</article-title>. <source>Arch Phys Med Rehabil</source>. <year>2017</year>; <volume>98</volume>(<issue>4</issue>): <fpage>622</fpage>–<lpage>630</lpage>. <pub-id pub-id-type="doi">10.1016/j.apmr.2016.11.003</pub-id>
<?supplied-pmid 27965005?><pub-id pub-id-type="pmid">27965005</pub-id></mixed-citation>
              </ref>
              <ref id="pone.0244583.ref010">
                <label>10</label>
                <mixed-citation publication-type="journal"><name><surname>Ginis</surname><given-names>P</given-names></name>, <name><surname>Nieuwboer</surname><given-names>A</given-names></name>, <name><surname>Dorfman</surname><given-names>M</given-names></name>, <name><surname>Ferrari</surname><given-names>A</given-names></name>, <name><surname>Gazit</surname><given-names>E</given-names></name>, <name><surname>Canning</surname><given-names>CG</given-names></name>, <etal>et al</etal><article-title>Feasibility and effects of home-based smartphone-delivered automated feedback training for gait in people with Parkinson's disease: A pilot randomized controlled trial</article-title>. <source>Parkinsonism Relat Disord</source>. <year>2016</year>; <volume>22</volume>: <fpage>28</fpage>–<lpage>34</lpage>. <pub-id pub-id-type="doi">10.1016/j.parkreldis.2015.11.004</pub-id>
<?supplied-pmid 26777408?><pub-id pub-id-type="pmid">26777408</pub-id></mixed-citation>
              </ref>
              <ref id="pone.0244583.ref011">
                <label>11</label>
                <mixed-citation publication-type="journal"><name><surname>Lakhani</surname><given-names>B</given-names></name>, <name><surname>Mansfield</surname><given-names>A</given-names></name>. <article-title>Visual feedback of the centre of gravity to optimize standing balance</article-title>. <source>Gait Posture</source>. <year>2015</year>; <volume>41</volume>(<issue>2</issue>): <fpage>499</fpage>–<lpage>503</lpage>. <pub-id pub-id-type="doi">10.1016/j.gaitpost.2014.12.003</pub-id>
<?supplied-pmid 25542399?><pub-id pub-id-type="pmid">25542399</pub-id></mixed-citation>
              </ref>
              <ref id="pone.0244583.ref012">
                <label>12</label>
                <mixed-citation publication-type="journal"><name><surname>Petersen</surname><given-names>H</given-names></name>, <name><surname>Magnusson</surname><given-names>M</given-names></name>, <name><surname>Johansson</surname><given-names>R</given-names></name>, <name><surname>Fransson</surname><given-names>PA</given-names></name>. <article-title>Auditory feedback regulation of perturbed stance in stroke patients</article-title>. <source>Scand J Rehabil Med</source><year>1996</year>; <volume>28</volume>: <fpage>217</fpage>–<lpage>223</lpage>. <?supplied-pmid 9122650?><pub-id pub-id-type="pmid">9122650</pub-id></mixed-citation>
              </ref>
              <ref id="pone.0244583.ref013">
                <label>13</label>
                <mixed-citation publication-type="journal"><name><surname>Pinsault</surname><given-names>N</given-names></name>, <name><surname>Vuillerme</surname><given-names>N</given-names></name>. <article-title>The effects of scale display of visual feedback on postural control during quiet standing in healthy elderly subjects</article-title>. <source>Arch Phys Med Rehabil</source>. <year>2008</year>; <volume>89</volume>(<issue>9</issue>): <fpage>1772</fpage>–<lpage>1774</lpage>. <pub-id pub-id-type="doi">10.1016/j.apmr.2008.02.024</pub-id>
<?supplied-pmid 18760162?><pub-id pub-id-type="pmid">18760162</pub-id></mixed-citation>
              </ref>
              <ref id="pone.0244583.ref014">
                <label>14</label>
                <mixed-citation publication-type="journal"><name><surname>Takeda</surname><given-names>K</given-names></name>, <name><surname>Mani</surname><given-names>H</given-names></name>, <name><surname>Hasegawa</surname><given-names>N</given-names></name>, <name><surname>Sato</surname><given-names>Y</given-names></name>, <name><surname>Tanaka</surname><given-names>S</given-names></name>, <name><surname>Maejima</surname><given-names>H</given-names></name>, <etal>et al</etal><article-title>Adaptation effects in static postural control by providing simultaneous visual feedback of center of pressure and center of gravity</article-title>. <source>J Physiol Anthropol</source>. <year>2017</year>; <volume>36</volume>(<issue>1</issue>): <fpage>31</fpage><pub-id pub-id-type="doi">10.1186/s40101-017-0147-5</pub-id><?supplied-pmid 28724444?><pub-id pub-id-type="pmid">28724444</pub-id></mixed-citation>
              </ref>
              <ref id="pone.0244583.ref015">
                <label>15</label>
                <mixed-citation publication-type="journal"><name><surname>Kim</surname><given-names>SJ</given-names></name>, <name><surname>Ogilvie</surname><given-names>M</given-names></name>, <name><surname>Shimabukuro</surname><given-names>N</given-names></name>, <name><surname>Stewart</surname><given-names>T</given-names></name>, <name><surname>Shin</surname><given-names>JH</given-names></name>. <article-title>Effects of Visual Feedback Distortion on Gait Adaptation: Comparison of Implicit Visual Distortion Versus Conscious Modulation on Retention of Motor Learning</article-title>. <source>IEEE Trans Biomed Eng</source>. <year>2015</year>; <volume>62</volume>(<issue>9</issue>): <fpage>2244</fpage>–<lpage>2250</lpage>. <pub-id pub-id-type="doi">10.1109/TBME.2015.2420851</pub-id>
<?supplied-pmid 25861084?><pub-id pub-id-type="pmid">25861084</pub-id></mixed-citation>
              </ref>
              <ref id="pone.0244583.ref016">
                <label>16</label>
                <mixed-citation publication-type="journal"><name><surname>Ranganathan</surname><given-names>R</given-names></name>, <name><surname>Newell</surname><given-names>KM</given-names></name>. <article-title>Influence of augmented feedback on coordination strategies</article-title>. <source>J Mot Behav</source>. <year>2009</year>; <volume>41</volume>(<issue>4</issue>): <fpage>317</fpage>–<lpage>330</lpage>. <pub-id pub-id-type="doi">10.3200/JMBR.41.4.317-330</pub-id>
<?supplied-pmid 19508958?><pub-id pub-id-type="pmid">19508958</pub-id></mixed-citation>
              </ref>
              <ref id="pone.0244583.ref017">
                <label>17</label>
                <mixed-citation publication-type="journal"><name><surname>Hasegawa</surname><given-names>N</given-names></name>, <name><surname>Takeda</surname><given-names>K</given-names></name>, <name><surname>Sakuma</surname><given-names>M</given-names></name>, <name><surname>Mani</surname><given-names>H</given-names></name>, <name><surname>Maejima</surname><given-names>H</given-names></name>, <name><surname>Asaka</surname><given-names>T</given-names></name>. <article-title>Learning effects of dynamic postural control by auditory biofeedback versus visual biofeedback training</article-title>. <source>Gait Posture</source>. <year>2017</year>; <volume>58</volume>: <fpage>188</fpage>–<lpage>193</lpage>. <pub-id pub-id-type="doi">10.1016/j.gaitpost.2017.08.001</pub-id>
<?supplied-pmid 28800501?><pub-id pub-id-type="pmid">28800501</pub-id></mixed-citation>
              </ref>
              <ref id="pone.0244583.ref018">
                <label>18</label>
                <mixed-citation publication-type="journal"><name><surname>Chiou</surname><given-names>SC</given-names></name>, <name><surname>Chang</surname><given-names>EC</given-names></name>. <article-title>Bimanual Coordination Learning with Different Augmented Feedback Modalities and Information Types</article-title>. <source>PLoS One</source>. <year>2016</year>; <volume>11</volume>(<issue>2</issue>): <fpage>e0149221</fpage><pub-id pub-id-type="doi">10.1371/journal.pone.0149221</pub-id><?supplied-pmid 26895286?><pub-id pub-id-type="pmid">26895286</pub-id></mixed-citation>
              </ref>
              <ref id="pone.0244583.ref019">
                <label>19</label>
                <mixed-citation publication-type="journal"><name><surname>Ronsse</surname><given-names>R</given-names></name>, <name><surname>Puttemans</surname><given-names>V</given-names></name>, <name><surname>Coxon</surname><given-names>JP</given-names></name>, <name><surname>Goble</surname><given-names>DJ</given-names></name>, <name><surname>Wagemans</surname><given-names>J</given-names></name>, <name><surname>Wenderoth</surname><given-names>N</given-names></name>, <etal>et al</etal><article-title>Motor learning with augmented feedback: modality-dependent behavioral and neural consequences</article-title>. <source>Cereb Cortex</source>. <year>2011</year>; <volume>21</volume>(<issue>6</issue>): <fpage>1283</fpage>–<lpage>1294</lpage>. <pub-id pub-id-type="doi">10.1093/cercor/bhq209</pub-id>
<?supplied-pmid 21030486?><pub-id pub-id-type="pmid">21030486</pub-id></mixed-citation>
              </ref>
              <ref id="pone.0244583.ref020">
                <label>20</label>
                <mixed-citation publication-type="journal"><name><surname>Nijhawan</surname><given-names>R.</given-names></name><article-title>Visual prediction: psychophysics and neurophysiology of compensation for time delays</article-title>. <source>Behav Brain Sci</source>. <year>2008</year>; <volume>31</volume>(<issue>2</issue>): <fpage>179</fpage>–<lpage>198</lpage>. <pub-id pub-id-type="doi">10.1017/S0140525X08003804</pub-id>
<?supplied-pmid 18479557?><pub-id pub-id-type="pmid">18479557</pub-id></mixed-citation>
              </ref>
              <ref id="pone.0244583.ref021">
                <label>21</label>
                <mixed-citation publication-type="journal"><name><surname>Holcombe</surname><given-names>AO</given-names></name>. <article-title>Seeing slow and seeing fast: two limits on perception.</article-title><source>Trends Cogn Sci</source>. <year>2009</year>; <volume>13</volume>(<issue>5</issue>): <fpage>216</fpage>–<lpage>221</lpage>. <pub-id pub-id-type="doi">10.1016/j.tics.2009.02.005</pub-id>
<?supplied-pmid 19386535?><pub-id pub-id-type="pmid">19386535</pub-id></mixed-citation>
              </ref>
              <ref id="pone.0244583.ref022">
                <label>22</label>
                <mixed-citation publication-type="journal"><name><surname>Hove</surname><given-names>MJ</given-names></name>, <name><surname>Fairhurst</surname><given-names>MT</given-names></name>, <name><surname>Kotz</surname><given-names>SA</given-names></name>, <name><surname>Keller</surname><given-names>PE</given-names></name>. <article-title>Synchronizing with auditory and visual rhythms: an fMRI assessment of modality differences and modality appropriateness</article-title>. <source>Neuroimage</source>. <year>2013</year>; <volume>67</volume>:<fpage>313</fpage>–<lpage>321</lpage>. <pub-id pub-id-type="doi">10.1016/j.neuroimage.2012.11.032</pub-id>
<?supplied-pmid 23207574?><pub-id pub-id-type="pmid">23207574</pub-id></mixed-citation>
              </ref>
              <ref id="pone.0244583.ref023">
                <label>23</label>
                <mixed-citation publication-type="journal"><name><surname>Chen</surname><given-names>HY</given-names></name>, <name><surname>Wing</surname><given-names>AM</given-names></name>. <article-title>Independent control of force and timing symmetry in dynamic standing balance: implications for rehabilitation of hemiparetic stroke patients</article-title>. <source>Hum Mov Sci</source>. <year>2012</year>; <volume>31</volume>(<issue>6</issue>): <fpage>1660</fpage>–<lpage>1669</lpage>. <pub-id pub-id-type="doi">10.1016/j.humov.2012.06.001</pub-id>
<?supplied-pmid 22939846?><pub-id pub-id-type="pmid">22939846</pub-id></mixed-citation>
              </ref>
              <ref id="pone.0244583.ref024">
                <label>24</label>
                <mixed-citation publication-type="journal"><name><surname>Radhakrishnan</surname><given-names>SM</given-names></name>, <name><surname>Hatzitaki</surname><given-names>V</given-names></name>, <name><surname>Vogiannou</surname><given-names>A</given-names></name>, <name><surname>Tzovaras</surname><given-names>D</given-names></name>. <article-title>The role of visual cues in the acquisition and transfer of a voluntary postural sway task</article-title>. <source>Gait Posture</source>. <year>2010</year>; <volume>32</volume>(<issue>4</issue>): <fpage>650</fpage>–<lpage>655</lpage>. <pub-id pub-id-type="doi">10.1016/j.gaitpost.2010.09.010</pub-id>
<?supplied-pmid 20934876?><pub-id pub-id-type="pmid">20934876</pub-id></mixed-citation>
              </ref>
              <ref id="pone.0244583.ref025">
                <label>25</label>
                <mixed-citation publication-type="journal"><name><surname>Stevens</surname><given-names>SS</given-names></name>. <article-title>On the psychophysical law</article-title>. <source>Psychol Rev</source>. <year>1957</year>; <volume>64</volume>(<issue>3</issue>): <fpage>153</fpage>–<lpage>181</lpage>. <pub-id pub-id-type="doi">10.1037/h0046162</pub-id>
<?supplied-pmid 13441853?><pub-id pub-id-type="pmid">13441853</pub-id></mixed-citation>
              </ref>
              <ref id="pone.0244583.ref026">
                <label>26</label>
                <mixed-citation publication-type="journal"><name><surname>Schmidt</surname><given-names>RC</given-names></name>, <name><surname>O’Brien</surname><given-names>B</given-names></name>. <article-title>Evaluating the Dynamics of Unintended Interpersonal Coordination</article-title>. <source>Ecol Psychol</source>. <year>1997</year>; <volume>9</volume>(<issue>3</issue>): <fpage>189</fpage>–<lpage>206</lpage>.</mixed-citation>
              </ref>
              <ref id="pone.0244583.ref027">
                <label>27</label>
                <mixed-citation publication-type="journal"><name><surname>Sotirakis</surname><given-names>H</given-names></name>, <name><surname>Hatzitaki</surname><given-names>V</given-names></name>, <name><surname>Munoz-Martel</surname><given-names>V</given-names></name>, <name><surname>Mademli</surname><given-names>L</given-names></name>, <name><surname>Arampatzis</surname><given-names>A</given-names></name>. <article-title>Center of Pressure Feedback Modulates the Entrainment of Voluntary Sway to the Motion of a Visual Target</article-title>. <source>Appl Sci</source>. <year>2019</year>; <volume>9</volume>(<issue>19</issue>): <fpage>3952</fpage><pub-id pub-id-type="doi">10.3390/app9193952</pub-id></mixed-citation>
              </ref>
              <ref id="pone.0244583.ref028">
                <label>28</label>
                <mixed-citation publication-type="journal"><name><surname>Halliday</surname><given-names>DM</given-names></name>, <name><surname>Rosenberg</surname><given-names>JR</given-names></name>, <name><surname>Amjad</surname><given-names>AM</given-names></name>, <name><surname>Breeze</surname><given-names>P</given-names></name>, <name><surname>Conway</surname><given-names>BA</given-names></name>, <name><surname>Farmer</surname><given-names>SF</given-names></name>. <article-title>A framework for the analysis of mixed time series/point process data—theory and application to the study of physiological tremor, single motor unit discharges and electromyograms</article-title>. <source>Prog Biophys Mol Biol</source>. <year>1995</year>; <volume>64</volume>(<issue>2–3</issue>): <fpage>237</fpage>–<lpage>278</lpage>. <pub-id pub-id-type="doi">10.1016/s0079-6107(96)00009-0</pub-id>
<?supplied-pmid 8987386?><pub-id pub-id-type="pmid">8987386</pub-id></mixed-citation>
              </ref>
              <ref id="pone.0244583.ref029">
                <label>29</label>
                <mixed-citation publication-type="journal"><name><surname>Foxe</surname><given-names>JJ</given-names></name>. <article-title>Multisensory integration: frequency tuning of audio-tactile integration</article-title>. <source>Curr Biol</source>. <year>2009</year>; <volume>19</volume>(<issue>9</issue>): <fpage>R373</fpage>–<lpage>375</lpage>. <pub-id pub-id-type="doi">10.1016/j.cub.2009.03.029</pub-id>
<?supplied-pmid 19439261?><pub-id pub-id-type="pmid">19439261</pub-id></mixed-citation>
              </ref>
              <ref id="pone.0244583.ref030">
                <label>30</label>
                <mixed-citation publication-type="journal"><name><surname>Ghai</surname><given-names>S</given-names></name>, <name><surname>Schmitz</surname><given-names>G</given-names></name>, <name><surname>Hwang</surname><given-names>TH</given-names></name>, <name><surname>Effenberg</surname><given-names>AO</given-names></name>. <article-title>Auditory Proprioceptive Integration: Effects of Real-Time Kinematic Auditory Feedback on Knee Proprioception</article-title>. <source>Front Neurosci</source>. <year>2018</year>; <volume>12</volume>: <fpage>142</fpage><pub-id pub-id-type="doi">10.3389/fnins.2018.00142</pub-id><?supplied-pmid 29568259?><pub-id pub-id-type="pmid">29568259</pub-id></mixed-citation>
              </ref>
              <ref id="pone.0244583.ref031">
                <label>31</label>
                <mixed-citation publication-type="journal"><name><surname>Ghai</surname><given-names>S</given-names></name>, <name><surname>Schmitz</surname><given-names>G</given-names></name>, <name><surname>Hwang</surname><given-names>TH</given-names></name>, <name><surname>Effenberg</surname><given-names>AO</given-names></name>. <article-title>Training proprioception with sound: effects of real-time auditory feedback on intermodal learning</article-title>. <source>Ann N Y Acad Sci</source>. <year>2019</year>; <volume>1438</volume>(<issue>1</issue>): <fpage>50</fpage>–<lpage>61</lpage>. <pub-id pub-id-type="doi">10.1111/nyas.13967</pub-id>
<?supplied-pmid 30221775?><pub-id pub-id-type="pmid">30221775</pub-id></mixed-citation>
              </ref>
              <ref id="pone.0244583.ref032">
                <label>32</label>
                <mixed-citation publication-type="journal"><name><surname>Schmitz</surname><given-names>G</given-names></name>, <name><surname>Mohammadi</surname><given-names>B</given-names></name>, <name><surname>Hammer</surname><given-names>A</given-names></name>, <name><surname>Heldmann</surname><given-names>M</given-names></name>, <name><surname>Samii</surname><given-names>A</given-names></name>, <name><surname>Münte</surname><given-names>TF</given-names></name>, <etal>et al</etal><article-title>Observation of sonified movements engages a basal ganglia frontocortical network</article-title>. <source>BMC Neurosci</source>. <year>2013</year>; <volume>14</volume>: <fpage>32</fpage><pub-id pub-id-type="doi">10.1186/1471-2202-14-32</pub-id><?supplied-pmid 23496827?><pub-id pub-id-type="pmid">23496827</pub-id></mixed-citation>
              </ref>
              <ref id="pone.0244583.ref033">
                <label>33</label>
                <mixed-citation publication-type="journal"><name><surname>Witt</surname><given-names>ST</given-names></name>, <name><surname>Laird</surname><given-names>AR</given-names></name>, <name><surname>Meyerand</surname><given-names>ME</given-names></name>. <article-title>Functional neuroimaging correlates of finger-tapping task variations: an ALE meta-analysis</article-title>. <source>Neuroimage</source>. <year>2008</year>; <volume>42</volume>(<issue>1</issue>): <fpage>343</fpage>–<lpage>356</lpage>. <pub-id pub-id-type="doi">10.1016/j.neuroimage.2008.04.025</pub-id>
<?supplied-pmid 18511305?><pub-id pub-id-type="pmid">18511305</pub-id></mixed-citation>
              </ref>
              <ref id="pone.0244583.ref034">
                <label>34</label>
                <mixed-citation publication-type="journal"><name><surname>Arrighi</surname><given-names>R</given-names></name>, <name><surname>Alais</surname><given-names>D</given-names></name>, <name><surname>Burr</surname><given-names>D</given-names></name>. <article-title>Perceptual synchrony of audiovisual streams for natural and artificial motion sequences</article-title>. <source>J Vis</source>. <year>2006</year>; <volume>6</volume>(<issue>3</issue>): <fpage>260</fpage>–<lpage>268</lpage>. <pub-id pub-id-type="doi">10.1167/6.3.6</pub-id>
<?supplied-pmid 16643094?><pub-id pub-id-type="pmid">16643094</pub-id></mixed-citation>
              </ref>
              <ref id="pone.0244583.ref035">
                <label>35</label>
                <mixed-citation publication-type="journal"><name><surname>Posner</surname><given-names>MI</given-names></name>, <name><surname>Nissen</surname><given-names>MJ</given-names></name>, <name><surname>Klein</surname><given-names>RM</given-names></name>. <article-title>Visual dominance: an information-processing account of its origins and significance</article-title>. <source>Psychol Rev</source>. <year>1976</year>; <volume>83</volume>(<issue>2</issue>): <fpage>157</fpage>–<lpage>171</lpage>. <?supplied-pmid 769017?><pub-id pub-id-type="pmid">769017</pub-id></mixed-citation>
              </ref>
              <ref id="pone.0244583.ref036">
                <label>36</label>
                <mixed-citation publication-type="journal"><name><surname>Radhakrishnan</surname><given-names>SM</given-names></name>, <name><surname>Hatzitaki</surname><given-names>V</given-names></name>, <name><surname>Patikas</surname><given-names>D</given-names></name>, <name><surname>Amiridis</surname><given-names>IG</given-names></name>. <article-title>Responses to Achilles tendon vibration during self-paced, visually and auditory-guided periodic sway</article-title>. <source>Exp Brain Res</source>. <year>2011</year>; <volume>213</volume>(<issue>4</issue>): <fpage>423</fpage>–<lpage>433</lpage>. <pub-id pub-id-type="doi">10.1007/s00221-011-2792-y</pub-id>
<?supplied-pmid 21748332?><pub-id pub-id-type="pmid">21748332</pub-id></mixed-citation>
              </ref>
            </ref-list>
          </back>
          <sub-article id="pone.0244583.r001" article-type="aggregated-review-documents">
            <front-stub>
              <article-id pub-id-type="doi">10.1371/journal.pone.0244583.r001</article-id>
              <title-group>
                <article-title>Decision Letter 0</article-title>
              </title-group>
              <contrib-group>
                <contrib contrib-type="author">
                  <name>
                    <surname>Barbieri</surname>
                    <given-names>Fabio A.</given-names>
                  </name>
                  <role>Academic Editor</role>
                </contrib>
              </contrib-group>
              <permissions>
                <copyright-statement>© 2020 Fabio A. Barbieri</copyright-statement>
                <copyright-year>2020</copyright-year>
                <copyright-holder>Fabio A. Barbieri</copyright-holder>
                <license xlink:href="http://creativecommons.org/licenses/by/4.0/">
                  <license-p>This is an open access article distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution License</ext-link>, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited.</license-p>
                </license>
              </permissions>
              <related-article id="rel-obj001" ext-link-type="doi" xlink:href="10.1371/journal.pone.0244583" related-article-type="reviewed-article"/>
              <custom-meta-group>
                <custom-meta>
                  <meta-name>Submission Version</meta-name>
                  <meta-value>0</meta-value>
                </custom-meta>
              </custom-meta-group>
            </front-stub>
            <body>
              <p>
                <named-content content-type="letter-date">17 Aug 2020</named-content>
              </p>
              <p>PONE-D-20-14779</p>
              <p>Differential effects of visual versus auditory biofeedback training for postural control</p>
              <p>PLOS ONE</p>
              <p>Dear Dr. Asaka,</p>
              <p>Thank you for submitting your manuscript to PLOS ONE. After careful consideration, we feel that it has merit but does not fully meet PLOS ONE’s publication criteria as it currently stands. Therefore, we invite you to submit a revised version of the manuscript that addresses the points raised during the review process.</p>
              <p>==============================</p>
              <p>Dear authors,</p>
              <p>Despite one reviewer suggested acceptation, there are important concerns indicated by second reviewer. The authors need to improve introduction rationality and results presentation (improve the writing and sequence). Both aspects are important to consider the manuscript for publication. </p>
              <p>==============================</p>
              <p>Please submit your revised manuscript by Oct 01 2020 11:59PM. If you will need more time than this to complete your revisions, please reply to this message or contact the journal office at <email>plosone@plos.org</email>. When you're ready to submit your revision, log on to <ext-link ext-link-type="uri" xlink:href="https://www.editorialmanager.com/pone/">https://www.editorialmanager.com/pone/</ext-link> and select the 'Submissions Needing Revision' folder to locate your manuscript file.</p>
              <p>Please include the following items when submitting your revised manuscript:</p>
              <p>
                <list list-type="bullet">
                  <list-item>
                    <p>A rebuttal letter that responds to each point raised by the academic editor and reviewer(s). You should upload this letter as a separate file labeled 'Response to Reviewers'.</p>
                  </list-item>
                  <list-item>
                    <p>A marked-up copy of your manuscript that highlights changes made to the original version. You should upload this as a separate file labeled 'Revised Manuscript with Track Changes'.</p>
                  </list-item>
                  <list-item>
                    <p>An unmarked version of your revised paper without tracked changes. You should upload this as a separate file labeled 'Manuscript'.</p>
                  </list-item>
                </list>
              </p>
              <p>If you would like to make changes to your financial disclosure, please include your updated statement in your cover letter. Guidelines for resubmitting your figure files are available below the reviewer comments at the end of this letter.</p>
              <p>If applicable, we recommend that you deposit your laboratory protocols in protocols.io to enhance the reproducibility of your results. Protocols.io assigns your protocol its own identifier (DOI) so that it can be cited independently in the future. For instructions see: <ext-link ext-link-type="uri" xlink:href="http://journals.plos.org/plosone/s/submission-guidelines#loc-laboratory-protocols">http://journals.plos.org/plosone/s/submission-guidelines#loc-laboratory-protocols</ext-link></p>
              <p>We look forward to receiving your revised manuscript.</p>
              <p>Kind regards,</p>
              <p>Fabio A. Barbieri, PhD</p>
              <p>Academic Editor</p>
              <p>PLOS ONE</p>
              <p>Journal Requirements:</p>
              <p>When submitting your revision, we need you to address these additional requirements.</p>
              <p>1. Please ensure that your manuscript meets PLOS ONE's style requirements, including those for file naming. The PLOS ONE style templates can be found at</p>
              <p><ext-link ext-link-type="uri" xlink:href="https://journals.plos.org/plosone/s/file?id=wjVg/PLOSOne_formatting_sample_main_body.pdf">https://journals.plos.org/plosone/s/file?id=wjVg/PLOSOne_formatting_sample_main_body.pdf</ext-link> and</p>
              <p>
                <ext-link ext-link-type="uri" xlink:href="https://journals.plos.org/plosone/s/file?id=ba62/PLOSOne_formatting_sample_title_authors_affiliations.pdf">https://journals.plos.org/plosone/s/file?id=ba62/PLOSOne_formatting_sample_title_authors_affiliations.pdf</ext-link>
              </p>
              <p>[Note: HTML markup is below. Please do not edit.]</p>
              <p>Reviewers' comments:</p>
              <p>Reviewer's Responses to Questions</p>
              <p>
                <bold>Comments to the Author</bold>
              </p>
              <p>1. Is the manuscript technically sound, and do the data support the conclusions?</p>
              <p>The manuscript must describe a technically sound piece of scientific research with data that supports the conclusions. Experiments must have been conducted rigorously, with appropriate controls, replication, and sample sizes. The conclusions must be drawn appropriately based on the data presented. </p>
              <p>Reviewer #1: Yes</p>
              <p>Reviewer #2: Partly</p>
              <p>**********</p>
              <p>2. Has the statistical analysis been performed appropriately and rigorously? </p>
              <p>Reviewer #1: Yes</p>
              <p>Reviewer #2: Yes</p>
              <p>**********</p>
              <p>3. Have the authors made all data underlying the findings in their manuscript fully available?</p>
              <p>The <ext-link ext-link-type="uri" xlink:href="http://www.plosone.org/static/policies.action#sharing">PLOS Data policy</ext-link> requires authors to make all data underlying the findings described in their manuscript fully available without restriction, with rare exception (please refer to the Data Availability Statement in the manuscript PDF file). The data should be provided as part of the manuscript or its supporting information, or deposited to a public repository. For example, in addition to summary statistics, the data points behind means, medians and variance measures should be available. If there are restrictions on publicly sharing data—e.g. participant privacy or use of data from a third party—those must be specified.</p>
              <p>Reviewer #1: No</p>
              <p>Reviewer #2: Yes</p>
              <p>**********</p>
              <p>4. Is the manuscript presented in an intelligible fashion and written in standard English?</p>
              <p>PLOS ONE does not copyedit accepted manuscripts, so the language in submitted articles must be clear, correct, and unambiguous. Any typographical or grammatical errors should be corrected at revision, so please note any specific errors here.</p>
              <p>Reviewer #1: Yes</p>
              <p>Reviewer #2: Yes</p>
              <p>**********</p>
              <p>5. Review Comments to the Author</p>
              <p>Please use the space provided to explain your answers to the questions above. You may also include additional comments for the author, including concerns about dual publication, research ethics, or publication ethics. (Please upload your review as an attachment if it exceeds 20,000 characters)</p>
              <p>Reviewer #1: The goal of this study was to determine if discrete auditory feedback during a body sway targeting task improved performance more than discrete visual feedback. The study was well designed, the methods were appropriate, and the manuscript is well written. The strengths of the paper include (1) random assignment of participants to two groups, and (2) the use of both spatial and temporal measures to assess performance. There are a few minor issues and one curiosity (#5) listed below. I would like to commend the authors for this work – it was a please to review this article. Overall, this manuscript is relevant and will be of interest to a wide range of researchers and clinicians.</p>
              <p>Minor Issues:</p>
              <p>1. Avoid acronyms, especially in the introduction/discussion – using the word feedback rather than BF doesn’t take much more space.</p>
              <p>2. Line 373 – The results of the study are not simply ‘suggestive’, given the randomized trial, it was demonstrated that auditory feedback was more effective than visual feedback for the swaying task.</p>
              <p>3. I recommend the authors reconsider the text on lines 315-317. Being the first to conduct an experiment is not relevant; adding knowledge to the field is relevant. If the authors choose to keep the wording about being first, I recommend they state they were the first to demonstrate that discrete auditory feedback was more effective at improving performance and learning (assuming no one else has demonstrated this).</p>
              <p>4. Line 81 – extra ‘e’</p>
              <p>5. This is a question I have, which may or may not be relevant for the text. Is there any indication that visual or auditory feedback have different cognitive demands? I would predict that, at least initially, auditory feedback would be more challenging due to the transformation of volume and pitch into sway magnitude and direction, whereas visual feedback does not (apparently) need to be transformed. Perhaps this distinction is related to improved learning – the auditory may be more attention-demanding, which may promote learning.</p>
              <p>Reviewer #2: The present manuscript investigated the effectiveness of discrete visual versus auditory biofeedback (BF) to improve a postural tracking task. Twenty-two young participants were assigned to either a visual or auditory BF group. Participants were asked to shift their center of pressure (CoP) by voluntarily swaying forward and backward following a hidden, moving in a sinusoidal fashion and displayed intermittently. Results showed that, according to the authors, auditory BF was better than visual BF improving spatial and temporal relationship between CoP and target positioning. Based upon these results it was concluded that motor learning of postural control was improved by discrete auditory BF training.</p>
              <p>Overall, the manuscript focuses on an important issue, which is related to improvement of postural positioning in a tracking task under different sensory cues displayed intermittently. The design and procedures seem to be sound, although a few and important issues still need to be clarified. Also the results and interpretation need to further explained. As a consequence, there a few issues that, as reviewer who is reading the manuscript for the first time, I would like to point out.</p>
              <p>The first issue that needs to be revised and/or further clarified in the manuscript is the idea of “training for postural control”. Such usage does not reflect the essence of what is training. In my opinion, the training focuses on one particular behavior of tracking, visual or auditory, intermittent target. The movement involving the whole body is nominated, correctly, as postural control, but there is no measure or intention (at least from my view) of measuring postural control performance other than following the target. Using the broader scope such as training for postural control might furnish an equivocal idea that the training is towards improvement of postural control as a daily use task. Certainty, this is not the case and the manipulation and results cannot be used for such usage. Thus, there is the need to clarify and better refer, throughout the manuscript and including the title, to the task involved in the study rather than generalize as “postural control”.</p>
              <p>Second, introduction needs to be improved in order to justify the rationality of the study. Why training effects would be potentially different between visual and auditory BF? Such clarification seems to be important even for preparing the reader for the proposed hypothesis. I am still wondering why was hypothesized that intermittent auditory BF would lead to learning but not visual BF. Yet, such learning effect would occur only in the temporal domain. Please further develop rationality for each of these issues even to support any discussion regarding the observed results.</p>
              <p>Procedures seem to be sound, but there is the need to further describe the conditions and instructions for the participants. Regarding the conditions, when the stimulus was auditory, participants had visual cues available? When participants were performing the visual training, auditory cues from the environment were available. A detailed description regarding the available cues is critical in order to further understand and discuss possible differences between sensory cues.</p>
              <p>Results are hard to follow, mainly for a couple of reasons. First, please refer in the text, because present the results where the reader can find the data. Refer to the Figure that the reader could see the results. Second, please clarify the statistical notation. It seems that statistical notation for auditory results are presented in the superior portion of the plot and for vision in the inferior part of it (I am not sure about this). For example, Figure 5B visual was not significant, but the note is nearby the symbol. On the other hand, auditory BF was significant and there is the notation indicating “NS”.</p>
              <p>Finally, I do not agree with the interpretation and conclusion that only auditory BF improved postural tracking. In this case, there are a few aspects that must be clarified. My first concern is regarding the use and interpretation of the variables employed in the study. From my understanding, the most important variable indicating if the task was accomplished is the average and SD of distance (even mentioned – line 193-194). The mean peak difference is the error at the time interval of BF. Thus, data show that both sensory cues were used to improve the tracking, but more erratically in the visual condition. Such difference is even noticed in the coherence values (lower values for the visual – moving the body less coherent with the stimulus) and in the phase (higher values for the visual – moving lagging behind the stimulus). These are the different strategies that participants adopted in using different sensory cues. The question is why participants adopt these different strategies? Is the visual processing slower than the auditory? Is visual used more likely a confirmatory cue? In my point of view, the manuscript should discuss and clarify these issues, but not saying the visual training did not improve postural tracking because it did (at least from what I could get from Figure 4). Finally, I did not understand the usage of the correlation analysis. Yet, the number of participants and the obtained results do not allow for a clear cut interpretation of this possible relationship. Why does use it?</p>
              <p>Minor issue:</p>
              <p>- Please revise abstract, reducing its size and presenting results and conclusion properly</p>
              <p>- Line 59-60: statement here contradicts the following sentence (lines 61-63). Please revise.</p>
              <p>- Hypothesis must be justified.</p>
              <p>- Please reference Figures in the text, before presenting results.</p>
              <p>- Figure 1 is hard to follow with those boxes “WithBF” and “WithoutBF”.</p>
              <p>- Please revise statistical notation in the Figures</p>
              <p>**********</p>
              <p>6. PLOS authors have the option to publish the peer review history of their article (<ext-link ext-link-type="uri" xlink:href="https://journals.plos.org/plosone/s/editorial-and-peer-review-process#loc-peer-review-history">what does this mean?</ext-link>). If published, this will include your full peer review and any attached files.</p>
              <p>If you choose “no”, your identity will remain anonymous but your review may still be made public.</p>
              <p><bold>Do you want your identity to be public for this peer review?</bold> For information about this choice, including consent withdrawal, please see our <ext-link ext-link-type="uri" xlink:href="https://www.plos.org/privacy-policy">Privacy Policy</ext-link>.</p>
              <p>Reviewer #1: No</p>
              <p>Reviewer #2: No</p>
              <p>[NOTE: If reviewer comments were submitted as an attachment file, they will be attached to this email and accessible via the submission site. Please log into your account, locate the manuscript record, and check for the action link "View Attachments". If this link does not appear, there are no attachment files.]</p>
              <p>While revising your submission, please upload your figure files to the Preflight Analysis and Conversion Engine (PACE) digital diagnostic tool, <ext-link ext-link-type="uri" xlink:href="https://pacev2.apexcovantage.com/">https://pacev2.apexcovantage.com/</ext-link>. PACE helps ensure that figures meet PLOS requirements. To use PACE, you must first register as a user. Registration is free. Then, login and navigate to the UPLOAD tab, where you will find detailed instructions on how to use the tool. If you encounter any issues or have any questions when using PACE, please email PLOS at <email>figures@plos.org</email>. Please note that Supporting Information files do not need this step.</p>
            </body>
          </sub-article>
          <sub-article id="pone.0244583.r002" article-type="author-comment">
            <front-stub>
              <article-id pub-id-type="doi">10.1371/journal.pone.0244583.r002</article-id>
              <title-group>
                <article-title>Author response to Decision Letter 0</article-title>
              </title-group>
              <related-article id="rel-obj002" ext-link-type="doi" xlink:href="10.1371/journal.pone.0244583" related-article-type="editor-report"/>
              <custom-meta-group>
                <custom-meta>
                  <meta-name>Submission Version</meta-name>
                  <meta-value>1</meta-value>
                </custom-meta>
              </custom-meta-group>
            </front-stub>
            <body>
              <p>
                <named-content content-type="author-response-date">17 Sep 2020</named-content>
              </p>
              <p>Dear Reviewers and Editor,</p>
              <p>We deeply thank the reviewers for their time and effort in peer-reviewing this manuscript. Your comments have been very useful and have helped to improve the manuscript. Below you can find a point-by-point response to each comment. All changes in the manuscript have been highlighted in yellow.</p>
              <p>Reviewer #1</p>
              <p>The goal of this study was to determine if discrete auditory feedback during a body sway targeting task improved performance more than discrete visual feedback. The study was well designed, the methods were appropriate, and the manuscript is well written. The strengths of the paper include (1) random assignment of participants to two groups, and (2) the use of both spatial and temporal measures to assess performance. There are a few minor issues and one curiosity (#5) listed below. I would like to commend the authors for this work – it was a please to review this article. Overall, this manuscript is relevant and will be of interest to a wide range of researchers and clinicians.</p>
              <p>Minor Issues:</p>
              <p>Point 1: Avoid acronyms, especially in the introduction/discussion – using the word feedback rather than BF doesn’t take much more space.</p>
              <p>Response 1: We changed BF to biofeedback in the whole manuscript, except for figures and tables.</p>
              <p>Point 2: Line 373 – The results of the study are not simply</p>
              <p>‘suggestive’, given the randomized trial, it was demonstrated that auditory feedback was more effective than visual feedback for the swaying task.</p>
              <p>Response 2: Thank you, we have modified the sentence in the conclusion.</p>
              <p>“This randomized trial demonstrated that discrete auditory biofeedback training was more effective than discrete visual biofeedback training for the motor learning of the voluntary postural sway task.”(Page 24, Line 436)</p>
              <p>Point 3: I recommend the authors reconsider the text on lines 315-317. Being the first to conduct an experiment is not relevant; adding knowledge to the field is relevant. If the authors choose to keep the wording about being first, I recommend they state they were the first to demonstrate that discrete auditory feedback was more effective at improving performance and learning (assuming no one else has demonstrated this).</p>
              <p>Response 3: Thank you for the suggestion. The first paragraph of the discussion leads as follow: </p>
              <p>“Our findings reveal that discrete auditory biofeedback was more effective than discrete visual biofeedback for motor learning of voluntary postural sway (even after equalizing the perceptual magnitude of each type of biofeedback)” (Page 20, Line 352).</p>
              <p>Point 4: Line 81 – extra ‘e’</p>
              <p>Response 4: We apologize for the typo. The extra ‘e’ was removed.</p>
              <p>Point 5: This is a question I have, which may or may not be relevant for the text. Is there any indication that visual or auditory feedback have different cognitive demands? I would predict that, at least initially, auditory feedback would be more challenging due to the transformation of volume and pitch into sway magnitude and direction, whereas visual feedback does not (apparently) need to be transformed. Perhaps this distinction is related to improved learning – the auditory may be more attention-demanding, which may promote learning.</p>
              <p>Response 5: The reviewer brought up a good point. Previous studies have suggested that auditory feedback training requires more cognitive involvement than visual feedback training on upper limb task. However, no studies investigated on balance tasks. We have added the following sentences in the discussion: </p>
              <p>Page 21, Line 383: Previous studies showed a stronger cognitive involvement, represented by increased brain activation of prefrontal areas [29] and putamen [36], in performing a sensory-motor task when using auditory biofeedback compared to visual biofeedback. More cognitive involvement may enhance attention to intrinsic sensory information, especially proprioceptive information, and that may explain why the postural performance with auditory biofeedback was better than performance with visual biofeedback, even after the biofeedback was removed.</p>
              <p>Reviewer #2</p>
              <p>The present manuscript investigated the effectiveness of discrete visual versus auditory biofeedback (BF) to improve a postural tracking task. Twenty-two young participants were assigned to either a visual or auditory BF group. Participants were asked to shift their center of pressure (CoP) by voluntarily swaying forward and backward following a hidden, moving in a sinusoidal fashion and displayed intermittently. Results showed that, according to the authors, auditory BF was better than visual BF improving spatial and temporal relationship between CoP and target positioning. Based upon these results it was concluded that motor learning of postural control was improved by discrete auditory BF training.</p>
              <p>Overall, the manuscript focuses on an important issue, which is related to improvement of postural positioning in a tracking task under different sensory cues displayed intermittently. The design and procedures seem to be sound, although a few and important issues still need to be clarified. Also the results and interpretation need to further explained. As a consequence, there a few issues that, as reviewer who is reading the manuscript for the first time, I would like to point out.</p>
              <p>Point 1: The first issue that needs to be revised and/or further clarified in the manuscript is the idea of “training for postural control”. Such usage does not reflect the essence of what is training. In my opinion, the training focuses on one particular behavior of tracking, visual or auditory, intermittent target. The movement involving the whole body is nominated, correctly, as postural control, but there is no measure or intention (at least from my view) of measuring postural control performance other than following the target. Using the broader scope such as training for postural control might furnish an equivocal idea that the training is towards improvement of postural control as a daily use task. Certainty, this is not the case and the manipulation and results cannot be used for such usage. Thus, there is the need to clarify and better refer, throughout the manuscript and including the title, to the task involved in the study rather than generalize as “postural control”.</p>
              <p>Response 1: We deeply appreciate the reviewer’s valuable comment. We changed “postural control” to “voluntary postural sway”.</p>
              <p>Point 2: Second, introduction needs to be improved in order to justify the rationality of the study. Why training effects would be potentially different between visual and auditory BF? Such clarification seems to be important even for preparing the reader for the proposed hypothesis. I am still wondering why was hypothesized that intermittent auditory BF would lead to learning but not visual BF. Yet, such learning effect would occur only in the temporal domain. Please further develop rationality for each of these issues even to support any discussion regarding the observed results.</p>
              <p>Response 2: Thank you for your important comments. We revised the last paragraph of the introduction.</p>
              <p>Page 5, Line 87: The goal of this study was to investigate the learning effects of discrete auditory versus visual biofeedback to improve postural control, using a voluntary postural sway task [17]. A previous study using functional magnetic resonance imaging showed that brain activation increased in sensory-specific areas during visual biofeedback training. In contrast, brain activation gradually decreased over time with auditory biofeedback training [19]. These findings suggest that auditory biofeedback training may suppress reliance on augmented biofeedback during training unlike visual biofeedback training which requires sustained dependence on vision. Moreover, previous studies showed that auditory inputs are processed more quickly, shorter reaction times, compared to visual inputs for motor response [20-22]. Thus, auditory biofeedback would result in faster, more accurate influence on the temporal domain of postural control compared to visual biofeedback. Therefore, we hypothesized that discrete auditory biofeedback training would result in better learning effects than visual biofeedback, especially in the temporal domain for control of voluntary postural sway.</p>
              <p>Point 3: Procedures seem to be sound, but there is the need to further describe the conditions and instructions for the participants. Regarding the conditions, when the stimulus was auditory, participants had visual cues available? When participants were performing the visual training, auditory cues from the environment were available. A detailed description regarding the available cues is critical in order to further understand and discuss possible differences between sensory cues.</p>
              <p>Response 3: We apologize for the insufficient description for the conditions and instructions. We added the following to the methods. </p>
              <p>Page 11, Line 208: When the biofeedback was auditory, visual environmental cues were available and when the biofeedback was visual, auditory environmental cues were available.</p>
              <p>Page 7, Line 128: After measuring the stability limits, participants were asked to perform the test and training sessions with the same stance and position of arms while maintaining attention on the monitor.</p>
              <p>Point 4: Results are hard to follow, mainly for a couple of reasons. First, please refer in the text, because present the results where the reader can find the data. Refer to the Figure that the reader could see the results. Second, please clarify the statistical notation. It seems that statistical notation for auditory results are presented in the superior portion of the plot and for vision in the inferior part of it (I am not sure about this). For example, Figure 5B visual was not significant, but the note is nearby the symbol. On the other hand, auditory BF was significant and there is the notation indicating “NS”.</p>
              <p>Response 4: We now describe each figure in the text. Also, we simplified Figure 4 and Figure 5 to be easier to understand. In addition, we added a table (Table 2) in the results section to explain the results from the two-way mixed-design ANOVA clearly.</p>
              <p>Point 5: Finally, I do not agree with the interpretation and conclusion that only auditory BF improved postural tracking. In this case, there are a few aspects that must be clarified. My first concern is regarding the use and interpretation of the variables employed in the study. From my understanding, the most important variable indicating if the task was accomplished is the average and SD of distance (even mentioned – line 193-194). The mean peak difference is the error at the time interval of BF. Thus, data show that both sensory cues were used to improve the tracking, but more erratically in the visual condition. Such difference is even noticed in the coherence values (lower values for the visual – moving the body less coherent with the stimulus) and in the phase (higher values for the visual – moving lagging behind the stimulus). These are the different strategies that participants adopted in using different sensory cues. The question is why participants adopt these different strategies? Is the visual processing slower than the auditory? Is visual used more likely a confirmatory cue? In my point of view, the manuscript should discuss and clarify these issues, but not saying the visual training did not improve postural tracking because it did (at least from what I could get from Figure 4). </p>
              <p>Response 5: Thank you for your thoughtful comments. We don’t seem that the visual biofeedback was more confirmative compared to auditory biofeedback in this study. This is because we tried to equalize the perceptual magnitude of both visual and auditory biofeedback using Stevens’ power law (Page 11, Line 203). However, previous studies showed the different cognitive challenges and temporal resolutions between visual and auditory biofeedback, which could lead to different strategies. </p>
              <p>As stated above, we added the impact of different cognitive challenges and temporal resolutions between auditory and visual biofeedback as follows: </p>
              <p>Page 21, Line 383: Previous studies showed a stronger cognitive involvement, represented by increased brain activation of prefrontal areas [19] and putamen [33], in performing a sensory-motor task when using auditory biofeedback compared to visual biofeedback. More cognitive involvement may enhance attention to intrinsic sensory information, especially proprioceptive information, and that may explain why the postural performance with auditory biofeedback was better than performance with visual biofeedback, even after the biofeedback was removed. Another explanation for the different learning strategies may be the different temporal accuracy between auditory and visual biofeedback. The stimulus-response reaction times for visual inputs are tens to one hundred milliseconds slower than that for auditory inputs [20,34]. Therefore, auditory biofeedback has an advantage in temporal resolution compared to visual biofeedback, which provides more temporal accuracy and reduced spatial error for auditory, than visual, biofeedback training. The slower visual processing results in delayed postural motor responses as apparent in the coherence values (lower values for the visual – moving the body less coherent with the stimulus) and in the phase (higher values for the visual – moving lagging behind the stimulus) (Fig .5).</p>
              <p>Point 6: Finally, I did not understand the usage of the correlation analysis. Yet, the number of participants and the obtained results do not allow for a clear cut interpretation of this possible relationship. Why does use it?</p>
              <p>Response 6: We tried to demonstrate that the visual biofeedback improved the performance using spontaneous learning effects, not via visual biofeedback. In fact, results suggest that the improved spatial error was mainly reached by the improved spatial error of the area without biofeedback in the training session (white area in Fig. 2). </p>
              <p>As stated above, we revised the sentences in the discussion as follows: </p>
              <p>Page 23, Line 413: In fact, the reduced mean peak difference was significantly associated with improvements of postural control in the spatial domain (Dmean) in the auditory biofeedback group only. Dmean indicates the average spatial error for one trial, which consists of the area with and without augmented sensory biofeedback in the training session. Therefore, the significant correlation between reduced mean peak difference and improvements of postural control in the spatial domain result suggests that enhanced accuracy when using auditory biofeedback is responsible for the reduced spatial error under the no-feedback condition. On the other hand, no significant correlation between the improvements in mean peak difference and Dmean was found in the discrete visual biofeedback training. This finding suggests that reduced the whole spatial error in the discrete visual biofeedback group may be mainly caused by reduced spatial error of the area without augmented sensory biofeedback in the training session. In other words, the discrete visual biofeedback improves voluntary postural sway performance in the spatial domain mainly using spontaneous motor learning, not based on enhanced sensory information.</p>
              <p>Minor issue:</p>
              <p>Point 7: Please revise abstract, reducing its size and presenting results and conclusion properly</p>
              <p>Response 7: Thank you. We revised the abstract as your suggestion.</p>
              <p>Point 8: Line 59-60: statement here contradicts the following sentence (lines 61-63). Please revise.</p>
              <p>Response 8: We apologize for the contradiction. We re-wrote the phrase as follows: </p>
              <p>Page 4, Line 63: Although a few studies have reported the effects of visual or auditory biofeedback training on postural control, in our knowledge, only our previous study reported that one modality was better than the other by direct comparison.</p>
              <p>Point 9: Hypothesis must be justified.</p>
              <p>Response 9: We revised the last paragraph of the introduction as Response 2. </p>
              <p>Point 10: Please reference Figures in the text, before presenting results.</p>
              <p>Response 10: We referred each figure in the text. </p>
              <p>Point 11: Figure 1 is hard to follow with those boxes “WithBF” and “WithoutBF”.</p>
              <p>Response 11: We modified Figure 1. </p>
              <p>Point 12: Please revise statistical notation in the Figures</p>
              <p>Response 12: Thank you, we modified Figure 4 and Figure 5 as Response 4.</p>
              <supplementary-material content-type="local-data" id="pone.0244583.s001">
                <label>Attachment</label>
                <caption>
                  <p>Submitted filename: <named-content content-type="submitted-filename">Response_to_Reviewers_v2.docx</named-content></p>
                </caption>
                <media xlink:href="pone.0244583.s001.docx">
                  <caption>
                    <p>Click here for additional data file.</p>
                  </caption>
                </media>
              </supplementary-material>
            </body>
          </sub-article>
          <sub-article id="pone.0244583.r003" article-type="aggregated-review-documents">
            <front-stub>
              <article-id pub-id-type="doi">10.1371/journal.pone.0244583.r003</article-id>
              <title-group>
                <article-title>Decision Letter 1</article-title>
              </title-group>
              <contrib-group>
                <contrib contrib-type="author">
                  <name>
                    <surname>Barbieri</surname>
                    <given-names>Fabio A.</given-names>
                  </name>
                  <role>Academic Editor</role>
                </contrib>
              </contrib-group>
              <permissions>
                <copyright-statement>© 2020 Fabio A. Barbieri</copyright-statement>
                <copyright-year>2020</copyright-year>
                <copyright-holder>Fabio A. Barbieri</copyright-holder>
                <license xlink:href="http://creativecommons.org/licenses/by/4.0/">
                  <license-p>This is an open access article distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution License</ext-link>, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited.</license-p>
                </license>
              </permissions>
              <related-article id="rel-obj003" ext-link-type="doi" xlink:href="10.1371/journal.pone.0244583" related-article-type="reviewed-article"/>
              <custom-meta-group>
                <custom-meta>
                  <meta-name>Submission Version</meta-name>
                  <meta-value>1</meta-value>
                </custom-meta>
              </custom-meta-group>
            </front-stub>
            <body>
              <p>
                <named-content content-type="letter-date">14 Dec 2020</named-content>
              </p>
              <p>Differential effects of visual versus auditory biofeedback training for voluntary postural sway</p>
              <p>PONE-D-20-14779R1</p>
              <p>Dear Dr. Asaka,</p>
              <p>We’re pleased to inform you that your manuscript has been judged scientifically suitable for publication and will be formally accepted for publication once it meets all outstanding technical requirements.</p>
              <p>Within one week, you’ll receive an e-mail detailing the required amendments. When these have been addressed, you’ll receive a formal acceptance letter and your manuscript will be scheduled for publication.</p>
              <p>An invoice for payment will follow shortly after the formal acceptance. To ensure an efficient process, please log into Editorial Manager at <ext-link ext-link-type="uri" xlink:href="http://www.editorialmanager.com/pone/">http://www.editorialmanager.com/pone/</ext-link>, click the 'Update My Information' link at the top of the page, and double check that your user information is up-to-date. If you have any billing related questions, please contact our Author Billing department directly at <email>authorbilling@plos.org</email>.</p>
              <p>If your institution or institutions have a press office, please notify them about your upcoming paper to help maximize its impact. If they’ll be preparing press materials, please inform our press team as soon as possible -- no later than 48 hours after receiving the formal acceptance. Your manuscript will remain under strict press embargo until 2 pm Eastern Time on the date of publication. For more information, please contact <email>onepress@plos.org</email>.</p>
              <p>Kind regards,</p>
              <p>Fabio A. Barbieri, PhD</p>
              <p>Academic Editor</p>
              <p>PLOS ONE</p>
              <p>Additional Editor Comments (optional):</p>
              <p>Reviewers' comments:</p>
              <p>Reviewer's Responses to Questions</p>
              <p>
                <bold>Comments to the Author</bold>
              </p>
              <p>1. If the authors have adequately addressed your comments raised in a previous round of review and you feel that this manuscript is now acceptable for publication, you may indicate that here to bypass the “Comments to the Author” section, enter your conflict of interest statement in the “Confidential to Editor” section, and submit your "Accept" recommendation.</p>
              <p>Reviewer #2: All comments have been addressed</p>
              <p>**********</p>
              <p>2. Is the manuscript technically sound, and do the data support the conclusions?</p>
              <p>The manuscript must describe a technically sound piece of scientific research with data that supports the conclusions. Experiments must have been conducted rigorously, with appropriate controls, replication, and sample sizes. The conclusions must be drawn appropriately based on the data presented. </p>
              <p>Reviewer #2: Yes</p>
              <p>**********</p>
              <p>3. Has the statistical analysis been performed appropriately and rigorously? </p>
              <p>Reviewer #2: Yes</p>
              <p>**********</p>
              <p>4. Have the authors made all data underlying the findings in their manuscript fully available?</p>
              <p>The <ext-link ext-link-type="uri" xlink:href="http://www.plosone.org/static/policies.action#sharing">PLOS Data policy</ext-link> requires authors to make all data underlying the findings described in their manuscript fully available without restriction, with rare exception (please refer to the Data Availability Statement in the manuscript PDF file). The data should be provided as part of the manuscript or its supporting information, or deposited to a public repository. For example, in addition to summary statistics, the data points behind means, medians and variance measures should be available. If there are restrictions on publicly sharing data—e.g. participant privacy or use of data from a third party—those must be specified.</p>
              <p>Reviewer #2: Yes</p>
              <p>**********</p>
              <p>5. Is the manuscript presented in an intelligible fashion and written in standard English?</p>
              <p>PLOS ONE does not copyedit accepted manuscripts, so the language in submitted articles must be clear, correct, and unambiguous. Any typographical or grammatical errors should be corrected at revision, so please note any specific errors here.</p>
              <p>Reviewer #2: Yes</p>
              <p>**********</p>
              <p>6. Review Comments to the Author</p>
              <p>Please use the space provided to explain your answers to the questions above. You may also include additional comments for the author, including concerns about dual publication, research ethics, or publication ethics. (Please upload your review as an attachment if it exceeds 20,000 characters)</p>
              <p>Reviewer #2: The revised version of the manuscript has been improved with all the suggested changes made. The authors also, based upon the suggestions from the reviewers, made substantial changes in the presentation and, most importantly, in the interpretation of the results, contributing significantly to the knowledge in the field. Thus, I congratulate with the authors for the submission and the presented manuscript.</p>
              <p>**********</p>
              <p>7. PLOS authors have the option to publish the peer review history of their article (<ext-link ext-link-type="uri" xlink:href="https://journals.plos.org/plosone/s/editorial-and-peer-review-process#loc-peer-review-history">what does this mean?</ext-link>). If published, this will include your full peer review and any attached files.</p>
              <p>If you choose “no”, your identity will remain anonymous but your review may still be made public.</p>
              <p><bold>Do you want your identity to be public for this peer review?</bold> For information about this choice, including consent withdrawal, please see our <ext-link ext-link-type="uri" xlink:href="https://www.plos.org/privacy-policy">Privacy Policy</ext-link>.</p>
              <p>Reviewer #2: <bold>Yes: </bold>Jose A. Barela</p>
            </body>
          </sub-article>
          <sub-article id="pone.0244583.r004" article-type="editor-report">
            <front-stub>
              <article-id pub-id-type="doi">10.1371/journal.pone.0244583.r004</article-id>
              <title-group>
                <article-title>Acceptance letter</article-title>
              </title-group>
              <contrib-group>
                <contrib contrib-type="author">
                  <name>
                    <surname>Barbieri</surname>
                    <given-names>Fabio A.</given-names>
                  </name>
                  <role>Academic Editor</role>
                </contrib>
              </contrib-group>
              <permissions>
                <copyright-statement>© 2020 Fabio A. Barbieri</copyright-statement>
                <copyright-year>2020</copyright-year>
                <copyright-holder>Fabio A. Barbieri</copyright-holder>
                <license xlink:href="http://creativecommons.org/licenses/by/4.0/">
                  <license-p>This is an open access article distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution License</ext-link>, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited.</license-p>
                </license>
              </permissions>
              <related-article id="rel-obj004" ext-link-type="doi" xlink:href="10.1371/journal.pone.0244583" related-article-type="reviewed-article"/>
            </front-stub>
            <body>
              <p>
                <named-content content-type="letter-date">16 Dec 2020</named-content>
              </p>
              <p>PONE-D-20-14779R1 </p>
              <p>Differential effects of visual versus auditory biofeedback training
for voluntary postural sway </p>
              <p>Dear Dr. Asaka:</p>
              <p>I'm pleased to inform you that your manuscript has been deemed suitable for publication in PLOS ONE. Congratulations! Your manuscript is now with our production department. </p>
              <p>If your institution or institutions have a press office, please let them know about your upcoming paper now to help maximize its impact. If they'll be preparing press materials, please inform our press team within the next 48 hours. Your manuscript will remain under strict press embargo until 2 pm Eastern Time on the date of publication. For more information please contact <email>onepress@plos.org</email>.</p>
              <p>If we can help with anything else, please email us at <email>plosone@plos.org</email>. </p>
              <p>Thank you for submitting your work to PLOS ONE and supporting open access. </p>
              <p>Kind regards, </p>
              <p>PLOS ONE Editorial Office Staff</p>
              <p>on behalf of</p>
              <p>Dr. Fabio A. Barbieri </p>
              <p>Academic Editor</p>
              <p>PLOS ONE</p>
            </body>
          </sub-article>
        </article>
      </metadata>
    </record>
  </GetRecord>
</OAI-PMH>
