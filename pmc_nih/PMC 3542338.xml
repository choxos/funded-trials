<?xml version="1.0" encoding="UTF-8"?>
<OAI-PMH xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/ http://www.openarchives.org/OAI/2.0/OAI-PMH.xsd">
  <responseDate>2024-05-03T07:31:48Z</responseDate>
  <request verb="GetRecord" identifier="oai:pubmedcentral.nih.gov:3542338" metadataPrefix="pmc">https:/www.ncbi.nlm.nih.gov/pmc/oai/oai.cgi</request>
  <GetRecord>
    <record>
      <header>
        <identifier>oai:pubmedcentral.nih.gov:3542338</identifier>
        <datestamp>2013-02-13</datestamp>
        <setSpec>plosone</setSpec>
        <setSpec>pmc-open</setSpec>
      </header>
      <metadata>
        <article xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" xsi:schemaLocation="https://jats.nlm.nih.gov/ns/archiving/1.3/ https://jats.nlm.nih.gov/archiving/1.3/xsd/JATS-archivearticle1-3.xsd" article-type="research-article">
          <front>
            <journal-meta>
              <journal-id journal-id-type="nlm-ta">PLoS One</journal-id>
              <journal-id journal-id-type="iso-abbrev">PLoS ONE</journal-id>
              <journal-id journal-id-type="publisher-id">plos</journal-id>
              <journal-id journal-id-type="pmc">plosone</journal-id>
              <journal-title-group>
                <journal-title>PLoS ONE</journal-title>
              </journal-title-group>
              <issn pub-type="epub">1932-6203</issn>
              <publisher>
                <publisher-name>Public Library of Science</publisher-name>
                <publisher-loc>San Francisco, USA</publisher-loc>
              </publisher>
            </journal-meta>
            <article-meta>
              <article-id pub-id-type="accession">PMC3542338</article-id>
              <article-id pub-id-type="pmcid">PMC3542338</article-id>
              <article-id pub-id-type="pmc-uid">3542338</article-id>
              <article-id pub-id-type="pmid">23408924</article-id>
              <article-id pub-id-type="pmid">23408924</article-id>
              <article-id pub-id-type="publisher-id">PONE-D-12-09052</article-id>
              <article-id pub-id-type="doi">10.1371/journal.pone.0053398</article-id>
              <article-categories>
                <subj-group subj-group-type="heading">
                  <subject>Research Article</subject>
                </subj-group>
                <subj-group subj-group-type="Discipline-v2">
                  <subject>Biology</subject>
                  <subj-group>
                    <subject>Anatomy and Physiology</subject>
                    <subj-group>
                      <subject>Neurological System</subject>
                      <subj-group>
                        <subject>Central Nervous System</subject>
                        <subject>Sensory Physiology</subject>
                      </subj-group>
                    </subj-group>
                  </subj-group>
                  <subj-group>
                    <subject>Neuroscience</subject>
                    <subj-group>
                      <subject>Cognitive Neuroscience</subject>
                      <subj-group>
                        <subject>Cognition</subject>
                      </subj-group>
                    </subj-group>
                    <subj-group>
                      <subject>Sensory Systems</subject>
                      <subj-group>
                        <subject>Auditory System</subject>
                      </subj-group>
                    </subj-group>
                    <subj-group>
                      <subject>Neurolinguistics</subject>
                      <subject>Neurophysiology</subject>
                    </subj-group>
                  </subj-group>
                </subj-group>
                <subj-group subj-group-type="Discipline-v2">
                  <subject>Social and Behavioral Sciences</subject>
                  <subj-group>
                    <subject>Linguistics</subject>
                    <subj-group>
                      <subject>Speech</subject>
                    </subj-group>
                  </subj-group>
                </subj-group>
              </article-categories>
              <title-group>
                <article-title>The Tracking of Speech Envelope in the Human Cortex</article-title>
                <alt-title alt-title-type="running-head">Speech Envelope Tracking in Human Cortex</alt-title>
              </title-group>
              <contrib-group>
                <contrib contrib-type="author">
                  <name>
                    <surname>Kubanek</surname>
                    <given-names>Jan</given-names>
                  </name>
                  <xref ref-type="aff" rid="aff1">
                    <sup>1</sup>
                  </xref>
                  <xref ref-type="aff" rid="aff2">
                    <sup>2</sup>
                  </xref>
                  <xref ref-type="corresp" rid="cor1">
                    <sup>*</sup>
                  </xref>
                </contrib>
                <contrib contrib-type="author">
                  <name>
                    <surname>Brunner</surname>
                    <given-names>Peter</given-names>
                  </name>
                  <xref ref-type="aff" rid="aff3">
                    <sup>3</sup>
                  </xref>
                  <xref ref-type="aff" rid="aff4">
                    <sup>4</sup>
                  </xref>
                </contrib>
                <contrib contrib-type="author">
                  <name>
                    <surname>Gunduz</surname>
                    <given-names>Aysegul</given-names>
                  </name>
                  <xref ref-type="aff" rid="aff3">
                    <sup>3</sup>
                  </xref>
                  <xref ref-type="aff" rid="aff4">
                    <sup>4</sup>
                  </xref>
                </contrib>
                <contrib contrib-type="author">
                  <name>
                    <surname>Poeppel</surname>
                    <given-names>David</given-names>
                  </name>
                  <xref ref-type="aff" rid="aff5">
                    <sup>5</sup>
                  </xref>
                </contrib>
                <contrib contrib-type="author">
                  <name>
                    <surname>Schalk</surname>
                    <given-names>Gerwin</given-names>
                  </name>
                  <xref ref-type="aff" rid="aff3">
                    <sup>3</sup>
                  </xref>
                  <xref ref-type="aff" rid="aff4">
                    <sup>4</sup>
                  </xref>
                </contrib>
              </contrib-group>
              <aff id="aff1">
                <label>1</label>
                <addr-line>Department of Anatomy &amp; Neurobiology, Washington University School of Medicine, St. Louis, Missouri, United States of America</addr-line>
              </aff>
              <aff id="aff2">
                <label>2</label>
                <addr-line>Department of Biomedical Engineering, Washington University in St. Louis, St. Louis, Missouri, United States of America</addr-line>
              </aff>
              <aff id="aff3">
                <label>3</label>
                <addr-line>Department of Neurology, Albany Medical College, Albany, New York, United States of America</addr-line>
              </aff>
              <aff id="aff4">
                <label>4</label>
                <addr-line>Brain-Computer Interface Research &amp; Development Program, Wadsworth Center, New York State Department of Health, Albany, New York, United States of America</addr-line>
              </aff>
              <aff id="aff5">
                <label>5</label>
                <addr-line>Department of Psychology, New York University, New York, New York, United States of America</addr-line>
              </aff>
              <contrib-group>
                <contrib contrib-type="editor">
                  <name>
                    <surname>Rodriguez-Fornells</surname>
                    <given-names>Antoni</given-names>
                  </name>
                  <role>Editor</role>
                  <xref ref-type="aff" rid="edit1"/>
                </contrib>
              </contrib-group>
              <aff id="edit1">
                <addr-line>University of Barcelona, Spain</addr-line>
              </aff>
              <author-notes>
                <corresp id="cor1">* E-mail: <email>kubanek@wustl.edu</email></corresp>
                <fn fn-type="COI-statement">
                  <p><bold>Competing Interests: </bold>The authors have declared that no competing interests exist.</p>
                </fn>
                <fn fn-type="con">
                  <p>Conceived and designed the experiments: JK PB AG GS. Performed the experiments: PB AG. Analyzed the data: JK. Wrote the paper: JK DP GS.</p>
                </fn>
              </author-notes>
              <pub-date pub-type="collection">
                <year>2013</year>
              </pub-date>
              <pub-date pub-type="epub">
                <day>10</day>
                <month>1</month>
                <year>2013</year>
              </pub-date>
              <pub-date pub-type="ecorrected">
                <day>5</day>
                <month>2</month>
                <year>2013</year>
              </pub-date>
              <volume>8</volume>
              <issue>1</issue>
              <elocation-id>e53398</elocation-id>
              <history>
                <date date-type="received">
                  <day>28</day>
                  <month>3</month>
                  <year>2012</year>
                </date>
                <date date-type="accepted">
                  <day>29</day>
                  <month>11</month>
                  <year>2012</year>
                </date>
              </history>
              <permissions>
                <copyright-statement>© 2013 Kubanek et al</copyright-statement>
                <copyright-year>2013</copyright-year>
                <copyright-holder>Kubanek et al</copyright-holder>
                <license xlink:href="http://creativecommons.org/licenses/by/4.0/">
                  <license-p>This is an open-access article distributed under the terms of the Creative Commons Attribution License, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are properly credited.</license-p>
                </license>
              </permissions>
              <abstract>
                <p>Humans are highly adept at processing speech. Recently, it has been shown that slow temporal information in speech (i.e., the envelope of speech) is critical for speech comprehension. Furthermore, it has been found that evoked electric potentials in human cortex are correlated with the speech envelope. However, it has been unclear whether this essential linguistic feature is encoded differentially in specific regions, or whether it is represented throughout the auditory system. To answer this question, we recorded neural data with high temporal resolution directly from the cortex while human subjects listened to a spoken story. We found that the gamma activity in human auditory cortex robustly tracks the speech envelope. The effect is so marked that it is observed during a single presentation of the spoken story to each subject. The effect is stronger in regions situated relatively early in the auditory pathway (belt areas) compared to other regions involved in speech processing, including the superior temporal gyrus (STG) and the posterior inferior frontal gyrus (Broca's region). To further distinguish whether speech envelope is encoded in the auditory system as a phonological (speech-related), or instead as a more general acoustic feature, we also probed the auditory system with a melodic stimulus. We found that belt areas track melody envelope weakly, and as the only region considered. Together, our data provide the first direct electrophysiological evidence that the envelope of speech is robustly tracked in non-primary auditory cortex (belt areas in particular), and suggest that the considered higher-order regions (STG and Broca's region) partake in a more abstract linguistic analysis.</p>
              </abstract>
              <funding-group>
                <funding-statement>This work was supported by grants from the National Institutes of Health (EB006356 and EB000856) and the United States Army Research Office (W911NF-07-1-0415 and W911NF-08-1-0216). The funders had no role in study design, data collection and analysis, decision to publish, or preparation of the manuscript.</funding-statement>
              </funding-group>
              <counts>
                <page-count count="9"/>
              </counts>
            </article-meta>
          </front>
          <body>
            <sec id="s1">
              <title>Introduction</title>
              <p>Spoken language is central to everyday communication. How speech is represented and processed in the nervous system is therefore of considerable interest to a wide range of scientists, clinicians, and engineers.</p>
              <p>Traditionally, the speech signal–-an output of the vocal tract resonating at specific frequencies–-has been viewed as a time-varying sonographic pattern of information in the frequency domain <xref rid="pone.0053398-Stevens1" ref-type="bibr">[1]</xref>. Recently, much interest has been sparked by purely temporal features of speech <xref rid="pone.0053398-Rosen1" ref-type="bibr">[2]</xref>–<xref rid="pone.0053398-Rauschecker1" ref-type="bibr">[5]</xref>. The importance of temporal information is demonstrated by many cases of language impairment <xref rid="pone.0053398-Zatorre1" ref-type="bibr">[3]</xref>. For instance, auditory neuropathy distorts temporal information transmitted to the brain. Patients with this condition can hear common sounds, but are severely impaired in understanding speech <xref rid="pone.0053398-Zeng1" ref-type="bibr">[6]</xref>. Other clinical evidence further supports the critical importance of temporal information. For example, early models of cochlear implants stimulated the cochlea with just one channel. This way, speech information was delivered to the brain entirely in the form of a time-varying waveform. Yet, patients with such implants were capable of understanding speech surprisingly well <xref rid="pone.0053398-Rosen1" ref-type="bibr">[2]</xref>.</p>
              <p>These salient clinical cases have stipulated questions about which component of the temporal signal is essential for speech understanding. Particular attention has focused on the slowly varying temporal component of speech (“envelope,” also referred to as “amplitude-envelope,” “time-amplitude,” or “time-intensity” <xref rid="pone.0053398-Rosen1" ref-type="bibr">[2]</xref>). Two main streams of evidence fuel this interest. First, it has been shown that manipulations of the speech envelope affect the recognition of consonants, vowels, and the understanding of sentences <xref rid="pone.0053398-Drullman1" ref-type="bibr">[7]</xref>–<xref rid="pone.0053398-Ahissar1" ref-type="bibr">[9]</xref>. This evidence indicates that speech envelope is an auditory feature that is necessary for speech understanding. Second, it has been shown <xref rid="pone.0053398-Shannon1" ref-type="bibr">[10]</xref> that human subjects can understand speech with a preserved temporal envelope but with severely degraded frequency content. This evidence indicates that speech envelope is an auditory feature that is sufficient for speech understanding.</p>
              <p>Given the essential role of the speech envelope in speech understanding, it is not surprising that speech envelope has been found to be represented in the human auditory system. In particular, studies that used brain recordings with high temporal resolution found that the variability in the speech envelope correlates with (i.e., is tracked by) the variability of electrical potentials and currents in human cortex <xref rid="pone.0053398-Abrams1" ref-type="bibr">[11]</xref>, <xref rid="pone.0053398-Abrams2" ref-type="bibr">[12]</xref>. Furthermore, the quality of this tracking predicts the quality of speech comprehension <xref rid="pone.0053398-Ahissar1" ref-type="bibr">[9]</xref>.</p>
              <p>However, it has been difficult to determine which cortical regions track this essential auditory feature. The availability of this information is a critical first step in understanding the individual stages of computations the auditory system uses to process speech-related signals. Progress in this direction is impeded by the limitations of the acquisition techniques used in current studies of the neural representation of the speech envelope. Specifically, the techniques used in current studies feature either high temporal resolution <xref rid="pone.0053398-Ahissar1" ref-type="bibr">[9]</xref>, <xref rid="pone.0053398-Abrams1" ref-type="bibr">[11]</xref>, <xref rid="pone.0053398-Abrams2" ref-type="bibr">[12]</xref> or high spatial resolution <xref rid="pone.0053398-Narain1" ref-type="bibr">[13]</xref>–<xref rid="pone.0053398-Obleser1" ref-type="bibr">[17]</xref>, but not both.</p>
              <p>To overcome these limitations, researchers have recently turned to electrocorticography (ECoG), an acquisition technique that combines high temporal resolution with favorable spatial resolution. Using this technique, it has been found that the speech envelope is tracked in the presumed core of human auditory cortex <xref rid="pone.0053398-Nourski1" ref-type="bibr">[18]</xref>.</p>
              <p>We asked whether and how speech envelope is tracked across human auditory cortex, outside the presumed core regions. In particular, we asked whether and how speech envelope is tracked in unisensory brain areas situated relatively early in the auditory pathway, and in higher multi- and supra-modal areas <xref rid="pone.0053398-Hickok1" ref-type="bibr">[4]</xref>. To provide an answer, we recorded neural activity using ECoG electrode grids placed on the left hemisphere of five human subjects listening to a spoken story. We found that human non-primary auditory cortex faithfully tracks speech envelope. The effect is stronger in areas situated relatively early in the auditory pathway (belt areas surrounding the auditory core) compared to higher-order regions including the superior temporal gyrus and the posterior inferior frontal gyrus.</p>
              <p>In a supplementary analysis, we investigated whether auditory cortex also tracks sound envelope of speech-unrelated stimuli <xref rid="pone.0053398-Nourski1" ref-type="bibr">[18]</xref>, <xref rid="pone.0053398-LigeoisChauvel1" ref-type="bibr">[19]</xref>. To do so, we presented the subjects with a song that featured a block of singing (i.e., a different kind of speech) and a block of pure melody (no speech). We found that the envelope of singing and pure melody is tracked only in the belt areas and to a lesser degree than speech.</p>
              <p>Together, we provide the first electrophysiological evidence that non-primary auditory cortex, in particular the cortex incorporating the belt areas surrounding the auditory core, tracks the temporal envelope of speech. To a lesser degree, this region also tracks the envelope of other naturalistic stimuli including lyrics and melody.</p>
            </sec>
            <sec id="s2">
              <title>Results</title>
              <p>We recorded the neural activity of the cortex using electrocorticographic (ECoG) electrode grids placed on the left hemisphere of the cortex of five human subjects (<xref ref-type="table" rid="pone-0053398-t001">Table 1</xref>) while they were attentively listening to a spoken story. The story was presented to each subject once, without repetition. Thus, in our study, data are not averaged across multiple stimulus presentations (typically referred to as trials in the literature).</p>
              <table-wrap id="pone-0053398-t001" orientation="portrait" position="float">
                <object-id pub-id-type="doi">10.1371/journal.pone.0053398.t001</object-id>
                <label>Table 1</label>
                <caption>
                  <title>Subject profiles. All subjects had normal cognitive capacity, as assessed by the Wechsler Adult Intelligence Scale-III <xref rid="pone.0053398-Wechsler1" ref-type="bibr">[45]</xref>.</title>
                </caption>
                <alternatives>
                  <graphic id="pone-0053398-t001-1" xlink:href="pone.0053398.t001"/>
                  <table frame="hsides" rules="groups">
                    <colgroup span="1">
                      <col align="left" span="1"/>
                      <col align="center" span="1"/>
                      <col align="center" span="1"/>
                      <col align="center" span="1"/>
                      <col align="center" span="1"/>
                      <col align="center" span="1"/>
                      <col align="center" span="1"/>
                    </colgroup>
                    <thead>
                      <tr>
                        <td align="left" rowspan="1" colspan="1">Subj.</td>
                        <td align="left" rowspan="1" colspan="1">Age</td>
                        <td align="left" rowspan="1" colspan="1">Sex</td>
                        <td align="left" rowspan="1" colspan="1">Handedness</td>
                        <td align="left" rowspan="1" colspan="1">Lang. dominance</td>
                        <td align="left" rowspan="1" colspan="1">Grid Locations</td>
                        <td align="left" rowspan="1" colspan="1">Channels</td>
                      </tr>
                    </thead>
                    <tbody>
                      <tr>
                        <td align="left" rowspan="1" colspan="1">A</td>
                        <td align="left" rowspan="1" colspan="1">29</td>
                        <td align="left" rowspan="1" colspan="1">F</td>
                        <td align="left" rowspan="1" colspan="1">R</td>
                        <td align="left" rowspan="1" colspan="1">L</td>
                        <td align="left" rowspan="1" colspan="1">Left fronto-parietal</td>
                        <td align="left" rowspan="1" colspan="1">64</td>
                      </tr>
                      <tr>
                        <td align="left" rowspan="1" colspan="1"/>
                        <td align="left" rowspan="1" colspan="1"/>
                        <td align="left" rowspan="1" colspan="1"/>
                        <td align="left" rowspan="1" colspan="1"/>
                        <td align="left" rowspan="1" colspan="1"/>
                        <td align="left" rowspan="1" colspan="1">Left temporal</td>
                        <td align="left" rowspan="1" colspan="1">23</td>
                      </tr>
                      <tr>
                        <td align="left" rowspan="1" colspan="1"/>
                        <td align="left" rowspan="1" colspan="1"/>
                        <td align="left" rowspan="1" colspan="1"/>
                        <td align="left" rowspan="1" colspan="1"/>
                        <td align="left" rowspan="1" colspan="1"/>
                        <td align="left" rowspan="1" colspan="1">Left temporal pole</td>
                        <td align="left" rowspan="1" colspan="1">3</td>
                      </tr>
                      <tr>
                        <td align="left" rowspan="1" colspan="1"/>
                        <td align="left" rowspan="1" colspan="1"/>
                        <td align="left" rowspan="1" colspan="1"/>
                        <td align="left" rowspan="1" colspan="1"/>
                        <td align="left" rowspan="1" colspan="1"/>
                        <td align="left" rowspan="1" colspan="1">Left occipital</td>
                        <td align="left" rowspan="1" colspan="1">6</td>
                      </tr>
                      <tr>
                        <td align="left" rowspan="1" colspan="1">B</td>
                        <td align="left" rowspan="1" colspan="1">30</td>
                        <td align="left" rowspan="1" colspan="1">M</td>
                        <td align="left" rowspan="1" colspan="1">R</td>
                        <td align="left" rowspan="1" colspan="1">L</td>
                        <td align="left" rowspan="1" colspan="1">Left frontal</td>
                        <td align="left" rowspan="1" colspan="1">40</td>
                      </tr>
                      <tr>
                        <td align="left" rowspan="1" colspan="1"/>
                        <td align="left" rowspan="1" colspan="1"/>
                        <td align="left" rowspan="1" colspan="1"/>
                        <td align="left" rowspan="1" colspan="1"/>
                        <td align="left" rowspan="1" colspan="1"/>
                        <td align="left" rowspan="1" colspan="1">Left temporal</td>
                        <td align="left" rowspan="1" colspan="1">35</td>
                      </tr>
                      <tr>
                        <td align="left" rowspan="1" colspan="1"/>
                        <td align="left" rowspan="1" colspan="1"/>
                        <td align="left" rowspan="1" colspan="1"/>
                        <td align="left" rowspan="1" colspan="1"/>
                        <td align="left" rowspan="1" colspan="1"/>
                        <td align="left" rowspan="1" colspan="1">Left temporal pole</td>
                        <td align="left" rowspan="1" colspan="1">4</td>
                      </tr>
                      <tr>
                        <td align="left" rowspan="1" colspan="1"/>
                        <td align="left" rowspan="1" colspan="1"/>
                        <td align="left" rowspan="1" colspan="1"/>
                        <td align="left" rowspan="1" colspan="1"/>
                        <td align="left" rowspan="1" colspan="1"/>
                        <td align="left" rowspan="1" colspan="1">Left occipital</td>
                        <td align="left" rowspan="1" colspan="1">4</td>
                      </tr>
                      <tr>
                        <td align="left" rowspan="1" colspan="1">C</td>
                        <td align="left" rowspan="1" colspan="1">26</td>
                        <td align="left" rowspan="1" colspan="1">F</td>
                        <td align="left" rowspan="1" colspan="1">R</td>
                        <td align="left" rowspan="1" colspan="1">L</td>
                        <td align="left" rowspan="1" colspan="1">Left frontal</td>
                        <td align="left" rowspan="1" colspan="1">64</td>
                      </tr>
                      <tr>
                        <td align="left" rowspan="1" colspan="1"/>
                        <td align="left" rowspan="1" colspan="1"/>
                        <td align="left" rowspan="1" colspan="1"/>
                        <td align="left" rowspan="1" colspan="1"/>
                        <td align="left" rowspan="1" colspan="1"/>
                        <td align="left" rowspan="1" colspan="1">Left temporal</td>
                        <td align="left" rowspan="1" colspan="1">35</td>
                      </tr>
                      <tr>
                        <td align="left" rowspan="1" colspan="1"/>
                        <td align="left" rowspan="1" colspan="1"/>
                        <td align="left" rowspan="1" colspan="1"/>
                        <td align="left" rowspan="1" colspan="1"/>
                        <td align="left" rowspan="1" colspan="1"/>
                        <td align="left" rowspan="1" colspan="1">Left temporal pole</td>
                        <td align="left" rowspan="1" colspan="1">4</td>
                      </tr>
                      <tr>
                        <td align="left" rowspan="1" colspan="1"/>
                        <td align="left" rowspan="1" colspan="1"/>
                        <td align="left" rowspan="1" colspan="1"/>
                        <td align="left" rowspan="1" colspan="1"/>
                        <td align="left" rowspan="1" colspan="1"/>
                        <td align="left" rowspan="1" colspan="1">Left occipital</td>
                        <td align="left" rowspan="1" colspan="1">6</td>
                      </tr>
                      <tr>
                        <td align="left" rowspan="1" colspan="1">D</td>
                        <td align="left" rowspan="1" colspan="1">56</td>
                        <td align="left" rowspan="1" colspan="1">M</td>
                        <td align="left" rowspan="1" colspan="1">R</td>
                        <td align="left" rowspan="1" colspan="1">L</td>
                        <td align="left" rowspan="1" colspan="1">Left frontal</td>
                        <td align="left" rowspan="1" colspan="1">56</td>
                      </tr>
                      <tr>
                        <td align="left" rowspan="1" colspan="1"/>
                        <td align="left" rowspan="1" colspan="1"/>
                        <td align="left" rowspan="1" colspan="1"/>
                        <td align="left" rowspan="1" colspan="1"/>
                        <td align="left" rowspan="1" colspan="1"/>
                        <td align="left" rowspan="1" colspan="1">Left temporal</td>
                        <td align="left" rowspan="1" colspan="1">35</td>
                      </tr>
                      <tr>
                        <td align="left" rowspan="1" colspan="1"/>
                        <td align="left" rowspan="1" colspan="1"/>
                        <td align="left" rowspan="1" colspan="1"/>
                        <td align="left" rowspan="1" colspan="1"/>
                        <td align="left" rowspan="1" colspan="1"/>
                        <td align="left" rowspan="1" colspan="1">Left occipital</td>
                        <td align="left" rowspan="1" colspan="1">6</td>
                      </tr>
                      <tr>
                        <td align="left" rowspan="1" colspan="1">E</td>
                        <td align="left" rowspan="1" colspan="1">45</td>
                        <td align="left" rowspan="1" colspan="1">M</td>
                        <td align="left" rowspan="1" colspan="1">R</td>
                        <td align="left" rowspan="1" colspan="1">L</td>
                        <td align="left" rowspan="1" colspan="1">Left fronto-temporal</td>
                        <td align="left" rowspan="1" colspan="1">54</td>
                      </tr>
                      <tr>
                        <td align="left" rowspan="1" colspan="1"/>
                        <td align="left" rowspan="1" colspan="1"/>
                        <td align="left" rowspan="1" colspan="1"/>
                        <td align="left" rowspan="1" colspan="1"/>
                        <td align="left" rowspan="1" colspan="1"/>
                        <td align="left" rowspan="1" colspan="1">Left temporal pole</td>
                        <td align="left" rowspan="1" colspan="1">4</td>
                      </tr>
                    </tbody>
                  </table>
                </alternatives>
              </table-wrap>
              <p>We quantified neural activity of our recordings in the high gamma range (75–115 Hz, see <xref ref-type="sec" rid="s4">Methods</xref>). We focus on the high gamma range, because activity in this range has been shown to reflect multi-unit discharge rates and local field potentials of neuronal ensembles underneath each electrode <xref rid="pone.0053398-Nir1" ref-type="bibr">[20]</xref>–<xref rid="pone.0053398-Mukamel1" ref-type="bibr">[22]</xref>. Furthermore, this signal has been shown to track the envelope of speech-related sounds in the putative core auditory cortex in humans <xref rid="pone.0053398-Nourski1" ref-type="bibr">[18]</xref>.</p>
              <p><xref ref-type="fig" rid="pone-0053398-g001">Figure 1</xref> shows the time course of high gamma activity of a channel located within the belt areas surrounding the auditory core (briefly, belt areas <xref rid="pone.0053398-Wessinger1" ref-type="bibr">[23]</xref>), superimposed on the time course of the envelope of the spoken story (<xref ref-type="sec" rid="s4">Methods</xref>). The figure demonstrates that the neural signal faithfully tracks the speech envelope (Spearman correlation <inline-formula><inline-graphic xlink:href="pone.0053398.e001.jpg"/></inline-formula>). This effect is intriguing given that the channel is positioned within the belt areas–-and not implanted in Heschl's gyrus as was the case in a previous study <xref rid="pone.0053398-Nourski1" ref-type="bibr">[18]</xref>–-and given that this tracking is observed without the necessity to average neural signals over many repeated trials as is typically done in the literature.</p>
              <fig id="pone-0053398-g001" orientation="portrait" position="float">
                <object-id pub-id-type="doi">10.1371/journal.pone.0053398.g001</object-id>
                <label>Figure 1</label>
                <caption>
                  <title>High gamma activity in human auditory cortex tracks the envelope of speech.</title>
                  <p>Black: Time course of the speech envelope. Green: High gamma activity recorded by a channel positioned in the belt areas in subject C (see <xref ref-type="fig" rid="pone-0053398-g005">Figure 5</xref>) while the subject listened to a narrated story. For the visualization purpose of this figure, we graphically scaled the magnitude of the neural signal to the magnitude of the envelope signal. The Spearman correlation between the two signals is <inline-formula><inline-graphic xlink:href="pone.0053398.e002.jpg"/></inline-formula>.</p>
                </caption>
                <graphic xlink:href="pone.0053398.g001"/>
              </fig>
              <p>The effect for each channel recorded in this region (11 channels in the 5 subjects, see <xref ref-type="sec" rid="s4">Methods</xref>) is given in <xref ref-type="fig" rid="pone-0053398-g002">Figure 2</xref>. The figure reveals that all (11/11) channels recorded in this region show a positive correlation. Furthermore, the correlation is significant (<inline-formula><inline-graphic xlink:href="pone.0053398.e003.jpg"/></inline-formula>) for most channels (10/11). Thus, these results demonstrate that the envelope of speech is faithfully tracked in human non-primary auditory cortex.</p>
              <fig id="pone-0053398-g002" orientation="portrait" position="float">
                <object-id pub-id-type="doi">10.1371/journal.pone.0053398.g002</object-id>
                <label>Figure 2</label>
                <caption>
                  <title>Tracking of speech envelope for each channel located in the belt areas.</title>
                  <p>The Spearman correlation coefficient (<inline-formula><inline-graphic xlink:href="pone.0053398.e004.jpg"/></inline-formula>) between the speech envelope and gamma activity for each channel in belt areas surrounding the auditory core. Filled bars denote the cases of significant correlation (<inline-formula><inline-graphic xlink:href="pone.0053398.e005.jpg"/></inline-formula>). The star refers to the example channel shown in <xref ref-type="fig" rid="pone-0053398-g001">Figure 1</xref> and <xref ref-type="fig" rid="pone-0053398-g005">Figure 5</xref>.</p>
                </caption>
                <graphic xlink:href="pone.0053398.g002"/>
              </fig>
              <p>We next tested whether the speech envelope is tracked, besides the gamma activity, also by a signal typically investigated in human studies. In particular, several studies identified correlations with signal envelope using time-locked average electric potentials or currents <xref rid="pone.0053398-Ahissar1" ref-type="bibr">[9]</xref>, <xref rid="pone.0053398-Abrams1" ref-type="bibr">[11]</xref>, <xref rid="pone.0053398-Abrams2" ref-type="bibr">[12]</xref>, <xref rid="pone.0053398-LigeoisChauvel1" ref-type="bibr">[19]</xref>. We investigated the effect of the raw potential in our neural recordings (see <xref ref-type="sec" rid="s4">Methods</xref>). We found (<xref ref-type="fig" rid="pone-0053398-g003">Figure 3</xref>) that high gamma activity is substantially more sensitive to the speech envelope compared to the raw potential (<inline-formula><inline-graphic xlink:href="pone.0053398.e006.jpg"/></inline-formula>, mean <inline-formula><inline-graphic xlink:href="pone.0053398.e007.jpg"/></inline-formula>(gamma)<inline-formula><inline-graphic xlink:href="pone.0053398.e008.jpg"/></inline-formula>, mean <inline-formula><inline-graphic xlink:href="pone.0053398.e009.jpg"/></inline-formula>(potential)<inline-formula><inline-graphic xlink:href="pone.0053398.e010.jpg"/></inline-formula>, paired two-tailed t-test <inline-formula><inline-graphic xlink:href="pone.0053398.e011.jpg"/></inline-formula> (<inline-formula><inline-graphic xlink:href="pone.0053398.e012.jpg"/></inline-formula>, <inline-formula><inline-graphic xlink:href="pone.0053398.e013.jpg"/></inline-formula> df)). The same result holds when the potential is computed as the rms value, and not as the mean–-see <xref ref-type="sec" rid="s4">Methods</xref> (mean <inline-formula><inline-graphic xlink:href="pone.0053398.e014.jpg"/></inline-formula>(potential<inline-formula><inline-graphic xlink:href="pone.0053398.e015.jpg"/></inline-formula>)<inline-formula><inline-graphic xlink:href="pone.0053398.e016.jpg"/></inline-formula>, gamma versus potential<inline-formula><inline-graphic xlink:href="pone.0053398.e017.jpg"/></inline-formula>
<inline-formula><inline-graphic xlink:href="pone.0053398.e018.jpg"/></inline-formula> (<inline-formula><inline-graphic xlink:href="pone.0053398.e019.jpg"/></inline-formula>, <inline-formula><inline-graphic xlink:href="pone.0053398.e020.jpg"/></inline-formula> df)). Due to the superior sensitivity of high gamma activity to envelope information, henceforth, we quantify all neural effects strictly using high gamma activity, and refer to this signal shortly as “neural activity”.</p>
              <fig id="pone-0053398-g003" orientation="portrait" position="float">
                <object-id pub-id-type="doi">10.1371/journal.pone.0053398.g003</object-id>
                <label>Figure 3</label>
                <caption>
                  <title>High gamma activity tracks the envelope of speech better than does the raw potential.</title>
                  <p>Mean<inline-formula><inline-graphic xlink:href="pone.0053398.e021.jpg"/></inline-formula>SEM Spearman correlation coefficient (<inline-formula><inline-graphic xlink:href="pone.0053398.e022.jpg"/></inline-formula>) between each neural signal and speech envelope. The mean is computed across all channels in the belt areas (<inline-formula><inline-graphic xlink:href="pone.0053398.e023.jpg"/></inline-formula>).</p>
                </caption>
                <graphic xlink:href="pone.0053398.g003"/>
              </fig>
              <p>Next, we investigated how the speech envelope is tracked in other cortical regions involved in speech processing. Specifically, we measured neural activity over the superior temporal gyrus (briefly, STG) and in the posterior inferior frontal gyrus (briefly, Broca's region). We focused on the STG because this region shows responses specific to intelligible speech <xref rid="pone.0053398-Narain1" ref-type="bibr">[13]</xref>, <xref rid="pone.0053398-Scott1" ref-type="bibr">[14]</xref>. Furthermore, posterior parts of this region have been traditionally associated with speech perception <xref rid="pone.0053398-Wernicke1" ref-type="bibr">[24]</xref>–<xref rid="pone.0053398-Geschwind2" ref-type="bibr">[26]</xref> and, more recently, also with speech generation <xref rid="pone.0053398-Rauschecker1" ref-type="bibr">[5]</xref>. We focused on the Broca's region as this region has traditionally been associated with speech production <xref rid="pone.0053398-Broca1" ref-type="bibr">[27]</xref>, and it is thought to be a part of the articulatory network in recent view <xref rid="pone.0053398-Hickok1" ref-type="bibr">[4]</xref>, <xref rid="pone.0053398-Rauschecker1" ref-type="bibr">[5]</xref>.</p>
              <p><xref ref-type="fig" rid="pone-0053398-g004">Figure 4</xref> compares the magnitude of the tracking of the speech envelope in these three regions of interest. The figure reveals that speech envelope is tracked predominantly in the belt areas (mean <inline-formula><inline-graphic xlink:href="pone.0053398.e024.jpg"/></inline-formula>, <inline-formula><inline-graphic xlink:href="pone.0053398.e025.jpg"/></inline-formula> (<inline-formula><inline-graphic xlink:href="pone.0053398.e026.jpg"/></inline-formula>), two-tailed t-test, <inline-formula><inline-graphic xlink:href="pone.0053398.e027.jpg"/></inline-formula>) compared to the STG (mean <inline-formula><inline-graphic xlink:href="pone.0053398.e028.jpg"/></inline-formula>, <inline-formula><inline-graphic xlink:href="pone.0053398.e029.jpg"/></inline-formula> (<inline-formula><inline-graphic xlink:href="pone.0053398.e030.jpg"/></inline-formula>), <inline-formula><inline-graphic xlink:href="pone.0053398.e031.jpg"/></inline-formula>) and the Broca's region (mean <inline-formula><inline-graphic xlink:href="pone.0053398.e032.jpg"/></inline-formula>, <inline-formula><inline-graphic xlink:href="pone.0053398.e033.jpg"/></inline-formula> (<inline-formula><inline-graphic xlink:href="pone.0053398.e034.jpg"/></inline-formula>), <inline-formula><inline-graphic xlink:href="pone.0053398.e035.jpg"/></inline-formula>, <inline-formula><inline-graphic xlink:href="pone.0053398.e036.jpg"/></inline-formula> df). Importantly, the belt areas track the speech envelope significantly better than the STG (<inline-formula><inline-graphic xlink:href="pone.0053398.e037.jpg"/></inline-formula> (<inline-formula><inline-graphic xlink:href="pone.0053398.e038.jpg"/></inline-formula>), two-tailed t-test, <inline-formula><inline-graphic xlink:href="pone.0053398.e039.jpg"/></inline-formula>, <inline-formula><inline-graphic xlink:href="pone.0053398.e040.jpg"/></inline-formula>) and significantly better than the Broca's region (<inline-formula><inline-graphic xlink:href="pone.0053398.e041.jpg"/></inline-formula>, (<inline-formula><inline-graphic xlink:href="pone.0053398.e042.jpg"/></inline-formula>), <inline-formula><inline-graphic xlink:href="pone.0053398.e043.jpg"/></inline-formula>, <inline-formula><inline-graphic xlink:href="pone.0053398.e044.jpg"/></inline-formula>). Note that the regions differ in the number of channels (<inline-formula><inline-graphic xlink:href="pone.0053398.e045.jpg"/></inline-formula>). Thus, an effect of a small magnitude (a small <inline-formula><inline-graphic xlink:href="pone.0053398.e046.jpg"/></inline-formula>) may be highly significant (a small <inline-formula><inline-graphic xlink:href="pone.0053398.e047.jpg"/></inline-formula>) for a region with a high <inline-formula><inline-graphic xlink:href="pone.0053398.e048.jpg"/></inline-formula> (e.g., STG).</p>
              <fig id="pone-0053398-g004" orientation="portrait" position="float">
                <object-id pub-id-type="doi">10.1371/journal.pone.0053398.g004</object-id>
                <label>Figure 4</label>
                <caption>
                  <title>Tracking of speech envelope in three auditory cortical regions.</title>
                  <p>Mean<inline-formula><inline-graphic xlink:href="pone.0053398.e049.jpg"/></inline-formula>SEM Spearman correlation coefficient (<inline-formula><inline-graphic xlink:href="pone.0053398.e050.jpg"/></inline-formula>) between neural activity and speech envelope in each region of interest. The mean is computed over all channels in each area (<inline-formula><inline-graphic xlink:href="pone.0053398.e051.jpg"/></inline-formula>, <inline-formula><inline-graphic xlink:href="pone.0053398.e052.jpg"/></inline-formula>, <inline-formula><inline-graphic xlink:href="pone.0053398.e053.jpg"/></inline-formula>). Stars denote the significance of the difference in means (two-tailed t-test), *<inline-formula><inline-graphic xlink:href="pone.0053398.e054.jpg"/></inline-formula>, **<inline-formula><inline-graphic xlink:href="pone.0053398.e055.jpg"/></inline-formula>.</p>
                </caption>
                <graphic xlink:href="pone.0053398.g004"/>
              </fig>
              <p>Similar results were obtained when we further evaluated the high gamma neural activity in a broader frequency range, 70–500 Hz (belt areas versus STG: <inline-formula><inline-graphic xlink:href="pone.0053398.e056.jpg"/></inline-formula>; belt areas versus Broca's region: <inline-formula><inline-graphic xlink:href="pone.0053398.e057.jpg"/></inline-formula>; mean <inline-formula><inline-graphic xlink:href="pone.0053398.e058.jpg"/></inline-formula>, <inline-formula><inline-graphic xlink:href="pone.0053398.e059.jpg"/></inline-formula>; mean <inline-formula><inline-graphic xlink:href="pone.0053398.e060.jpg"/></inline-formula>, <inline-formula><inline-graphic xlink:href="pone.0053398.e061.jpg"/></inline-formula>; mean <inline-formula><inline-graphic xlink:href="pone.0053398.e062.jpg"/></inline-formula>, <inline-formula><inline-graphic xlink:href="pone.0053398.e063.jpg"/></inline-formula>). Similar results are also obtained, for the frequency range 75–115 Hz, when we use Pearson's instead of Spearman's correlation (belt areas versus STG: <inline-formula><inline-graphic xlink:href="pone.0053398.e064.jpg"/></inline-formula>, <inline-formula><inline-graphic xlink:href="pone.0053398.e065.jpg"/></inline-formula> df; belt areas versus Broca's region: <inline-formula><inline-graphic xlink:href="pone.0053398.e066.jpg"/></inline-formula>; mean <inline-formula><inline-graphic xlink:href="pone.0053398.e067.jpg"/></inline-formula>, <inline-formula><inline-graphic xlink:href="pone.0053398.e068.jpg"/></inline-formula>; mean <inline-formula><inline-graphic xlink:href="pone.0053398.e069.jpg"/></inline-formula>; mean <inline-formula><inline-graphic xlink:href="pone.0053398.e070.jpg"/></inline-formula>).</p>
              <p>We further validated these results by characterizing the spatial topography (see <xref ref-type="sec" rid="s4">Methods</xref>) of the tracking effect (<inline-formula><inline-graphic xlink:href="pone.0053398.e071.jpg"/></inline-formula>) in each subject (<xref ref-type="fig" rid="pone-0053398-g005">Figure 5</xref>). This analysis confirms that speech envelope is tracked predominantly by the regions within or close to posterior parts of the superior temporal gyrus. As discussed in more detail in the <xref ref-type="sec" rid="s4">Methods</xref>, our automatized procedure of co-registration of electrode locations with anatomical data may be imperfect (e.g., one channel in subject D and some channels in subject E). Nonetheless, the same principal results as those shown in <xref ref-type="fig" rid="pone-0053398-g004">Figure 4</xref> hold when subject D (mean <inline-formula><inline-graphic xlink:href="pone.0053398.e072.jpg"/></inline-formula>, <inline-formula><inline-graphic xlink:href="pone.0053398.e073.jpg"/></inline-formula>, <inline-formula><inline-graphic xlink:href="pone.0053398.e074.jpg"/></inline-formula>; belt areas versus STG: <inline-formula><inline-graphic xlink:href="pone.0053398.e075.jpg"/></inline-formula>, belt areas versus Broca's region: <inline-formula><inline-graphic xlink:href="pone.0053398.e076.jpg"/></inline-formula>, <inline-formula><inline-graphic xlink:href="pone.0053398.e077.jpg"/></inline-formula> df) or subject E (<inline-formula><inline-graphic xlink:href="pone.0053398.e078.jpg"/></inline-formula>, <inline-formula><inline-graphic xlink:href="pone.0053398.e079.jpg"/></inline-formula>, <inline-formula><inline-graphic xlink:href="pone.0053398.e080.jpg"/></inline-formula>; belt areas versus STG: <inline-formula><inline-graphic xlink:href="pone.0053398.e081.jpg"/></inline-formula>, belt areas versus Broca's region: <inline-formula><inline-graphic xlink:href="pone.0053398.e082.jpg"/></inline-formula>) are excluded from the analyses.</p>
              <fig id="pone-0053398-g005" orientation="portrait" position="float">
                <object-id pub-id-type="doi">10.1371/journal.pone.0053398.g005</object-id>
                <label>Figure 5</label>
                <caption>
                  <title>Neural tracking of speech envelope at each recording site in each subject.</title>
                  <p>Color hue (see colorbars) gives <inline-formula><inline-graphic xlink:href="pone.0053398.e083.jpg"/></inline-formula> at each channel for the individual subjects (A–E), and for the subject average (AVG). Individual channels implanted in each subject are shown in green (belt areas), orange (STG), red (Broca's region), or black (other regions). The location of each channel was determined using the Talairach Atlas daemon (see <xref ref-type="sec" rid="s4">Methods</xref>). In subject C, the arrow points to the channel for which we illustrated the tracking effect (<xref ref-type="fig" rid="pone-0053398-g001">Figure 1</xref>). STS: superior temporal sulcus; SF: Sylvian fissure; TTS: transverse temporal sulcus (perpendicular to the view plane).</p>
                </caption>
                <graphic xlink:href="pone.0053398.g005"/>
              </fig>
              <p><xref ref-type="fig" rid="pone-0053398-g005">Figure 5</xref> shows that the tracking effect is observed more strongly in regions situated relatively early in the auditory pathway compared to other cortical regions. We quantified this impression by comparing the mean tracking effect for channels positioned within the belt areas and channels in all other regions (<xref ref-type="fig" rid="pone-0053398-g006">Figure 6</xref>). The figure reveals a highly significant difference (<inline-formula><inline-graphic xlink:href="pone.0053398.e084.jpg"/></inline-formula>, two-tailed t-test). Thus, the speech envelope tracking effect is observed predominantly early in the auditory pathway.</p>
              <fig id="pone-0053398-g006" orientation="portrait" position="float">
                <object-id pub-id-type="doi">10.1371/journal.pone.0053398.g006</object-id>
                <label>Figure 6</label>
                <caption>
                  <title>Tracking of speech envelope in early auditory regions compared to all other regions.</title>
                  <p>Mean<inline-formula><inline-graphic xlink:href="pone.0053398.e085.jpg"/></inline-formula>SEM Spearman correlation coefficient (<inline-formula><inline-graphic xlink:href="pone.0053398.e086.jpg"/></inline-formula>) between neural activity and speech envelope in belt areas (green) and all other regions (gray). The mean is computed over all channels in each case (<inline-formula><inline-graphic xlink:href="pone.0053398.e087.jpg"/></inline-formula>, <inline-formula><inline-graphic xlink:href="pone.0053398.e088.jpg"/></inline-formula>).</p>
                </caption>
                <graphic xlink:href="pone.0053398.g006"/>
              </fig>
              <p>Both speech and animal vocalization carry information also in the frequency domain, in the form of frequency-modulated (FM) sweeps <xref rid="pone.0053398-Stevens1" ref-type="bibr">[1]</xref>, <xref rid="pone.0053398-Rauschecker1" ref-type="bibr">[5]</xref>. Given this frequency composition of the speech signal, it is possible that each of the three speech-sensitive regions of interest is sensitive to envelope information in particular range of stimulus frequencies. We thus extracted the envelope of the speech signal at each frequency in the range from 16 Hz to 16 kHz (see <xref ref-type="sec" rid="s4">Methods</xref>). We then computed the correlation (<inline-formula><inline-graphic xlink:href="pone.0053398.e089.jpg"/></inline-formula>) between neural activity and the envelope of speech at each frequency in this range. The result is shown in <xref ref-type="fig" rid="pone-0053398-g007">Figure 7</xref>. Two effects are observed. First, this figure confirms the result reported in <xref ref-type="fig" rid="pone-0053398-g004">Figure 4</xref> and of <xref ref-type="fig" rid="pone-0053398-g005">Figure 5</xref> that the speech envelope is predominantly tracked in the belt areas, and shows that this is true regardless of the frequency at which the envelope is assessed. Second, belt areas activity tracks the speech envelope starting at a sound frequency of about 100 Hz, which interestingly approximately equals the lower limit of the fundamental frequency of human utterances <xref rid="pone.0053398-Stevens1" ref-type="bibr">[1]</xref>, <xref rid="pone.0053398-Schwartz1" ref-type="bibr">[28]</xref>.</p>
              <fig id="pone-0053398-g007" orientation="portrait" position="float">
                <object-id pub-id-type="doi">10.1371/journal.pone.0053398.g007</object-id>
                <label>Figure 7</label>
                <caption>
                  <title>Neural tracking of speech envelope at each frequency of the sound.</title>
                  <p>Mean<inline-formula><inline-graphic xlink:href="pone.0053398.e090.jpg"/></inline-formula>SEM Spearman correlation coefficient (<inline-formula><inline-graphic xlink:href="pone.0053398.e091.jpg"/></inline-formula>) between neural activity and speech envelope for each frequency, for each region of interest. The dashed line gives the average spectrum of the speech in logarithmic units.</p>
                </caption>
                <graphic xlink:href="pone.0053398.g007"/>
              </fig>
              <p>Thus far, we have assessed the extent to which the human auditory system tracks the envelope of speech. In an additional analysis, we assessed the strength of the tracking effect also for other kinds of naturalistic stimuli, speech-related and speech-unrelated. To this end, we presented to our subjects–-besides the speech stimulus–-also a song (see <xref ref-type="sec" rid="s4">Methods</xref>). We extracted from the song periods of singing (“lyrics”), and used the melodic part of the song in which no singing occurs as a speech-unrelated stimulus (“melody”). We assessed envelope tracking for these two additional stimuli the same way as we did for the speech stimulus. The result is shown in <xref ref-type="fig" rid="pone-0053398-g008">Figure 8</xref>. Two main effects are observed. First–-and in line with the observations made for the speech stimulus–-stimulus envelope is predominantly tracked in the belt areas (lyrics: belt areas versus STG, <inline-formula><inline-graphic xlink:href="pone.0053398.e092.jpg"/></inline-formula> (<inline-formula><inline-graphic xlink:href="pone.0053398.e093.jpg"/></inline-formula>), two-tailed t-test; belt areas versus Broca's region, <inline-formula><inline-graphic xlink:href="pone.0053398.e094.jpg"/></inline-formula> (<inline-formula><inline-graphic xlink:href="pone.0053398.e095.jpg"/></inline-formula>); melody: belt areas versus STG, <inline-formula><inline-graphic xlink:href="pone.0053398.e096.jpg"/></inline-formula> (<inline-formula><inline-graphic xlink:href="pone.0053398.e097.jpg"/></inline-formula>), two-tailed t-test; belt areas versus Broca's region, <inline-formula><inline-graphic xlink:href="pone.0053398.e098.jpg"/></inline-formula> (<inline-formula><inline-graphic xlink:href="pone.0053398.e099.jpg"/></inline-formula>, <inline-formula><inline-graphic xlink:href="pone.0053398.e100.jpg"/></inline-formula> df)). Second, the envelope of lyrics and melody seems to be tracked by the considered cortical regions substantially worse than speech. We assessed these two main effects on envelope tracking (<inline-formula><inline-graphic xlink:href="pone.0053398.e101.jpg"/></inline-formula>) using a two-way ANOVA, with factors cortical region (belt areas, STG, Broca's region) and stimulus type (speech, lyrics, melody). Both factors had a highly significant impact on <inline-formula><inline-graphic xlink:href="pone.0053398.e102.jpg"/></inline-formula> (cortical region, <inline-formula><inline-graphic xlink:href="pone.0053398.e103.jpg"/></inline-formula> (<inline-formula><inline-graphic xlink:href="pone.0053398.e104.jpg"/></inline-formula>); stimulus type, <inline-formula><inline-graphic xlink:href="pone.0053398.e105.jpg"/></inline-formula> (<inline-formula><inline-graphic xlink:href="pone.0053398.e106.jpg"/></inline-formula>)). Thus, this analysis, along with the data shown in <xref ref-type="fig" rid="pone-0053398-g004">Figure 4</xref> and <xref ref-type="fig" rid="pone-0053398-g008">Figure 8</xref>, suggests that stimulus envelope is tracked predominantly in the belt areas, and suggests that the envelope of other types of stimuli, including lyrics and melody, is encoded relatively weakly compared to speech. Besides these findings, an important observation is that belt areas, albeit somewhat weakly, significantly track the envelope of melody (<inline-formula><inline-graphic xlink:href="pone.0053398.e107.jpg"/></inline-formula>, two-tailed t-test, <inline-formula><inline-graphic xlink:href="pone.0053398.e108.jpg"/></inline-formula>, <inline-formula><inline-graphic xlink:href="pone.0053398.e109.jpg"/></inline-formula> df).</p>
              <fig id="pone-0053398-g008" orientation="portrait" position="float">
                <object-id pub-id-type="doi">10.1371/journal.pone.0053398.g008</object-id>
                <label>Figure 8</label>
                <caption>
                  <title>Tracking of speech-related and melodic stimuli in human cortex.</title>
                  <p>Mean<inline-formula><inline-graphic xlink:href="pone.0053398.e110.jpg"/></inline-formula>SEM Spearman correlation coefficient (<inline-formula><inline-graphic xlink:href="pone.0053398.e111.jpg"/></inline-formula>) between neural activity and the speech envelope, for each region of interest. Left: stimulus containing lyrics. Right: melody. The mean is computed across all channels in each area. Stars denote the significance of the difference in means (t-test), *<inline-formula><inline-graphic xlink:href="pone.0053398.e112.jpg"/></inline-formula>, **<inline-formula><inline-graphic xlink:href="pone.0053398.e113.jpg"/></inline-formula>.</p>
                </caption>
                <graphic xlink:href="pone.0053398.g008"/>
              </fig>
              <p>The above effects could not be observed had we used–-instead of high gamma activity–-the less sensitive raw electric potential, whose time-averaged form has been employed in other studies <xref rid="pone.0053398-Ahissar1" ref-type="bibr">[9]</xref>, <xref rid="pone.0053398-Abrams1" ref-type="bibr">[11]</xref>, <xref rid="pone.0053398-Abrams2" ref-type="bibr">[12]</xref>, <xref rid="pone.0053398-LigeoisChauvel1" ref-type="bibr">[19]</xref>. Using the raw potential, the two-way ANOVA with factors brain region and stimulus type fails to detect differences in envelope tracking (<inline-formula><inline-graphic xlink:href="pone.0053398.e114.jpg"/></inline-formula>) among cortical regions (main effect of cortical region, <inline-formula><inline-graphic xlink:href="pone.0053398.e115.jpg"/></inline-formula> (<inline-formula><inline-graphic xlink:href="pone.0053398.e116.jpg"/></inline-formula>)), and is weakly sensitive to stimulus type (main effect of stimulus type, <inline-formula><inline-graphic xlink:href="pone.0053398.e117.jpg"/></inline-formula> (<inline-formula><inline-graphic xlink:href="pone.0053398.e118.jpg"/></inline-formula>, <inline-formula><inline-graphic xlink:href="pone.0053398.e119.jpg"/></inline-formula> df)). This is in contrast to the sensitivity of high gamma (main effect of cortical region, <inline-formula><inline-graphic xlink:href="pone.0053398.e120.jpg"/></inline-formula> (<inline-formula><inline-graphic xlink:href="pone.0053398.e121.jpg"/></inline-formula>); main effect of stimulus type, <inline-formula><inline-graphic xlink:href="pone.0053398.e122.jpg"/></inline-formula> (<inline-formula><inline-graphic xlink:href="pone.0053398.e123.jpg"/></inline-formula>)).</p>
            </sec>
            <sec id="s3">
              <title>Discussion</title>
              <p>We recorded cortical responses in humans listening to naturalistic auditory stimuli, including speech and singing (speech-related stimuli), and music (speech-unrelated stimulus). We observed that high gamma activity in the belt areas surrounding the auditory core robustly tracks the envelope of speech-related stimuli. The effect is observed during a single presentation of the stimulus to each subject. We found that the tracking effect is strongest in the region incorporating the belt areas. This region tracks, besides speech-related stimuli, also the envelope of speech-unrelated stimuli (melody), albeit to a lesser degree compared to speech. Other regions involved in speech processing, including the STG and the Broca's region, track the envelope of speech-related stimuli only, and the effect in these regions is substantially weaker compared to the effect of speech in the belt areas.</p>
              <p>These findings are consistent with the idea of hierarchical representation of speech-related sounds in the auditory system <xref rid="pone.0053398-Rauschecker1" ref-type="bibr">[5]</xref>. In this regard, our data show that the belt areas represent a simple acoustic feature–-envelope–-strongly, and regardless of what kind of stimulus is presented (speech-related or speech-unrelated). This suggests that the belt areas process simple acoustic features of the stimulus, and thus represent a low stage in the speech-processing hierarchy. In contrast, the considered higher-order regions (STG, Broca's region) represent envelope only weakly, and specifically for speech. This suggests that these regions are more invariant to representing simple acoustic features of the stimulus such as the speech envelope and are thus positioned higher in the speech-processing hierarchy. The idea that these regions specialize in a higher, abstract (lexical/syntactic/semantic) level of speech analysis is supported also by imaging studies <xref rid="pone.0053398-Narain1" ref-type="bibr">[13]</xref>, <xref rid="pone.0053398-Scott1" ref-type="bibr">[14]</xref>, <xref rid="pone.0053398-Liebenthal1" ref-type="bibr">[16]</xref>, <xref rid="pone.0053398-Obleser1" ref-type="bibr">[17]</xref>. These studies have revealed that non-primary cortical regions (i.e., left middle and anterior superior temporal sulcus) show differential responses when complex sound features are compared with simple acoustic features. This holds true for both the comparison of phonetic vs. acoustic sound features <xref rid="pone.0053398-Liebenthal1" ref-type="bibr">[16]</xref>, <xref rid="pone.0053398-Obleser1" ref-type="bibr">[17]</xref>, and for the comparison of semantic vs. acoustic features <xref rid="pone.0053398-Narain1" ref-type="bibr">[13]</xref>, <xref rid="pone.0053398-Scott1" ref-type="bibr">[14]</xref>.</p>
              <p>Notably, the finding that the considered higher-order auditory regions represent speech envelope only weakly is consistent with but does not prove the validity of the hierarchical processing model. In particular, these regions, while invariant to speech envelope or possibly not encoding speech envelope at all, may at the same time act as lower-order processing nodes for simple acoustic features of speech other than the speech envelope. This way, these putative higher-order speech processing regions may not neatly fit the hierarchical speech-processing model.</p>
              <p>In this study, we also tested whether the envelope-tracking effect is specific to speech or whether it can be observed also for other kinds of stimuli (speech-unrelated stimuli). We found that the envelope of melody is in the considered auditory regions represented weakly compared to speech. Nonetheless, the effect does reach significance in the belt areas. This suggests that the stimulus envelope is expressed in the belt areas as an acoustic feature, not as a purely phonetic feature specific to speech. This is in a good agreement with studies that found, in regions in or close to belt areas, similar activation for both phonemic and nonphonemic sounds <xref rid="pone.0053398-Liebenthal1" ref-type="bibr">[16]</xref>–<xref rid="pone.0053398-Nourski1" ref-type="bibr">[18]</xref>. Furthermore, the belt areas may be sensitive to temporal information in general <xref rid="pone.0053398-Binder1" ref-type="bibr">[29]</xref>. The weak coding of music envelope in the left cortex (all 5 subjects had a left coverage) is in line with proposals that the temporal variability in music is relatively sluggish compared to speech, and that music carries a substantial amount of information in the frequency domain <xref rid="pone.0053398-Zatorre1" ref-type="bibr">[3]</xref>. It has been suggested <xref rid="pone.0053398-Zatorre1" ref-type="bibr">[3]</xref> that the left auditory cortex specializes in processing of temporal information, whereas the right auditory cortex is more sensitive to information in the frequency domain. A more recent piece of evidence comes from an imaging study <xref rid="pone.0053398-Hyde1" ref-type="bibr">[30]</xref>, which found that a manipulation in pitch of melodic sounds was reflected mainly in the right hemisphere, and much less in the left. Future electrophysiological studies could determine how the envelope of music is tracked in the right hemisphere.</p>
              <p>Temporal information related to primordial forms of speech–-animal vocalization–-has been shown to be represented in discharge rates of neurons in cat and marmoset primary auditory cortex <xref rid="pone.0053398-Gehr1" ref-type="bibr">[31]</xref>–<xref rid="pone.0053398-Nagarajan1" ref-type="bibr">[34]</xref>. One of these studies <xref rid="pone.0053398-Gehr1" ref-type="bibr">[31]</xref> reports that about 60% of units in primary auditory cortex (A1) track the onset of a vocalization, and about 40% of A1 units track major peaks in that sound. Interestingly, the tracking of the vocalization envelope becomes particularly salient when the phase-locked multi-unit responses are summed together. Given that synchronized ensembles of multi-unit activity are correlated with high gamma power of the LFPs <xref rid="pone.0053398-Nir1" ref-type="bibr">[20]</xref>–<xref rid="pone.0053398-Mukamel1" ref-type="bibr">[22]</xref>, an intriguing possibility is that the high gamma activity that we report in this study is closely related to the multi-unit discharges recorded in animal primary auditory cortex during vocalization. Indeed, it has been found that the high gamma activity can be closely tied to neuronal discharge activity in human auditory cortex when subjects listen to naturalistic stimuli <xref rid="pone.0053398-Nir1" ref-type="bibr">[20]</xref>, <xref rid="pone.0053398-Mukamel1" ref-type="bibr">[22]</xref>.</p>
              <p>Most previous studies that reported tracking of speech envelope used the averaged time-locked raw electric potential <xref rid="pone.0053398-Ahissar1" ref-type="bibr">[9]</xref>, <xref rid="pone.0053398-Abrams1" ref-type="bibr">[11]</xref>, <xref rid="pone.0053398-Abrams2" ref-type="bibr">[12]</xref>, <xref rid="pone.0053398-LigeoisChauvel1" ref-type="bibr">[19]</xref>. One of these studies <xref rid="pone.0053398-LigeoisChauvel1" ref-type="bibr">[19]</xref> also used ECoG–-the modality we worked with in the present study. Although that study found large effects in the amplitude of cortical potentials as a function of modulation frequency, it found a relatively uniform representation of envelope information across the studied cortical regions (including primary/secondary auditory cortex, and posterior and anterior parts of superior temporal gyrus). Indeed, these results are congruent to our results when we work with raw electric potential–-this signal tracks speech envelope weakly, and its low sensitivity does not detect the differences in coding of envelope information among the individual cortical regions. Furthermore, this finding is congruent with the result of <xref rid="pone.0053398-Nourski1" ref-type="bibr">[18]</xref>. This study reports that that gamma power measured at ECoG channels implanted in Heschl's gyrus tracks sound envelope significantly more strongly than cortical potentials. However, future work shall elucidate how the results obtained using a time-locked and averaged raw electric potential in previous studies compare to the results obtained using the temporally unconstrained raw potential considered in our study.</p>
              <p>In summary, the speech envelope is an auditory feature that is essential for speech understanding. We provide the first electrophysiological account of the tracking effect in human non-primary auditory regions. Our data reveal that the speech envelope is encoded most strongly relatively early in the auditory pathway, in particular in the belt areas surrounding the auditory core. These regions encode, to a lesser degree, also the envelope of a melody. In comparison, higher-order regions (STG and Broca's region) track the envelope of speech only, thus indicating that these regions encode speech envelope as a phonological–-not purely acoustic–-feature, and they do so only weakly. These results are in line with previous suggestions that these regions specialize in more abstract, high-level (lexical/syntactic/semantic) analysis of speech.</p>
              <p>Looking forward, the high sensitivity of neural signals recorded using electrocorticography to temporal information in the stimulus reported in this study may serve as a powerful tool to study other fine temporal aspects of auditory processing in humans, while providing sufficient spatial detail to characterize the individual cortical regions involved.</p>
            </sec>
            <sec sec-type="methods" id="s4">
              <title>Methods</title>
              <sec id="s4a">
                <title>Subjects</title>
                <p>Five patients with intractable epilepsy, two women (Subjects A and C) and three men (Subjects B, D, and E), participated in this study. All subjects were left language dominant (Wada test). These patients underwent temporary implantation of subdural electrode arrays for the localization of seizure foci prior to surgical resection. <xref ref-type="table" rid="pone-0053398-t001">Table 1</xref> summarizes the subjects' clinical profiles. All subjects gave written informed consent through a protocol reviewed and approved by the Wadsworth Center Institutional Review Board. In all subjects, the seizure focus was localized to the anterior left temporal lobe using visual inspection of ictal ECoG signals <xref rid="pone.0053398-Williams1" ref-type="bibr">[35]</xref>. Prior to resection, the seizure focus was delineated from eloquent auditory and language cortex using electrical cortical stimulation mapping <xref rid="pone.0053398-Ojemann1" ref-type="bibr">[36]</xref>. The implanted electrode grids (Ad-Tech Medical Corp., Racine, WI) consisted of platinum-iridium electrodes, 4 mm in diameter (2.3 mm exposed) with an inter-electrode distance of 10 mm. Each subject had postoperative anterior-posterior and lateral radiographs, as well as computer tomography (CT) scans to verify grid locations.</p>
              </sec>
              <sec id="s4b">
                <title>Auditory Stimuli</title>
                <p>Subjects were asked to listen to a male voice narrating four fictional stories from daily life, which were part of the Boston Aphasia Battery <xref rid="pone.0053398-Goodglass1" ref-type="bibr">[37]</xref>. The fictional stories were 1∶42 minutes (102 s) long, digitized at 44.1 kHz in waveform audio file format, and were binaurally presented to each subject using in-ear monitoring earphones (AKG IP 2, 12 Hz to 23.5 kHz audio bandwidth, 20 dB isolation from environmental noise). The sound volume was set to a comfortable level. The envelope of the stimulus is shown in <xref ref-type="fig" rid="pone-0053398-g001">Figure 1</xref>. The spectrum of the stimulus is shown in <xref ref-type="fig" rid="pone-0053398-g007">Figure 7</xref>. Subjects were also asked to listen to the song Another Brick in the Wall - Part 1 (Pink Floyd, Columbia Records, 1979). The song was 3∶10 minutes long. We chose this song because it features speech-related (lyrics) and speech-unrelated (melody) parts. We extracted from the song periods of singing (0∶41 minutes in total) and periods of instrumentally-carried melody (2∶29 minutes in total). The periods of singing and melody were interleaved in the first 1∶20 minutes of the song. The remaining part of the song consisted of pure melody. We obtained similar results when we considered as the melody stimulus all melody periods (2∶29 minutes in total) or only the last 1∶50 minutes of the song (continuous segment of pure melody). Thus, we used all melody periods. Each stimulus was presented to each subject once and only once. Thus, in our study, data are not averaged over multiple stimulus presentations (referred to as trials in the literature).</p>
              </sec>
              <sec id="s4c">
                <title>Extraction of Sound Features</title>
                <p>We extracted the envelope of a given stimulus by computing the power of the raw sound signal in each time window (consecutive windows of 50 ms duration, no overlap). The length of the analysis window (50 ms) was chosen as short enough to capture the variation in speech envelope, and long enough to allow for meaningful estimation of the gamma component of the neural signal. An example result for the speech stimulus is given in <xref ref-type="fig" rid="pone-0053398-g001">Figure 1</xref>. Separately, for the purpose of <xref ref-type="fig" rid="pone-0053398-g007">Figure 7</xref>, we further extracted the envelope at each frequency of a given stimulus. To do so, we computed spectral power, for each frequency, of the raw sound signal in each time window. To compute spectral power, we used the fast Fourier transform. (Some studies used the Hilbert transform <xref rid="pone.0053398-Ahissar1" ref-type="bibr">[9]</xref>, <xref rid="pone.0053398-Nourski1" ref-type="bibr">[18]</xref> for the same purpose. We opted for the Fourier transform, as in general this method has been the prevalent method when extracting frequency information from sound signals.)</p>
              </sec>
              <sec id="s4d">
                <title>Electrophysiological Recording</title>
                <p>Data collection and stimulus presentation were realized using the general-purpose software BCI2000 <xref rid="pone.0053398-Schalk1" ref-type="bibr">[38]</xref>, <xref rid="pone.0053398-Schalk2" ref-type="bibr">[39]</xref> and g.USBamp biosignal acquisition devices (g.tec Medical Engineering, Schiedlberg, Austria). The g.USBamp devices amplified the ECoG signals, low-pass filtered them at 5000 Hz, digitized them at 38400 Hz, and finally downsampled the result to 1200 Hz. The downsampling step preceded all analyses performed in the paper. Electrodes that clearly did not contain ECoG activity (e.g., due to broken wires, reference location, etc.) were excluded from our analyses (subject A: 1 channel, B: 1, C: 1, D: 2, E: 2).</p>
              </sec>
              <sec id="s4e">
                <title>Cortical Mapping</title>
                <p>We used the software package Curry (Neuroscan Inc., El Paso, TX) to create subject-specific 3D cortical brain models from high resolution pre-op MRI scans, and to co-register the MRIs with post-op CTs and extract the stereotactic coordinates of each grid electrode. We acquired two sets of T1 weighted MRI scans, a sagittal one to define the origin of the coordinate system (i.e., anterior/posterior commissure), and a coronal one to reconstruct the cortical surface. Both scans were acquired on a 1.5 Tesla General Electric MRI scanner with 3 mm and 1 mm slice thickness for the sagittal and coronal scans, respectively. The anatomical and functional (Brodmann) areas of each channel were assigned using the Talairach Atlas daemon <xref rid="pone.0053398-Lancaster1" ref-type="bibr">[40]</xref> (<ext-link ext-link-type="uri" xlink:href="http://www.talairach.org/daemon.html">http://www.talairach.org/daemon.html</ext-link>). Using this procedure, we identified 11 channels in the belt areas surrounding the auditory core (“belt areas” <xref rid="pone.0053398-Wessinger1" ref-type="bibr">[23]</xref>, BA 42, extending to planum temporale, and possibly including the parabelt), subject A: 3 channels, B: 2, C: 3, D: 2, E: 1), 31 channels in BA 22 extending over the superior temporal gyrus (“STG”), and 13 channels in BA 44/45, a part of the posterior inferior frontal gyrus (“Broca's region”). All channels in a given region were included in the analyses, i.e., we do not restrict our analyses solely to channels that show a significant relationship to stimulus envelope. We projected the electrode coordinates onto the reconstructed brain models and generated activation maps using a custom program (<email>jan@eye-hand.wustl.edu</email>) written in Matlab (The Mathworks Inc., Natick MA). Activations were smoothed using a linear kernel falling from 1 to 0 over the distance of 10 mm. In <xref ref-type="fig" rid="pone-0053398-g005">Figure 5</xref> AVG, we plotted neural activations that were averaged over all subjects on a template cortical (pial surface) model. We obtained this model from the source code provided on the AFNI SUMA website (<ext-link ext-link-type="uri" xlink:href="http://afni.nimh.nih.gov/afni/suma">http://afni.nimh.nih.gov/afni/suma</ext-link>). The electrode coordinates for each subject were projected on this model and average activations were computed using the same custom Matlab program. Although the brain model of each subject was expressed in the same Talairach space as the generic AFNI model, each individualized brain model slightly differs from this generic model. The average activation shown in <xref ref-type="fig" rid="pone-0053398-g005">Figure 5</xref> AVG should thus serve only an illustrative purpose. Notably, the electrode locations are determined by lateral skull radiographs to identify the stereotactic coordinates of each grid electrode using automated software <xref rid="pone.0053398-Miller2" ref-type="bibr">[41]</xref>. This procedure inherently leads to several millimeters of variance in the coordinate estimates <xref rid="pone.0053398-Miller2" ref-type="bibr">[41]</xref>. At the same time, the extent of each cortical regions considered in this study was much greater than this expected localization variance. Furthermore, using this procedure, we detected significant differences in the envelope tracking neural effect across the considered regions. Thus, the relatively small variance in the estimation of stereotactic coordinates does not affect our principal results.</p>
              </sec>
              <sec id="s4f">
                <title>Extraction of Neural Activity</title>
                <p>We first pre-processed the raw ECoG signals using a common average reference (CAR) spatial filter (as in <xref rid="pone.0053398-Schalk3" ref-type="bibr">[42]</xref>, <xref rid="pone.0053398-Kubanek1" ref-type="bibr">[43]</xref>). To extract the high gamma activity, we converted the time-series ECoG data into the frequency domain by applying an autoregressive model of order 12 (a different value has minimal effects on the results) to each of the 50 ms time windows. We averaged the obtained spectral amplitudes in the high gamma (75–115 Hz) frequency range. We used this frequency range to match the range of a previous study <xref rid="pone.0053398-Kubanek1" ref-type="bibr">[43]</xref>; this range avoids the frequency of the line noise (60 Hz) and its harmonics (120 Hz, 180 Hz, etc.). Similar results (see <xref ref-type="sec" rid="s2">Results</xref>) are obtained when we used a broader frequency range, 70–500 Hz. Besides activity in the high gamma band, we also computed the raw unrectified ECoG potential, which has previously been shown to correlate with different aspects of motor function <xref rid="pone.0053398-Schalk3" ref-type="bibr">[42]</xref>–<xref rid="pone.0053398-Acharya1" ref-type="bibr">[44]</xref> and labeled local motor potential (LMP), by averaging the raw time-series ECoG samples in each of the 50 ms windows. Notice that the raw potential is a purely time-domain signal, whereas the high gamma signal represents the evolution of the high gamma amplitude over time. Both signals are extracted (in 50 ms windows) from the same raw time-series ECoG samples. For completeness, we computed the potential also as a root-mean-square (rms) value, instead of the mean, over each window. The same principle results hold, including the same frequency profiles (<xref ref-type="fig" rid="pone-0053398-g007">Figure 7</xref>), when we evaluated our data in 100 ms instead of 50 ms windows.</p>
              </sec>
              <sec id="s4g">
                <title>Assessment of Envelope Tracking</title>
                <p>We quantified the relationship between neural activity and stimulus envelope by computing the Spearman correlation <inline-formula><inline-graphic xlink:href="pone.0053398.e124.jpg"/></inline-formula> between these two quantities. To account for the temporal lag between these two quantities, we computed this correlation for each lag between the two signals in the range <inline-formula><inline-graphic xlink:href="pone.0053398.e125.jpg"/></inline-formula> ms to <inline-formula><inline-graphic xlink:href="pone.0053398.e126.jpg"/></inline-formula> ms, in 10 ms steps. Throughout the paper, we report the maximum value of the correlation over this range. The time of this maximum represents the optimal lag between neural activity and stimulus envelope. We estimated the average value of this lag for all channel-envelope pairs that were at least weakly correlated (<inline-formula><inline-graphic xlink:href="pone.0053398.e127.jpg"/></inline-formula>) to ensure that an optimal lag could in principle be found. Neural activity lags, on average, over all stimuli and all frequencies, behind stimulus envelope by 88.6<inline-formula><inline-graphic xlink:href="pone.0053398.e128.jpg"/></inline-formula>3.5 (mean<inline-formula><inline-graphic xlink:href="pone.0053398.e129.jpg"/></inline-formula>SEM) ms in the belt areas, 89.9<inline-formula><inline-graphic xlink:href="pone.0053398.e130.jpg"/></inline-formula>5.9 ms in the STG, and 86.7<inline-formula><inline-graphic xlink:href="pone.0053398.e131.jpg"/></inline-formula>3.0 ms in the Broca's region. Notice that these values are potentially not precise, and thus no conclusions should be drawn based on these values. A more precise value of the lag should be determined in a separate study, by removing the autocorrelation structure in the stimuli, and by carefully comparing only those channels in each area that encode envelope similarly strongly. Throughout the study, <inline-formula><inline-graphic xlink:href="pone.0053398.e132.jpg"/></inline-formula> values are Fisher-transformed prior to any test.</p>
                <p>The nonparametric Spearman's statistic gave similar results (see <xref ref-type="sec" rid="s2">Results</xref>) as the parametric Pearson's. We used the Spearman's statistic because it is potentially more robust and has fewer assumptions about signal properties than the Pearson's statistic.</p>
                <p>To assess the relationship between neural signals and the sound signal, we calculated, for each location in a particular region, the correlation coefficient between the sound signal and the neural signal. We then asked whether this distribution of correlation coefficients is significantly different from zero (see <xref ref-type="sec" rid="s2">Results</xref>). Using this measure, we are able to obtain both a highly significant effect (e.g., for speech in the belt areas, mean <inline-formula><inline-graphic xlink:href="pone.0053398.e133.jpg"/></inline-formula>, <inline-formula><inline-graphic xlink:href="pone.0053398.e134.jpg"/></inline-formula>, <inline-formula><inline-graphic xlink:href="pone.0053398.e135.jpg"/></inline-formula>), as well as no effect that serves as the negative control (e.g., for melody in the STG, mean <inline-formula><inline-graphic xlink:href="pone.0053398.e136.jpg"/></inline-formula>, <inline-formula><inline-graphic xlink:href="pone.0053398.e137.jpg"/></inline-formula>, <inline-formula><inline-graphic xlink:href="pone.0053398.e138.jpg"/></inline-formula>, or for speech in the Broca's region, mean <inline-formula><inline-graphic xlink:href="pone.0053398.e139.jpg"/></inline-formula>, <inline-formula><inline-graphic xlink:href="pone.0053398.e140.jpg"/></inline-formula>, <inline-formula><inline-graphic xlink:href="pone.0053398.e141.jpg"/></inline-formula>).</p>
              </sec>
            </sec>
          </body>
          <back>
            <ref-list>
              <title>References</title>
              <ref id="pone.0053398-Stevens1">
                <label>1</label>
                <mixed-citation publication-type="other">Stevens K (2000) Acoustic phonetics, volume 30. The MIT press.</mixed-citation>
              </ref>
              <ref id="pone.0053398-Rosen1">
                <label>2</label>
                <mixed-citation publication-type="journal"><name><surname>Rosen</surname><given-names>S</given-names></name> (<year>1992</year>) <article-title>Temporal information in speech: acoustic, auditory and linguistic aspects</article-title>. <source>Philos Trans R Soc Lond, B, Biol Sci</source>
<volume>336</volume>: <fpage>367</fpage>–<lpage>73</lpage>.<pub-id pub-id-type="pmid">1354376</pub-id></mixed-citation>
              </ref>
              <ref id="pone.0053398-Zatorre1">
                <label>3</label>
                <mixed-citation publication-type="journal"><name><surname>Zatorre</surname><given-names>RJ</given-names></name>, <name><surname>Belin</surname><given-names>P</given-names></name>, <name><surname>Penhune</surname><given-names>VB</given-names></name> (<year>2002</year>) <article-title>Structure and function of auditory cortex: music and speech</article-title>. <source>Trends Cogn Sci (Regul Ed)</source>
<volume>6</volume>: <fpage>37</fpage>–<lpage>46</lpage>.<pub-id pub-id-type="pmid">11849614</pub-id></mixed-citation>
              </ref>
              <ref id="pone.0053398-Hickok1">
                <label>4</label>
                <mixed-citation publication-type="journal"><name><surname>Hickok</surname><given-names>G</given-names></name>, <name><surname>Poeppel</surname><given-names>D</given-names></name> (<year>2007</year>) <article-title>The cortical organization of speech processing</article-title>. <source>Nat Rev Neurosci</source>
<volume>8</volume>: <fpage>393</fpage>–<lpage>402</lpage>.<pub-id pub-id-type="pmid">17431404</pub-id></mixed-citation>
              </ref>
              <ref id="pone.0053398-Rauschecker1">
                <label>5</label>
                <mixed-citation publication-type="journal"><name><surname>Rauschecker</surname><given-names>JP</given-names></name>, <name><surname>Scott</surname><given-names>SK</given-names></name> (<year>2009</year>) <article-title>Maps and streams in the auditory cortex: nonhuman primates illuminate human speech processing</article-title>. <source>Nature Neuroscience</source>
<volume>12</volume>: <fpage>718</fpage>–<lpage>24</lpage>.<pub-id pub-id-type="pmid">19471271</pub-id></mixed-citation>
              </ref>
              <ref id="pone.0053398-Zeng1">
                <label>6</label>
                <mixed-citation publication-type="journal"><name><surname>Zeng</surname><given-names>FG</given-names></name>, <name><surname>Oba</surname><given-names>S</given-names></name>, <name><surname>Garde</surname><given-names>S</given-names></name>, <name><surname>Sininger</surname><given-names>Y</given-names></name>, <name><surname>Starr</surname><given-names>A</given-names></name> (<year>1999</year>) <article-title>Temporal and speech processing deficits in auditory neuropathy</article-title>. <source>Neuroreport</source>
<volume>10</volume>: <fpage>3429</fpage>–<lpage>35</lpage>.<pub-id pub-id-type="pmid">10599857</pub-id></mixed-citation>
              </ref>
              <ref id="pone.0053398-Drullman1">
                <label>7</label>
                <mixed-citation publication-type="journal"><name><surname>Drullman</surname><given-names>R</given-names></name>, <name><surname>Festen</surname><given-names>JM</given-names></name>, <name><surname>Plomp</surname><given-names>R</given-names></name> (<year>1994</year>) <article-title>Effect of reducing slow temporal modulations on speech reception</article-title>. <source>J Acoust Soc Am</source>
<volume>95</volume>: <fpage>2670</fpage>–<lpage>80</lpage>.<pub-id pub-id-type="pmid">8207140</pub-id></mixed-citation>
              </ref>
              <ref id="pone.0053398-Drullman2">
                <label>8</label>
                <mixed-citation publication-type="journal"><name><surname>Drullman</surname><given-names>R</given-names></name>, <name><surname>Festen</surname><given-names>JM</given-names></name>, <name><surname>Plomp</surname><given-names>R</given-names></name> (<year>1994</year>) <article-title>Effect of temporal envelope smearing on speech reception</article-title>. <source>J Acoust Soc Am</source>
<volume>95</volume>: <fpage>1053</fpage>–<lpage>64</lpage>.<pub-id pub-id-type="pmid">8132899</pub-id></mixed-citation>
              </ref>
              <ref id="pone.0053398-Ahissar1">
                <label>9</label>
                <mixed-citation publication-type="journal"><name><surname>Ahissar</surname><given-names>E</given-names></name>, <name><surname>Nagarajan</surname><given-names>S</given-names></name>, <name><surname>Ahissar</surname><given-names>M</given-names></name>, <name><surname>Protopapas</surname><given-names>A</given-names></name>, <name><surname>Mahncke</surname><given-names>H</given-names></name>, <etal>et al</etal> (<year>2001</year>) <article-title>Speech comprehension is correlated with temporal response patterns recorded from auditory cortex</article-title>. <source>Proc Natl Acad Sci USA</source>
<volume>98</volume>: <fpage>13367</fpage>–<lpage>72</lpage>.<pub-id pub-id-type="pmid">11698688</pub-id></mixed-citation>
              </ref>
              <ref id="pone.0053398-Shannon1">
                <label>10</label>
                <mixed-citation publication-type="journal"><name><surname>Shannon</surname><given-names>RV</given-names></name>, <name><surname>Zeng</surname><given-names>FG</given-names></name>, <name><surname>Kamath</surname><given-names>V</given-names></name>, <name><surname>Wygonski</surname><given-names>J</given-names></name>, <name><surname>Ekelid</surname><given-names>M</given-names></name> (<year>1995</year>) <article-title>Speech recognition with primarily temporal cues</article-title>. <source>Science</source>
<volume>270</volume>: <fpage>303</fpage>–<lpage>4</lpage>.<pub-id pub-id-type="pmid">7569981</pub-id></mixed-citation>
              </ref>
              <ref id="pone.0053398-Abrams1">
                <label>11</label>
                <mixed-citation publication-type="journal"><name><surname>Abrams</surname><given-names>DA</given-names></name>, <name><surname>Nicol</surname><given-names>T</given-names></name>, <name><surname>Zecker</surname><given-names>S</given-names></name>, <name><surname>Kraus</surname><given-names>N</given-names></name> (<year>2008</year>) <article-title>Right-hemisphere auditory cortex is dominant for coding syllable patterns in speech</article-title>. <source>J Neurosci</source>
<volume>28</volume>: <fpage>3958</fpage>–<lpage>65</lpage>.<pub-id pub-id-type="pmid">18400895</pub-id></mixed-citation>
              </ref>
              <ref id="pone.0053398-Abrams2">
                <label>12</label>
                <mixed-citation publication-type="journal"><name><surname>Abrams</surname><given-names>DA</given-names></name>, <name><surname>Nicol</surname><given-names>T</given-names></name>, <name><surname>Zecker</surname><given-names>S</given-names></name>, <name><surname>Kraus</surname><given-names>N</given-names></name> (<year>2009</year>) <article-title>Abnormal cortical processing of the syllable rate of speech in poor readers</article-title>. <source>J Neurosci</source>
<volume>29</volume>: <fpage>7686</fpage>–<lpage>93</lpage>.<pub-id pub-id-type="pmid">19535580</pub-id></mixed-citation>
              </ref>
              <ref id="pone.0053398-Narain1">
                <label>13</label>
                <mixed-citation publication-type="journal"><name><surname>Narain</surname><given-names>C</given-names></name>, <name><surname>Scott</surname><given-names>SK</given-names></name>, <name><surname>Wise</surname><given-names>RJS</given-names></name>, <name><surname>Rosen</surname><given-names>S</given-names></name>, <name><surname>Leff</surname><given-names>A</given-names></name>, <etal>et al</etal> (<year>2003</year>) <article-title>De_ning a left-lateralized response specific to intelligible speech using fMRI</article-title>. <source>Cereb Cortex</source>
<volume>13</volume>: <fpage>1362</fpage>–<lpage>8</lpage>.<pub-id pub-id-type="pmid">14615301</pub-id></mixed-citation>
              </ref>
              <ref id="pone.0053398-Scott1">
                <label>14</label>
                <mixed-citation publication-type="journal"><name><surname>Scott</surname><given-names>SK</given-names></name>, <name><surname>Blank</surname><given-names>CC</given-names></name>, <name><surname>Rosen</surname><given-names>S</given-names></name>, <name><surname>Wise</surname><given-names>RJ</given-names></name> (<year>2000</year>) <article-title>Identification of a pathway for intelligible speech in the left temporal lobe</article-title>. <source>Brain</source>
<volume>123 Pt 12</volume>: <fpage>2400</fpage>–<lpage>6</lpage>.<pub-id pub-id-type="pmid">11099443</pub-id></mixed-citation>
              </ref>
              <ref id="pone.0053398-Boemio1">
                <label>15</label>
                <mixed-citation publication-type="journal"><name><surname>Boemio</surname><given-names>A</given-names></name>, <name><surname>Fromm</surname><given-names>S</given-names></name>, <name><surname>Braun</surname><given-names>A</given-names></name>, <name><surname>Poeppel</surname><given-names>D</given-names></name> (<year>2005</year>) <article-title>Hierarchical and asymmetric temporal sensitivity in human auditory cortices</article-title>. <source>Nature Neuroscience</source>
<volume>8</volume>: <fpage>389</fpage>–<lpage>95</lpage>.<pub-id pub-id-type="pmid">15723061</pub-id></mixed-citation>
              </ref>
              <ref id="pone.0053398-Liebenthal1">
                <label>16</label>
                <mixed-citation publication-type="journal"><name><surname>Liebenthal</surname><given-names>E</given-names></name>, <name><surname>Binder</surname><given-names>JR</given-names></name>, <name><surname>Spitzer</surname><given-names>SM</given-names></name>, <name><surname>Possing</surname><given-names>ET</given-names></name>, <name><surname>Medler</surname><given-names>DA</given-names></name> (<year>2005</year>) <article-title>Neural substrates of phonemic perception</article-title>. <source>Cereb Cortex</source>
<volume>15</volume>: <fpage>1621</fpage>–<lpage>31</lpage>.<pub-id pub-id-type="pmid">15703256</pub-id></mixed-citation>
              </ref>
              <ref id="pone.0053398-Obleser1">
                <label>17</label>
                <mixed-citation publication-type="journal"><name><surname>Obleser</surname><given-names>J</given-names></name>, <name><surname>Zimmermann</surname><given-names>J</given-names></name>, <name><surname>Meter</surname><given-names>JV</given-names></name>, <name><surname>Rauschecker</surname><given-names>JP</given-names></name> (<year>2007</year>) <article-title>Multiple stages of auditory speech perception reected in event-related fMRI</article-title>. <source>Cereb Cortex</source>
<volume>17</volume>: <fpage>2251</fpage>–<lpage>7</lpage>.<pub-id pub-id-type="pmid">17150986</pub-id></mixed-citation>
              </ref>
              <ref id="pone.0053398-Nourski1">
                <label>18</label>
                <mixed-citation publication-type="journal"><name><surname>Nourski</surname><given-names>KV</given-names></name>, <name><surname>Reale</surname><given-names>RA</given-names></name>, <name><surname>Oya</surname><given-names>H</given-names></name>, <name><surname>Kawasaki</surname><given-names>H</given-names></name>, <name><surname>Kovach</surname><given-names>CK</given-names></name>, <etal>et al</etal> (<year>2009</year>) <article-title>Temporal envelope of time-compressed speech represented in the human auditory cortex</article-title>. <source>J Neurosci</source>
<volume>29</volume>: <fpage>15564</fpage>–<lpage>74</lpage>.<pub-id pub-id-type="pmid">20007480</pub-id></mixed-citation>
              </ref>
              <ref id="pone.0053398-LigeoisChauvel1">
                <label>19</label>
                <mixed-citation publication-type="journal"><name><surname>Liégeois-Chauvel</surname><given-names>C</given-names></name>, <name><surname>Lorenzi</surname><given-names>C</given-names></name>, <name><surname>Trébuchon</surname><given-names>A</given-names></name>, <name><surname>Régis</surname><given-names>J</given-names></name>, <name><surname>Chauvel</surname><given-names>P</given-names></name> (<year>2004</year>) <article-title>Temporal envelope pro-cessing in the human left and right auditory cortices</article-title>. <source>Cereb Cortex</source>
<volume>14</volume>: <fpage>731</fpage>–<lpage>40</lpage>.<pub-id pub-id-type="pmid">15054052</pub-id></mixed-citation>
              </ref>
              <ref id="pone.0053398-Nir1">
                <label>20</label>
                <mixed-citation publication-type="journal"><name><surname>Nir</surname><given-names>Y</given-names></name>, <name><surname>Fisch</surname><given-names>L</given-names></name>, <name><surname>Mukamel</surname><given-names>R</given-names></name>, <name><surname>Gelbard-Sagiv</surname><given-names>H</given-names></name>, <name><surname>Arieli</surname><given-names>A</given-names></name>, <etal>et al</etal> (<year>2007</year>) <article-title>Coupling between neuronal firing rate, gamma lfp, and bold fmri is related to interneuronal correlations</article-title>. <source>Current Biology</source>
<volume>17</volume>: <fpage>1275</fpage>–<lpage>1285</lpage>.<pub-id pub-id-type="pmid">17686438</pub-id></mixed-citation>
              </ref>
              <ref id="pone.0053398-Miller1">
                <label>21</label>
                <mixed-citation publication-type="journal"><name><surname>Miller</surname><given-names>KJ</given-names></name> (<year>2010</year>) <article-title>Broadband spectral change: evidence for a macroscale correlate of population firing rate</article-title>? <source>J Neurosci</source>
<volume>30</volume>: <fpage>6477</fpage>–<lpage>9</lpage>.<pub-id pub-id-type="pmid">20463210</pub-id></mixed-citation>
              </ref>
              <ref id="pone.0053398-Mukamel1">
                <label>22</label>
                <mixed-citation publication-type="journal"><name><surname>Mukamel</surname><given-names>R</given-names></name>, <name><surname>Nir</surname><given-names>Y</given-names></name>, <name><surname>Harel</surname><given-names>M</given-names></name>, <name><surname>Arieli</surname><given-names>A</given-names></name>, <name><surname>Malach</surname><given-names>R</given-names></name>, <etal>et al</etal> (<year>2011</year>) <article-title>Invariance of firing rate and field potential dynamics to stimulus modulation rate in human auditory cortex</article-title>. <source>Human brain mapping</source>
<volume>32</volume>: <fpage>1181</fpage>–<lpage>1193</lpage>.<pub-id pub-id-type="pmid">20665720</pub-id></mixed-citation>
              </ref>
              <ref id="pone.0053398-Wessinger1">
                <label>23</label>
                <mixed-citation publication-type="journal"><name><surname>Wessinger</surname><given-names>C</given-names></name>, <name><surname>VanMeter</surname><given-names>J</given-names></name>, <name><surname>Tian</surname><given-names>B</given-names></name>, <name><surname>Van Lare</surname><given-names>J</given-names></name>, <name><surname>Pekar</surname><given-names>J</given-names></name>, <etal>et al</etal> (<year>2001</year>) <article-title>Hierarchical organization of the human auditory cortex revealed by functional magnetic resonance imaging</article-title>. <source>Journal of Cognitive Neuroscience</source>
<volume>13</volume>: <fpage>1</fpage>–<lpage>7</lpage>.<pub-id pub-id-type="pmid">11224904</pub-id></mixed-citation>
              </ref>
              <ref id="pone.0053398-Wernicke1">
                <label>24</label>
                <mixed-citation publication-type="other">Wernicke C (1874) Der aphasische Symptomencomplex: Eine psychologische Studie auf anatomis-cher Basis. Cohn &amp; Weigert, Breslau, Germany.</mixed-citation>
              </ref>
              <ref id="pone.0053398-Geschwind1">
                <label>25</label>
                <mixed-citation publication-type="journal"><name><surname>Geschwind</surname><given-names>N</given-names></name> (<year>1965</year>) <article-title>Disconnexion syndromes in animals and man. i</article-title>. <source>Brain</source>
<volume>88</volume>: <fpage>237</fpage>–<lpage>94</lpage>.<pub-id pub-id-type="pmid">5318481</pub-id></mixed-citation>
              </ref>
              <ref id="pone.0053398-Geschwind2">
                <label>26</label>
                <mixed-citation publication-type="journal"><name><surname>Geschwind</surname><given-names>N</given-names></name> (<year>1965</year>) <article-title>Disconnexion syndromes in animals and man. ii</article-title>. <source>Brain</source>
<volume>88</volume>: <fpage>585</fpage>–<lpage>644</lpage>.<pub-id pub-id-type="pmid">5318824</pub-id></mixed-citation>
              </ref>
              <ref id="pone.0053398-Broca1">
                <label>27</label>
                <mixed-citation publication-type="journal"><name><surname>Broca</surname><given-names>P</given-names></name> (<year>1861</year>) <article-title>Remarques sur le siège de la faculté du language articulé: suivies d'une observation d'aphémie</article-title>. <source>Bull Soc Anat Paris</source>
<volume>6</volume>: <fpage>330357</fpage>.</mixed-citation>
              </ref>
              <ref id="pone.0053398-Schwartz1">
                <label>28</label>
                <mixed-citation publication-type="journal"><name><surname>Schwartz</surname><given-names>DA</given-names></name>, <name><surname>Howe</surname><given-names>CQ</given-names></name>, <name><surname>Purves</surname><given-names>D</given-names></name> (<year>2003</year>) <article-title>The statistical structure of human speech sounds predicts musical universals</article-title>. <source>J Neurosci</source>
<volume>23</volume>: <fpage>7160</fpage>–<lpage>8</lpage>.<pub-id pub-id-type="pmid">12904476</pub-id></mixed-citation>
              </ref>
              <ref id="pone.0053398-Binder1">
                <label>29</label>
                <mixed-citation publication-type="journal"><name><surname>Binder</surname><given-names>JR</given-names></name>, <name><surname>Frost</surname><given-names>JA</given-names></name>, <name><surname>Hammeke</surname><given-names>TA</given-names></name>, <name><surname>Bellgowan</surname><given-names>PS</given-names></name>, <name><surname>Springer</surname><given-names>JA</given-names></name>, <etal>et al</etal> (<year>2000</year>) <article-title>Human temporal lobe activation by speech and nonspeech sounds</article-title>. <source>Cereb Cortex</source>
<volume>10</volume>: <fpage>512</fpage>–<lpage>28</lpage>.<pub-id pub-id-type="pmid">10847601</pub-id></mixed-citation>
              </ref>
              <ref id="pone.0053398-Hyde1">
                <label>30</label>
                <mixed-citation publication-type="journal"><name><surname>Hyde</surname><given-names>KL</given-names></name>, <name><surname>Peretz</surname><given-names>I</given-names></name>, <name><surname>Zatorre</surname><given-names>RJ</given-names></name> (<year>2008</year>) <article-title>Evidence for the role of the right auditory cortex in fine pitch resolution</article-title>. <source>Neuropsychologia</source>
<volume>46</volume>: <fpage>632</fpage>–<lpage>9</lpage>.<pub-id pub-id-type="pmid">17959204</pub-id></mixed-citation>
              </ref>
              <ref id="pone.0053398-Gehr1">
                <label>31</label>
                <mixed-citation publication-type="journal"><name><surname>Gehr</surname><given-names>DD</given-names></name>, <name><surname>Komiya</surname><given-names>H</given-names></name>, <name><surname>Eggermont</surname><given-names>JJ</given-names></name> (<year>2000</year>) <article-title>Neuronal responses in cat primary auditory cortex to natural and altered species-specific calls</article-title>. <source>Hear Res</source>
<volume>150</volume>: <fpage>27</fpage>–<lpage>42</lpage>.<pub-id pub-id-type="pmid">11077191</pub-id></mixed-citation>
              </ref>
              <ref id="pone.0053398-Gourvitch1">
                <label>32</label>
                <mixed-citation publication-type="journal"><name><surname>Gourévitch</surname><given-names>B</given-names></name>, <name><surname>Eggermont</surname><given-names>JJ</given-names></name> (<year>2007</year>) <article-title>Spatial representation of neural responses to natural and altered conspecific vocalizations in cat auditory cortex</article-title>. <source>J Neurophysiol</source>
<volume>97</volume>: <fpage>144</fpage>–<lpage>58</lpage>.<pub-id pub-id-type="pmid">17021022</pub-id></mixed-citation>
              </ref>
              <ref id="pone.0053398-Wang1">
                <label>33</label>
                <mixed-citation publication-type="journal"><name><surname>Wang</surname><given-names>X</given-names></name>, <name><surname>Merzenich</surname><given-names>MM</given-names></name>, <name><surname>Beitel</surname><given-names>R</given-names></name>, <name><surname>Schreiner</surname><given-names>CE</given-names></name> (<year>1995</year>) <article-title>Representation of a species-specific vocal-ization in the primary auditory cortex of the common marmoset: temporal and spectral charac-teristics</article-title>. <source>J Neurophysiol</source>
<volume>74</volume>: <fpage>2685</fpage>–<lpage>706</lpage>.<pub-id pub-id-type="pmid">8747224</pub-id></mixed-citation>
              </ref>
              <ref id="pone.0053398-Nagarajan1">
                <label>34</label>
                <mixed-citation publication-type="journal"><name><surname>Nagarajan</surname><given-names>SS</given-names></name>, <name><surname>Cheung</surname><given-names>SW</given-names></name>, <name><surname>Bedenbaugh</surname><given-names>P</given-names></name>, <name><surname>Beitel</surname><given-names>RE</given-names></name>, <name><surname>Schreiner</surname><given-names>CE</given-names></name>, <etal>et al</etal> (<year>2002</year>) <article-title>Representation of spectral and temporal envelope of twitter vocalizations in common marmoset primary auditory cortex</article-title>. <source>J Neurophysiol</source>
<volume>87</volume>: <fpage>1723</fpage>–<lpage>37</lpage>.<pub-id pub-id-type="pmid">11929894</pub-id></mixed-citation>
              </ref>
              <ref id="pone.0053398-Williams1">
                <label>35</label>
                <mixed-citation publication-type="journal"><name><surname>Williams</surname><given-names>D</given-names></name>, <name><surname>Gibbs</surname><given-names>F</given-names></name> (<year>1938</year>) <article-title>The localization of intracranial lesions by electroencephalography</article-title>. <source>New England Journal of Medicine</source>
<volume>218</volume>: <fpage>998</fpage>–<lpage>1002</lpage>.</mixed-citation>
              </ref>
              <ref id="pone.0053398-Ojemann1">
                <label>36</label>
                <mixed-citation publication-type="journal"><name><surname>Ojemann</surname><given-names>G</given-names></name>, <name><surname>Ojemann</surname><given-names>J</given-names></name>, <name><surname>Lettich</surname><given-names>E</given-names></name>, <name><surname>Berger</surname><given-names>M</given-names></name> (<year>1989</year>) <article-title>Cortical language localization in left, dominant hemisphere. an electrical stimulation mapping investigation in 117 patients</article-title>. <source>J Neurosurg</source>
<volume>71</volume>: <fpage>316</fpage>–<lpage>26</lpage>.<pub-id pub-id-type="pmid">2769383</pub-id></mixed-citation>
              </ref>
              <ref id="pone.0053398-Goodglass1">
                <label>37</label>
                <mixed-citation publication-type="other">Goodglass H, Kaplan E, Barresi B (1983) Boston diagnostic aphasia examination (BDAE). Philadelphia: Lea and Febiger.</mixed-citation>
              </ref>
              <ref id="pone.0053398-Schalk1">
                <label>38</label>
                <mixed-citation publication-type="journal"><name><surname>Schalk</surname><given-names>G</given-names></name>, <name><surname>McFarland</surname><given-names>DJ</given-names></name>, <name><surname>Hinterberger</surname><given-names>T</given-names></name>, <name><surname>Birbaumer</surname><given-names>N</given-names></name>, <name><surname>Wolpaw</surname><given-names>JR</given-names></name> (<year>2004</year>) <article-title>BCI2000: a general-purpose brain-computer interface (BCI) system</article-title>. <source>IEEE transactions on bio-medical engineering</source>
<volume>51</volume>: <fpage>1034</fpage>–<lpage>43</lpage>.<pub-id pub-id-type="pmid">15188875</pub-id></mixed-citation>
              </ref>
              <ref id="pone.0053398-Schalk2">
                <label>39</label>
                <mixed-citation publication-type="other">Schalk G, Mellinger J (2010) A Practical Guide to Brain-Computer Interfacing with BCI2000. London, UK: Springer, 1st edition.</mixed-citation>
              </ref>
              <ref id="pone.0053398-Lancaster1">
                <label>40</label>
                <mixed-citation publication-type="journal"><name><surname>Lancaster</surname><given-names>JL</given-names></name>, <name><surname>Woldorff</surname><given-names>MG</given-names></name>, <name><surname>Parsons</surname><given-names>LM</given-names></name>, <name><surname>Liotti</surname><given-names>M</given-names></name>, <name><surname>Freitas</surname><given-names>CS</given-names></name>, <etal>et al</etal> (<year>2000</year>) <article-title>Automated Talairach atlas labels for functional brain mapping</article-title>. <source>Hum Brain Mapp</source>
<volume>10</volume>: <fpage>120</fpage>–<lpage>31</lpage>.<pub-id pub-id-type="pmid">10912591</pub-id></mixed-citation>
              </ref>
              <ref id="pone.0053398-Miller2">
                <label>41</label>
                <mixed-citation publication-type="journal"><name><surname>Miller</surname><given-names>K</given-names></name>, <name><surname>Makeig</surname><given-names>S</given-names></name>, <name><surname>Hebb</surname><given-names>A</given-names></name>, <name><surname>Rao</surname><given-names>R</given-names></name>, <name><surname>dennijs</surname><given-names>M</given-names></name>, <etal>et al</etal> (<year>2007</year>) <article-title>Cortical electrode localization from x-rays and simple mapping for electrocorticographic research: The location on cortex(loc) package for matlab</article-title>. <source>Journal of neuroscience methods</source>
<volume>162</volume>: <fpage>303</fpage>–<lpage>308</lpage>.<pub-id pub-id-type="pmid">17343918</pub-id></mixed-citation>
              </ref>
              <ref id="pone.0053398-Schalk3">
                <label>42</label>
                <mixed-citation publication-type="journal"><name><surname>Schalk</surname><given-names>G</given-names></name>, <name><surname>Kubanek</surname><given-names>J</given-names></name>, <name><surname>Miller</surname><given-names>KJ</given-names></name>, <name><surname>Anderson</surname><given-names>NR</given-names></name>, <name><surname>Leuthardt</surname><given-names>EC</given-names></name>, <etal>et al</etal> (<year>2007</year>) <article-title>Decoding two-dimensional movement trajectories using electrocorticographic signals in humans</article-title>. <source>Journal of Neural Engineering</source>
<volume>4</volume>: <fpage>264</fpage>.<pub-id pub-id-type="pmid">17873429</pub-id></mixed-citation>
              </ref>
              <ref id="pone.0053398-Kubanek1">
                <label>43</label>
                <mixed-citation publication-type="journal"><name><surname>Kubanek</surname><given-names>J</given-names></name>, <name><surname>Miller</surname><given-names>KJ</given-names></name>, <name><surname>Ojemann</surname><given-names>JG</given-names></name>, <name><surname>Wolpaw</surname><given-names>JR</given-names></name>, <name><surname>Schalk</surname><given-names>G</given-names></name> (<year>2009</year>) <article-title>Decoding exion of individual fingers using electrocorticographic signals in humans</article-title>. <source>J Neural Eng</source>
<volume>6</volume>: <fpage>066001</fpage>.<pub-id pub-id-type="pmid">19794237</pub-id></mixed-citation>
              </ref>
              <ref id="pone.0053398-Acharya1">
                <label>44</label>
                <mixed-citation publication-type="journal"><name><surname>Acharya</surname><given-names>S</given-names></name>, <name><surname>Fifer</surname><given-names>MS</given-names></name>, <name><surname>Benz</surname><given-names>HL</given-names></name>, <name><surname>Crone</surname><given-names>NE</given-names></name>, <name><surname>Thakor</surname><given-names>NV</given-names></name> (<year>2010</year>) <article-title>Electrocorticographic amplitude predicts finger positions during slow grasping motions of the hand</article-title>. <source>J Neural Eng</source>
<volume>7</volume>: <fpage>046002</fpage>–<lpage>046002</lpage>.<pub-id pub-id-type="pmid">20489239</pub-id></mixed-citation>
              </ref>
              <ref id="pone.0053398-Wechsler1">
                <label>45</label>
                <mixed-citation publication-type="other">Wechsler D (1997) Wechsler adult intelligence scale-(wais-iii). San Antonio, TX: Psychological Corporation.</mixed-citation>
              </ref>
            </ref-list>
          </back>
        </article>
      </metadata>
    </record>
  </GetRecord>
</OAI-PMH>
