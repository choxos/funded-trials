<?xml version="1.0" encoding="UTF-8"?>
<OAI-PMH xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/ http://www.openarchives.org/OAI/2.0/OAI-PMH.xsd">
  <responseDate>2024-05-03T02:31:10Z</responseDate>
  <request verb="GetRecord" identifier="oai:pubmedcentral.nih.gov:9314349" metadataPrefix="pmc">https:/www.ncbi.nlm.nih.gov/pmc/oai/oai.cgi</request>
  <GetRecord>
    <record>
      <header>
        <identifier>oai:pubmedcentral.nih.gov:9314349</identifier>
        <datestamp>2022-07-27</datestamp>
        <setSpec>scirep</setSpec>
        <setSpec>pmc-open</setSpec>
      </header>
      <metadata>
        <article xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:mml="http://www.w3.org/1998/Math/MathML" xsi:schemaLocation="https://jats.nlm.nih.gov/ns/archiving/1.3/ https://jats.nlm.nih.gov/archiving/1.3/xsd/JATS-archivearticle1-3.xsd" article-type="research-article" dtd-version="1.3">
          <processing-meta base-tagset="archiving" mathml-version="3.0" table-model="xhtml" tagset-family="jats">
            <restricted-by>pmc</restricted-by>
          </processing-meta>
          <front>
            <journal-meta>
              <journal-id journal-id-type="nlm-ta">Sci Rep</journal-id>
              <journal-id journal-id-type="iso-abbrev">Sci Rep</journal-id>
              <journal-title-group>
                <journal-title>Scientific Reports</journal-title>
              </journal-title-group>
              <issn pub-type="epub">2045-2322</issn>
              <publisher>
                <publisher-name>Nature Publishing Group UK</publisher-name>
                <publisher-loc>London</publisher-loc>
              </publisher>
            </journal-meta>
            <article-meta>
              <article-id pub-id-type="accession">PMC9314349</article-id>
              <article-id pub-id-type="pmcid">PMC9314349</article-id>
              <article-id pub-id-type="pmc-uid">9314349</article-id>
              <article-id pub-id-type="pmid">35879360</article-id>
              <article-id pub-id-type="publisher-id">15649</article-id>
              <article-id pub-id-type="doi">10.1038/s41598-022-15649-x</article-id>
              <article-categories>
                <subj-group subj-group-type="heading">
                  <subject>Article</subject>
                </subj-group>
              </article-categories>
              <title-group>
                <article-title>Change-detection training and its effects on visual processing skills</article-title>
              </title-group>
              <contrib-group>
                <contrib contrib-type="author" corresp="yes">
                  <name>
                    <surname>Truong</surname>
                    <given-names>Jennifer</given-names>
                  </name>
                  <address>
                    <email>jennit9@uci.edu</email>
                  </address>
                  <xref ref-type="aff" rid="Aff1">1</xref>
                </contrib>
                <contrib contrib-type="author">
                  <name>
                    <surname>Buschkuehl</surname>
                    <given-names>Martin</given-names>
                  </name>
                  <xref ref-type="aff" rid="Aff2">2</xref>
                </contrib>
                <contrib contrib-type="author">
                  <name>
                    <surname>Smith-Peirce</surname>
                    <given-names>Rachel N.</given-names>
                  </name>
                  <xref ref-type="aff" rid="Aff1">1</xref>
                </contrib>
                <contrib contrib-type="author">
                  <name>
                    <surname>Carrillo</surname>
                    <given-names>Audrey A.</given-names>
                  </name>
                  <xref ref-type="aff" rid="Aff3">3</xref>
                </contrib>
                <contrib contrib-type="author">
                  <name>
                    <surname>Seitz</surname>
                    <given-names>Aaron R.</given-names>
                  </name>
                  <xref ref-type="aff" rid="Aff3">3</xref>
                </contrib>
                <contrib contrib-type="author" corresp="yes">
                  <name>
                    <surname>Jaeggi</surname>
                    <given-names>Susanne M.</given-names>
                  </name>
                  <address>
                    <email>smjaeggi@uci.edu</email>
                  </address>
                  <xref ref-type="aff" rid="Aff1">1</xref>
                </contrib>
                <aff id="Aff1"><label>1</label><institution-wrap><institution-id institution-id-type="GRID">grid.266093.8</institution-id><institution-id institution-id-type="ISNI">0000 0001 0668 7243</institution-id><institution>School of Education, </institution><institution>University of California-Irvine, </institution></institution-wrap>Irvine, CA USA </aff>
                <aff id="Aff2"><label>2</label><institution-wrap><institution-id institution-id-type="GRID">grid.429635.f</institution-id><institution-id institution-id-type="ISNI">0000 0004 6023 2129</institution-id><institution>MIND Research Institute, </institution></institution-wrap>Irvine, CA USA </aff>
                <aff id="Aff3"><label>3</label><institution-wrap><institution-id institution-id-type="GRID">grid.266097.c</institution-id><institution-id institution-id-type="ISNI">0000 0001 2222 1582</institution-id><institution>Department of Psychology, </institution><institution>University of California-Riverside, </institution></institution-wrap>Riverside, CA USA </aff>
              </contrib-group>
              <pub-date pub-type="epub">
                <day>25</day>
                <month>7</month>
                <year>2022</year>
              </pub-date>
              <pub-date pub-type="pmc-release">
                <day>25</day>
                <month>7</month>
                <year>2022</year>
              </pub-date>
              <pub-date pub-type="collection">
                <year>2022</year>
              </pub-date>
              <volume>12</volume>
              <elocation-id>12646</elocation-id>
              <history>
                <date date-type="received">
                  <day>28</day>
                  <month>1</month>
                  <year>2022</year>
                </date>
                <date date-type="accepted">
                  <day>27</day>
                  <month>6</month>
                  <year>2022</year>
                </date>
              </history>
              <permissions>
                <copyright-statement>© The Author(s) 2022</copyright-statement>
                <license>
                  <ali:license_ref specific-use="textmining" content-type="ccbylicense">https://creativecommons.org/licenses/by/4.0/</ali:license_ref>
                  <license-p><bold>Open Access</bold> This article is licensed under a Creative Commons Attribution 4.0 International License, which permits use, sharing, adaptation, distribution and reproduction in any medium or format, as long as you give appropriate credit to the original author(s) and the source, provide a link to the Creative Commons licence, and indicate if changes were made. The images or other third party material in this article are included in the article's Creative Commons licence, unless indicated otherwise in a credit line to the material. If material is not included in the article's Creative Commons licence and your intended use is not permitted by statutory regulation or exceeds the permitted use, you will need to obtain permission directly from the copyright holder. To view a copy of this licence, visit <ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by/4.0/">http://creativecommons.org/licenses/by/4.0/</ext-link>.</license-p>
                </license>
              </permissions>
              <abstract id="Abs1">
                <p id="Par1">Previous cognitive training research with the change-detection paradigm found only sparse effects that went beyond improvements in the training task but stressed an increase in fidelity of internal memory representations. Motivated by the demanding visual processing requirements of change-detection training, we extended this work by focusing on whether training on a change-detection task would improve visual processing skills. Fifty participants were randomly assigned to train on a change-detection task or on a control task for seven sessions. Participants’ visual processing skills were assessed before and after the intervention, focusing on visual search, contrast sensitivity, and contour integration. Our results suggest a general improvement in perceptual skills that was primarily driven by a conjunction search task and to a much lesser extent by a complex visual search task and a contrast sensitivity task. The data from the conjunction search task further suggest a causal link between training and improvements of perceptual as opposed to attentional processes. Since the change-detection paradigm is commonly used to assess working memory capacity, future research needs to investigate how much of its variance is explained by memory performance and how much is explained by perceptual processes.</p>
              </abstract>
              <kwd-group kwd-group-type="npg-subject">
                <title>Subject terms</title>
                <kwd>Human behaviour</kwd>
                <kwd>Learning and memory</kwd>
                <kwd>Sensory processing</kwd>
              </kwd-group>
              <funding-group>
                <award-group>
                  <funding-source>
                    <institution>UC Irvine Undergraduate Research Opportunities Program (UROP)</institution>
                  </funding-source>
                </award-group>
              </funding-group>
              <funding-group>
                <award-group>
                  <funding-source>
                    <institution>UC Irvine Summer Undergraduate Research Program (SURP)</institution>
                  </funding-source>
                </award-group>
              </funding-group>
              <funding-group>
                <award-group>
                  <funding-source>
                    <institution-wrap>
                      <institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100000049</institution-id>
                      <institution>National Institute on Aging</institution>
                    </institution-wrap>
                  </funding-source>
                  <award-id>1K02AG054665</award-id>
                  <principal-award-recipient>
                    <name>
                      <surname>Jaeggi</surname>
                      <given-names>Susanne M.</given-names>
                    </name>
                  </principal-award-recipient>
                </award-group>
              </funding-group>
              <custom-meta-group>
                <custom-meta>
                  <meta-name>issue-copyright-statement</meta-name>
                  <meta-value>© The Author(s) 2022</meta-value>
                </custom-meta>
              </custom-meta-group>
            </article-meta>
          </front>
          <body>
            <sec id="Sec1">
              <title>Introduction</title>
              <p id="Par2">Detecting visual change in the environment is very important and in its most dramatic case even makes a difference over life and death, for example by detecting threats in the form of predators or oncoming traffic. A popular laboratory paradigm to assess change-detection performance focuses on the capacity of visual short-term memory. It presents participants with an initial set of stimuli such as colored squares<sup><xref ref-type="bibr" rid="CR1">1</xref></sup> or blocks oriented at different angles<sup><xref ref-type="bibr" rid="CR2">2</xref></sup>. After a brief presentation of these stimuli, followed by a pause, the task requires to determine whether a probe is identical to the initial set or not. By varying the number of stimuli, memory capacity can be assessed. The task has been a preferred means to assess working memory capacity because it is assumed that strategies such as rehearsal only minimally affect performance<sup><xref ref-type="bibr" rid="CR1">1</xref></sup>. It has further been argued that individual differences remain stable across an extended period of time<sup><xref ref-type="bibr" rid="CR3">3</xref></sup>, that change detection represents a stable trait<sup><xref ref-type="bibr" rid="CR4">4</xref>,<xref ref-type="bibr" rid="CR5">5</xref></sup>, and that change-detection performance correlates well with measures of intelligence<sup><xref ref-type="bibr" rid="CR6">6</xref></sup>. Nonetheless, previous training research has revealed that participants are able to considerably improve their performance in such change-detection tasks over time<sup><xref ref-type="bibr" rid="CR2">2</xref>,<xref ref-type="bibr" rid="CR7">7</xref>,<xref ref-type="bibr" rid="CR8">8</xref></sup>. Despite limited evidence that training on change-detection tasks impacts performance in untrained measures<sup><xref ref-type="bibr" rid="CR7">7</xref></sup>, there is accumulating evidence indicating that change-detection training improves the fidelity of internal memory representations<sup><xref ref-type="bibr" rid="CR2">2</xref>,<xref ref-type="bibr" rid="CR9">9</xref>,<xref ref-type="bibr" rid="CR10">10</xref></sup>, but see<sup><xref ref-type="bibr" rid="CR7">7</xref></sup> and furthermore, this literature also points towards an increase in visual working memory capacity<sup><xref ref-type="bibr" rid="CR2">2</xref>,<xref ref-type="bibr" rid="CR7">7</xref>,<xref ref-type="bibr" rid="CR8">8</xref>,<xref ref-type="bibr" rid="CR10">10</xref></sup>, but see<sup><xref ref-type="bibr" rid="CR11">11</xref></sup>.</p>
              <p id="Par3">Although the change-detection task is mostly described as a measure to assess visual memory capacity, the task also requires participants to rapidly process visual stimuli. As such, change-detection training will potentially not only foster visual working memory capacity, but more generally, processes related to processing speed and perceptual learning<sup><xref ref-type="bibr" rid="CR12">12</xref></sup>. Specifically, when using a change-detection task for training where the number of stimuli is adaptively increased while holding the presentation rate constant, an increasingly higher rate of processing is required for successful task completion<sup><xref ref-type="bibr" rid="CR7">7</xref></sup>. It has been argued that higher visual processing speed may be the basis for generally improved visual functions<sup><xref ref-type="bibr" rid="CR13">13</xref>–<xref ref-type="bibr" rid="CR16">16</xref></sup>. This line of research has focused on training contrast sensitivity using Gabor patches as stimuli and has repeatedly resulted in generalizing effects going beyond the specific training paradigm<sup><xref ref-type="bibr" rid="CR16">16</xref>,<xref ref-type="bibr" rid="CR17">17</xref></sup>. Such generalizing effects led to the emerging view that perceptual learning does not exclusively lead to improvements of low-level processes, but instead, involves optimization of an extensive array of brain networks<sup><xref ref-type="bibr" rid="CR12">12</xref></sup>. The goal of the present study was to explore whether targeted training on a change-detection task has the potential to improve specific visual perceptual processes, and as such, generalizes beyond improvements in memory representations and memory capacity. For that purpose, we focused on potential transfer to perceptual processes that to our knowledge have not received any attention in this training literature. In a first step to investigate such transfer, we focused on visual figure-ground discrimination, contrast sensitivity, and contour integration.</p>
              <p id="Par4"><italic>Visual figure-ground discrimination</italic> is the concept of being able to discriminate an object from its background<sup><xref ref-type="bibr" rid="CR18">18</xref>,<xref ref-type="bibr" rid="CR19">19</xref></sup>. Visual search is considered a sub-skill of visual discrimination and refers to the process of detecting a target among distractors<sup><xref ref-type="bibr" rid="CR20">20</xref></sup>. Performance on a visual search task directly depends on visual processing speed, that is, the faster an individual is able to process individual stimuli, the better an individual’s performance. It has also been shown that visual search tasks are malleable, and susceptible to training<sup><xref ref-type="bibr" rid="CR21">21</xref></sup>. Another factor that determines performance on a visual search task is working memory. Several studies have shown considerable similarities between visual search and working memory<sup><xref ref-type="bibr" rid="CR22">22</xref>,<xref ref-type="bibr" rid="CR23">23</xref></sup>. For example, Emrich et al.<sup><xref ref-type="bibr" rid="CR24">24</xref></sup> asked their participants to perform a visual search task and a visual working memory task. During both tasks, the authors recorded contralateral delay activity (CDA) levels via an electroencephalogram (EEG). Results showed that the CDA amplitudes were comparable in both tasks, suggesting a similar underlying neural mechanism. Using CDA amplitudes as well, Luria and Vogel<sup><xref ref-type="bibr" rid="CR25">25</xref></sup> found that an increase of visual search task difficulty corresponded with an increase of CDA amplitudes, indicating an increased reliance on working memory capacity to perform the visual search task. This finding corresponds well with the notion that visual working memory capacity is critical to perform well on visual search tasks due to the need to remember the visual target, compare target information with distractor information, and organize the search array<sup><xref ref-type="bibr" rid="CR26">26</xref>,<xref ref-type="bibr" rid="CR27">27</xref></sup>. While the reason for any potential transfer to visual search tasks can emerge due to perceptual learning and/or induced changes to the working memory system, the visual search task data will allow us to make inferences whether any effects are based on improved perceptual processes and/or increased search efficiency (see also below).</p>
              <p id="Par5"><italic>Contrast sensitivity</italic> refers to the skill to perceive small differences in luminescence between an object and its background<sup><xref ref-type="bibr" rid="CR28">28</xref></sup>. Often operationalized with Gabor stimuli, contrast sensitivity tasks have often been used as a training vehicle<sup><xref ref-type="bibr" rid="CR17">17</xref></sup>. The data from several studies provide evidence for a link between working memory and contrast features in visual processing. For example, Xing et al.<sup><xref ref-type="bibr" rid="CR29">29</xref></sup> were able to decode the content of working memory by analyzing the brain activation pattern of the sensory cortices by means of functional Magnetic Resonance Imaging (fMRI). While such decoding has been shown before for orientation features of visual stimuli<sup><xref ref-type="bibr" rid="CR30">30</xref></sup>, Xing et al.<sup><xref ref-type="bibr" rid="CR29">29</xref></sup> were able to do this for stimuli that differed in contrast. Despite a possible connection between contrast sensitivity and working memory, our operationalization to assess contrast sensitivity did not include a working memory component and therefore, any potential transfer in this task is likely to be deemed of a perceptual nature.</p>
              <p id="Par6"><italic>Contour integration</italic> is based on the Gestalt law of “good continuation”, referring to the preference in viewing an object as something that is smooth and continuous rather than something that is abrupt and disconnected through the integration of local elements to form a global contour<sup><xref ref-type="bibr" rid="CR31">31</xref>,<xref ref-type="bibr" rid="CR32">32</xref></sup>. Contour integration is a special case of visual grouping, which refers to the idea that the visual system tends to group elements into visual wholes through simple rules<sup><xref ref-type="bibr" rid="CR33">33</xref></sup>. Visual grouping has been found to benefit visual working memory, as illustrated by Li et al.<sup><xref ref-type="bibr" rid="CR34">34</xref></sup>. Their study focused on the grouping effect of illusory contours, which is a phenomenon in contour integration in which a person observes contours that are not physically present<sup><xref ref-type="bibr" rid="CR35">35</xref></sup>. Similar to the contrast sensitivity task, our implementation here did not include a working memory component and consequently, any observed transfer to this untrained task would more likely be attributed to improved perceptual processes.</p>
              <p id="Par7">To test our hypothesis whether training on a change-detection task has a causal impact on visual perception, we conducted an intervention study in which participants trained for seven sessions on a similar change-detection paradigm that we used for training before<sup><xref ref-type="bibr" rid="CR7">7</xref></sup>. In this previous work, participants trained ten sessions for a total of 3000 trials. In order to shorten the study time in terms of days participants are trained and are tested on the outcome measures, we opted for seven days of training but increased the daily training time by 25%. This adjustment resulted in 2800 training trials (a 7% reduction of the original total training time) at the end of the study but allowed a participant to complete the study within a time frame of nine days. In addition, we chose to implement a slightly different training task compared to what we have used before. In particular, this involved including a whole array probe because we assumed it would foster configurational grouping that could be beneficial, especially for contour integration, and we also included a mask to reduce the influence of iconic memory, and we excluded trial feedback to reduce the affordance to develop very task-specific strategies. The intervention was book-ended by assessment sessions that consisted of a conjunction search task, a complex visual search task, a contrast sensitivity task, and a contour integration task. The data from the experimental group was compared to an active control group that was tested on the same measures as the experimental group. We expected to find an overall effect of change-detection training on visual processing skills. We expected the strongest effects in the visual search tasks, given their discussed connection to perceptual processes and working memory. However, we predicted a stronger effect in the conjunction search task due to its similarity with the experimental training task for example regarding the discrete nature of stimuli in both tasks and a related task objective which consisted of identifying a target stimulus among a set of discrete distractors. We hypothesized that such task similarity fosters the applicability of rules acquired during perceptual learning to a novel task<sup><xref ref-type="bibr" rid="CR36">36</xref></sup>. In contrast, we expected a smaller effect in a more complex visual search task because we hypothesized that the discrete nature of the training task would not provide enough affordances to increase performance in a task that uses scenes as stimuli that are substantially larger and more complex. Regarding the contour integration task, we argued before that change-detection training could help participants to become better at configurational grouping, which, if indeed true, could have an especially beneficial effect on its performance<sup><xref ref-type="bibr" rid="CR7">7</xref></sup>.</p>
            </sec>
            <sec id="Sec2">
              <title>Methods</title>
              <sec id="Sec3">
                <title>Participants</title>
                <p id="Par8">Undergraduate students from UC Irvine (UCI) were recruited to participate in the study via email and the UCI Human Subjects Lab Pool between June 2020 and February 2021. All experimental procedures performed in this study were approved by the UCI Undergraduate Research Opportunities Program (UROP) following guidelines by the UCI Institutional Review Board and in accordance with the Declaration of Helsinki. Informed consent was obtained from all individual participants included in the study. A total of 53 students participated in the study. Two participants dropped out due to scheduling conflicts and one participant was excluded due to non-compliance with instructions, leaving a final sample of 50 participants. 25 participants (20.48 years old [SD = 1.19]; 72% women) were randomly assigned to the experimental group and the other 25 participants (20.20 years old [SD = 1.38]; 84% women) were assigned to an active control group. The inclusion criteria required participants to have normal or corrected to normal vision and to not be colorblind. Participants' vision health was assessed by self-report that focused on topics such as current eye correction, last vision test, and participants’ general feeling about their vision (i.e., clarity and pain). Participants were asked to fill out a visual activities questionnaire<sup><xref ref-type="bibr" rid="CR37">37</xref></sup>, and color blindness was assessed through a color discrimination test where they were required to choose appropriate color words that matched a colored object. For their participation, participants received either a $65 gift card or a $20 gift card and course credit.</p>
              </sec>
              <sec id="Sec4">
                <title>Training tasks</title>
                <sec id="Sec5">
                  <title>Experimental group: change-detection training</title>
                  <p id="Par9">Participants were asked to train once per day for seven sessions with the change-detection task at home on their own computer. Each training session consisted of 20 rounds, each round comprising 20 trials. A trial started with a fixation cross presented in the center of the screen (Fig. <xref rid="Fig1" ref-type="fig">1</xref>). After 1000 ms, the fixation cross was replaced by colored squares that stayed on the screen for 250 ms. After a 200 ms blank screen, a mask was presented for 700 ms, followed by a 100 ms blank screen. The mask consisted of randomly striped squares which appeared in the same locations as the initial stimuli and the colors of the stripes were the same as the ones used for the squares. Next, participants were presented with a probe of colored squares with one square encircled. Participants had to indicate whether the encircled square was of the same color or not as in the initial display. Assuming an eye-to-monitor distance of 50 cm, the array of squares was presented in an imaginary rectangle with a horizontal size of about 12.66° of visual angle and a vertical size of about 9.5° of visual angle. The squares were presented in random locations in every trial with the restrictions that the squares did not appear within an imaginary circle of 2° diameter located in the center of the screen and the center-to-center distance of the squares was at least 2° of visual angle. The squares’ size was about 0.8° × 0.8° of visual angle. The colors used for the squares were black, blue, green, purple, red, white, and yellow. In case of a change-trial (i.e., the cued square was of a different color compared to the initial display), the probe color was randomly selected from this set of colors, excluding the color the square had in the initial display. To account for the variation in participants’ screen sizes and resolutions, they were asked to measure the physical size of their screen in the first training session. After submitting the measurement to the program, the stimuli were adjusted as best as possible to adhere to the indicated degrees of visual angles.<fig id="Fig1"><label>Figure 1</label><caption><p>A sample trial of the change-detection training task. Participants were asked to focus on a fixation cross, which was then replaced by a set of colored squares (shown here with set size four). After a blank screen, followed by a mask, followed by another blank screen, a probe of colored squares was presented. Participants had to indicate whether or not the encircled square was of the same color as the one in the initial display.</p></caption><graphic xlink:href="41598_2022_15649_Fig1_HTML" id="MO1"/></fig></p>
                  <p id="Par10">Participants started with a set size of two squares in their first session. The training difficulty of every round was adaptive and depended on participants’ performance on their previous round. If participants achieved an accuracy greater than 85% in a round, the set size would increase by one in the next round. If participants achieved an accuracy of less than 70%, the set size would decrease by one in the next round. If participants scored between 70 and 85%, then the set size would not change in the next round. The starting set size of subsequent sessions was determined by the participants’ last set size in the previous session minus two in order to allow for a brief warm-up period. The minimum set size was two squares, and the maximum set size was 20 squares, but this maximum was not achieved by any of the participants. The dependent measure to quantify training progress was the average set size per session.</p>
                </sec>
                <sec id="Sec6">
                  <title>Active control group</title>
                  <p id="Par11">Participants assigned to the active control group were asked to train on a general knowledge task, once per day for seven sessions at home on their own computer. Participants were presented with general knowledge and vocabulary questions and had to select the correct answer out of four provided answer choices within a 15 s time limit. After every response, participants were provided with feedback and incorrectly answered questions were shown again in the next session, offering a learning opportunity. The task consisted of 20 rounds with each round consisting of eight trials. The task was adaptive. If participants scored 75% or better, the level of difficulty of the next round increased by one. If participants scored 50% or less, the level of difficulty of the next round decreased by one. If participants scored between 50 and 75%, the level of difficulty of the next round did not change. The dependent variable to quantify training progress was the average level of difficulty achieved in each session.</p>
                  <p id="Par12">A very similar control task has been used before by our lab in other cognitive training work<sup><xref ref-type="bibr" rid="CR38">38</xref>,<xref ref-type="bibr" rid="CR39">39</xref></sup>. The main rationale of including a knowledge-based task is that it shares only very few if any processes of interest with the experimental task. For example, the presentation rate was not nearly as fast as in the experimental group and the focus of the task was on retrieval of semantic information and not on processing of information that differed on a trial-by-trial basis. In addition, our knowledge-based control task is typically perceived as an actual intervention, and thus, participant expectations have shown to be matched with other cognitive interventions<sup><xref ref-type="bibr" rid="CR39">39</xref></sup>. A minimal overlap of processes of interest and the believability of the control condition to be an actual intervention are both recommended features of control tasks in the cognitive training field<sup><xref ref-type="bibr" rid="CR38">38</xref></sup>.</p>
                </sec>
              </sec>
              <sec id="Sec7">
                <title>Outcome measures</title>
                <sec id="Sec8">
                  <title>Conjunction search</title>
                  <p id="Par13">Similar to a task used by Stoet<sup><xref ref-type="bibr" rid="CR40">40</xref></sup>, participants were presented with an array of stimuli and asked to find a target among distractors. The target consisted of a regularly presented orange letter T, the distractors of an orange letter T presented upside-down and a regularly presented blue letter T. Participants were presented with 5, 10, 15, or 20 stimuli (= set size) that were with one restriction randomly distributed on a centrally presented, imaginary five by five grid (Fig. <xref rid="Fig2" ref-type="fig">2</xref>). The only exception to the random distribution of stimuli within the grid was that the target could not appear in the center location. The two distractor types were as evenly distributed as possible so that the number of upside-down orange Ts and the number of blue Ts differed by maximally one item (e.g., in even set sizes with a target). The set size was randomly selected for each trial as opposed to a blocked presentation. The stimuli had a size of 50 by 50 pixels and the center-to-center distance in the grid was 100 pixels. In contrast to the training task, the size of the stimuli was not adjusted to different screen resolutions and sizes. For each set size, 20 trials were presented and half of them included a target stimulus. A trial started with a fixation cross presented for 2000 ms in the center of the screen, followed by the array of stimuli. Participants were instructed to press the spacebar when they found a target and they were given a maximum of 3000 ms to respond. Participants did not have to press a key if they determined that there was no target on the screen but instead had to wait until the trial timed-out. A response or time-out was followed by a fixation cross presented for 1000 ms, followed by a mask presented for 200 ms. The mask consisted of interleaved orange and blue squares sized 28 by 28 pixels that changed to the opposite color at a rate of 50 ms. Participants were provided with eight practice trials at set sizes 3 and 6, half of them including a target. The dependent variables were the reaction times to correct responses both, across all trials and separately as a function of set size.<fig id="Fig2"><label>Figure 2</label><caption><p>Conjunction search task. Participants were presented with an array of orange letter Ts, orange Ts that were presented upside-down, and blue Ts. The stimuli were presented in set sizes 5, 10, 15, and 20 (Panels A through D). Participants were instructed to look for an orange T and ignore all other stimuli.</p></caption><graphic xlink:href="41598_2022_15649_Fig2_HTML" id="MO2"/></fig></p>
                </sec>
                <sec id="Sec9">
                  <title>Complex visual search</title>
                  <p id="Par14">This computerized task was based on the well-known ‘Where’s Waldo’ search images<sup><xref ref-type="bibr" rid="CR41">41</xref>,<xref ref-type="bibr" rid="CR42">42</xref></sup>. Participants were presented with images that were very rich in detail and were instructed to look for ‘Waldo’, a cartoon character with round glasses and a red-white hat. A trial started with the message ‘Get ready for the next image! Press mouse button to start’. After a mouse click, followed by a 500 ms delay, a search image was presented. Instructions stressed to work as quickly as possible without making any errors. Within a time-limit of three minutes, participants had to find the target by clicking on it with the mouse pointer. Participants could click on potential targets as often as they wanted to. After a target was found or the time ran out, the next search image was presented. Through piloting, we created two parallel-versions of the task, each consisting of 12 images, that were comparable in reaction times of finding the target. The dependent variable was the reaction time for correct responses to find the target.</p>
                </sec>
                <sec id="Sec10">
                  <title>Contrast sensitivity</title>
                  <p id="Par15">This task was based upon low-contrast target detection tasks previously used in studies of visual perceptual learning<sup><xref ref-type="bibr" rid="CR43">43</xref></sup>. Participants were presented with Gabors (mathematically a sine-wave windowed by a Gaussian) shown one at a time at random positions on the screen (Fig. <xref rid="Fig3" ref-type="fig">3</xref>). Gabors were presented at 12 cycles per degree (with a standard deviation of 0.75 degrees for the Gaussian window and thus an extent of ~ 2.5° on screen—based on a typical viewing distance of 18″) for up to 4 s each, with a 50 ms ramp to mask the onset. A linear color look-up table was used to ensure proper presentation of the Gabors’ luminance profile and Michelson contrast was calculated. While we cannot guarantee that this leads to perfect rendering of the luminance profile of the Gabors on all devices, these settings are reliable on most modern tablets and smartphones and to whatever extent that there was any distortion of the luminance profile for some participants this would be equated in the pre- and post-test sessions. As it is standard for estimates of contrast sensitivity, the Gabors were presented on a mid-gray background<sup><xref ref-type="bibr" rid="CR28">28</xref></sup>. Participants were instructed to tap on them as soon as they became aware of them. As soon as participants responded, a new Gabor was presented and a 3-down, 1-up staircase was run on the contrast of the Gabors, with approximately 40 trials per run (partially dependent upon the staircase). Thus, subsequent patches were harder to detect. The dependent variable was the contrast threshold (estimated as the average of the last 3 reversals of the staircase) for each participant determined by the staircase procedure.<fig id="Fig3"><label>Figure 3</label><caption><p>Contrast sensitivity task. Gabors appeared one at a time on the screen against a gray background. Participants were instructed to tap on the patch as quickly as possible. The circle surrounding the Gabor is for illustrational purposes.</p></caption><graphic xlink:href="41598_2022_15649_Fig3_HTML" id="MO3"/></fig></p>
                </sec>
                <sec id="Sec11">
                  <title>Contour integration</title>
                  <p id="Par16">In this shape detection task<sup><xref ref-type="bibr" rid="CR44">44</xref></sup>, participants were asked to identify a circle that was hidden amongst a field of randomly oriented Gabors (Fig. <xref rid="Fig4" ref-type="fig">4</xref>). The circle was comprised of spaced oriented Gabor inducers, similar to a dotted line drawing. Participants were asked to tap on the circle as quickly as possible. To increase the task difficulty, a 3-down, 1-up staircase was run on the orientation jitter of the inducers (i.e., the offset of their orientation from the true orientation appropriate to properly define the contour) such that the smooth curvature of the circle was broken up by the inducers having systematically increasing errors in local slope. In this way, as the orientation jitter increased, the lines of the circle were harder to follow, and the circle faded into the background that was made of randomly oriented Gabors. There were approximately 40 trials per participant depending on the staircase. The dependent variable was the participants’ threshold of orientation jitter of the Gabors (estimated as the average of the last 3 reversals of the staircase).<fig id="Fig4"><label>Figure 4</label><caption><p>Contour integration task. Gabors appeared on a gray background. Participants were asked to tap on the circle that was formed by the Gabors. The contour is outlined for illustrational purposes.</p></caption><graphic xlink:href="41598_2022_15649_Fig4_HTML" id="MO4"/></fig></p>
                </sec>
              </sec>
              <sec id="Sec12">
                <title>Procedure</title>
                <p id="Par17">The study was conducted remotely on participants’ own computers. On the first day of the study, all necessary computer tasks were installed under the guidance of research personnel. For that purpose, PsychoPy<sup><xref ref-type="bibr" rid="CR45">45</xref></sup>, Sightseeing<sup><xref ref-type="bibr" rid="CR46">46</xref></sup>, and corresponding experiment scripts were installed. This session lasted about 30 min. Each participant completed a pre-test session consisting of a demographic survey that also included questions related to vision health, a conjunction search task, a complex visual search task, a contrast sensitivity task, and a contour integration task. The pre-test took about 90 min to complete. Following random assignment, participants trained for seven sessions either on the change-detection task or the control task. Participants completed one session per day (excluding weekends) which lasted about 30 min each. The pre-test and the first training session were completed on the same day. Following their last training day, participants completed a post-test session that included the same tasks as the pre-test session. The post-test lasted about 60 min. The study procedure is represented in Fig. <xref rid="Fig5" ref-type="fig">5</xref>. During all pre- and post-test assessments, as well as during training, participants were supervised via video conferencing software that allowed us to monitor compliance and answer any potential questions participants might have; a procedure that has shown to result in similar data quality and performance as compared to in-lab studies<sup><xref ref-type="bibr" rid="CR47">47</xref></sup>.<fig id="Fig5"><label>Figure 5</label><caption><p>Procedure of the study. Participants were randomly assigned to either the change-detection training group (n = 25) or the active control group (n = 25).</p></caption><graphic xlink:href="41598_2022_15649_Fig5_HTML" id="MO5"/></fig></p>
              </sec>
            </sec>
            <sec id="Sec13">
              <title>Results</title>
              <p id="Par18">In our statistical approach, after screening for outliers, we first tested for baseline differences and then for an overall effect of training on all our outcome measures with a multivariate analysis of variance (MANOVA) and an independent samples t-test for group differences on the averaged z-scores of our four outcome measures. Next, we followed-up on these analyses with independent samples t-tests, analyses of covariance (ANCOVAs), and repeated measures analyses of variance (ANOVAs). We report and mainly rely on frequentist analyses, but we also provide Bayes Factors using default priors to provide a more comprehensive analysis picture. The study procedures and analytic approaches were pre-registered at AsPredicted (Protocol #46326).</p>
              <sec id="Sec14">
                <title>Training outlier analyses and evaluation of baseline differences</title>
                <p id="Par19">All analyses were carried out with JASP<sup><xref ref-type="bibr" rid="CR48">48</xref></sup>. To screen for non-compliance and to identify low-performing outliers in the change-detection training data, we analyzed the R<sup>2</sup> of individual regression models with logarithmic transformations of session number. Outliers were defined as regression models with an R<sup>2</sup> value that was more than three median absolute deviations below the overall median<sup><xref ref-type="bibr" rid="CR49">49</xref></sup>. This analysis revealed no training outliers in our data. However, there were two participants, including the only one who performed worse at the end of training than in the beginning, with an R<sup>2</sup> = 0.000. Given that the fit of the average curve across all participants was R<sup>2</sup> = 0.996, we ran our analyses with and without these two participants. Because there were no substantial differences between both analyses and to avoid the risk of bias, we only report the analysis with the whole sample.</p>
                <p id="Par20">Independent samples t-tests and Mann–Whitney U-tests revealed no significant group differences at pre-test (all <italic>p</italic>s &gt; 0.199; all <italic>BF</italic><sub><italic>10</italic></sub> &lt; 0.403) in any of our dependent variables. The descriptive statistics of the raw data are provided in Table <xref rid="Tab1" ref-type="table">1</xref> and the Pearson’s correlations between pre-test measures in Table <xref rid="Tab2" ref-type="table">2</xref>.<table-wrap id="Tab1"><label>Table 1</label><caption><p>Descriptive data of the outcome measures for both groups.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left" rowspan="2"/><th align="left" colspan="5">Pre-test</th><th align="left" colspan="5">Post-test</th><th align="left" colspan="5">Pre-post comparison</th></tr><tr><th align="left"><italic>N</italic></th><th align="left">Mean</th><th align="left">SD</th><th align="left">Min</th><th align="left">Max</th><th align="left"><italic>N</italic></th><th align="left">Mean</th><th align="left">SD</th><th align="left">Min</th><th align="left">Max</th><th align="left">BF<sub>10</sub></th><th align="left"><italic>t</italic></th><th align="left"><italic>p</italic></th><th align="left"><italic>r</italic></th><th align="left">Cohen's d</th></tr></thead><tbody><tr><td align="left" colspan="16"><bold>Experimental group</bold></td></tr><tr><td align="left">Conjunction search (ms)</td><td align="left">25</td><td align="left">1010</td><td align="left">197</td><td align="left">685</td><td align="left">1469</td><td align="left">25</td><td align="left">924</td><td align="left">182</td><td align="left">614</td><td align="left">1332</td><td align="left">6.735</td><td align="left">3.434</td><td align="left">0.002</td><td align="left">0.776</td><td align="left">0.687</td></tr><tr><td align="left">Complex visual search (s)</td><td align="left">24</td><td align="left">73</td><td align="left">22</td><td align="left">35</td><td align="left">127</td><td align="left">24</td><td align="left">65</td><td align="left">15</td><td align="left">32</td><td align="left">91</td><td align="left">1.001</td><td align="left">1.901</td><td align="left">0.070</td><td align="left">0.304</td><td align="left">0.388</td></tr><tr><td align="left">Contrast sensitivity</td><td align="left">25</td><td align="left">0.168</td><td align="left">0.135</td><td align="left">0.033</td><td align="left">0.451</td><td align="left">25</td><td align="left">0.103</td><td align="left">0.114</td><td align="left">0.012</td><td align="left">0.451</td><td align="left">4627</td><td align="left">9.155</td><td align="left"> &lt; 0.001</td><td align="left">0.916</td><td align="left">1.831</td></tr><tr><td align="left">Contour integration</td><td align="left">20</td><td align="left">0.314</td><td align="left">0.096</td><td align="left">0.083</td><td align="left">0.467</td><td align="left">20</td><td align="left">0.294</td><td align="left">0.092</td><td align="left">0.153</td><td align="left">0.489</td><td align="left">0.368</td><td align="left">1.021</td><td align="left">0.320</td><td align="left">0.541</td><td align="left">0.228</td></tr><tr><td align="left" colspan="16"><bold>Active control group</bold></td></tr><tr><td align="left">Conjunction search (ms)</td><td align="left">25</td><td align="left">997</td><td align="left">148</td><td align="left">750</td><td align="left">1396</td><td align="left">25</td><td align="left">1042</td><td align="left">222</td><td align="left">715</td><td align="left">1529</td><td align="left">0.51</td><td align="left">− 1.064</td><td align="left">0.298</td><td align="left">0.687</td><td align="left">− 0.213</td></tr><tr><td align="left">Complex visual search (s)</td><td align="left">24</td><td align="left">71</td><td align="left">19</td><td align="left">32</td><td align="left">115</td><td align="left">24</td><td align="left">72</td><td align="left">16</td><td align="left">33</td><td align="left">100</td><td align="left">0.217</td><td align="left">− 0.077</td><td align="left">0.881</td><td align="left">− 0.034</td><td align="left">− 0.031</td></tr><tr><td align="left">Contrast sensitivity</td><td align="left">25</td><td align="left">0.150</td><td align="left">0.111</td><td align="left">0.041</td><td align="left">0.387</td><td align="left">25</td><td align="left">0.095</td><td align="left">0.085</td><td align="left">0.017</td><td align="left">0.332</td><td align="left">26,415</td><td align="left">12.655</td><td align="left"> &lt; 0.001</td><td align="left">0.948</td><td align="left">2.531</td></tr><tr><td align="left">Contour integration</td><td align="left">19</td><td align="left">0.289</td><td align="left">0.129</td><td align="left">0.039</td><td align="left">0.600</td><td align="left">19</td><td align="left">0.286</td><td align="left">0.071</td><td align="left">0.178</td><td align="left">0.415</td><td align="left">0.238</td><td align="left">0.095</td><td align="left">0.925</td><td align="left">0.079</td><td align="left">0.022</td></tr></tbody></table></table-wrap><table-wrap id="Tab2"><label>Table 2</label><caption><p>Pearson's correlations between all four outcome measures at pre-test.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left"/><th align="left">Conjunction search</th><th align="left">Complex visual search</th><th align="left">Contrast sensitivity</th></tr></thead><tbody><tr><td align="left">Conjunction search</td><td align="left">–</td><td align="left"/><td align="left"/></tr><tr><td align="left">Complex visual search</td><td align="left">− 0.040</td><td align="left">–</td><td align="left"/></tr><tr><td align="left">Contrast sensitivity</td><td align="left">0.261</td><td align="left">0.044</td><td align="left">–</td></tr><tr><td align="left">Contour integration</td><td align="left">− 0.161</td><td align="left">0.021</td><td align="left">− 0.496**</td></tr></tbody></table><table-wrap-foot><p>**p &lt; 0.01.</p></table-wrap-foot></table-wrap></p>
              </sec>
              <sec id="Sec15">
                <title>Training performance</title>
                <p id="Par21">Participants in the experimental group increased their performance by 50% over the course of seven training days (Fig. <xref rid="Fig6" ref-type="fig">6</xref>). A one-way (training sessions: 1–7) repeated measures ANOVA revealed a significant main effect of session, <italic>F</italic>(2.856,144) = 19.767, <italic>p</italic> &lt; 0.001, <italic>η</italic><sup>2</sup><sub>p</sub> = 0.452, <italic>BF</italic><sub><italic>10</italic></sub> = 5.288e + 13. A regression analysis with logarithmic transformations of session number revealed an excellent fit (R<sup>2</sup> = 0.996) that was better than the fit of a linear regression model (R<sup>2</sup> = 0.927).<fig id="Fig6"><label>Figure 6</label><caption><p>Training curve of the current study compared to the first seven training sessions of the two training groups reported in Buschkuehl et al.<sup><xref ref-type="bibr" rid="CR7">7</xref></sup>. Despite some conceptual task differences, training performance of the current study is similar to that of Training Group 2 (TG2) from Buschkuehl et al.<sup><xref ref-type="bibr" rid="CR7">7</xref></sup>, however, the overall performance is descriptively higher throughout most of the training sessions. Error bars represent standard error of the mean. TG1 stands for ‘Training Group 1’.</p></caption><graphic xlink:href="41598_2022_15649_Fig6_HTML" id="MO6"/></fig></p>
                <p id="Par22">Participants in the control group demonstrated a performance increase of 34% from the first to the last session. A one-way (training sessions: 1–7) repeated measures ANOVA revealed a significant main effect of session, <italic>F</italic>(6,144) = 10.095, <italic>p</italic> &lt; 0.001, <italic>η</italic><sup>2</sup><sub>p</sub>  = 0.296, <italic>BF</italic><sub><italic>10</italic></sub> = 3.977e + 6.</p>
              </sec>
              <sec id="Sec16">
                <title>Outcome variables</title>
                <sec id="Sec17">
                  <title>Outlier control</title>
                  <sec id="Sec18">
                    <title>Conjunction search task</title>
                    <p id="Par23">We screened for outliers and deleted all data points that were more than three median absolute deviations above or below the overall median<sup><xref ref-type="bibr" rid="CR49">49</xref></sup>. This screening was conducted separately within each participant, for each set size (5, 10, 15, and 20), and both sessions (pre- and post-test). It resulted in the removal of 259 out of 3948 data points (6.6%). We then averaged the reaction times for each set size separately. For the purpose of the main analysis, the mean reaction times for each set size were averaged again, resulting in a single performance metric for each participant, from which we then calculated the gain scores. We also note that participants made on average fewer than 2 errors per test occasion.</p>
                  </sec>
                  <sec id="Sec19">
                    <title>Complex visual search task</title>
                    <p id="Par24">We excluded two participants (1 experimental, 1 control) from the analysis because they did not find the target in any of the 12 search images presented to them at pre-test. Overall, we had eight participants (four in each group) at pre-test and four participants at post-test (2 in each group) who found fewer than four out of the 12 targets. Nevertheless, Cronbach’s α on the accuracy scores was 0.735 at pre-test and 0.695 at post-test. Due to the variation of inherent difficulty between the search images, it did not make sense to control for outliers and therefore mean reaction times were created for each participant, which served as the basis for the gain score calculation.</p>
                  </sec>
                  <sec id="Sec20">
                    <title>Contour integration task</title>
                    <p id="Par25">We had to exclude a total of 11 participants (5 experimental, 6 controls), 7 (3 experimental, 4 controls) due to insufficient screen resolution that was required to run the task, and 4 (2 experimental, 2 controls) due to non-compliance with instructions.</p>
                    <p id="Par26">Finally, in order to fit the data of the conjunction search task and the contrast sensitivity task better to a normal distribution, the corresponding pre- and post-test data were log-transformed before calculating gain scores.</p>
                  </sec>
                </sec>
                <sec id="Sec21">
                  <title>Overall analysis</title>
                  <p id="Par27">We conducted a MANOVA to investigate whether change-detection training resulted in an overall improvement in visual processing. For that purpose, we generated gain scores (pre-test minus post-test) for all four of our outcome measures that served as the dependent variables; group constituted the between-subjects variable. The overall MANOVA was significant, <italic>F</italic>(4,34) = 3.051, <italic>p</italic> = 0.030, <italic>η</italic><sup>2</sup><sub>p</sub>  = 0.27. Following-up on the MANOVA, independent samples t-tests on the gain scores yielded a significant group effect for the conjunction search task, <italic>t</italic>(37) = 3.316, <italic>p</italic> = 0.002, <italic>d</italic> = 1.062, <italic>BF</italic><sub><italic>10</italic></sub> = 17.339, but not the other tasks: complex visual search, <italic>t</italic>(37) = 1.017, <italic>p</italic> = 0.316 <italic>d</italic> = 0.326, <italic>BF</italic><sub><italic>10</italic></sub> = 0.469; contrast sensitivity, <italic>t</italic>(37) = 1.313, <italic>p</italic> = 0.197, <italic>d</italic> = 0.421, <italic>BF</italic><sub><italic>10</italic></sub> = 0.613; contour integration, <italic>t</italic>(37) = 0.461, <italic>p</italic> = 0.648, <italic>d</italic> = 0.148, <italic>BF</italic><sub><italic>10</italic></sub> = 0.339.</p>
                  <p id="Par28">An issue with the MANOVA was that the analytical sample was reduced to N = 37 (excluding 6 participants from the experimental group and 7 controls) due to missing data, especially in the contour integration task (11 missing data points) but also the complex visual search task (2 missing data points). Therefore, in order to increase the statistical power by including all available data, we z-scored all gain scores that constituted the dependent variables of the MANOVA and averaged these z-scores across individual participants. An independent samples t-test revealed a significant group effect, <italic>t</italic>(48) = 2.742, <italic>p</italic> = 0.009, <italic>d</italic> = 0.776, <italic>BF</italic><sub><italic>10</italic></sub> = 5.471. To further analyze the data of the four outcome measures, we ran separate ANCOVAs with the post-test scores as the dependent variable and the pre-test scores as the covariate. These analyses considered all available data points per outcome measure and revealed a significant group effect for the conjunction search task, <italic>F</italic>(1,47) = 9.179, <italic>p</italic> = 0.004, <italic>η</italic><sup>2</sup><sub>p</sub>  = 0.163, <italic>BF</italic><sub><italic>incl</italic></sub> = 9.348, but not the other tasks: complex visual search, <italic>F</italic>(1,45) = 2.941, <italic>p</italic> = 0.093, <italic>η</italic><sup>2</sup><sub>p</sub>  = 0.061, <italic>BF</italic><sub><italic>incl</italic></sub> = 0.895; contrast sensitivity, <italic>F</italic>(1,47) = 2.351, <italic>p</italic> = 0.132, <italic>η</italic><sup>2</sup><sub>p</sub>  = 0.048, <italic>BF</italic><sub><italic>incl</italic></sub> = 0.708; contour integration, <italic>F</italic>(1,36) = 0.010, <italic>p</italic> = 0.919, <italic>η</italic><sup>2</sup><sub>p</sub>  = 0.000, <italic>BF</italic><sub><italic>incl</italic></sub> = 0.319. We note that the significance levels of this set of ANCOVAs do not change if the p-values are adjusted for multiple comparisons, for example through a Bonferroni correction. Corresponding repeated-measure ANOVAs led to nearly identical results and are therefore not reported. Figure <xref rid="Fig7" ref-type="fig">7</xref> provides an illustration of the effect sizes for each of the four outcome measures.
<fig id="Fig7"><label>Figure 7</label><caption><p>Effect sizes for each outcome measure. The effect sizes are based on individual ANCOVAs and the p-values above each bar represent the significance level of the corresponding analysis.</p></caption><graphic xlink:href="41598_2022_15649_Fig7_HTML" id="MO7"/></fig></p>
                </sec>
                <sec id="Sec22">
                  <title>Detailed conjunction search task analysis</title>
                  <p id="Par29">To further elucidate the outcome of the conjunction search task, we analyzed the reaction time data as a function of set size (Fig. <xref rid="Fig8" ref-type="fig">8</xref>). The basis for our reaction time analysis was the outlier-controlled data, averaged for each set size. To improve the fit of the data to a normal distribution, all means were log transformed. A repeated measures ANOVA with the factors group (experimental vs control), session (pre vs post), and set size (5, 10, 15, 20) revealed a significant main effect for set size, <italic>F</italic>(3,144) = 202.488, <italic>p</italic> &lt; 0.001, <italic>η</italic><sup>2</sup><sub>p</sub>  = 0.808, <italic>BF</italic><sub><italic>incl</italic></sub> = 7.967e + 56, and importantly, a significant session by group interaction, <italic>F</italic>(1,144) = 9.408, <italic>p</italic> = 0.004, <italic>η</italic><sup>2</sup><sub>p</sub>  = 0.164, <italic>BF</italic><sub><italic>incl</italic></sub> = 59,348. There was no main effect for session <italic>F</italic>(1,144) = 2.262, <italic>p</italic> = 0.139, <italic>η</italic><sup>2</sup><sub>p</sub>  = 0.045, <italic>BF</italic><sub><italic>incl</italic></sub> = 2.137 and no main effect for group <italic>F</italic>(1,48) = 1.228, <italic>p</italic> = 0.273, <italic>η</italic><sup>2</sup><sub>p</sub>  = 0.025, <italic>BF</italic><sub><italic>incl</italic></sub> = 0.568. Bonferroni corrected post-hoc tests comparing the effects of set size revealed that all comparisons were significant (all <italic>p</italic>s &lt; 0.001, all <italic>d</italic>s &gt; 0.683, all <italic>BF</italic><sub><italic>10, U</italic></sub> &gt; 2328). Note that we report <italic>BF</italic><sub><italic>incl</italic></sub> according to the suggested method by Sebastiaan Mathôd<sup><xref ref-type="bibr" rid="CR50">50</xref></sup> where applicable.<fig id="Fig8"><label>Figure 8</label><caption><p>Reaction time data of the conjunction search task. The data show the typical slope that indicates how much longer it takes to find a target with an increasing number of distractors. While both groups performed similarly at pre-test, the experimental group improved at post-test and the control group got worse, although to a smaller degree. The averaged slope of fitted linear regression models to the actual reaction times at pre-test resulted in a value of 21.26 ms (SE = 1.40) which is well in line with previously reported data<sup><xref ref-type="bibr" rid="CR40">40</xref></sup>. Error bars represent standard error of the mean.</p></caption><graphic xlink:href="41598_2022_15649_Fig8_HTML" id="MO8"/></fig></p>
                  <p id="Par30">As can be seen in Fig. <xref rid="Fig8" ref-type="fig">8</xref>, the session by group interaction was driven by the improvement of the experimental group, but it is also the case that the control group got worse at post-test, although to a smaller degree. To quantify any group differences of the reaction time x set size functions, we fitted linear regression models for each participant to the mean log-transformed reaction times. ANCOVAs with the post-test scores as dependent variables and the pre-test scores as covariates yielded a significant group effect for the intercept, <italic>F</italic>(1,47) = 4.057, <italic>p</italic> &lt; 0.05, <italic>η</italic><sup>2</sup><sub>p</sub>  = 0.079, <italic>BF</italic><sub><italic>incl</italic></sub> = 1.379; but not the slope, <italic>F</italic>(1,47) = 0.046, <italic>p</italic> = 0.832, <italic>η</italic><sup>2</sup><sub>p</sub>  = 0.000, <italic>BF</italic><sub><italic>incl</italic></sub> = 0.287.</p>
                </sec>
              </sec>
            </sec>
            <sec id="Sec23">
              <title>Discussion</title>
              <p id="Par31">The present study aimed to explore the potential benefits of change-detection training on the perceptual processes of visual search, contrast sensitivity, and contour integration. After seven days of training, participants improved on their initial change-detection performance by 50% on average, providing further evidence for the malleability of change-detection skills with adaptive training. Compared to an active control group, participants also showed generalized improvements across the visual perceptual tasks. However, the improvement was most pronounced in the conjunction search task (<italic>η</italic><sup>2</sup><sub>p</sub>  = 0.163). The overall effect was driven to a much lesser extent by the complex visual search task (<italic>η</italic><sup>2</sup><sub>p</sub>  = 0.061) and the contrast sensitivity task (<italic>η</italic><sup>2</sup><sub>p</sub>  = 0.048). No effect was observed for contour integration (<italic>η</italic><sup>2</sup><sub>p</sub>  = 0.000).</p>
              <sec id="Sec24">
                <title>Specific training effects</title>
                <p id="Par32">Although our implemented training paradigm was not identical to the ones used by Buschkuehl et al.<sup><xref ref-type="bibr" rid="CR7">7</xref></sup>, the paradigms are similar enough to warrant a brief comparison. As illustrated in Fig. <xref rid="Fig6" ref-type="fig">6</xref>, the training performance observed in the present study is situated between the two training paradigms used by Buschkuehl et al.<sup><xref ref-type="bibr" rid="CR7">7</xref></sup>, which may be due to the fact that the current change-detection training represents a “mix-and-match” of Training Group 1 (TG1) and Training Group 2 (TG2). The current change-detection training task adopted the mask from TG2 and the whole array probe from TG1. The element that notably differentiates the current change-detection design from TG2 is the test display which likely contributes to the better overall training performance. It has been found that such cues lead to improved change-detection performance as they assist in retrieval of memory representations<sup><xref ref-type="bibr" rid="CR51">51</xref>,<xref ref-type="bibr" rid="CR52">52</xref></sup>. In addition, configurational grouping may have been a strategy used by participants to result in better performance<sup><xref ref-type="bibr" rid="CR7">7</xref></sup>. By possibly grouping nearby squares together, participants may have an easier time recalling the color of the squares as they are now part of “smaller wholes” instead of being perceived or remembered as “many individual objects”<sup><xref ref-type="bibr" rid="CR53">53</xref></sup>. Another area in which the present training paradigm differs from the ones used previously is training time. The current training task consisted of 20 rounds with each round being composed of 20 trials. In contrast, previously we included only 15 rounds, each round consisting of 20 trials but trained for 10 days instead of only 7. However, the trial lengths were quite similar across the three paradigms. The additional 25% of training time in the first seven sessions in the present study could contribute to the explanation why overall training performance was better here.</p>
              </sec>
              <sec id="Sec25">
                <title>Outcome measures</title>
                <p id="Par33">The main question we aimed to answer was whether change-detection training impacts visual perceptual processes beyond the trained task. At a global level, our data indicate that this is indeed the case, however, the degree of improvement varied substantially at the individual task level. The strongest support for a causal relationship between change-detection training and improved visual perceptual processes comes from the conjunction search task (<italic>η</italic><sup>2</sup><sub>p</sub>  = 0.163). Performance on the conjunction search task is captured by a baseline processing time (often referred to as the intercept of a linear regression model) and the rate of processing (often referred to as the slope of a linear regression model). While the rate of processing is an indicator for search efficiency that reflects the rate of attentional shifting between items for example, the baseline processing time is assumed to represent components that are not search related such as perceptual processes and motor-related response processes<sup><xref ref-type="bibr" rid="CR54">54</xref>,<xref ref-type="bibr" rid="CR55">55</xref></sup>. Figure <xref rid="Fig8" ref-type="fig">8</xref> illustrates that the baseline processing time is different between sessions and groups which was confirmed by a significant group difference of the intercept as shown with the ANCOVA. However, participants processed items at a similar rate before and after training (i.e., all four curves are parallel) as indicated by a non-significant group comparison of the slopes. Therefore, it seems that change-detection training facilitates perceptual processes that are required in our conjunction visual search task. If the training resulted mainly in improved post-search processes, such as motor responses, or a general improvement solely because participants were part of the experimental group, then we would expect to find a more universal improvement across outcome measures, a pattern that we neither observed in the present nor our previous training study<sup><xref ref-type="bibr" rid="CR7">7</xref></sup>. Joseph et al.<sup><xref ref-type="bibr" rid="CR54">54</xref></sup> have argued that it is hard to distinguish between perceptual and attentional processes using baseline processing and the rate of processing. However, we note that the data of our conjunction search task are strikingly clear in that the only pre- to post-test change was in baseline processing, suggesting that what participants seem to mainly improve on are in fact perceptual processes. This raises the question how much of the observed improvement in the training task can be attributed to improved (task-specific) memory capacity and how much can be attributed to improved perceptual processes. The currently available change-detection training literature suggests that there is negligible transfer potential to tasks that also rely on memory capacity but involve perceptual processes to a far lesser extent, such as an n-back task or a complex span task<sup><xref ref-type="bibr" rid="CR2">2</xref>,<xref ref-type="bibr" rid="CR7">7</xref>,<xref ref-type="bibr" rid="CR8">8</xref>,<xref ref-type="bibr" rid="CR10">10</xref></sup>. This pattern of findings in the literature along with the current outcome suggest that change-detection training seems to most likely improve perceptual processes, but future studies are needed to further clarify this issue. Future research also needs to clarify under what perceptual conditions a transfer from change-detection training to a conjunction search task might occur. For example, the change-detection training task used here covers a relatively wide area of 12.66° by 9.5° of visual angle, and therefore, it is reasonable to assume that participants learn to rely on peripheral cues<sup><xref ref-type="bibr" rid="CR56">56</xref></sup>. A straight-forward way to investigate this assumption would be to parametrically vary the size of the visual field in the conjunction search task.</p>
                <p id="Par34">Although the effect size of the complex visual search task (<italic>η</italic><sup>2</sup><sub>p</sub>  = 0.061) indicates that the task contributed to the overall effect of change-detection training on visual processing to some extent, the individual task analysis did not reach significance. We included this task assuming that it differed to a higher degree relative to the conjunction search task from the experimental training task, such that the lack of task similarity would only minimally foster the applicability of rules acquired through perceptual learning<sup><xref ref-type="bibr" rid="CR36">36</xref></sup>. In retrospect, however, we identified a few issues with the task variant we used here, in particular, we think that only having 12 trials in the pre- and post-test session, respectively, was not optimal as only 20 participants were able to find more than half of the targets in the pre- and post-test, respectively. Another issue we identified was that the search images varied in size as a function of the different screen sizes on which the task was run because participants performed the training as well as the outcome measures on their own hardware. The issue is not severe within-participants, but more so on a between-participants level because on larger screens it is intuitively easier to find the targets compared to smaller screens. For that reason, it is important to replicate our initial data in the laboratory using a more controlled environment. Aside from these affordances for methodological improvements, we note that the lack of transfer is in line with our prediction: the complex search task differs from the conjunction search task in that the search images did not consist of discrete items but rather a very complex scene containing many different stimuli. Therefore, it is possible that participants learned to work with highly artificial stimuli such as the squares in the change-detection task, and thus, they were able to apply their learned skills to the orange T stimuli in the conjunction search task but not beyond this specific context. The fact that the initial performance in the conjunction task was relatively low could have further pronounced the effects.</p>
                <p id="Par35">The effect size of the contrast sensitivity task (<italic>η</italic><sup>2</sup><sub>p</sub>  = 0.048) was comparable with the one of the complex visual search task (<italic>η</italic><sup>2</sup> = 0.061). We note that extensive training is generally required to find robust changes in contrast sensitivity<sup><xref ref-type="bibr" rid="CR57">57</xref></sup>, with typical vision training studies targeting contrast sensitivity employing 20 training sessions<sup><xref ref-type="bibr" rid="CR17">17</xref>,<xref ref-type="bibr" rid="CR58">58</xref>–<xref ref-type="bibr" rid="CR60">60</xref></sup>. Still, we note that there were notable improvements in both groups, which is not unusual as there are also known practice effects especially in perceptual or attention-based tasks when comparing two initial subsequent task sessions<sup><xref ref-type="bibr" rid="CR61">61</xref>,<xref ref-type="bibr" rid="CR62">62</xref></sup>. While some may question why change-detection training would lead to changes on contrast discrimination, which is typically considered a low-level visual feature, one explanation could be on the extent to which attention is focused on luminance properties of stimuli in both the change-detection and the contrast sensitivity tasks. Further, while classical studies have assumed that perceptual learning is driven by low-level processes, it is clear that learning can occur at a variety of levels of processing<sup><xref ref-type="bibr" rid="CR12">12</xref></sup> and that potential transfer can depend upon on how applicable newly learned rules (e.g., attend to the luminance distribution of a stimulus) are to new situations<sup><xref ref-type="bibr" rid="CR36">36</xref></sup>. Future studies need to be conducted to further quantify the effects of change-detection training and whether longer training would lead to larger effect sizes and more specifically what brain structures, or rule processes, lead to transfer between the tasks.</p>
                <p id="Par36">Compared to the three other outcome measures, there was no effect of training on contour integration (<italic>η</italic><sup>2</sup><sub>p</sub>  = 0.000). We speculate that the change-detection training task does not provide enough affordances to improve on this specific perceptual skill. A striking difference between our training task and the contour integration task was that while the training relied on straight lines of its stimuli, the outcome measure focused on circles. Since contour integration performance deteriorates for contours that become increasingly curved compared to contours that are straight or contain corners, the level of difficulty of our contour integration might have been too high to detect any changes as a function of training<sup><xref ref-type="bibr" rid="CR63">63</xref>,<xref ref-type="bibr" rid="CR64">64</xref></sup>. It is also possible that the focus on individual features that was trained in the change-detection task was inappropriate to lead to positive changes in integrating multiple features into wholes as needed in the contour integration task. Finally, our contour integration task allowed participants to respond within a relatively wide time window which further increases task difficulty, as it has been shown that the ability to detect a contour worsens with prolonged presentation<sup><xref ref-type="bibr" rid="CR31">31</xref></sup>.</p>
              </sec>
              <sec id="Sec26">
                <title>Potential mechanisms of change-detection training</title>
                <p id="Par37">The change-detection task is often used as a measure to assess working memory capacity and some have argued that it is especially well suited to do so because certain strategies such as rehearsal are only minimally helpful due to the fast timing of the task<sup><xref ref-type="bibr" rid="CR1">1</xref></sup>. It has also been argued that change-detection performance represents a fairly stable trait<sup><xref ref-type="bibr" rid="CR5">5</xref></sup>. Nevertheless, a growing number of training studies using change-detection tasks, including the current experiment, demonstrated that participants can improve on the task to an impressive degree if the training is implemented in a certain way, for example by adaptively adjusting the number of stimuli participants train with. The question then is, what do participants actually learn? While previous training studies suggested that training effects are quite task specific in that there are only very minimal benefits beyond the task context, previous experiments observed an improved fidelity of internal memory representations. We have argued earlier that training on change-detection may lead to improved processes related to perceptual learning and predicted such transfer to most likely occur in visual search tasks given the presumed overlap in cognitive processes shared between working memory and visual search<sup><xref ref-type="bibr" rid="CR22">22</xref>,<xref ref-type="bibr" rid="CR23">23</xref></sup>. While some might find such a transfer not very surprising, our study is nevertheless the first that causally links change-detection performance, a measure traditionally used to assess working memory capacity, with visual search performance. Future research needs to investigate to what extent the variance in the change-detection task might be explained by working memory processes and/or perceptual processes. One way to investigate this further is by means of mediation models<sup><xref ref-type="bibr" rid="CR65">65</xref></sup>. Critically, this is also an example where cognitive training studies might provide causal insights that correlational studies are not able to provide.</p>
              </sec>
              <sec id="Sec27">
                <title>Limitations</title>
                <p id="Par38">Our entire study, including pre- and post-test assessments, was conducted on participants’ own hardware, which, for example, led to slight variations in stimuli sizes and presentation times. While such studies have been conducted successfully before<sup><xref ref-type="bibr" rid="CR47">47</xref>,<xref ref-type="bibr" rid="CR66">66</xref></sup>, we would have preferred to better standardize the assessment of our outcome measures by using identical screens and computers across all participants, as well as ensuring that participants completed their sessions in a quiet environment with minimal distractions. Despite those circumstances, our training data were consistent and well in line with findings of our previous study<sup><xref ref-type="bibr" rid="CR7">7</xref></sup> in which participants also trained on their own hardware at home. Furthermore, the data from the conjunction search task clearly show the signature picture for the reaction time x set size functions one expects based on decades of prior research. In addition, our data also indicate acceptable reliability values in both groups (<italic>r</italic> = 0.776 and <italic>r</italic> = 0.687, respectively). Overall, our results suggest that the data quality for the training task and the conjunction search task was adequate. Taking a closer look at the three other outcome measures, the pre- to post-test reliabilities of the complex visual search task were low in both groups (<italic>r</italic> = 0.304 and <italic>r</italic> = − 0.034, respectively), although internal reliability on accuracy was in an acceptable range for the present purposes (α = 0.735 at pre-test and α = 0.695 at post-test). As discussed before, we believe that a future administration of this or a similar task could benefit from additional trials and a more standardized administration. The pre- to post-test reliability data of the contrast sensitivity task were very high in both groups (<italic>r</italic> = 0.916 and <italic>r</italic> = 0.948, respectively) and we are confident that our task measured this construct appropriately. The reliabilities in the contour integration task, on the other hand, were low (<italic>r</italic> = 0.541 and <italic>r</italic> = 0.079) in both groups. An issue with this task was also that we had to exclude 11 participants due to insufficient hardware specifications or non-compliance with task instructions which indicates that also here, a more standardized task administration would be beneficial.</p>
                <p id="Par39">We also want to note that 78% of all participants were women. However, our gender distribution is comparable to other change-detection training studies. In six different experiments that reported gender information<sup><xref ref-type="bibr" rid="CR2">2</xref>,<xref ref-type="bibr" rid="CR7">7</xref>,<xref ref-type="bibr" rid="CR9">9</xref>,<xref ref-type="bibr" rid="CR10">10</xref></sup>, the average percentage of included women was 76%. There is currently only one study available<sup><xref ref-type="bibr" rid="CR8">8</xref></sup>, where more men than women participated (10 out of 15). Therefore, our sample is in line with the existing change-detection training literature. However, future research is needed to determine whether there might be any gender differences in change-detection training.</p>
                <p id="Par40">These limitations notwithstanding, our training data are very well in line with previous research, and we contend that the data collected with the conjunction search task and the contrast sensitivity task are reliable and robust. However, reliability indices in the complex search task and contour integration task are not ideal which might have prevented adequate assessment of the transfer potential of these tasks, either because of non-standardized task administration, their task design, or a combination of both.</p>
              </sec>
            </sec>
            <sec id="Sec28">
              <title>Conclusion</title>
              <p id="Par41">Our training study revealed impressive specific training effects on a change-detection task as well as transfer effects to perceptual processes. The data indicate a causal relationship between change-detection training and perceptual processes as required in conjunction search tasks. Since the change-detection paradigm is commonly used to assess working memory capacity, future research should focus on how much of its variance is explained by memory performance and how much is explained by perceptual processes.</p>
            </sec>
          </body>
          <back>
            <fn-group>
              <fn>
                <p>
                  <bold>Publisher's note</bold>
                </p>
                <p>Springer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations.</p>
              </fn>
            </fn-group>
            <ack>
              <title>Acknowledgements</title>
              <p>The authors would like to thank Helen Alsunna, Mary Zheng, Naomi Meave, Selene Leal, and Christina Xia for their assistance in data collection and Trevor Stravropoulos for developing the contrast sensitivity and contour integration task for the current project.</p>
            </ack>
            <notes notes-type="author-contribution">
              <title>Author contributions</title>
              <p>S.M.J., J.T., and M.B. designed the experiment; J.T., R.N.S.-P., and A.A.C. conducted the experiment; S.M.J. supervised the project and provided resources; M.B. and A.R.S. provided resources. M.B. and J.T. analyzed the data; M.B., J.T., A.R.S., and S.M.J. prepared the manuscript. All authors reviewed and approved the manuscript.</p>
            </notes>
            <notes notes-type="funding-information">
              <title>Funding</title>
              <p>This work was supported by the UC Irvine Undergraduate Research Opportunities Program (UROP) and Summer Undergraduate Research Program (SURP) to J.T. S.M.J. is supported by the National Institute on Aging (Grant No. 1K02AG054665).</p>
            </notes>
            <notes notes-type="data-availability">
              <title>Data availability</title>
              <p>The datasets used in the current study are available from the corresponding authors on reasonable request.</p>
            </notes>
            <notes id="FPar1" notes-type="COI-statement">
              <title>Competing interests</title>
              <p id="Par42">M.B. is employed at the MIND Research Institute whose interest is related to this work. S.M.J. has an indirect financial interest in the MIND Research Institute. All other authors declare no competing interests.</p>
            </notes>
            <ref-list id="Bib1">
              <title>References</title>
              <ref id="CR1">
                <label>1.</label>
                <element-citation publication-type="journal">
                  <person-group person-group-type="author">
                    <name>
                      <surname>Cowan</surname>
                      <given-names>N</given-names>
                    </name>
                    <etal/>
                  </person-group>
                  <article-title>On the capacity of attention: Its estimation and its role in working memory and cognitive aptitudes</article-title>
                  <source>Cognit. Psychol.</source>
                  <year>2005</year>
                  <volume>51</volume>
                  <fpage>42</fpage>
                  <lpage>100</lpage>
                  <pub-id pub-id-type="doi">10.1016/j.cogpsych.2004.12.001</pub-id>
                  <?supplied-pmid 16039935?>
                  <pub-id pub-id-type="pmid">16039935</pub-id>
                </element-citation>
              </ref>
              <ref id="CR2">
                <label>2.</label>
                <element-citation publication-type="journal">
                  <person-group person-group-type="author">
                    <name>
                      <surname>Moriya</surname>
                      <given-names>J</given-names>
                    </name>
                  </person-group>
                  <article-title>Visual-working-memory training improves both quantity and quality</article-title>
                  <source>J. Cogn. Enhanc.</source>
                  <year>2019</year>
                  <volume>3</volume>
                  <fpage>221</fpage>
                  <lpage>232</lpage>
                  <pub-id pub-id-type="doi">10.1007/s41465-018-00120-5</pub-id>
                </element-citation>
              </ref>
              <ref id="CR3">
                <label>3.</label>
                <element-citation publication-type="journal">
                  <person-group person-group-type="author">
                    <name>
                      <surname>Xu</surname>
                      <given-names>Z</given-names>
                    </name>
                    <name>
                      <surname>Adam</surname>
                      <given-names>KCS</given-names>
                    </name>
                    <name>
                      <surname>Fang</surname>
                      <given-names>X</given-names>
                    </name>
                    <name>
                      <surname>Vogel</surname>
                      <given-names>EK</given-names>
                    </name>
                  </person-group>
                  <article-title>The reliability and stability of visual working memory capacity</article-title>
                  <source>Behav. Res. Methods</source>
                  <year>2017</year>
                  <pub-id pub-id-type="doi">10.3758/s13428-017-0886-6</pub-id>
                </element-citation>
              </ref>
              <ref id="CR4">
                <label>4.</label>
                <element-citation publication-type="journal">
                  <person-group person-group-type="author">
                    <name>
                      <surname>Rouder</surname>
                      <given-names>JN</given-names>
                    </name>
                    <etal/>
                  </person-group>
                  <article-title>An assessment of fixed-capacity models of visual working memory</article-title>
                  <source>Proc. Natl. Acad. Sci. USA.</source>
                  <year>2008</year>
                  <volume>105</volume>
                  <fpage>5975</fpage>
                  <lpage>5979</lpage>
                  <pub-id pub-id-type="doi">10.1073/pnas.0711295105</pub-id>
                  <?supplied-pmid 18420818?>
                  <pub-id pub-id-type="pmid">18420818</pub-id>
                </element-citation>
              </ref>
              <ref id="CR5">
                <label>5.</label>
                <element-citation publication-type="journal">
                  <person-group person-group-type="author">
                    <name>
                      <surname>Zhang</surname>
                      <given-names>W</given-names>
                    </name>
                    <name>
                      <surname>Luck</surname>
                      <given-names>SJ</given-names>
                    </name>
                  </person-group>
                  <article-title>The number and quality of representations in working memory</article-title>
                  <source>Psychol. Sci.</source>
                  <year>2011</year>
                  <volume>22</volume>
                  <fpage>1434</fpage>
                  <lpage>1441</lpage>
                  <pub-id pub-id-type="doi">10.1177/0956797611417006</pub-id>
                  <?supplied-pmid 21987693?>
                  <pub-id pub-id-type="pmid">21987693</pub-id>
                </element-citation>
              </ref>
              <ref id="CR6">
                <label>6.</label>
                <element-citation publication-type="journal">
                  <person-group person-group-type="author">
                    <name>
                      <surname>Johnson</surname>
                      <given-names>MK</given-names>
                    </name>
                    <etal/>
                  </person-group>
                  <article-title>The relationship between working memory capacity and broad measures of cognitive ability in healthy adults and people with schizophrenia</article-title>
                  <source>Neuropsychology</source>
                  <year>2013</year>
                  <volume>27</volume>
                  <fpage>220</fpage>
                  <lpage>229</lpage>
                  <pub-id pub-id-type="doi">10.1037/a0032060</pub-id>
                  <?supplied-pmid 23527650?>
                  <pub-id pub-id-type="pmid">23527650</pub-id>
                </element-citation>
              </ref>
              <ref id="CR7">
                <label>7.</label>
                <element-citation publication-type="journal">
                  <person-group person-group-type="author">
                    <name>
                      <surname>Buschkuehl</surname>
                      <given-names>M</given-names>
                    </name>
                    <name>
                      <surname>Jaeggi</surname>
                      <given-names>SM</given-names>
                    </name>
                    <name>
                      <surname>Mueller</surname>
                      <given-names>ST</given-names>
                    </name>
                    <name>
                      <surname>Shah</surname>
                      <given-names>P</given-names>
                    </name>
                    <name>
                      <surname>Jonides</surname>
                      <given-names>J</given-names>
                    </name>
                  </person-group>
                  <article-title>Training change detection leads to substantial task-specific improvement</article-title>
                  <source>J. Cogn. Enhanc.</source>
                  <year>2017</year>
                  <volume>1</volume>
                  <fpage>419</fpage>
                  <lpage>433</lpage>
                  <pub-id pub-id-type="doi">10.1007/s41465-017-0055-y</pub-id>
                </element-citation>
              </ref>
              <ref id="CR8">
                <label>8.</label>
                <element-citation publication-type="journal">
                  <person-group person-group-type="author">
                    <name>
                      <surname>Kuo</surname>
                      <given-names>C-C</given-names>
                    </name>
                    <name>
                      <surname>Zhang</surname>
                      <given-names>C</given-names>
                    </name>
                    <name>
                      <surname>Rissman</surname>
                      <given-names>RA</given-names>
                    </name>
                    <name>
                      <surname>Chiu</surname>
                      <given-names>AWL</given-names>
                    </name>
                  </person-group>
                  <article-title>Long-term electrophysiological and behavioral analysis on the improvement of visual working memory load, training gains, and transfer benefits</article-title>
                  <source>J. Behav. Brain Sci.</source>
                  <year>2014</year>
                  <volume>04</volume>
                  <fpage>234</fpage>
                  <lpage>246</lpage>
                  <pub-id pub-id-type="doi">10.4236/jbbs.2014.45025</pub-id>
                </element-citation>
              </ref>
              <ref id="CR9">
                <label>9.</label>
                <element-citation publication-type="journal">
                  <person-group person-group-type="author">
                    <name>
                      <surname>Ovalle Fresa</surname>
                      <given-names>R</given-names>
                    </name>
                    <name>
                      <surname>Rothen</surname>
                      <given-names>N</given-names>
                    </name>
                  </person-group>
                  <article-title>Training enhances fidelity of color representations in visual long-term memory</article-title>
                  <source>J. Cogn. Enhanc.</source>
                  <year>2019</year>
                  <volume>3</volume>
                  <fpage>315</fpage>
                  <lpage>327</lpage>
                  <pub-id pub-id-type="doi">10.1007/s41465-019-00121-y</pub-id>
                </element-citation>
              </ref>
              <ref id="CR10">
                <label>10.</label>
                <element-citation publication-type="journal">
                  <person-group person-group-type="author">
                    <name>
                      <surname>Wang</surname>
                      <given-names>K</given-names>
                    </name>
                    <name>
                      <surname>Qian</surname>
                      <given-names>J</given-names>
                    </name>
                  </person-group>
                  <article-title>Training with high perceptual difficulty improves the capacity and fidelity of internal representation in VWM</article-title>
                  <source>Psychol. Res.</source>
                  <year>2020</year>
                  <pub-id pub-id-type="doi">10.1007/s00426-020-01404-2</pub-id>
                  <?supplied-pmid 33211160?>
                  <pub-id pub-id-type="pmid">33211160</pub-id>
                </element-citation>
              </ref>
              <ref id="CR11">
                <label>11.</label>
                <element-citation publication-type="journal">
                  <person-group person-group-type="author">
                    <name>
                      <surname>Gaspar</surname>
                      <given-names>JG</given-names>
                    </name>
                    <name>
                      <surname>Neider</surname>
                      <given-names>MB</given-names>
                    </name>
                    <name>
                      <surname>Simons</surname>
                      <given-names>DJ</given-names>
                    </name>
                    <name>
                      <surname>McCarley</surname>
                      <given-names>JS</given-names>
                    </name>
                    <name>
                      <surname>Kramer</surname>
                      <given-names>AF</given-names>
                    </name>
                  </person-group>
                  <article-title>Change detection: Training and transfer</article-title>
                  <source>PLoS ONE</source>
                  <year>2013</year>
                  <volume>8</volume>
                  <fpage>e67781</fpage>
                  <pub-id pub-id-type="doi">10.1371/journal.pone.0067781</pub-id>
                  <?supplied-pmid 23840775?>
                  <pub-id pub-id-type="pmid">23840775</pub-id>
                </element-citation>
              </ref>
              <ref id="CR12">
                <label>12.</label>
                <element-citation publication-type="journal">
                  <person-group person-group-type="author">
                    <name>
                      <surname>Maniglia</surname>
                      <given-names>M</given-names>
                    </name>
                    <name>
                      <surname>Seitz</surname>
                      <given-names>AR</given-names>
                    </name>
                  </person-group>
                  <article-title>Towards a whole brain model of perceptual learning</article-title>
                  <source>Curr. Opin. Behav. Sci.</source>
                  <year>2018</year>
                  <volume>20</volume>
                  <fpage>47</fpage>
                  <lpage>55</lpage>
                  <pub-id pub-id-type="doi">10.1016/j.cobeha.2017.10.004</pub-id>
                  <?supplied-pmid 29457054?>
                  <pub-id pub-id-type="pmid">29457054</pub-id>
                </element-citation>
              </ref>
              <ref id="CR13">
                <label>13.</label>
                <element-citation publication-type="journal">
                  <person-group person-group-type="author">
                    <name>
                      <surname>Lev</surname>
                      <given-names>M</given-names>
                    </name>
                    <etal/>
                  </person-group>
                  <article-title>Training improves visual processing speed and generalizes to untrained functions</article-title>
                  <source>Sci. Rep.</source>
                  <year>2014</year>
                  <volume>4</volume>
                  <fpage>7251</fpage>
                  <pub-id pub-id-type="doi">10.1038/srep07251</pub-id>
                  <?supplied-pmid 25431233?>
                  <pub-id pub-id-type="pmid">25431233</pub-id>
                </element-citation>
              </ref>
              <ref id="CR14">
                <label>14.</label>
                <element-citation publication-type="journal">
                  <person-group person-group-type="author">
                    <name>
                      <surname>Polat</surname>
                      <given-names>U</given-names>
                    </name>
                    <etal/>
                  </person-group>
                  <article-title>Training the brain to overcome the effect of aging on the human eye</article-title>
                  <source>Sci. Rep.</source>
                  <year>2012</year>
                  <volume>2</volume>
                  <fpage>278</fpage>
                  <pub-id pub-id-type="doi">10.1038/srep00278</pub-id>
                  <?supplied-pmid 22363834?>
                  <pub-id pub-id-type="pmid">22363834</pub-id>
                </element-citation>
              </ref>
              <ref id="CR15">
                <label>15.</label>
                <element-citation publication-type="journal">
                  <person-group person-group-type="author">
                    <name>
                      <surname>Sterkin</surname>
                      <given-names>A</given-names>
                    </name>
                    <etal/>
                  </person-group>
                  <article-title>Vision improvement in pilots with presbyopia following perceptual learning</article-title>
                  <source>Vis. Res.</source>
                  <year>2018</year>
                  <volume>152</volume>
                  <fpage>61</fpage>
                  <lpage>73</lpage>
                  <pub-id pub-id-type="doi">10.1016/j.visres.2017.09.003</pub-id>
                  <?supplied-pmid 29154795?>
                  <pub-id pub-id-type="pmid">29154795</pub-id>
                </element-citation>
              </ref>
              <ref id="CR16">
                <label>16.</label>
                <element-citation publication-type="journal">
                  <person-group person-group-type="author">
                    <name>
                      <surname>Yehezkel</surname>
                      <given-names>O</given-names>
                    </name>
                    <name>
                      <surname>Sterkin</surname>
                      <given-names>A</given-names>
                    </name>
                    <name>
                      <surname>Lev</surname>
                      <given-names>M</given-names>
                    </name>
                    <name>
                      <surname>Polat</surname>
                      <given-names>U</given-names>
                    </name>
                  </person-group>
                  <article-title>Training on spatiotemporal masking improves crowded and uncrowded visual acuity</article-title>
                  <source>J. Vis.</source>
                  <year>2015</year>
                  <volume>15</volume>
                  <fpage>12</fpage>
                  <pub-id pub-id-type="doi">10.1167/15.6.12</pub-id>
                  <?supplied-pmid 26024459?>
                  <pub-id pub-id-type="pmid">26024459</pub-id>
                </element-citation>
              </ref>
              <ref id="CR17">
                <label>17.</label>
                <element-citation publication-type="journal">
                  <person-group person-group-type="author">
                    <name>
                      <surname>Deveau</surname>
                      <given-names>J</given-names>
                    </name>
                    <name>
                      <surname>Ozer</surname>
                      <given-names>DJ</given-names>
                    </name>
                    <name>
                      <surname>Seitz</surname>
                      <given-names>AR</given-names>
                    </name>
                  </person-group>
                  <article-title>Improved vision and on-field performance in baseball through perceptual learning</article-title>
                  <source>Curr. Biol.</source>
                  <year>2014</year>
                  <volume>24</volume>
                  <fpage>R146</fpage>
                  <lpage>R147</lpage>
                  <pub-id pub-id-type="doi">10.1016/j.cub.2014.01.004</pub-id>
                  <?supplied-pmid 24556432?>
                  <pub-id pub-id-type="pmid">24556432</pub-id>
                </element-citation>
              </ref>
              <ref id="CR18">
                <label>18.</label>
                <element-citation publication-type="journal">
                  <person-group person-group-type="author">
                    <name>
                      <surname>Ritter</surname>
                      <given-names>DR</given-names>
                    </name>
                    <name>
                      <surname>Ysseldyke</surname>
                      <given-names>JE</given-names>
                    </name>
                  </person-group>
                  <article-title>Convergent and discriminant validation of the trait of visual figure-ground perception</article-title>
                  <source>J. Learn. Disabil.</source>
                  <year>1976</year>
                  <volume>9</volume>
                  <fpage>319</fpage>
                  <lpage>325</lpage>
                  <pub-id pub-id-type="doi">10.1177/002221947600900511</pub-id>
                </element-citation>
              </ref>
              <ref id="CR19">
                <label>19.</label>
                <element-citation publication-type="journal">
                  <person-group person-group-type="author">
                    <name>
                      <surname>Zusne</surname>
                      <given-names>L</given-names>
                    </name>
                  </person-group>
                  <article-title>Visual figure-ground and perceptual stability</article-title>
                  <source>Percept. Mot. Skills</source>
                  <year>1993</year>
                  <volume>77</volume>
                  <fpage>564</fpage>
                  <lpage>566</lpage>
                  <pub-id pub-id-type="doi">10.2466/pms.1993.77.2.564</pub-id>
                  <?supplied-pmid 8247680?>
                  <pub-id pub-id-type="pmid">8247680</pub-id>
                </element-citation>
              </ref>
              <ref id="CR20">
                <label>20.</label>
                <element-citation publication-type="journal">
                  <person-group person-group-type="author">
                    <name>
                      <surname>Clark</surname>
                      <given-names>K</given-names>
                    </name>
                    <name>
                      <surname>Appelbaum</surname>
                      <given-names>LG</given-names>
                    </name>
                    <name>
                      <surname>van den Berg</surname>
                      <given-names>B</given-names>
                    </name>
                    <name>
                      <surname>Mitroff</surname>
                      <given-names>SR</given-names>
                    </name>
                    <name>
                      <surname>Woldorff</surname>
                      <given-names>MG</given-names>
                    </name>
                  </person-group>
                  <article-title>Improvement in visual search with practice: Mapping learning-related changes in neurocognitive stages of processing</article-title>
                  <source>J. Neurosci.</source>
                  <year>2015</year>
                  <volume>35</volume>
                  <fpage>5351</fpage>
                  <lpage>5359</lpage>
                  <pub-id pub-id-type="doi">10.1523/JNEUROSCI.1152-14.2015</pub-id>
                  <?supplied-pmid 25834059?>
                  <pub-id pub-id-type="pmid">25834059</pub-id>
                </element-citation>
              </ref>
              <ref id="CR21">
                <label>21.</label>
                <element-citation publication-type="journal">
                  <person-group person-group-type="author">
                    <name>
                      <surname>Le Dantec</surname>
                      <given-names>CC</given-names>
                    </name>
                    <name>
                      <surname>Melton</surname>
                      <given-names>EE</given-names>
                    </name>
                    <name>
                      <surname>Seitz</surname>
                      <given-names>AR</given-names>
                    </name>
                  </person-group>
                  <article-title>A triple dissociation between learning of target, distractors, and spatial contexts</article-title>
                  <source>J. Vis.</source>
                  <year>2012</year>
                  <volume>12</volume>
                  <fpage>1</fpage>
                  <lpage>12</lpage>
                  <pub-id pub-id-type="doi">10.1167/12.2.5</pub-id>
                </element-citation>
              </ref>
              <ref id="CR22">
                <label>22.</label>
                <element-citation publication-type="journal">
                  <person-group person-group-type="author">
                    <name>
                      <surname>Kuo</surname>
                      <given-names>B-C</given-names>
                    </name>
                    <name>
                      <surname>Rao</surname>
                      <given-names>A</given-names>
                    </name>
                    <name>
                      <surname>Lepsien</surname>
                      <given-names>J</given-names>
                    </name>
                    <name>
                      <surname>Nobre</surname>
                      <given-names>AC</given-names>
                    </name>
                  </person-group>
                  <article-title>Searching for targets within the spatial layout of visual short-term memory</article-title>
                  <source>J. Neurosci.</source>
                  <year>2009</year>
                  <volume>29</volume>
                  <fpage>8032</fpage>
                  <lpage>8038</lpage>
                  <pub-id pub-id-type="doi">10.1523/JNEUROSCI.0952-09.2009</pub-id>
                  <?supplied-pmid 19553443?>
                  <pub-id pub-id-type="pmid">19553443</pub-id>
                </element-citation>
              </ref>
              <ref id="CR23">
                <label>23.</label>
                <element-citation publication-type="journal">
                  <person-group person-group-type="author">
                    <name>
                      <surname>Kong</surname>
                      <given-names>G</given-names>
                    </name>
                    <name>
                      <surname>Fougnie</surname>
                      <given-names>D</given-names>
                    </name>
                  </person-group>
                  <article-title>Visual search within working memory</article-title>
                  <source>J. Exp. Psychol. Gen.</source>
                  <year>2019</year>
                  <volume>148</volume>
                  <fpage>1688</fpage>
                  <lpage>1700</lpage>
                  <pub-id pub-id-type="doi">10.1037/xge0000555</pub-id>
                  <?supplied-pmid 30667264?>
                  <pub-id pub-id-type="pmid">30667264</pub-id>
                </element-citation>
              </ref>
              <ref id="CR24">
                <label>24.</label>
                <element-citation publication-type="journal">
                  <person-group person-group-type="author">
                    <name>
                      <surname>Emrich</surname>
                      <given-names>SM</given-names>
                    </name>
                    <name>
                      <surname>Al-Aidroos</surname>
                      <given-names>N</given-names>
                    </name>
                    <name>
                      <surname>Pratt</surname>
                      <given-names>J</given-names>
                    </name>
                    <name>
                      <surname>Ferber</surname>
                      <given-names>S</given-names>
                    </name>
                  </person-group>
                  <article-title>Visual search elicits the electrophysiological marker of visual working memory</article-title>
                  <source>PLoS ONE</source>
                  <year>2009</year>
                  <volume>4</volume>
                  <fpage>e8042</fpage>
                  <pub-id pub-id-type="doi">10.1371/journal.pone.0008042</pub-id>
                  <?supplied-pmid 19956663?>
                  <pub-id pub-id-type="pmid">19956663</pub-id>
                </element-citation>
              </ref>
              <ref id="CR25">
                <label>25.</label>
                <element-citation publication-type="journal">
                  <person-group person-group-type="author">
                    <name>
                      <surname>Luria</surname>
                      <given-names>R</given-names>
                    </name>
                    <name>
                      <surname>Vogel</surname>
                      <given-names>EK</given-names>
                    </name>
                  </person-group>
                  <article-title>Visual search demands dictate reliance on working memory storage</article-title>
                  <source>J. Neurosci.</source>
                  <year>2011</year>
                  <volume>31</volume>
                  <fpage>6199</fpage>
                  <lpage>6207</lpage>
                  <pub-id pub-id-type="doi">10.1523/JNEUROSCI.6453-10.2011</pub-id>
                  <?supplied-pmid 21508243?>
                  <pub-id pub-id-type="pmid">21508243</pub-id>
                </element-citation>
              </ref>
              <ref id="CR26">
                <label>26.</label>
                <element-citation publication-type="journal">
                  <person-group person-group-type="author">
                    <name>
                      <surname>Bundesen</surname>
                      <given-names>C</given-names>
                    </name>
                  </person-group>
                  <article-title>A theory of visual attention</article-title>
                  <source>Psychol. Rev.</source>
                  <year>1990</year>
                  <volume>97</volume>
                  <fpage>523</fpage>
                  <lpage>547</lpage>
                  <pub-id pub-id-type="doi">10.1037/0033-295X.97.4.523</pub-id>
                  <?supplied-pmid 2247540?>
                  <pub-id pub-id-type="pmid">2247540</pub-id>
                </element-citation>
              </ref>
              <ref id="CR27">
                <label>27.</label>
                <element-citation publication-type="journal">
                  <person-group person-group-type="author">
                    <name>
                      <surname>Desimone</surname>
                      <given-names>R</given-names>
                    </name>
                    <name>
                      <surname>Duncan</surname>
                      <given-names>J</given-names>
                    </name>
                  </person-group>
                  <article-title>Neural mechanisms of selective visual attention</article-title>
                  <source>Annu. Rev. Neurosci.</source>
                  <year>1995</year>
                  <volume>18</volume>
                  <fpage>193</fpage>
                  <lpage>222</lpage>
                  <pub-id pub-id-type="doi">10.1146/annurev.ne.18.030195.001205</pub-id>
                  <?supplied-pmid 7605061?>
                  <pub-id pub-id-type="pmid">7605061</pub-id>
                </element-citation>
              </ref>
              <ref id="CR28">
                <label>28.</label>
                <element-citation publication-type="journal">
                  <person-group person-group-type="author">
                    <name>
                      <surname>Thurman</surname>
                      <given-names>SM</given-names>
                    </name>
                    <name>
                      <surname>Davey</surname>
                      <given-names>PG</given-names>
                    </name>
                    <name>
                      <surname>McCray</surname>
                      <given-names>KL</given-names>
                    </name>
                    <name>
                      <surname>Paronian</surname>
                      <given-names>V</given-names>
                    </name>
                    <name>
                      <surname>Seitz</surname>
                      <given-names>AR</given-names>
                    </name>
                  </person-group>
                  <article-title>Predicting individual contrast sensitivity functions from acuity and letter contrast sensitivity measurements</article-title>
                  <source>J. Vis.</source>
                  <year>2016</year>
                  <volume>16</volume>
                  <fpage>15</fpage>
                  <pub-id pub-id-type="doi">10.1167/16.15.15</pub-id>
                  <?supplied-pmid 28006065?>
                  <pub-id pub-id-type="pmid">28006065</pub-id>
                </element-citation>
              </ref>
              <ref id="CR29">
                <label>29.</label>
                <element-citation publication-type="journal">
                  <person-group person-group-type="author">
                    <name>
                      <surname>Xing</surname>
                      <given-names>Y</given-names>
                    </name>
                    <name>
                      <surname>Ledgeway</surname>
                      <given-names>T</given-names>
                    </name>
                    <name>
                      <surname>McGraw</surname>
                      <given-names>PV</given-names>
                    </name>
                    <name>
                      <surname>Schluppeck</surname>
                      <given-names>D</given-names>
                    </name>
                  </person-group>
                  <article-title>Decoding working memory of stimulus contrast in early visual cortex</article-title>
                  <source>J. Neurosci.</source>
                  <year>2013</year>
                  <volume>33</volume>
                  <fpage>10301</fpage>
                  <lpage>10311</lpage>
                  <pub-id pub-id-type="doi">10.1523/JNEUROSCI.3754-12.2013</pub-id>
                  <?supplied-pmid 23785144?>
                  <pub-id pub-id-type="pmid">23785144</pub-id>
                </element-citation>
              </ref>
              <ref id="CR30">
                <label>30.</label>
                <element-citation publication-type="journal">
                  <person-group person-group-type="author">
                    <name>
                      <surname>Harrison</surname>
                      <given-names>SA</given-names>
                    </name>
                    <name>
                      <surname>Tong</surname>
                      <given-names>F</given-names>
                    </name>
                  </person-group>
                  <article-title>Decoding reveals the contents of visual working memory in early visual areas</article-title>
                  <source>Nature</source>
                  <year>2009</year>
                  <volume>458</volume>
                  <fpage>632</fpage>
                  <lpage>635</lpage>
                  <pub-id pub-id-type="doi">10.1038/nature07832</pub-id>
                  <?supplied-pmid 19225460?>
                  <pub-id pub-id-type="pmid">19225460</pub-id>
                </element-citation>
              </ref>
              <ref id="CR31">
                <label>31.</label>
                <element-citation publication-type="journal">
                  <person-group person-group-type="author">
                    <name>
                      <surname>Grzymisch</surname>
                      <given-names>A</given-names>
                    </name>
                    <name>
                      <surname>Grimsen</surname>
                      <given-names>C</given-names>
                    </name>
                    <name>
                      <surname>Ernst</surname>
                      <given-names>UA</given-names>
                    </name>
                  </person-group>
                  <article-title>Contour integration in dynamic scenes: impaired detection performance in extended presentations</article-title>
                  <source>Front. Psychol.</source>
                  <year>2017</year>
                  <volume>8</volume>
                  <fpage>1501</fpage>
                  <pub-id pub-id-type="doi">10.3389/fpsyg.2017.01501</pub-id>
                  <?supplied-pmid 28928692?>
                  <pub-id pub-id-type="pmid">28928692</pub-id>
                </element-citation>
              </ref>
              <ref id="CR32">
                <label>32.</label>
                <element-citation publication-type="journal">
                  <person-group person-group-type="author">
                    <name>
                      <surname>Hansen</surname>
                      <given-names>T</given-names>
                    </name>
                    <name>
                      <surname>Neumann</surname>
                      <given-names>H</given-names>
                    </name>
                  </person-group>
                  <article-title>A recurrent model of contour integration in primary visual cortex</article-title>
                  <source>J. Vis.</source>
                  <year>2008</year>
                  <volume>8</volume>
                  <fpage>1</fpage>
                  <lpage>25</lpage>
                  <?supplied-pmid 18842084?>
                  <pub-id pub-id-type="pmid">18842084</pub-id>
                </element-citation>
              </ref>
              <ref id="CR33">
                <label>33.</label>
                <element-citation publication-type="journal">
                  <person-group person-group-type="author">
                    <name>
                      <surname>Volberg</surname>
                      <given-names>G</given-names>
                    </name>
                    <name>
                      <surname>Wutz</surname>
                      <given-names>A</given-names>
                    </name>
                    <name>
                      <surname>Greenlee</surname>
                      <given-names>MW</given-names>
                    </name>
                  </person-group>
                  <article-title>Top-down control in contour grouping</article-title>
                  <source>PLoS ONE</source>
                  <year>2013</year>
                  <volume>8</volume>
                  <fpage>e54085</fpage>
                  <pub-id pub-id-type="doi">10.1371/journal.pone.0054085</pub-id>
                  <?supplied-pmid 23326575?>
                  <pub-id pub-id-type="pmid">23326575</pub-id>
                </element-citation>
              </ref>
              <ref id="CR34">
                <label>34.</label>
                <element-citation publication-type="journal">
                  <person-group person-group-type="author">
                    <name>
                      <surname>Li</surname>
                      <given-names>J</given-names>
                    </name>
                    <name>
                      <surname>Qian</surname>
                      <given-names>J</given-names>
                    </name>
                    <name>
                      <surname>Liang</surname>
                      <given-names>F</given-names>
                    </name>
                  </person-group>
                  <article-title>Evidence for the beneficial effect of perceptual grouping on visual working memory: An empirical study on illusory contour and a meta-analytic study</article-title>
                  <source>Sci. Rep.</source>
                  <year>2018</year>
                  <volume>8</volume>
                  <fpage>13864</fpage>
                  <pub-id pub-id-type="doi">10.1038/s41598-018-32039-4</pub-id>
                  <?supplied-pmid 30218056?>
                  <pub-id pub-id-type="pmid">30218056</pub-id>
                </element-citation>
              </ref>
              <ref id="CR35">
                <label>35.</label>
                <element-citation publication-type="journal">
                  <person-group person-group-type="author">
                    <name>
                      <surname>Zhou</surname>
                      <given-names>Y</given-names>
                    </name>
                    <name>
                      <surname>Acerbi</surname>
                      <given-names>L</given-names>
                    </name>
                    <name>
                      <surname>Ma</surname>
                      <given-names>WJ</given-names>
                    </name>
                  </person-group>
                  <article-title>The role of sensory uncertainty in simple contour integration</article-title>
                  <source>PLoS Comput. Biol.</source>
                  <year>2020</year>
                  <volume>16</volume>
                  <fpage>e1006308</fpage>
                  <pub-id pub-id-type="doi">10.1371/journal.pcbi.1006308</pub-id>
                  <?supplied-pmid 33253195?>
                  <pub-id pub-id-type="pmid">33253195</pub-id>
                </element-citation>
              </ref>
              <ref id="CR36">
                <label>36.</label>
                <element-citation publication-type="journal">
                  <person-group person-group-type="author">
                    <name>
                      <surname>Green</surname>
                      <given-names>CS</given-names>
                    </name>
                    <name>
                      <surname>Kattner</surname>
                      <given-names>F</given-names>
                    </name>
                    <name>
                      <surname>Siegel</surname>
                      <given-names>MH</given-names>
                    </name>
                    <name>
                      <surname>Kersten</surname>
                      <given-names>D</given-names>
                    </name>
                    <name>
                      <surname>Schrater</surname>
                      <given-names>PR</given-names>
                    </name>
                  </person-group>
                  <article-title>Differences in perceptual learning transfer as a function of training task</article-title>
                  <source>J. Vis.</source>
                  <year>2015</year>
                  <volume>15</volume>
                  <fpage>5</fpage>
                  <pub-id pub-id-type="doi">10.1167/15.10.5</pub-id>
                  <?supplied-pmid 26305737?>
                  <pub-id pub-id-type="pmid">26305737</pub-id>
                </element-citation>
              </ref>
              <ref id="CR37">
                <label>37.</label>
                <mixed-citation publication-type="other">Sloane, M. E., Ball, K., Owsley, C., Bruni, J. R. &amp; Roenker, D. L. The visual activities questionnaire: Developing an instrument for assessing problems in everyday visual tasks. in <italic>Technical Digest, Noninvasive Assessment of the Visual System</italic> (1992).</mixed-citation>
              </ref>
              <ref id="CR38">
                <label>38.</label>
                <element-citation publication-type="journal">
                  <person-group person-group-type="author">
                    <name>
                      <surname>Jaeggi</surname>
                      <given-names>SM</given-names>
                    </name>
                    <etal/>
                  </person-group>
                  <article-title>Investigating the effects of spacing on working memory training outcome: A randomized, controlled, multisite trial in older adults</article-title>
                  <source>J. Gerontol. B. Psychol. Sci. Soc. Sci.</source>
                  <year>2020</year>
                  <volume>75</volume>
                  <fpage>1181</fpage>
                  <lpage>1192</lpage>
                  <pub-id pub-id-type="doi">10.1093/geronb/gbz090</pub-id>
                  <?supplied-pmid 31353413?>
                  <pub-id pub-id-type="pmid">31353413</pub-id>
                </element-citation>
              </ref>
              <ref id="CR39">
                <label>39.</label>
                <element-citation publication-type="journal">
                  <person-group person-group-type="author">
                    <name>
                      <surname>Tsai</surname>
                      <given-names>N</given-names>
                    </name>
                    <etal/>
                  </person-group>
                  <article-title>(Un)Great expectations: The role of placebo effects in cognitive training</article-title>
                  <source>J. Appl. Res. Mem. Cogn.</source>
                  <year>2018</year>
                  <volume>7</volume>
                  <fpage>564</fpage>
                  <lpage>573</lpage>
                  <pub-id pub-id-type="doi">10.1037/h0101826</pub-id>
                  <?supplied-pmid 31660288?>
                  <pub-id pub-id-type="pmid">31660288</pub-id>
                </element-citation>
              </ref>
              <ref id="CR40">
                <label>40.</label>
                <element-citation publication-type="journal">
                  <person-group person-group-type="author">
                    <name>
                      <surname>Stoet</surname>
                      <given-names>G</given-names>
                    </name>
                  </person-group>
                  <article-title>Sex differences in search and gathering skills</article-title>
                  <source>Evol. Hum. Behav.</source>
                  <year>2011</year>
                  <volume>32</volume>
                  <fpage>416</fpage>
                  <lpage>422</lpage>
                  <pub-id pub-id-type="doi">10.1016/j.evolhumbehav.2011.03.001</pub-id>
                </element-citation>
              </ref>
              <ref id="CR41">
                <label>41.</label>
                <element-citation publication-type="book">
                  <person-group person-group-type="author">
                    <name>
                      <surname>Handford</surname>
                      <given-names>M</given-names>
                    </name>
                  </person-group>
                  <source>Where’s Waldo? The Great Picture Hunt!</source>
                  <year>2010</year>
                  <publisher-name>Candlewick Press</publisher-name>
                </element-citation>
              </ref>
              <ref id="CR42">
                <label>42.</label>
                <element-citation publication-type="book">
                  <person-group person-group-type="author">
                    <name>
                      <surname>Handford</surname>
                      <given-names>M</given-names>
                    </name>
                  </person-group>
                  <source>Where’s Waldo? The Wonder Book</source>
                  <year>2017</year>
                  <publisher-name>Candlewick Press</publisher-name>
                </element-citation>
              </ref>
              <ref id="CR43">
                <label>43.</label>
                <element-citation publication-type="journal">
                  <person-group person-group-type="author">
                    <name>
                      <surname>Deveau</surname>
                      <given-names>J</given-names>
                    </name>
                    <name>
                      <surname>Lovcik</surname>
                      <given-names>G</given-names>
                    </name>
                    <name>
                      <surname>Seitz</surname>
                      <given-names>AR</given-names>
                    </name>
                  </person-group>
                  <article-title>Broad-based visual benefits from training with an integrated perceptual-learning video game</article-title>
                  <source>Vis. Res.</source>
                  <year>2014</year>
                  <volume>99</volume>
                  <fpage>134</fpage>
                  <lpage>140</lpage>
                  <pub-id pub-id-type="doi">10.1016/j.visres.2013.12.015</pub-id>
                  <?supplied-pmid 24406157?>
                  <pub-id pub-id-type="pmid">24406157</pub-id>
                </element-citation>
              </ref>
              <ref id="CR44">
                <label>44.</label>
                <element-citation publication-type="journal">
                  <person-group person-group-type="author">
                    <name>
                      <surname>Silverstein</surname>
                      <given-names>S</given-names>
                    </name>
                    <name>
                      <surname>Keane</surname>
                      <given-names>B</given-names>
                    </name>
                  </person-group>
                  <article-title>Perceptual organization in schizophrenia: Plasticity and state-related change</article-title>
                  <source>Learn. Percept.</source>
                  <year>2009</year>
                  <volume>1</volume>
                  <fpage>229</fpage>
                  <lpage>261</lpage>
                  <pub-id pub-id-type="doi">10.1556/LP.1.2009.2.111</pub-id>
                </element-citation>
              </ref>
              <ref id="CR45">
                <label>45.</label>
                <element-citation publication-type="journal">
                  <person-group person-group-type="author">
                    <name>
                      <surname>Peirce</surname>
                      <given-names>J</given-names>
                    </name>
                    <etal/>
                  </person-group>
                  <article-title>PsychoPy2: Experiments in behavior made easy</article-title>
                  <source>Behav. Res. Methods</source>
                  <year>2019</year>
                  <volume>51</volume>
                  <fpage>195</fpage>
                  <lpage>203</lpage>
                  <pub-id pub-id-type="doi">10.3758/s13428-018-01193-y</pub-id>
                  <?supplied-pmid 30734206?>
                  <pub-id pub-id-type="pmid">30734206</pub-id>
                </element-citation>
              </ref>
              <ref id="CR46">
                <label>46.</label>
                <mixed-citation publication-type="other">Sightseeing. <italic>UCR|Brain Game Center</italic><ext-link ext-link-type="uri" xlink:href="https://braingamecenter.ucr.edu/perceptual-training-apps/">https://braingamecenter.ucr.edu/perceptual-training-apps/</ext-link>.</mixed-citation>
              </ref>
              <ref id="CR47">
                <label>47.</label>
                <element-citation publication-type="journal">
                  <person-group person-group-type="author">
                    <name>
                      <surname>Collins</surname>
                      <given-names>CL</given-names>
                    </name>
                    <etal/>
                  </person-group>
                  <article-title>Video-based remote administration of cognitive assessments and interventions: a comparison with in-lab administration</article-title>
                  <source>J. Cogn. Enhanc.</source>
                  <year>2022</year>
                  <pub-id pub-id-type="doi">10.1007/s41465-022-00240-z</pub-id>
                  <?supplied-pmid 35261961?>
                  <pub-id pub-id-type="pmid">35261961</pub-id>
                </element-citation>
              </ref>
              <ref id="CR48">
                <label>48.</label>
                <mixed-citation publication-type="other">JASP Team. <italic>JASP</italic>. (2021).</mixed-citation>
              </ref>
              <ref id="CR49">
                <label>49.</label>
                <element-citation publication-type="journal">
                  <person-group person-group-type="author">
                    <name>
                      <surname>Leys</surname>
                      <given-names>C</given-names>
                    </name>
                    <name>
                      <surname>Ley</surname>
                      <given-names>C</given-names>
                    </name>
                    <name>
                      <surname>Klein</surname>
                      <given-names>O</given-names>
                    </name>
                    <name>
                      <surname>Bernard</surname>
                      <given-names>P</given-names>
                    </name>
                    <name>
                      <surname>Licata</surname>
                      <given-names>L</given-names>
                    </name>
                  </person-group>
                  <article-title>Detecting outliers: Do not use standard deviation around the mean, use absolute deviation around the median</article-title>
                  <source>J. Exp. Soc. Psychol.</source>
                  <year>2013</year>
                  <volume>49</volume>
                  <fpage>764</fpage>
                  <lpage>766</lpage>
                  <pub-id pub-id-type="doi">10.1016/j.jesp.2013.03.013</pub-id>
                </element-citation>
              </ref>
              <ref id="CR50">
                <label>50.</label>
                <element-citation publication-type="journal">
                  <person-group person-group-type="author">
                    <name>
                      <surname>van den Bergh</surname>
                      <given-names>D</given-names>
                    </name>
                    <etal/>
                  </person-group>
                  <article-title>A tutorial on conducting and interpreting a Bayesian ANOVA in JASP</article-title>
                  <source>L’Année Psychol.</source>
                  <year>2020</year>
                  <volume>120</volume>
                  <fpage>73</fpage>
                  <lpage>96</lpage>
                  <pub-id pub-id-type="doi">10.3917/anpsy1.201.0073</pub-id>
                </element-citation>
              </ref>
              <ref id="CR51">
                <label>51.</label>
                <element-citation publication-type="journal">
                  <person-group person-group-type="author">
                    <name>
                      <surname>Beck</surname>
                      <given-names>MR</given-names>
                    </name>
                    <name>
                      <surname>van Lamsweerde</surname>
                      <given-names>AE</given-names>
                    </name>
                  </person-group>
                  <article-title>Accessing long-term memory representations during visual change detection</article-title>
                  <source>Mem. Cognit.</source>
                  <year>2011</year>
                  <volume>39</volume>
                  <fpage>433</fpage>
                  <lpage>446</lpage>
                  <pub-id pub-id-type="doi">10.3758/s13421-010-0033-4</pub-id>
                  <?supplied-pmid 21264606?>
                  <pub-id pub-id-type="pmid">21264606</pub-id>
                </element-citation>
              </ref>
              <ref id="CR52">
                <label>52.</label>
                <element-citation publication-type="journal">
                  <person-group person-group-type="author">
                    <name>
                      <surname>Herman</surname>
                      <given-names>JP</given-names>
                    </name>
                    <name>
                      <surname>Bogadhi</surname>
                      <given-names>AR</given-names>
                    </name>
                    <name>
                      <surname>Krauzlis</surname>
                      <given-names>RJ</given-names>
                    </name>
                  </person-group>
                  <article-title>Effects of spatial cues on color-change detection in humans</article-title>
                  <source>J. Vis.</source>
                  <year>2015</year>
                  <volume>15</volume>
                  <fpage>3</fpage>
                  <pub-id pub-id-type="doi">10.1167/15.6.3</pub-id>
                  <?supplied-pmid 26047359?>
                  <pub-id pub-id-type="pmid">26047359</pub-id>
                </element-citation>
              </ref>
              <ref id="CR53">
                <label>53.</label>
                <element-citation publication-type="journal">
                  <person-group person-group-type="author">
                    <name>
                      <surname>Jiang</surname>
                      <given-names>Y</given-names>
                    </name>
                    <name>
                      <surname>Chun</surname>
                      <given-names>MM</given-names>
                    </name>
                    <name>
                      <surname>Olson</surname>
                      <given-names>IR</given-names>
                    </name>
                  </person-group>
                  <article-title>Perceptual grouping in change detection</article-title>
                  <source>Percept. Psychophys.</source>
                  <year>2004</year>
                  <volume>66</volume>
                  <fpage>446</fpage>
                  <lpage>453</lpage>
                  <pub-id pub-id-type="doi">10.3758/BF03194892</pub-id>
                  <?supplied-pmid 15283069?>
                  <pub-id pub-id-type="pmid">15283069</pub-id>
                </element-citation>
              </ref>
              <ref id="CR54">
                <label>54.</label>
                <element-citation publication-type="journal">
                  <person-group person-group-type="author">
                    <name>
                      <surname>Joseph</surname>
                      <given-names>RM</given-names>
                    </name>
                    <name>
                      <surname>Keehn</surname>
                      <given-names>B</given-names>
                    </name>
                    <name>
                      <surname>Connolly</surname>
                      <given-names>C</given-names>
                    </name>
                    <name>
                      <surname>Wolfe</surname>
                      <given-names>JM</given-names>
                    </name>
                    <name>
                      <surname>Horowitz</surname>
                      <given-names>TS</given-names>
                    </name>
                  </person-group>
                  <article-title>Why is visual search superior in autism spectrum disorder?: Visual search in ASD</article-title>
                  <source>Dev. Sci.</source>
                  <year>2009</year>
                  <volume>12</volume>
                  <fpage>1083</fpage>
                  <lpage>1096</lpage>
                  <pub-id pub-id-type="doi">10.1111/j.1467-7687.2009.00855.x</pub-id>
                  <?supplied-pmid 19840062?>
                  <pub-id pub-id-type="pmid">19840062</pub-id>
                </element-citation>
              </ref>
              <ref id="CR55">
                <label>55.</label>
                <element-citation publication-type="journal">
                  <person-group person-group-type="author">
                    <name>
                      <surname>Yoshimura</surname>
                      <given-names>N</given-names>
                    </name>
                    <name>
                      <surname>Yonemitsu</surname>
                      <given-names>F</given-names>
                    </name>
                    <name>
                      <surname>Marmolejo-Ramos</surname>
                      <given-names>F</given-names>
                    </name>
                    <name>
                      <surname>Ariga</surname>
                      <given-names>A</given-names>
                    </name>
                    <name>
                      <surname>Yamada</surname>
                      <given-names>Y</given-names>
                    </name>
                  </person-group>
                  <article-title>Task difficulty modulates the disrupting effects of oral respiration on visual search performance</article-title>
                  <source>J. Cogn.</source>
                  <year>2019</year>
                  <volume>2</volume>
                  <fpage>21</fpage>
                  <pub-id pub-id-type="doi">10.5334/joc.77</pub-id>
                  <?supplied-pmid 31517239?>
                  <pub-id pub-id-type="pmid">31517239</pub-id>
                </element-citation>
              </ref>
              <ref id="CR56">
                <label>56.</label>
                <element-citation publication-type="journal">
                  <person-group person-group-type="author">
                    <name>
                      <surname>Hulleman</surname>
                      <given-names>J</given-names>
                    </name>
                    <name>
                      <surname>Olivers</surname>
                      <given-names>CNL</given-names>
                    </name>
                  </person-group>
                  <article-title>The impending demise of the item in visual search</article-title>
                  <source>Behav. Brain Sci.</source>
                  <year>2017</year>
                  <volume>40</volume>
                  <fpage>e132</fpage>
                  <pub-id pub-id-type="doi">10.1017/S0140525X15002794</pub-id>
                  <?supplied-pmid 26673054?>
                  <pub-id pub-id-type="pmid">26673054</pub-id>
                </element-citation>
              </ref>
              <ref id="CR57">
                <label>57.</label>
                <element-citation publication-type="journal">
                  <person-group person-group-type="author">
                    <name>
                      <surname>Furmanski</surname>
                      <given-names>CS</given-names>
                    </name>
                    <name>
                      <surname>Schluppeck</surname>
                      <given-names>D</given-names>
                    </name>
                    <name>
                      <surname>Engel</surname>
                      <given-names>SA</given-names>
                    </name>
                  </person-group>
                  <article-title>Learning strengthens the response of primary visual cortex to simple patterns</article-title>
                  <source>Curr. Biol.</source>
                  <year>2004</year>
                  <volume>14</volume>
                  <fpage>573</fpage>
                  <lpage>578</lpage>
                  <pub-id pub-id-type="doi">10.1016/j.cub.2004.03.032</pub-id>
                  <?supplied-pmid 15062097?>
                  <pub-id pub-id-type="pmid">15062097</pub-id>
                </element-citation>
              </ref>
              <ref id="CR58">
                <label>58.</label>
                <element-citation publication-type="journal">
                  <person-group person-group-type="author">
                    <name>
                      <surname>Deveau</surname>
                      <given-names>J</given-names>
                    </name>
                    <name>
                      <surname>Seitz</surname>
                      <given-names>AR</given-names>
                    </name>
                  </person-group>
                  <article-title>Applying perceptual learning to achieve practical changes in vision</article-title>
                  <source>Front. Psychol.</source>
                  <year>2014</year>
                  <volume>5</volume>
                  <fpage>1166</fpage>
                  <pub-id pub-id-type="doi">10.3389/fpsyg.2014.01166</pub-id>
                  <?supplied-pmid 25360128?>
                  <pub-id pub-id-type="pmid">25360128</pub-id>
                </element-citation>
              </ref>
              <ref id="CR59">
                <label>59.</label>
                <element-citation publication-type="journal">
                  <person-group person-group-type="author">
                    <name>
                      <surname>Li</surname>
                      <given-names>R</given-names>
                    </name>
                    <name>
                      <surname>Polat</surname>
                      <given-names>U</given-names>
                    </name>
                    <name>
                      <surname>Makous</surname>
                      <given-names>W</given-names>
                    </name>
                    <name>
                      <surname>Bavelier</surname>
                      <given-names>D</given-names>
                    </name>
                  </person-group>
                  <article-title>Enhancing the contrast sensitivity function through action video game training</article-title>
                  <source>Nat. Neurosci.</source>
                  <year>2009</year>
                  <volume>12</volume>
                  <fpage>549</fpage>
                  <lpage>551</lpage>
                  <pub-id pub-id-type="doi">10.1038/nn.2296</pub-id>
                  <?supplied-pmid 19330003?>
                  <pub-id pub-id-type="pmid">19330003</pub-id>
                </element-citation>
              </ref>
              <ref id="CR60">
                <label>60.</label>
                <element-citation publication-type="journal">
                  <person-group person-group-type="author">
                    <name>
                      <surname>Polat</surname>
                      <given-names>U</given-names>
                    </name>
                  </person-group>
                  <article-title>Making perceptual learning practical to improve visual functions</article-title>
                  <source>Vis. Res.</source>
                  <year>2009</year>
                  <volume>49</volume>
                  <fpage>2566</fpage>
                  <lpage>2573</lpage>
                  <pub-id pub-id-type="doi">10.1016/j.visres.2009.06.005</pub-id>
                  <?supplied-pmid 19520103?>
                  <pub-id pub-id-type="pmid">19520103</pub-id>
                </element-citation>
              </ref>
              <ref id="CR61">
                <label>61.</label>
                <element-citation publication-type="journal">
                  <person-group person-group-type="author">
                    <name>
                      <surname>Seitz</surname>
                      <given-names>AR</given-names>
                    </name>
                    <name>
                      <surname>Nanez</surname>
                      <given-names>JE</given-names>
                    </name>
                    <name>
                      <surname>Holloway</surname>
                      <given-names>SR</given-names>
                    </name>
                    <name>
                      <surname>Koyama</surname>
                      <given-names>S</given-names>
                    </name>
                    <name>
                      <surname>Watanabe</surname>
                      <given-names>T</given-names>
                    </name>
                  </person-group>
                  <article-title>Seeing what is not there shows the costs of perceptual learning</article-title>
                  <source>Proc. Natl. Acad. Sci.</source>
                  <year>2005</year>
                  <volume>102</volume>
                  <fpage>9080</fpage>
                  <lpage>9085</lpage>
                  <pub-id pub-id-type="doi">10.1073/pnas.0501026102</pub-id>
                  <?supplied-pmid 15956204?>
                  <pub-id pub-id-type="pmid">15956204</pub-id>
                </element-citation>
              </ref>
              <ref id="CR62">
                <label>62.</label>
                <element-citation publication-type="journal">
                  <person-group person-group-type="author">
                    <name>
                      <surname>Pahor</surname>
                      <given-names>A</given-names>
                    </name>
                    <etal/>
                  </person-group>
                  <article-title>UCancellation: A new mobile measure of selective attention and concentration</article-title>
                  <source>Behav. Res. Methods</source>
                  <year>2022</year>
                  <pub-id pub-id-type="doi">10.3758/s13428-021-01765-5</pub-id>
                  <?supplied-pmid 35106729?>
                  <pub-id pub-id-type="pmid">35106729</pub-id>
                </element-citation>
              </ref>
              <ref id="CR63">
                <label>63.</label>
                <element-citation publication-type="journal">
                  <person-group person-group-type="author">
                    <name>
                      <surname>Hess</surname>
                      <given-names>RF</given-names>
                    </name>
                    <name>
                      <surname>Hayes</surname>
                      <given-names>A</given-names>
                    </name>
                    <name>
                      <surname>Field</surname>
                      <given-names>DJ</given-names>
                    </name>
                  </person-group>
                  <article-title>Contour integration and cortical processing</article-title>
                  <source>J. Physiol.</source>
                  <year>2003</year>
                  <volume>97</volume>
                  <fpage>105</fpage>
                  <lpage>119</lpage>
                </element-citation>
              </ref>
              <ref id="CR64">
                <label>64.</label>
                <element-citation publication-type="journal">
                  <person-group person-group-type="author">
                    <name>
                      <surname>Persike</surname>
                      <given-names>M</given-names>
                    </name>
                    <name>
                      <surname>Meinhardt</surname>
                      <given-names>G</given-names>
                    </name>
                  </person-group>
                  <article-title>A new angle on contour integration: The role of corners</article-title>
                  <source>J. Vis.</source>
                  <year>2017</year>
                  <volume>17</volume>
                  <fpage>9</fpage>
                  <lpage>9</lpage>
                  <pub-id pub-id-type="doi">10.1167/17.12.9</pub-id>
                </element-citation>
              </ref>
              <ref id="CR65">
                <label>65.</label>
                <element-citation publication-type="journal">
                  <person-group person-group-type="author">
                    <name>
                      <surname>Pahor</surname>
                      <given-names>A</given-names>
                    </name>
                    <name>
                      <surname>Seitz</surname>
                      <given-names>AR</given-names>
                    </name>
                    <name>
                      <surname>Jaeggi</surname>
                      <given-names>SM</given-names>
                    </name>
                  </person-group>
                  <article-title>Near transfer to an unrelated N-back task mediates the effect of N-back working memory training on matrix reasoning</article-title>
                  <source>Nat. Hum. Behav.</source>
                  <year>2022</year>
                  <volume>2022</volume>
                  <fpage>1</fpage>
                  <lpage>14</lpage>
                </element-citation>
              </ref>
              <ref id="CR66">
                <label>66.</label>
                <element-citation publication-type="journal">
                  <person-group person-group-type="author">
                    <name>
                      <surname>Bridges</surname>
                      <given-names>D</given-names>
                    </name>
                    <name>
                      <surname>Pitiot</surname>
                      <given-names>A</given-names>
                    </name>
                    <name>
                      <surname>MacAskill</surname>
                      <given-names>MR</given-names>
                    </name>
                    <name>
                      <surname>Peirce</surname>
                      <given-names>JW</given-names>
                    </name>
                  </person-group>
                  <article-title>The timing mega-study: comparing a range of experiment generators, both lab-based and online</article-title>
                  <source>PeerJ</source>
                  <year>2020</year>
                  <volume>8</volume>
                  <fpage>e9414</fpage>
                  <pub-id pub-id-type="doi">10.7717/peerj.9414</pub-id>
                  <?supplied-pmid 33005482?>
                  <pub-id pub-id-type="pmid">33005482</pub-id>
                </element-citation>
              </ref>
            </ref-list>
          </back>
        </article>
      </metadata>
    </record>
  </GetRecord>
</OAI-PMH>
