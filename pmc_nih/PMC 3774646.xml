<?xml version="1.0" encoding="UTF-8"?>
<OAI-PMH xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/ http://www.openarchives.org/OAI/2.0/OAI-PMH.xsd">
  <responseDate>2024-05-03T06:17:52Z</responseDate>
  <request verb="GetRecord" identifier="oai:pubmedcentral.nih.gov:3774646" metadataPrefix="pmc">https:/www.ncbi.nlm.nih.gov/pmc/oai/oai.cgi</request>
  <GetRecord>
    <record>
      <header>
        <identifier>oai:pubmedcentral.nih.gov:3774646</identifier>
        <datestamp>2013-09-24</datestamp>
        <setSpec>plosone</setSpec>
        <setSpec>pmc-open</setSpec>
      </header>
      <metadata>
        <article xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" xsi:schemaLocation="https://jats.nlm.nih.gov/ns/archiving/1.3/ https://jats.nlm.nih.gov/archiving/1.3/xsd/JATS-archivearticle1-3.xsd" article-type="research-article">
          <front>
            <journal-meta>
              <journal-id journal-id-type="nlm-ta">PLoS One</journal-id>
              <journal-id journal-id-type="iso-abbrev">PLoS ONE</journal-id>
              <journal-id journal-id-type="publisher-id">plos</journal-id>
              <journal-id journal-id-type="pmc">plosone</journal-id>
              <journal-title-group>
                <journal-title>PLoS ONE</journal-title>
              </journal-title-group>
              <issn pub-type="epub">1932-6203</issn>
              <publisher>
                <publisher-name>Public Library of Science</publisher-name>
                <publisher-loc>San Francisco, USA</publisher-loc>
              </publisher>
            </journal-meta>
            <article-meta>
              <article-id pub-id-type="accession">PMC3774646</article-id>
              <article-id pub-id-type="pmcid">PMC3774646</article-id>
              <article-id pub-id-type="pmc-uid">3774646</article-id>
              <article-id pub-id-type="pmid">24066179</article-id>
              <article-id pub-id-type="pmid">24066179</article-id>
              <article-id pub-id-type="publisher-id">PONE-D-13-27884</article-id>
              <article-id pub-id-type="doi">10.1371/journal.pone.0075410</article-id>
              <article-categories>
                <subj-group subj-group-type="heading">
                  <subject>Research Article</subject>
                </subj-group>
              </article-categories>
              <title-group>
                <article-title>The Effect of Instrumental Timbre on Interval Discrimination</article-title>
                <alt-title alt-title-type="running-head">Interval Discrimination and Timbral Effects</alt-title>
              </title-group>
              <contrib-group>
                <contrib contrib-type="author">
                  <name>
                    <surname>Zarate</surname>
                    <given-names>Jean Mary</given-names>
                  </name>
                  <xref ref-type="aff" rid="aff1"/>
                  <xref ref-type="corresp" rid="cor1">
                    <sup>*</sup>
                  </xref>
                </contrib>
                <contrib contrib-type="author">
                  <name>
                    <surname>Ritson</surname>
                    <given-names>Caroline R.</given-names>
                  </name>
                  <xref ref-type="aff" rid="aff1"/>
                </contrib>
                <contrib contrib-type="author">
                  <name>
                    <surname>Poeppel</surname>
                    <given-names>David</given-names>
                  </name>
                  <xref ref-type="aff" rid="aff1"/>
                </contrib>
              </contrib-group>
              <aff id="aff1">
                <addr-line>Department of Psychology, New York University, New York, New York, United States of America</addr-line>
              </aff>
              <contrib-group>
                <contrib contrib-type="editor">
                  <name>
                    <surname>Balasubramaniam</surname>
                    <given-names>Ramesh</given-names>
                  </name>
                  <role>Editor</role>
                  <xref ref-type="aff" rid="edit1"/>
                </contrib>
              </contrib-group>
              <aff id="edit1">
                <addr-line>University of California, Merced, United States of America</addr-line>
              </aff>
              <author-notes>
                <corresp id="cor1">* E-mail: <email>jean.m.zarate@nyu.edu</email></corresp>
                <fn fn-type="COI-statement">
                  <p><bold>Competing Interests: </bold>The authors have declared that no competing interests exist.</p>
                </fn>
                <fn fn-type="con">
                  <p>Conceived and designed the experiments: JMZ CRR DP. Performed the experiments: JMZ CRR. Analyzed the data: JMZ CRR. Wrote the paper: JMZ CRR DP.</p>
                </fn>
              </author-notes>
              <pub-date pub-type="collection">
                <year>2013</year>
              </pub-date>
              <pub-date pub-type="epub">
                <day>16</day>
                <month>9</month>
                <year>2013</year>
              </pub-date>
              <volume>8</volume>
              <issue>9</issue>
              <elocation-id>e75410</elocation-id>
              <history>
                <date date-type="received">
                  <day>5</day>
                  <month>7</month>
                  <year>2013</year>
                </date>
                <date date-type="accepted">
                  <day>13</day>
                  <month>8</month>
                  <year>2013</year>
                </date>
              </history>
              <permissions>
                <copyright-statement>© 2013 Zarate et al</copyright-statement>
                <copyright-year>2013</copyright-year>
                <copyright-holder>Zarate et al</copyright-holder>
                <license xlink:href="http://creativecommons.org/licenses/by/4.0/">
                  <license-p>This is an open-access article distributed under the terms of the Creative Commons Attribution License, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are properly credited.</license-p>
                </license>
              </permissions>
              <abstract>
                <p>We tested non-musicians and musicians in an auditory psychophysical experiment to assess the effects of timbre manipulation on pitch-interval discrimination. Both groups were asked to indicate the larger of two presented intervals, comprised of four sequentially presented pitches; the second or fourth stimulus within a trial was either a sinusoidal (or “pure”), flute, piano, or synthetic voice tone, while the remaining three stimuli were all pure tones. The interval-discrimination tasks were administered parametrically to assess performance across varying pitch distances between intervals (“interval-differences”). Irrespective of timbre, musicians displayed a steady improvement across interval-differences, while non-musicians only demonstrated enhanced interval discrimination at an interval-difference of 100 cents (one semitone in Western music). Surprisingly, the best discrimination performance across both groups was observed with pure-tone intervals, followed by intervals containing a piano tone. More specifically, we observed that: 1) timbre changes within a trial affect interval discrimination; and 2) the broad spectral characteristics of an instrumental timbre may influence perceived pitch or interval magnitude and make interval discrimination more difficult.</p>
              </abstract>
              <funding-group>
                <funding-statement>The research was funded in part by a grant from the GRAMMY Foundation® to J.M.Z. and by a grant from the National Institutes of Health (NIH R01 05660 to D.P.). The funders had no role in study design, data collection and analysis, decision to publish, or preparation of the manuscript.</funding-statement>
              </funding-group>
              <counts>
                <page-count count="9"/>
              </counts>
            </article-meta>
          </front>
          <body>
            <sec id="s1">
              <title>Introduction</title>
              <p>The ability to perceive changing pitch in sounds is crucial for both speech and music. The contour of pitch changes in speech can determine the linguistic-communicative intent of a sentence (e.g., interrogative versus declarative versus imperative) and its affective content (happy, angry, sad, etc.), or—at the word level—distinguish lexical-semantic meanings in tonal languages <xref rid="pone.0075410-Dowling1" ref-type="bibr">[1]</xref>. The pitch contour in music outlines the melody. In more detail, the melodic contour can be subdivided into particular frequency ratios or intervals that have specific labels (and functions) in Western music composition, such as the minor third, perfect fifth, or the major seventh.</p>
              <p>Since pitch intervals serve such a fundamental role in music, numerous studies have investigated the ability to discriminate pitch intervals in Western musical contexts (for a comprehensive review, see <xref rid="pone.0075410-Burns1" ref-type="bibr">[2]</xref>). These experiments included tasks such as interval categorization or discrimination of interval magnitudes at or around musically relevant intervals <xref rid="pone.0075410-Burns2" ref-type="bibr">[3]</xref>, <xref rid="pone.0075410-Hill1" ref-type="bibr">[4]</xref>, <xref rid="pone.0075410-Houtsma1" ref-type="bibr">[5]</xref>, <xref rid="pone.0075410-Zatorre1" ref-type="bibr">[6]</xref>, <xref rid="pone.0075410-Zatorre2" ref-type="bibr">[7]</xref>, correcting mistuned intervals <xref rid="pone.0075410-Rakowski1" ref-type="bibr">[8]</xref>, <xref rid="pone.0075410-Ward1" ref-type="bibr">[9]</xref>, and assessment of performance intonation <xref rid="pone.0075410-Dowling2" ref-type="bibr">[10]</xref>, <xref rid="pone.0075410-Ward2" ref-type="bibr">[11]</xref>. It is likely that the explicitly musical contexts of these experiments—in which the discrimination tasks were based on musically relevant intervals—may have given musicians a significant advantage over non-musicians. In a recent experiment <xref rid="pone.0075410-Zarate1" ref-type="bibr">[12]</xref>, we reduced the musical context by choosing frequencies that were not easily assigned to note names and interval magnitudes that are not often used in Western music (e.g., 25, 50, 75 cents), except one interval at 100 cents (a semitone). People with extensive musical expertise exhibited interval-discrimination thresholds of 100 cents, and non-musicians displayed larger thresholds <xref rid="pone.0075410-Zarate1" ref-type="bibr">[12]</xref>, which: 1) corroborates McDermott et al.'s findings obtained with an adaptive procedure <xref rid="pone.0075410-McDermott1" ref-type="bibr">[13]</xref>, and 2) suggests that an explicitly musical context in these studies may not influence the basic interval-discrimination thresholds in these groups. These thresholds may be established via repeated exposure to similar intervals in Western music and languages <xref rid="pone.0075410-Han1" ref-type="bibr">[14]</xref>.</p>
              <p>While musically relevant frequencies or interval magnitudes may not affect interval-discrimination thresholds, changes in a tone's frequency spectrum—which creates a particular timbre—may influence pitch and/or interval perception (see <xref ref-type="fig" rid="pone-0075410-g001">Figure 1</xref>) <xref rid="pone.0075410-Moore1" ref-type="bibr">[15]</xref>, <xref rid="pone.0075410-Singh1" ref-type="bibr">[16]</xref>; however, earlier experiments have yielded conflicting accounts of these effects in musicians and non-musicians. When music students were asked to tune musical intervals containing pure or synthetic complex tones, Rakowski determined that regardless of tone timbre, melodic intervals of a minor third or less are judged as even smaller in size than their actual pitch magnitude, and conversely larger intervals are perceived as bigger that their magnitude <xref rid="pone.0075410-Rakowski2" ref-type="bibr">[17]</xref>. In contrast, Russo and Thompson found that timbre affected the perceived size of a melodic interval for both musicians and non-musicians, depending on whether synthetic timbre changed from a dull to a brighter sound (or the reverse manipulation) between the two tones <xref rid="pone.0075410-Russo1" ref-type="bibr">[18]</xref>. Spiegel and Watson <xref rid="pone.0075410-Spiegel1" ref-type="bibr">[19]</xref> and Micheyl et al. <xref rid="pone.0075410-Micheyl1" ref-type="bibr">[20]</xref> reported that both musicians and non-musicians had better two-tone discrimination thresholds with synthetic complex tones than with pure tones; they argued that the enhanced frequency discrimination observed with complex tones, which are closer to real instrumental timbres that musicians hear during training, may be generalized to artificial pure tones. Demany and Semal suggested that this generalization is only partial, since they found that pitch discrimination abilities may be specific for the timbres used during training <xref rid="pone.0075410-Demany1" ref-type="bibr">[21]</xref>. Finally, McDermott and his colleagues found that musicians and non-musicians had similar pitch- and interval-discrimination thresholds for both synthetic complex and pure-tone stimuli <xref rid="pone.0075410-McDermott1" ref-type="bibr">[13]</xref>.</p>
              <fig id="pone-0075410-g001" orientation="portrait" position="float">
                <object-id pub-id-type="doi">10.1371/journal.pone.0075410.g001</object-id>
                <label>Figure 1</label>
                <caption>
                  <title>Spectrograms (A) and fundamental frequency traces (B) of the four tones at F4 (target base frequency of 349.23 Hz).</title>
                  <p>The variations in spectral energy influence the estimated fundamental frequency for each of the instrumental tones.</p>
                </caption>
                <graphic xlink:href="pone.0075410.g001"/>
              </fig>
              <p>Given these disparate accounts of timbral effects on interval discrimination, we designed this study to assess the phenomenon employing different levels of controls. Most auditory discrimination studies have typically used pure or synthetic complex tones; here, we employed more naturalistic, instrument sound samples to assess the effects of timbre. Non-musicians and musicians were asked to indicate the larger of two presented intervals. We manipulated the timbre in one of the four presented stimuli (either pure, flute, piano, or synthetic voice) per trial; the other three stimuli were pure tones, as in a previous experiment <xref rid="pone.0075410-Zarate1" ref-type="bibr">[12]</xref>. We altered only one out of four tones to determine whether the introduction of a different timbre would alter the perceived pitch of a tone and, therefore, the perceived interval size. Additionally, we only changed one note per trial to prevent any confounding variables, such as interactions between two different non-sinusoidal timbres (one in each interval) and perceived pitch and/or interval size. Unlike our previous experiment, we sought to make this task more musically relevant by employing a base frequency (349.23 Hz or F4) and a large set of intervals that could be assigned to Western conventions: 100, 200, 300, 400, 500, 600 cents (i.e., the minor and major seconds, minor and major thirds, fourth, and the tritone). Based on earlier research <xref rid="pone.0075410-Zarate1" ref-type="bibr">[12]</xref>, <xref rid="pone.0075410-McDermott1" ref-type="bibr">[13]</xref>, <xref rid="pone.0075410-Spiegel1" ref-type="bibr">[19]</xref>, <xref rid="pone.0075410-Micheyl1" ref-type="bibr">[20]</xref>, <xref rid="pone.0075410-KishonRabin1" ref-type="bibr">[22]</xref>, we predicted that the musicians would perform better on this discrimination task, due to their training-enhanced auditory skills. We also hypothesized—based on Spiegel and Watson's and Micheyl et al.'s reports of improved discrimination with complex tones—that instrumental timbres may improve all subjects' performances, since instrumental sounds are more naturalistic than pure tones that are only encountered in a laboratory setting. Finally, based on Demany and Semal's (2002) suggestion that enhanced auditory skills may be linked with exposure to specific timbres during musical training, musicians may exhibit greater improvement in interval discrimination during instrumental-timbre trials than non-musicians.</p>
              <p>To summarize, we designed the present experiment to determine whether employing more naturalistic, instrumental timbres would improve interval discrimination in non-musicians and musicians, relative to only pure-tone stimuli. As detailed below, musicians discriminated between intervals better than non-musicians across all timbres, and interval discrimination was best with pure tones.</p>
            </sec>
            <sec sec-type="methods" id="s2">
              <title>Methods</title>
              <sec id="s2a">
                <title>Ethics Statement</title>
                <p>All testing was performed with the subjects' informed written consent and in accordance with procedures approved by the NYU University Committee on Activities Involving Human Subjects.</p>
              </sec>
              <sec id="s2b">
                <title>Subjects</title>
                <p>A total of 29 subjects were recruited from the New York University (NYU) community and surrounding areas. All subjects (mean age  = 24.8 years, SD = 6.56 years) were right-handed and had normal hearing. All subjects were categorized as non-musicians or musicians according to self-report of musical experience, as assessed by an in-house survey. Fourteen non-musicians (7 female) had minimal musical experience (mean  = 0.78 years, SD = 0.66 years) and did not play music regularly at the time of study. Fifteen musicians (7 female) had an average of 11.7 years of musical experience (SD = 5.83 years) and were practicing or performing music at the time of study. None of the subjects reported having absolute pitch.</p>
              </sec>
              <sec id="s2c">
                <title>Stimuli</title>
                <p>We used MATLAB (Mathworks, Natick, MA, USA) to create sinusoidal tones at a base frequency of 349.23 Hz, which corresponds to an F4 in Western music. The instrument sounds (piano, flute, voice) were MIDI-generated (Musical Instrument Digital Interface) from a Yamaha YPT-220 keyboard (Yamaha Corporation of America, Buena Park, CA, USA) at this same base frequency. All instrument sounds included the attack (or onset) of the sound and had no vibrato. Additional sinusoidal and instrumental tones were generated at specific pitch distances—50 to 600 cents at 25-cent increments—from this base frequency. All tones (200-ms duration, 16-bit depth, 44100-Hz sampling frequency) were gated with 7-ms cosine ramps in MATLAB and then normalized to 0.8 dB in Audacity (open-source freeware, <ext-link ext-link-type="uri" xlink:href="http://audacity.sourceforge.net/">http://audacity.sourceforge.net/</ext-link>). <xref ref-type="fig" rid="pone-0075410-g001">Figure 1</xref> displays the spectrograms and the fundamental frequency (F0) estimated with YIN <xref rid="pone.0075410-deCheveign1" ref-type="bibr">[23]</xref> for each timbre at F4. Compared to the pure tone, all instrumental timbres have more energy across a broader span of frequencies (including harmonics of the base frequency). Additionally, the flute and synthetic voice tones have more diffuse onsets, relative to the sharper onsets of the piano and pure tones. Finally, the frequency range of spectral energy increases from the piano to the flute tones, with the synthetic voice timbre displaying the broadest frequency distribution of energy compared to all other tones. The various changes in spectral energy apparently result in slight fluctuations of estimated F0 in the piano and synthetic voice timbres (<xref ref-type="fig" rid="pone-0075410-g001">Figure 1B</xref>).</p>
                <p>In MATLAB, tones were paired with a 50-ms gap of silence in between to create interval sizes ranging from 50 to 600 cents (respectively, a quarter-tone to a tritone in Western music), and intervals were combined (ISI = 0.8, 0.9, or 1 s) to create individual test trials. We parametrically varied the magnitude differences between intervals within a trial from 0 to 100 cents (a semitone in Western music) in 25-cent increments. We chose 100 cents as the maximum interval-difference due to both its musical relevance and the observation that musicians' performances approach a ceiling of maximum accuracy at around this magnitude <xref rid="pone.0075410-Zarate1" ref-type="bibr">[12]</xref>. Zero-cent differences between intervals in a trial were included to observe whether subjects—when forced to guess— had a response bias based on timbre type. For all trials, the first tone of each interval was a pure tone at the base frequency of 349.23 Hz. In trials with an instrumental timbre, the instrument sound could occur either as the second or the fourth tone.</p>
              </sec>
              <sec id="s2d">
                <title>Experimental procedure</title>
                <p>In a sound-attenuated booth, subjects sat in front of a lab computer and wore headphones (Sennheiser HD 380 Professional, Sennheiser Electronic Corporation, Wedemark, Germany), through which all auditory stimuli were delivered via MATLAB at a comfortable level (∼77.5 dB SPL). A 10-trial demonstration was presented at the beginning of the session to familiarize subjects with the different sounds presented during the experiment. Prior to interval discrimination, pitch-discrimination thresholds were determined with pure tones at 349.23 Hz in a “2 down – 1 up” staircase procedure <xref rid="pone.0075410-Levitt1" ref-type="bibr">[24]</xref>, implemented as part of the MLP toolbox for auditory psychophysical testing <xref rid="pone.0075410-Grassi1" ref-type="bibr">[25]</xref>. After discrimination-threshold testing, subjects were presented with intervals in a two-alternative forced-choice design, indicating by button press which pair contained the larger interval. Subjects received visual feedback (“correct” or “incorrect”) on the computer screen after making each decision. There were at least 15 trials of each interval type (pure, flute, piano, and voice) for each interval-difference, all presented in a pseudo-randomized order. In total, there were 5 blocks with 100 trials each, and subjects were allowed to take a short break between blocks.</p>
              </sec>
              <sec id="s2e">
                <title>Analyses</title>
                <p>Subjects' performances were measured as percent-correct scores for each of the interval-differences (25–100 cents). We also calculated <italic>d</italic>-prime and <italic>β</italic>normalized values to measure detector sensitivity and response bias, respectively <xref rid="pone.0075410-Dorfman1" ref-type="bibr">[26]</xref>, <xref rid="pone.0075410-Rosenblith1" ref-type="bibr">[27]</xref>, <xref rid="pone.0075410-Swets1" ref-type="bibr">[28]</xref>. The hit and false alarm (FA) rates, <italic>d</italic>-prime (<italic>d′</italic>), and <italic>β</italic>normalized values were calculated as follows:</p>
                <list list-type="simple">
                  <list-item>
                    <p>Hit  = H(# times 1<sup>st</sup> pair was chosen/# trials with larger 1<sup>st</sup> pair)  =  score for 1<sup>st</sup>-pair trials.</p>
                  </list-item>
                  <list-item>
                    <p>FA  = 1 – H(# times 2<sup>nd</sup> pair was chosen/# trials with larger 2<sup>nd</sup> pair)  = 1 – score for 2<sup>nd</sup>-pair trials.</p>
                  </list-item>
                  <list-item>
                    <p><italic>d′</italic>  =  Zscore(Hit) – Zscore(FA)</p>
                  </list-item>
                  <list-item>
                    <p><italic>β</italic> = −0.5*(Zscore(Hit) + Zscore(FA))</p>
                  </list-item>
                  <list-item>
                    <p><italic>β</italic><sub>normalized</sub>  =  <italic>β</italic>/<italic>d′</italic>; 0 =  no bias; negative values  =  bias towards selecting 1<sup>st</sup> pair; positive values  =  bias towards selecting 2<sup>nd</sup> pair.</p>
                  </list-item>
                </list>
                <p>For equal-interval trials (0-cent difference between intervals), response bias was calculated as a proportion of the total number of trials in which subjects selected the interval with an instrumental timbre, instead of the pure-tone interval; higher proportions reflect a stronger bias towards selecting the instrumental-timbre intervals.</p>
                <p>Pitch-discrimination thresholds (in cents), percent-correct scores, <italic>d′</italic> values, <italic>β</italic>normalized values, mean reaction time, and standard deviation of reaction times were analyzed using an independent-samples <italic>t</italic>-test or repeated-measures analyses of variance (ANOVA). Tukey's Honestly Significant Difference test and planned comparisons were used for post-hoc analyses of significant main effects and interactions, respectively.</p>
              </sec>
            </sec>
            <sec id="s3">
              <title>Results</title>
              <sec id="s3a">
                <title>Effects of musical expertise on pitch-discrimination thresholds</title>
                <p>An independent samples <italic>t</italic>-test performed on the musicians' and non-musicians' pitch-discrimination thresholds measured at F4 (349.23 Hz) determined that musicians' thresholds (mean ± SEM  = 14.6±2.7 cents) were significantly lower than non-musicians' thresholds (44.1±9.7 cents) as expected [<italic>t</italic>(27)  = 1.22, <italic>p</italic>&lt;0.05].</p>
              </sec>
              <sec id="s3b">
                <title>Effects of musical expertise and timbre on interval-discrimination accuracy</title>
                <p><xref ref-type="fig" rid="pone-0075410-g002">Figure 2</xref> depicts the significant results of a three-way repeated-measures ANOVA performed on percent-correct scores, with group as the between-subject factor and timbre and interval-difference as the repeated within-subject factors. The analysis revealed significant main effects of group [<italic>F</italic>(1,27)  = 33.64, <italic>p</italic>&lt;0.001], timbre [<italic>F</italic>(3,81)  = 22.87, <italic>p</italic>&lt;0.001], and interval-difference [<italic>F</italic>(3,81)  = 88.47, <italic>p</italic>&lt;0.001], and significant two-way interactions between group and timbre [<italic>F</italic>(3,81)  = 5.75, <italic>p</italic>&lt;0.01], group and interval-difference [<italic>F</italic>(3,81)  = 4.65, <italic>p</italic>&lt;0.01], and timbre and interval-difference [<italic>F</italic>(9,243)  = 8.44, <italic>p</italic>&lt;0.001]. No other interactions were significant.</p>
                <fig id="pone-0075410-g002" orientation="portrait" position="float">
                  <object-id pub-id-type="doi">10.1371/journal.pone.0075410.g002</object-id>
                  <label>Figure 2</label>
                  <caption>
                    <title>Interval-discrimination accuracy.</title>
                    <p>(A) Mean ± SEM percent-correct scores of musicians and non-musicians for interval discrimination, across the four timbre types and collapsed across all interval-differences. Musicians performed more accurately than non-musicians across all timbres (indicated by<bold>!</bold>, <italic>p</italic>&lt;0.001), but were marginally most accurate with pure-tone intervals (denoted by <bold>*</bold>, <italic>p</italic>&lt;0.06). Non-musicians showed more accurate interval discrimination with pure and piano tones than with the other two timbres (marked by <bold>+</bold>, <italic>p</italic>&lt;0.05). (B) Mean ± SEM percent-correct scores of musicians and non-musicians for each interval-difference collapsed across timbre types. Musicians were more accurate than non-musicians overall (marked by<bold>!</bold>, <italic>p</italic>&lt;0.001). Both musicians (denoted by <bold>*</bold>, <italic>p</italic>&lt;0.001) and non-musicians (indicated by <bold>+</bold>, <italic>p</italic>&lt;0.05) displayed the least accuracy at the 25-cent interval-difference. (C) Mean ± SEM percent-correct scores for each timbre type at each interval-difference, averaged across both groups. All subjects discriminated pure-tone intervals more accurately than flute- and voice-tone intervals at all interval-differences except 25 cents (shown by<bold>!</bold>, <italic>p</italic>&lt;0.001), and better than piano-tone intervals at all interval-differences except 50 cents (marked with <bold>*</bold>, <italic>p</italic>&lt;0.06). Flute-tone discrimination only improved at 100 cents (shown by <bold>a</bold>, <italic>p</italic>&lt;0.001), while voice-tone performance significantly improved at interval-differences of 75 cents and higher (indicated by <bold>b</bold>, <italic>p</italic>&lt;0.01).</p>
                  </caption>
                  <graphic xlink:href="pone.0075410.g002"/>
                </fig>
                <p>Planned comparisons performed on the group-by-timbre interaction determined that musicians discriminated intervals of all timbres more accurately than non-musicians (<xref ref-type="fig" rid="pone-0075410-g002">Figure 2A</xref>; <italic>p</italic>s&lt;0.001), which reiterated the significant group main effect, but musicians discriminated intervals with all pure tones marginally better than with other timbres (<italic>p</italic>s&lt;0.06). Non-musicians on the whole performed more accurately with pure and piano tones, which may be due to their sharper onsets and/or relatively compact distribution of sound energy (see <xref ref-type="fig" rid="pone-0075410-g001">Figure 1</xref>), compared to flute and synthetic voice timbres (<italic>p</italic>s&lt;0.05). Planned comparisons on the two-way interaction between group and interval-difference showed that musicians discriminated between intervals more accurately at all interval-differences than non-musicians (<xref ref-type="fig" rid="pone-0075410-g002">Figure 2B</xref>; <italic>p</italic>s&lt;0.001). Both groups showed a significant, steady improvement in accuracy as interval-differences increased, and had the worst discrimination accuracy at the 25-cent interval-difference compared to all other magnitudes (<italic>p</italic>s&lt;0.001 for musicians; <italic>p</italic>s&lt;0.05 for non-musicians).</p>
                <p>Planned comparisons on the timbre-by-interval-difference interaction determined that discrimination with pure-tone intervals significantly improved at each larger interval-difference (<xref ref-type="fig" rid="pone-0075410-g002">Figure 2C</xref>; <italic>p</italic>s&lt;0.05). Pure-tone discrimination was also better than with flute- and voice-tone intervals at all interval-differences except 25 cents (<italic>p</italic>s&lt;0.001), and marginally more accurate than with piano-tone intervals at all interval-differences other than 50 cents (<italic>p</italic>s&lt;0.06). This suggests that in general, interval discrimination with pure tones was better than with any other timbre. Performance with flute tones only significantly improved at the 100-cent interval-difference (<italic>p</italic>s&lt;0.001). Discrimination accuracy with piano-tone intervals improved as interval-differences increased (<italic>p</italic>s&lt;0.05), but with no significant change in accuracy between interval-differences of 50 and 75 cents. Voice-tone interval discrimination improved significantly beginning at the 75-cent interval-difference (<italic>p</italic>s&lt;0.01); there were no significant changes in accuracy between 25- and 50-cent interval-differences and between 75- and 100-cent interval-differences. Notably, voice-tone interval discrimination was worse at 100 cents than with any other timbre (<italic>p</italic>s&lt;0.01).</p>
              </sec>
              <sec id="s3c">
                <title>Effects of musical expertise and timbre on interval-discrimination sensitivity</title>
                <p><xref ref-type="fig" rid="pone-0075410-g003">Figure 3A</xref> shows the results of a three-way repeated-measures ANOVA (group by timbre by interval-difference) performed on <italic>d′</italic> prime values to assess the influence of musical expertise on interval-discrimination sensitivity. The analysis resulted in significant main effects of group [<italic>F</italic>(1,27)  = 23.28, p&lt;0.001], timbre [<italic>F</italic>(3,81)  = 12.23, <italic>p</italic>&lt;0.001], and interval-difference [<italic>F</italic>(3,81)  = 28.04, <italic>p</italic>&lt;0.001], and significant two-way interactions between group and interval-difference [<italic>F</italic>(3,81)  = 7.02, <italic>p</italic>&lt;0.001] and timbre and interval-difference [<italic>F</italic>(9,243)  = 2.47, <italic>p</italic>&lt;0.05]. Planned comparisons on the group-by-interval-difference interaction determined that musicians displayed more discrimination sensitivity than non-musicians across all interval-differences (<xref ref-type="fig" rid="pone-0075410-g003">Figure 3A</xref>, left; <italic>p</italic>s&lt;0.01), as reflected by the main group effect. Among musicians, sensitivity improved as a function of increasing interval-difference (<italic>p</italic>s&lt;0.01) irrespective of timbre, with the exception of no significant change in sensitivity between interval-differences of 50 and 75 cents. Within the non-musician group, discrimination sensitivity was only significantly enhanced at an interval-difference of 100 cents (<italic>p</italic>s&lt;0.01); there were no other changes in sensitivity seen among smaller interval-differences (<italic>p</italic>s&gt;0.5).</p>
                <fig id="pone-0075410-g003" orientation="portrait" position="float">
                  <object-id pub-id-type="doi">10.1371/journal.pone.0075410.g003</object-id>
                  <label>Figure 3</label>
                  <caption>
                    <title>Interval-discrimination sensitivity and response bias.</title>
                    <p>(A) Left: Mean ± SEM <italic>d′</italic> values of musicians and non-musicians across all timbres at each interval-difference. Musicians exhibited more discrimination sensitivity across all interval-differences, compared to non-musicians (denoted by<bold>!</bold>, <italic>p</italic>&lt;0.01). Across interval-differences, on-musicians only showed enhanced discrimination sensitivity at a difference of 100 cents (marked by <bold>*</bold>, <italic>p</italic>&lt;0.01). Right: Mean ± SEM <italic>d′</italic> values for each timbre at each interval-difference, averaged across both groups. Listeners exhibited enhanced discrimination sensitivity at interval-differences of 75 cents with pure and voice tones (indicated with <bold>a</bold>, <italic>p</italic>s&lt;0.05), 100 cents with flute tones (marked with <bold>b</bold>, <italic>p</italic>&lt;0.001), and 50 cents with a piano timbre (denoted by <bold>c</bold>, <italic>p</italic>&lt;0.06). (B) Left: Mean ± SEM <italic>β<sub>normalized</sub></italic> values (measure of response bias) for musicians and non-musicians at each interval-difference, collapsed across all timbre types. In general, non-musicians showed greater bias towards selecting the second interval as the larger interval than non-musicians (shown with<bold>!</bold>, <italic>p</italic>&lt;0.01), and this bias only significantly decreased within the group at the 100-cent interval-difference (indicated by <bold>*</bold>, p&lt;0.001). Right: Mean ± SEM <italic>β<sub>normalized</sub></italic> values for each timbre at all interval-differences, averaged across both groups. Response bias decreased significantly with pure and voice-tones at 75 cents (marked by <bold>a</bold>, <italic>p</italic>s&lt;0.05). Interestingly, response bias was highest at a 75-cent interval-difference with flute tones, compared to all other timbres (shown by<bold>!</bold>, <italic>p</italic>&lt;0.05).</p>
                  </caption>
                  <graphic xlink:href="pone.0075410.g003"/>
                </fig>
                <p>Planned comparisons on the interaction between timbre and interval-difference revealed that <italic>d′</italic> values increased significantly across both groups for both pure- and voice-tone interval discrimination at interval-differences of 75 cents and larger (<xref ref-type="fig" rid="pone-0075410-g003">Figure 3A</xref>, right; <italic>p</italic>s&lt;0.05). The <italic>d′</italic> values were higher for pure-tone interval discrimination than for flute-tone discrimination at all interval-differences (<italic>p</italic>s&lt;0.05) except 50 cents, piano-tone discrimination at only 75 cents (<italic>p</italic>&lt;0.001), and discrimination with voice tones overall (<italic>p</italic>s&lt;0.05). Among the instrumental timbres, sensitivity for piano tones was marginally higher at 50–100 cents than for flute tones (<italic>p</italic>s&lt;0.08) and at 50- and 100-cent interval-differences than for voice tones (<italic>p</italic>s&lt;0.07). Sensitivity to flute-tone intervals increased significantly between 25 and 50 cents (<italic>p</italic>&lt;0.01) and at the 100-cent interval-difference (<italic>p</italic>s&lt;0.001). The <italic>d′</italic> values for piano-tone discrimination marginally increased at interval-differences of 50 cents and higher (<italic>p</italic>s&lt;0.06), with no significant changes in sensitivity between 50- and 75-cent interval-differences.</p>
                <p>Overall, interval-discrimination performance was best with pure-tone intervals, except at the smallest interval-differences of 25 cents; changes in timbral qualities did not enhance interval discrimination at this very small magnitude. Discrimination sensitivity was poorest with flute tones, whereas the poorest accuracy was observed with voice-tone intervals—accuracy with voice tones was still significantly lower than all other timbres at the 100-cent interval-difference. The discrepancy between accuracy and discrimination sensitivity may be explained by response bias, as discussed below.</p>
              </sec>
              <sec id="s3d">
                <title>Effects of musical expertise and timbre on response bias during interval discrimination</title>
                <p><xref ref-type="fig" rid="pone-0075410-g003">Figure 3B</xref> shows results from a three-way repeated measures ANOVA (group by timbre by interval-difference) on <italic>β<sub>normalized</sub></italic> values to determine the effects of musical training and timbre on response bias. We found significant main effects of group [<italic>F</italic>(1,27)  = 21.70, <italic>p</italic>&lt;0.001], timbre [<italic>F</italic>(3,81)  = 16.70, <italic>p</italic>&lt;0.001], and interval-difference [<italic>F</italic>(3,81)  = 27.89, <italic>p</italic>&lt;0.001], as well as significant group-by-interval-difference [<italic>F</italic>(3,81)  = 5.61, <italic>p</italic>&lt;0.01] and timbre-by-interval-difference interactions [<italic>F</italic>(9, 243)  = 3.57, <italic>p</italic>&lt;0.001].</p>
                <p>Planned comparisons on the group-by-interval-difference interaction revealed that non-musicians showed greater bias than musicians towards choosing the second interval as the larger interval across all interval-differences (<xref ref-type="fig" rid="pone-0075410-g003">Figure 3B</xref>, left; <italic>p</italic>s&lt;0.01), as also indicated by the significant group main effect. Musicians' response bias decreased as interval-differences grew (<italic>p</italic>s&lt;0.05), except for no change in response bias between interval-differences of 50 and 75 cents. Among non-musicians, response bias only significantly decreased at the 100-cent interval-difference (<italic>p</italic>s&lt;0.001); no other significant changes in bias were seen at smaller magnitudes (<italic>p</italic>s&gt;0.8). This result is mirrored by non-musicians' significant increase in discrimination sensitivity at only a 100-cent interval-difference (see results for <italic>d′</italic> values above).</p>
                <p>Planned comparisons performed on the timbre-by-interval-difference interaction determined that response bias significantly decreased with pure tones as interval-differences increased to 75 cents and larger (<xref ref-type="fig" rid="pone-0075410-g003">Figure 3B</xref>, right; <italic>p</italic>s&lt;0.05). There was also less response bias with pure-tone intervals compared to flute and voice tones at all interval-differences (<italic>p</italic>s&lt;0.05), and relative to piano tones at 25- and 75-cent interval-differences (<italic>p</italic>s&lt;0.01). During interval discrimination with flute tones, response bias was higher at interval-differences of 25 and 75 cents than at 50 and 100 cents (<italic>p</italic>s&lt;0.05); notably, bias unexpectedly increased between interval-differences of 50 and 75 cents (<italic>p</italic>&lt;0.05). The bias observed at 75 cents with flute tones was the largest compared to all other timbres (<italic>p</italic>s&lt;0.05), which may explain why sensitivity was worst with flute tones and only improved at a 100-cent interval-difference. Response bias with piano-tone intervals reduced as a function of increasing interval-difference (<italic>p</italic>s&lt;0.05), with the exception of no significant bias change between interval-differences of 50 and 75 cents. During interval discrimination with voice tones, response bias decreased significantly at 75 cents and larger (<italic>p</italic>s&lt;0.05); bias did not change between 25- and 50-cent and between 75- and 100-cent differences between intervals.</p>
                <p>To assess whether timbre influenced response bias with equal-magnitude intervals as subjects were forced to guess the larger interval, we performed an additional three-way repeated measures ANOVA on response-bias scores from the 0-cent interval-difference trials; response bias was measured as the proportion of trials in which subjects selected the interval with an instrumental timbre instead of the pure-tone interval. The analysis resulted in a significant group effect [<italic>F</italic>(1,27)  = 16.10, <italic>p</italic>&lt;0.001]—non-musicians exhibited a greater bias (mean ± SEM: 0.66±0.04) than musicians (0.53±0.03) towards selecting intervals with instrumental timbres when there was no difference in interval magnitude.</p>
              </sec>
              <sec id="s3e">
                <title>Effects of musical expertise and timbre on reaction time during interval discrimination</title>
                <p><xref ref-type="fig" rid="pone-0075410-g004">Figure 4</xref> displays results from analyses on mean reaction times obtained during interval discrimination. We found significant main effects of timbre [<italic>F</italic>(3,81)  = 4.68, <italic>p</italic>&lt;0.01] and interval-difference [<italic>F</italic>(3,81)  = 23.77, <italic>p</italic>&lt;0.001], and a significant group-by-interval-difference interaction [<italic>F</italic>(3,81)  = 6.40, <italic>p</italic>&lt;0.001]. Post-hoc tests performed on the timbre main effect determined that all subjects responded marginally faster during interval discrimination with pure tones than any other timbre (<xref ref-type="table" rid="pone-0075410-t001">Table 1</xref>; <italic>p</italic>s&lt;0.06). Planned comparisons on the group-by-interval-difference interaction revealed that musicians were marginally faster than non-musicians in discriminating intervals that were 100 cents apart (<xref ref-type="fig" rid="pone-0075410-g004">Figure 4</xref>; <italic>p</italic>&lt;0.09), but not at any other interval-difference. Within the musician group, reaction times reduced as interval-differences increased (<italic>p</italic>s&lt;0.01), except for no significant change in reaction time between interval-differences of 50 and 75 cents. Non-musicians responded more quickly at interval-differences of 100 cents than at 25 and 50 cents (<xref ref-type="fig" rid="pone-0075410-g004">Figure 4</xref>; <italic>p</italic>s&lt;0.01).</p>
                <fig id="pone-0075410-g004" orientation="portrait" position="float">
                  <object-id pub-id-type="doi">10.1371/journal.pone.0075410.g004</object-id>
                  <label>Figure 4</label>
                  <caption>
                    <title>Mean ± SEM reaction times (in seconds) for musicians and non-musicians, averaged across all timbres.</title>
                    <p>Musicians answered marginally faster than non-musicians at a 100-cent interval-difference (marked by <bold>*</bold>, <italic>p</italic>&lt;0.09). Within the non-musician group, reaction times were faster at an interval-difference of 100 cents, compared to 25 and 50 cents (indicated by<bold>!</bold>, <italic>p</italic>&lt;0.01).</p>
                  </caption>
                  <graphic xlink:href="pone.0075410.g004"/>
                </fig>
                <table-wrap id="pone-0075410-t001" orientation="portrait" position="float">
                  <object-id pub-id-type="doi">10.1371/journal.pone.0075410.t001</object-id>
                  <label>Table 1</label>
                  <caption>
                    <title>Mean ± SEM reaction times (in seconds) during interval discrimination with each timbre type.</title>
                  </caption>
                  <alternatives>
                    <graphic id="pone-0075410-t001-1" xlink:href="pone.0075410.t001"/>
                    <table frame="hsides" rules="groups">
                      <colgroup span="1">
                        <col align="left" span="1"/>
                        <col align="center" span="1"/>
                      </colgroup>
                      <thead>
                        <tr>
                          <td align="left" rowspan="1" colspan="1">Timbre</td>
                          <td align="left" rowspan="1" colspan="1">Mean Reaction Time ± SEM (s)</td>
                        </tr>
                      </thead>
                      <tbody>
                        <tr>
                          <td align="left" rowspan="1" colspan="1">pure</td>
                          <td align="left" rowspan="1" colspan="1">1.329±0.066*</td>
                        </tr>
                        <tr>
                          <td align="left" rowspan="1" colspan="1">flute</td>
                          <td align="left" rowspan="1" colspan="1">1.428±0.089</td>
                        </tr>
                        <tr>
                          <td align="left" rowspan="1" colspan="1">piano</td>
                          <td align="left" rowspan="1" colspan="1">1.400±0.088</td>
                        </tr>
                        <tr>
                          <td align="left" rowspan="1" colspan="1">voice</td>
                          <td align="left" rowspan="1" colspan="1">1.399±0.078</td>
                        </tr>
                      </tbody>
                    </table>
                  </alternatives>
                  <table-wrap-foot>
                    <fn id="nt101">
                      <label/>
                      <p>Both musicians and non-musicians discriminated between pure-tone intervals faster than intervals with other timbres (marked by <bold>*</bold>, <italic>p</italic>&lt;0.06), regardless of interval-difference.</p>
                    </fn>
                  </table-wrap-foot>
                </table-wrap>
                <p>Analyses of the variability of reaction time (measured as a subject's standard deviation of reaction time) revealed a marginally significant main effect of timbre [<italic>F</italic>(3,81)  = 2.20, <italic>p</italic>&lt;0.1]; post-hoc tests determined that response times were marginally more variable during flute-tone discrimination than during pure-tone discrimination (<xref ref-type="table" rid="pone-0075410-t002">Table 2</xref>; <italic>p</italic>&lt;0.08); no other significant differences were found. Based on accuracy and sensitivity scores, interval discrimination with flute tones may be more difficult, and this may be accompanied by more variable response times.</p>
                <table-wrap id="pone-0075410-t002" orientation="portrait" position="float">
                  <object-id pub-id-type="doi">10.1371/journal.pone.0075410.t002</object-id>
                  <label>Table 2</label>
                  <caption>
                    <title>Average variability ± SEM of reaction time (in seconds) for each timbre type.</title>
                  </caption>
                  <alternatives>
                    <graphic id="pone-0075410-t002-2" xlink:href="pone.0075410.t002"/>
                    <table frame="hsides" rules="groups">
                      <colgroup span="1">
                        <col align="left" span="1"/>
                        <col align="center" span="1"/>
                      </colgroup>
                      <thead>
                        <tr>
                          <td align="left" rowspan="1" colspan="1">Timbre</td>
                          <td align="left" rowspan="1" colspan="1">Mean Variability of Reaction time ± SEM (s)</td>
                        </tr>
                      </thead>
                      <tbody>
                        <tr>
                          <td align="left" rowspan="1" colspan="1">pure</td>
                          <td align="left" rowspan="1" colspan="1">0.540±0.060</td>
                        </tr>
                        <tr>
                          <td align="left" rowspan="1" colspan="1">flute</td>
                          <td align="left" rowspan="1" colspan="1">0.720±0.146 *</td>
                        </tr>
                        <tr>
                          <td align="left" rowspan="1" colspan="1">piano</td>
                          <td align="left" rowspan="1" colspan="1">0.669±0.177</td>
                        </tr>
                        <tr>
                          <td align="left" rowspan="1" colspan="1">voice</td>
                          <td align="left" rowspan="1" colspan="1">0.607±0.073</td>
                        </tr>
                      </tbody>
                    </table>
                  </alternatives>
                  <table-wrap-foot>
                    <fn id="nt102">
                      <label/>
                      <p>Across all interval-differences, all subjects showed more variable response times during interval discrimination with flute tones than with pure tones (indicated by <bold>*</bold>, <italic>p</italic>&lt;0.08).</p>
                    </fn>
                  </table-wrap-foot>
                </table-wrap>
                <p>Analyses of the reaction times during the equal-interval trials resulted in a significant timbre effect [<italic>F</italic>(3,81)  = 4.06, <italic>p</italic>&lt;0.01]—all subjects responded more quickly during flute-tone trials than during pure- and voice-tone trials (<xref ref-type="table" rid="pone-0075410-t003">Table 3</xref>; <italic>p</italic>s&lt;0.05), but not compared to during piano-tone trials. Analyses of the variability of reaction time during equal-interval trials did not reveal any significant other main effects or interactions (<italic>p</italic>s&gt;0.2).</p>
                <table-wrap id="pone-0075410-t003" orientation="portrait" position="float">
                  <object-id pub-id-type="doi">10.1371/journal.pone.0075410.t003</object-id>
                  <label>Table 3</label>
                  <caption>
                    <title>Mean ± SEM of reaction time (in seconds) during equal-interval trials.</title>
                  </caption>
                  <alternatives>
                    <graphic id="pone-0075410-t003-3" xlink:href="pone.0075410.t003"/>
                    <table frame="hsides" rules="groups">
                      <colgroup span="1">
                        <col align="left" span="1"/>
                        <col align="center" span="1"/>
                      </colgroup>
                      <thead>
                        <tr>
                          <td align="left" rowspan="1" colspan="1">Timbre</td>
                          <td align="left" rowspan="1" colspan="1">Mean Reaction Time ± SEM (s)</td>
                        </tr>
                      </thead>
                      <tbody>
                        <tr>
                          <td align="left" rowspan="1" colspan="1">pure</td>
                          <td align="left" rowspan="1" colspan="1">1.625±0.089</td>
                        </tr>
                        <tr>
                          <td align="left" rowspan="1" colspan="1">flute</td>
                          <td align="left" rowspan="1" colspan="1">1.469±0.068 *</td>
                        </tr>
                        <tr>
                          <td align="left" rowspan="1" colspan="1">piano</td>
                          <td align="left" rowspan="1" colspan="1">1.578±0.091</td>
                        </tr>
                        <tr>
                          <td align="left" rowspan="1" colspan="1">voice</td>
                          <td align="left" rowspan="1" colspan="1">1.600±0.088</td>
                        </tr>
                      </tbody>
                    </table>
                  </alternatives>
                  <table-wrap-foot>
                    <fn id="nt103">
                      <label/>
                      <p>Regardless of interval-difference, flute-tone intervals elicited shorter reaction times from all subjects, compared to intervals with pure and voice tones (shown with <bold>*</bold>, <italic>p</italic>&lt;0.05).</p>
                    </fn>
                  </table-wrap-foot>
                </table-wrap>
              </sec>
            </sec>
            <sec id="s4">
              <title>Discussion</title>
              <p>As expected, musicians discriminated between intervals more accurately than non-musicians across all interval-differences. In general, accuracy (as measured by percent-correct scores) improved in both groups as interval-differences increased. When examining discrimination sensitivity (represented by <italic>d′</italic> values), musicians displayed a significant increase in discrimination sensitivity across most interval-differences, while non-musicians exhibited an interval-discrimination sensitivity threshold of 100 cents. Originally, we hypothesized that instrumental sounds would enhance interval-discrimination performance in all subjects, relative to pure tones. However, although non-musicians displayed a stronger bias to select the interval with any instrumental sound among pairs of equal-magnitude intervals, both groups typically discriminated between intervals better and faster with pure tones than with any of the instrumental timbres; this evidence contradicts Micheyl and colleagues' report of enhanced auditory perception with complex timbres relative to pure tones <xref rid="pone.0075410-Micheyl1" ref-type="bibr">[20]</xref>. Among the instrumental timbres, interval discrimination was best with piano tones (perhaps due to a sharper onset or attack), and intervals with flute tones (with diffuse tone onsets) elicited arguably the worst discrimination sensitivity.</p>
              <p>Since both groups also took longer to respond during instrumental-timbre trials, it is possible that changing the timbre of only one out of four stimuli presented within a trial could have caused a distraction during discrimination; sequential tones with different timbres may be difficult to group together as intervals in our experiment <xref rid="pone.0075410-Bregman1" ref-type="bibr">[29]</xref>. Indeed, a previous study has suggested that the introduction of a new timbre can violate expectations that each successive sound will match the timbre of the previous one(s); in that study, this violation of expectation manifested as decreased discrimination accuracy <xref rid="pone.0075410-Warrier1" ref-type="bibr">[30]</xref>. Our observed interaction between timbre changes and decreased task performance is supported by Borchert et al.'s observations <xref rid="pone.0075410-Borchert1" ref-type="bibr">[31]</xref>: subjects had difficulty discriminating between two sequentially presented tones with different timbres, compared to two simultaneous tones with different timbres. Moreover, timbre seems to have a stronger interaction with pitch extraction/judgment and the evaluation of interval size when there is no tonal context (as in our interval-discrimination task), as opposed to tasks with a tonal reference point (or “key”) <xref rid="pone.0075410-Warrier1" ref-type="bibr">[30]</xref>. In fact, pitch changes seem to be best perceived (regardless of timbre) when F0 changes by at least 4%; the perception of smaller F0 changes is more influenced by timbre changes <xref rid="pone.0075410-Singh1" ref-type="bibr">[16]</xref>. In our task, two interval sizes and interval-differences have F0 changes of less than 4% of the base frequency (i.e., 25 and 50 cents from 349.23 Hz). Thus, our experimental design perhaps allowed for a stronger influence of timbral manipulation on pitch perception and interval discrimination than expected, which may have rendered interval comparisons with instrumental timbres more difficult. This may also explain the non-musicians' bias towards selecting the interval with an instrumental timbre as the larger interval, even when the interval magnitudes were equal; timbral changes may have altered non-musicians' perception of the instrumental tones' spectral centroid and consequently the interval magnitude (see <xref rid="pone.0075410-Russo1" ref-type="bibr">[18]</xref>). In contrast, musicians are reported to be less susceptible to timbre-based illusions of interval size, making them less likely to perceive the instrumental interval as larger <xref rid="pone.0075410-Russo1" ref-type="bibr">[18]</xref>.</p>
              <p>Although we expected an improvement in interval discrimination based on both instrumental timbre and musical expertise, such an interaction was not observed in this study. However, when comparing the present results to those of our previous study with only pure-tone intervals <xref rid="pone.0075410-Zarate1" ref-type="bibr">[12]</xref>, the use of instrumental timbre with intervals based on a musically relevant frequency appears to improve interval discrimination in each group, specifically among our parametrically varied interval-differences. In our earlier study, we reported pure-tone interval-discrimination thresholds of 100 cents in musicians, and 150 cents in non-musicians, which echoed McDermott et al.'s (2010) findings with both pure- and complex-tone intervals. Surprisingly, the musicians enrolled in this study did not display a threshold in interval discrimination (when averaging across all timbres), but rather a steady, significant increase in performance as the interval-differences grew larger. In addition, the non-musicians here demonstrated a smaller interval-discrimination threshold of 100 cents (across all timbres), rather than the threshold seen in our earlier experiment with only pure tones. In general, these qualitative performance changes across the two studies suggest that timbre specifically from musical instruments may improve or aid interval discrimination. Whether this instrumental-timbre effect can be disentangled from the effect of implementing a musically relevant base frequency (F4), rather than the base frequencies employed in our earlier study or in McDermott et al.'s (2010) study, must be explored further in later research.</p>
              <p>However, these interpretations of timbral effects should be taken cautiously, since the best interval discrimination was still observed with pure-tone stimuli, which may have been due partly to higher presentation rates of pure-tone intervals throughout the experiment, compared to instrumental-tone intervals; a practice effect with this particular timbre may have enhanced pure-tone interval discrimination. Nevertheless, compared to previous conflicting accounts of the timbral effects on interval discrimination, we observed that: 1) changes from one timbre to another, especially within the same trial, significantly affect interval discrimination, and 2) the varied spectral energy of instrumental timbre can alter pitch perception and/or interval discrimination.</p>
            </sec>
          </body>
          <back>
            <ack>
              <p>The authors thank Keith Doelling for his helpful feedback on this manuscript.</p>
            </ack>
            <ref-list>
              <title>References</title>
              <ref id="pone.0075410-Dowling1">
                <label>1</label>
                <mixed-citation publication-type="other">Dowling WJ, Harwood D (1986) Music Cognition. New York: Academic Press. 1–258 .</mixed-citation>
              </ref>
              <ref id="pone.0075410-Burns1">
                <label>2</label>
                <mixed-citation publication-type="other">Burns EM (1999) Intervals, Scales, and Tuning. In: Deutsch D, The Psychology of Music. San Diego: Academic Press. 215–264.</mixed-citation>
              </ref>
              <ref id="pone.0075410-Burns2">
                <label>3</label>
                <mixed-citation publication-type="journal"><name><surname>Burns</surname><given-names>EM</given-names></name>, <name><surname>Ward</surname><given-names>WD</given-names></name> (<year>1978</year>) <article-title>Categorical perception–phenomenon or epiphenomenon: evidence from experiments in the perception of melodic musical intervals</article-title>. <source>J AcoustSoc Am</source>
<volume>63</volume>: <fpage>456</fpage>–<lpage>468</lpage>.</mixed-citation>
              </ref>
              <ref id="pone.0075410-Hill1">
                <label>4</label>
                <mixed-citation publication-type="journal"><name><surname>Hill</surname><given-names>TJ</given-names></name>, <name><surname>Summers</surname><given-names>IR</given-names></name> (<year>2007</year>) <article-title>Discrimination of interval size in short tone sequences</article-title>. <source>J AcoustSoc Am</source>
<volume>121</volume>: <fpage>2376</fpage>–<lpage>2383</lpage>.</mixed-citation>
              </ref>
              <ref id="pone.0075410-Houtsma1">
                <label>5</label>
                <mixed-citation publication-type="journal"><name><surname>Houtsma</surname><given-names>AJM</given-names></name> (<year>1968</year>) <article-title>Discrimination of frequency ratios</article-title>. <source>J AcoustSocAm</source>
<volume>44</volume>: <fpage>383</fpage>.</mixed-citation>
              </ref>
              <ref id="pone.0075410-Zatorre1">
                <label>6</label>
                <mixed-citation publication-type="journal"><name><surname>Zatorre</surname><given-names>RJ</given-names></name> (<year>1983</year>) <article-title>Category-boundary effects and speeded sorting with a harmonic musical-interval continuum: evidence for dual processing</article-title>. <source>J Exp Psychol HumPerceptPerform</source>
<volume>9</volume>: <fpage>739</fpage>–<lpage>752</lpage>.</mixed-citation>
              </ref>
              <ref id="pone.0075410-Zatorre2">
                <label>7</label>
                <mixed-citation publication-type="journal"><name><surname>Zatorre</surname><given-names>RJ</given-names></name>, <name><surname>Halpern</surname><given-names>AR</given-names></name> (<year>1979</year>) <article-title>Identification, discrimination, and selective adaptation of simultaneous musical intervals</article-title>. <source>PerceptPsychophys</source>
<volume>26</volume>: <fpage>384</fpage>–<lpage>395</lpage>.</mixed-citation>
              </ref>
              <ref id="pone.0075410-Rakowski1">
                <label>8</label>
                <mixed-citation publication-type="journal"><name><surname>Rakowski</surname><given-names>A</given-names></name> (<year>1976</year>) <article-title>Tuning of isolated musical intervals</article-title>. <source>JAcoustSocAm</source>
<volume>59</volume>: <fpage>S50</fpage>.</mixed-citation>
              </ref>
              <ref id="pone.0075410-Ward1">
                <label>9</label>
                <mixed-citation publication-type="journal"><name><surname>Ward</surname><given-names>WD</given-names></name> (<year>1954</year>) <article-title>Subjective musical pitch</article-title>. <source>J AcoustSocAm</source>
<volume>26</volume>: <fpage>369</fpage>–<lpage>380</lpage>.</mixed-citation>
              </ref>
              <ref id="pone.0075410-Dowling2">
                <label>10</label>
                <mixed-citation publication-type="journal"><name><surname>Dowling</surname><given-names>WJ</given-names></name> (<year>1978</year>) <article-title>Scale and contour: two components of a theory of memory for melodies</article-title>. <source>Psychol Rev</source>
<volume>85</volume>: <fpage>341</fpage>–<lpage>354</lpage>.</mixed-citation>
              </ref>
              <ref id="pone.0075410-Ward2">
                <label>11</label>
                <mixed-citation publication-type="other">Ward WD (1970) Musical perception. In: Tobias JV, Foundations of modern auditory theory. New York: Academic Press. 405–447.</mixed-citation>
              </ref>
              <ref id="pone.0075410-Zarate1">
                <label>12</label>
                <mixed-citation publication-type="journal"><name><surname>Zarate</surname><given-names>JM</given-names></name>, <name><surname>Ritson</surname><given-names>CR</given-names></name>, <name><surname>Poeppel</surname><given-names>D</given-names></name> (<year>2012</year>) <article-title>Pitch-interval discrimination and musical expertise: is the semitone a perceptual boundary?</article-title>
<source>J AcoustSoc Am</source>
<volume>132</volume>: <fpage>984</fpage>–<lpage>993</lpage>.</mixed-citation>
              </ref>
              <ref id="pone.0075410-McDermott1">
                <label>13</label>
                <mixed-citation publication-type="journal"><name><surname>McDermott</surname><given-names>JH</given-names></name>, <name><surname>Keebler</surname><given-names>MV</given-names></name>, <name><surname>Micheyl</surname><given-names>C</given-names></name>, <name><surname>Oxenham</surname><given-names>AJ</given-names></name> (<year>2010</year>) <article-title>Musical intervals and relative pitch: frequency resolution, not interval resolution, is special</article-title>. <source>J AcoustSoc Am</source>
<volume>128</volume>: <fpage>1943</fpage>–<lpage>1951</lpage>.</mixed-citation>
              </ref>
              <ref id="pone.0075410-Han1">
                <label>14</label>
                <mixed-citation publication-type="journal"><name><surname>Han</surname><given-names>S</given-names></name>, <name><surname>Sundararajan</surname><given-names>J</given-names></name>, <name><surname>Bowling</surname><given-names>DL</given-names></name>, <name><surname>Lake</surname><given-names>J</given-names></name>, <name><surname>Purves</surname><given-names>D</given-names></name> (<year>2011</year>) <article-title>Co-variation of tonality in the music and speech of different cultures</article-title>. <source>PLoS One</source>
<volume>6</volume>: <fpage>e20160</fpage>.<pub-id pub-id-type="pmid">21637716</pub-id></mixed-citation>
              </ref>
              <ref id="pone.0075410-Moore1">
                <label>15</label>
                <mixed-citation publication-type="journal"><name><surname>Moore</surname><given-names>BCJ</given-names></name>, <name><surname>Glasberg</surname><given-names>BR</given-names></name> (<year>1990</year>) <article-title>Frequency discrimination of complex tones with overlapping and non-overlapping harmonics</article-title>. <source>The Journal of the Acoustical Society of America</source>
<volume>87</volume>: <fpage>2163</fpage>–<lpage>2177</lpage>.<pub-id pub-id-type="pmid">2348021</pub-id></mixed-citation>
              </ref>
              <ref id="pone.0075410-Singh1">
                <label>16</label>
                <mixed-citation publication-type="journal"><name><surname>Singh</surname><given-names>PG</given-names></name>, <name><surname>Hirsh</surname><given-names>IJ</given-names></name> (<year>1992</year>) <article-title>Influence of spectral locus and F0 changes on the pitch and timbre of complex tones</article-title>. <source>Journal of the Acoustical Society of America</source>
<volume>92</volume>: <fpage>2650</fpage>–<lpage>2661</lpage>.<pub-id pub-id-type="pmid">1479128</pub-id></mixed-citation>
              </ref>
              <ref id="pone.0075410-Rakowski2">
                <label>17</label>
                <mixed-citation publication-type="journal"><name><surname>Rakowski</surname><given-names>A</given-names></name> (<year>1990</year>) <article-title>Intonation variants of musical intervals in isolation and in musical contexts</article-title>. <source>Psychology of Music</source>
<volume>18</volume>: <fpage>60</fpage>–<lpage>72</lpage>.</mixed-citation>
              </ref>
              <ref id="pone.0075410-Russo1">
                <label>18</label>
                <mixed-citation publication-type="journal"><name><surname>Russo</surname><given-names>FA</given-names></name>, <name><surname>Thompson</surname><given-names>WF</given-names></name> (<year>2005</year>) <article-title>An interval size illusion: the influence of timbre on the perceived size of melodic intervals</article-title>. <source>Percept Psychophys</source>
<volume>67</volume>: <fpage>559</fpage>–<lpage>568</lpage>.<pub-id pub-id-type="pmid">16134451</pub-id></mixed-citation>
              </ref>
              <ref id="pone.0075410-Spiegel1">
                <label>19</label>
                <mixed-citation publication-type="journal"><name><surname>Spiegel</surname><given-names>MF</given-names></name>, <name><surname>Watson</surname><given-names>CS</given-names></name> (<year>1984</year>) <article-title>Performance on frequency-discrimination tasks by musicians and non-musicians</article-title>. <source>JAcoustSocAm</source>
<volume>76</volume>: <fpage>1690</fpage>–<lpage>1695</lpage>.</mixed-citation>
              </ref>
              <ref id="pone.0075410-Micheyl1">
                <label>20</label>
                <mixed-citation publication-type="journal"><name><surname>Micheyl</surname><given-names>C</given-names></name>, <name><surname>Delhommeau</surname><given-names>K</given-names></name>, <name><surname>Perrot</surname><given-names>X</given-names></name>, <name><surname>Oxenham</surname><given-names>AJ</given-names></name> (<year>2006</year>) <article-title>Influence of musical and psychoacoustical training on pitch discrimination</article-title>. <source>HearRes</source>
<volume>219</volume>: <fpage>36</fpage>–<lpage>47</lpage>.</mixed-citation>
              </ref>
              <ref id="pone.0075410-Demany1">
                <label>21</label>
                <mixed-citation publication-type="journal"><name><surname>Demany</surname><given-names>L</given-names></name>, <name><surname>Semal</surname><given-names>C</given-names></name> (<year>2002</year>) <article-title>Learning to perceive pitch differences</article-title>. <source>J AcoustSoc Am</source>
<volume>111</volume>: <fpage>1377</fpage>–<lpage>1388</lpage>.</mixed-citation>
              </ref>
              <ref id="pone.0075410-KishonRabin1">
                <label>22</label>
                <mixed-citation publication-type="journal"><name><surname>Kishon-Rabin</surname><given-names>L</given-names></name>, <name><surname>Amir</surname><given-names>O</given-names></name>, <name><surname>Vexler</surname><given-names>Y</given-names></name>, <name><surname>Zaltz</surname><given-names>Y</given-names></name> (<year>2001</year>) <article-title>Pitch discrimination: are professional musicians better than non-musicians?</article-title>
<source>J Basic ClinPhysiol Pharmacol</source>
<volume>12</volume>: <fpage>125</fpage>–<lpage>143</lpage>.</mixed-citation>
              </ref>
              <ref id="pone.0075410-deCheveign1">
                <label>23</label>
                <mixed-citation publication-type="journal"><name><surname>de Cheveigné</surname><given-names>A</given-names></name>, <name><surname>Kawahara</surname><given-names>H</given-names></name> (<year>2002</year>) <article-title>YIN, a fundamental frequency estimator for speech and music</article-title>. <source>J AcoustSoc Am</source>
<volume>111</volume>: <fpage>1917</fpage>–<lpage>1930</lpage>.</mixed-citation>
              </ref>
              <ref id="pone.0075410-Levitt1">
                <label>24</label>
                <mixed-citation publication-type="journal"><name><surname>Levitt</surname><given-names>H</given-names></name> (<year>1971</year>) <article-title>Transformed up-down methods in psychoacoustics</article-title>. <source>J AcoustSoc Am</source>
<volume>49</volume>: <fpage>467</fpage>–<lpage>477</lpage>.</mixed-citation>
              </ref>
              <ref id="pone.0075410-Grassi1">
                <label>25</label>
                <mixed-citation publication-type="journal"><name><surname>Grassi</surname><given-names>M</given-names></name>, <name><surname>Soranzo</surname><given-names>A</given-names></name> (<year>2009</year>) <article-title>MLP: a MATLAB toolbox for rapid and reliable auditory threshold estimation</article-title>. <source>BehavRes Methods</source>
<volume>41</volume>: <fpage>20</fpage>–<lpage>28</lpage>.</mixed-citation>
              </ref>
              <ref id="pone.0075410-Dorfman1">
                <label>26</label>
                <mixed-citation publication-type="journal"><name><surname>Dorfman</surname><given-names>DD</given-names></name>, <name><surname>Alf</surname><given-names>E</given-names><suffix>Jr</suffix></name> (<year>1968</year>) <article-title>Maximum likelihood estimation of parameters of signal detection theory–a direct solution</article-title>. <source>Psychometrika</source>
<volume>33</volume>: <fpage>117</fpage>–<lpage>124</lpage>.<pub-id pub-id-type="pmid">5239566</pub-id></mixed-citation>
              </ref>
              <ref id="pone.0075410-Rosenblith1">
                <label>27</label>
                <mixed-citation publication-type="journal"><name><surname>Rosenblith</surname><given-names>WA</given-names></name>, <name><surname>Stevens</surname><given-names>KN</given-names></name> (<year>1953</year>) <article-title>On the DL for frequency</article-title>. <source>JAcoustSocAm</source>
<volume>25</volume>: <fpage>980</fpage>–<lpage>985</lpage>.</mixed-citation>
              </ref>
              <ref id="pone.0075410-Swets1">
                <label>28</label>
                <mixed-citation publication-type="journal"><name><surname>Swets</surname><given-names>J</given-names></name> (<year>1982</year>) <article-title>Recent theoretical developments in signal-detection and recognition</article-title>. <source>Psychophysiology</source>
<volume>19</volume>: <fpage>300</fpage>.</mixed-citation>
              </ref>
              <ref id="pone.0075410-Bregman1">
                <label>29</label>
                <mixed-citation publication-type="other">Bregman AS (1990) Auditory scene analysis: The perceptual organization of sound. Cambridge: MIT Press.</mixed-citation>
              </ref>
              <ref id="pone.0075410-Warrier1">
                <label>30</label>
                <mixed-citation publication-type="journal"><name><surname>Warrier</surname><given-names>CM</given-names></name>, <name><surname>Zatorre</surname><given-names>RJ</given-names></name> (<year>2002</year>) <article-title>Influence of tonal context and timbral variation on perception of pitch</article-title>. <source>Percept Psychophys</source>
<volume>64</volume>: <fpage>198</fpage>–<lpage>207</lpage>.<pub-id pub-id-type="pmid">12013375</pub-id></mixed-citation>
              </ref>
              <ref id="pone.0075410-Borchert1">
                <label>31</label>
                <mixed-citation publication-type="journal"><name><surname>Borchert</surname><given-names>EM</given-names></name>, <name><surname>Micheyl</surname><given-names>C</given-names></name>, <name><surname>Oxenham</surname><given-names>AJ</given-names></name> (<year>2011</year>) <article-title>Perceptual grouping affects pitch judgments across time and frequency</article-title>. <source>J Exp Psychol Hum Percept Perform</source>
<volume>37</volume>: <fpage>257</fpage>–<lpage>269</lpage>.<pub-id pub-id-type="pmid">21077719</pub-id></mixed-citation>
              </ref>
            </ref-list>
          </back>
        </article>
      </metadata>
    </record>
  </GetRecord>
</OAI-PMH>
