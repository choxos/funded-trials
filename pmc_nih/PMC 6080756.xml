<?xml version="1.0" encoding="UTF-8"?>
<OAI-PMH xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/ http://www.openarchives.org/OAI/2.0/OAI-PMH.xsd">
  <responseDate>2024-05-03T04:22:01Z</responseDate>
  <request verb="GetRecord" identifier="oai:pubmedcentral.nih.gov:6080756" metadataPrefix="pmc">https:/www.ncbi.nlm.nih.gov/pmc/oai/oai.cgi</request>
  <GetRecord>
    <record>
      <header>
        <identifier>oai:pubmedcentral.nih.gov:6080756</identifier>
        <datestamp>2018-08-16</datestamp>
        <setSpec>plosone</setSpec>
        <setSpec>pmc-open</setSpec>
      </header>
      <metadata>
        <article xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" xsi:schemaLocation="https://jats.nlm.nih.gov/ns/archiving/1.3/ https://jats.nlm.nih.gov/archiving/1.3/xsd/JATS-archivearticle1-3.xsd" article-type="research-article">
          <front>
            <journal-meta>
              <journal-id journal-id-type="nlm-ta">PLoS One</journal-id>
              <journal-id journal-id-type="iso-abbrev">PLoS ONE</journal-id>
              <journal-id journal-id-type="publisher-id">plos</journal-id>
              <journal-id journal-id-type="pmc">plosone</journal-id>
              <journal-title-group>
                <journal-title>PLoS ONE</journal-title>
              </journal-title-group>
              <issn pub-type="epub">1932-6203</issn>
              <publisher>
                <publisher-name>Public Library of Science</publisher-name>
                <publisher-loc>San Francisco, CA USA</publisher-loc>
              </publisher>
            </journal-meta>
            <article-meta>
              <article-id pub-id-type="accession">PMC6080756</article-id>
              <article-id pub-id-type="pmcid">PMC6080756</article-id>
              <article-id pub-id-type="pmc-uid">6080756</article-id>
              <article-id pub-id-type="pmid">30086140</article-id>
              <article-id pub-id-type="doi">10.1371/journal.pone.0199358</article-id>
              <article-id pub-id-type="publisher-id">PONE-D-18-02988</article-id>
              <article-categories>
                <subj-group subj-group-type="heading">
                  <subject>Research Article</subject>
                </subj-group>
                <subj-group subj-group-type="Discipline-v3">
                  <subject>Social Sciences</subject>
                  <subj-group>
                    <subject>Linguistics</subject>
                    <subj-group>
                      <subject>Speech</subject>
                    </subj-group>
                  </subj-group>
                </subj-group>
                <subj-group subj-group-type="Discipline-v3">
                  <subject>Engineering and Technology</subject>
                  <subj-group>
                    <subject>Signal Processing</subject>
                    <subj-group>
                      <subject>Speech Signal Processing</subject>
                    </subj-group>
                  </subj-group>
                </subj-group>
                <subj-group subj-group-type="Discipline-v3">
                  <subject>Social Sciences</subject>
                  <subj-group>
                    <subject>Linguistics</subject>
                    <subj-group>
                      <subject>Grammar</subject>
                      <subj-group>
                        <subject>Phonology</subject>
                      </subj-group>
                    </subj-group>
                  </subj-group>
                </subj-group>
                <subj-group subj-group-type="Discipline-v3">
                  <subject>Biology and Life Sciences</subject>
                  <subj-group>
                    <subject>Neuroscience</subject>
                    <subj-group>
                      <subject>Cognitive Science</subject>
                      <subj-group>
                        <subject>Cognitive Psychology</subject>
                        <subj-group>
                          <subject>Language</subject>
                        </subj-group>
                      </subj-group>
                    </subj-group>
                  </subj-group>
                </subj-group>
                <subj-group subj-group-type="Discipline-v3">
                  <subject>Biology and Life Sciences</subject>
                  <subj-group>
                    <subject>Psychology</subject>
                    <subj-group>
                      <subject>Cognitive Psychology</subject>
                      <subj-group>
                        <subject>Language</subject>
                      </subj-group>
                    </subj-group>
                  </subj-group>
                </subj-group>
                <subj-group subj-group-type="Discipline-v3">
                  <subject>Social Sciences</subject>
                  <subj-group>
                    <subject>Psychology</subject>
                    <subj-group>
                      <subject>Cognitive Psychology</subject>
                      <subj-group>
                        <subject>Language</subject>
                      </subj-group>
                    </subj-group>
                  </subj-group>
                </subj-group>
                <subj-group subj-group-type="Discipline-v3">
                  <subject>Social Sciences</subject>
                  <subj-group>
                    <subject>Linguistics</subject>
                    <subj-group>
                      <subject>Grammar</subject>
                      <subj-group>
                        <subject>Phonology</subject>
                        <subj-group>
                          <subject>Phonemes</subject>
                        </subj-group>
                      </subj-group>
                    </subj-group>
                  </subj-group>
                </subj-group>
                <subj-group subj-group-type="Discipline-v3">
                  <subject>Social Sciences</subject>
                  <subj-group>
                    <subject>Linguistics</subject>
                    <subj-group>
                      <subject>Phonetics</subject>
                      <subj-group>
                        <subject>Vowels</subject>
                      </subj-group>
                    </subj-group>
                  </subj-group>
                </subj-group>
                <subj-group subj-group-type="Discipline-v3">
                  <subject>Social Sciences</subject>
                  <subj-group>
                    <subject>Linguistics</subject>
                    <subj-group>
                      <subject>Grammar</subject>
                      <subj-group>
                        <subject>Phonology</subject>
                        <subj-group>
                          <subject>Syllables</subject>
                        </subj-group>
                      </subj-group>
                    </subj-group>
                  </subj-group>
                </subj-group>
                <subj-group subj-group-type="Discipline-v3">
                  <subject>Engineering and Technology</subject>
                  <subj-group>
                    <subject>Equipment</subject>
                    <subj-group>
                      <subject>Audio Equipment</subject>
                    </subj-group>
                  </subj-group>
                </subj-group>
              </article-categories>
              <title-group>
                <article-title>Maintaining information about speech input during accent adaptation</article-title>
                <alt-title alt-title-type="running-head">Maintaining information about speech input during accent adaptation</alt-title>
              </title-group>
              <contrib-group>
                <contrib contrib-type="author">
                  <contrib-id authenticated="true" contrib-id-type="orcid">http://orcid.org/0000-0003-0103-941X</contrib-id>
                  <name>
                    <surname>Burchill</surname>
                    <given-names>Zachary</given-names>
                  </name>
                  <role content-type="http://credit.casrai.org/">Conceptualization</role>
                  <role content-type="http://credit.casrai.org/">Data curation</role>
                  <role content-type="http://credit.casrai.org/">Investigation</role>
                  <role content-type="http://credit.casrai.org/">Methodology</role>
                  <role content-type="http://credit.casrai.org/">Project administration</role>
                  <role content-type="http://credit.casrai.org/">Supervision</role>
                  <role content-type="http://credit.casrai.org/">Visualization</role>
                  <role content-type="http://credit.casrai.org/">Writing – original draft</role>
                  <role content-type="http://credit.casrai.org/">Writing – review &amp; editing</role>
                  <xref ref-type="aff" rid="aff001">
                    <sup>1</sup>
                  </xref>
                  <xref ref-type="corresp" rid="cor001">*</xref>
                </contrib>
                <contrib contrib-type="author">
                  <name>
                    <surname>Liu</surname>
                    <given-names>Linda</given-names>
                  </name>
                  <role content-type="http://credit.casrai.org/">Conceptualization</role>
                  <role content-type="http://credit.casrai.org/">Investigation</role>
                  <role content-type="http://credit.casrai.org/">Software</role>
                  <role content-type="http://credit.casrai.org/">Writing – review &amp; editing</role>
                  <xref ref-type="aff" rid="aff001">
                    <sup>1</sup>
                  </xref>
                </contrib>
                <contrib contrib-type="author">
                  <contrib-id authenticated="true" contrib-id-type="orcid">http://orcid.org/0000-0002-1158-7308</contrib-id>
                  <name>
                    <surname>Jaeger</surname>
                    <given-names>T. Florian</given-names>
                  </name>
                  <role content-type="http://credit.casrai.org/">Conceptualization</role>
                  <role content-type="http://credit.casrai.org/">Formal analysis</role>
                  <role content-type="http://credit.casrai.org/">Funding acquisition</role>
                  <role content-type="http://credit.casrai.org/">Investigation</role>
                  <role content-type="http://credit.casrai.org/">Methodology</role>
                  <role content-type="http://credit.casrai.org/">Supervision</role>
                  <role content-type="http://credit.casrai.org/">Validation</role>
                  <role content-type="http://credit.casrai.org/">Writing – original draft</role>
                  <role content-type="http://credit.casrai.org/">Writing – review &amp; editing</role>
                  <xref ref-type="aff" rid="aff001">
                    <sup>1</sup>
                  </xref>
                  <xref ref-type="aff" rid="aff002">
                    <sup>2</sup>
                  </xref>
                </contrib>
              </contrib-group>
              <aff id="aff001">
                <label>1</label>
                <addr-line>Department of Brain and Cognitive Sciences, University of Rochester, Rochester, New York, United States of America</addr-line>
              </aff>
              <aff id="aff002">
                <label>2</label>
                <addr-line>Department of Computer Science, University of Rochester, Rochester, New York, United States of America</addr-line>
              </aff>
              <contrib-group>
                <contrib contrib-type="editor">
                  <name>
                    <surname>Malmierca</surname>
                    <given-names>Manuel S.</given-names>
                  </name>
                  <role>Editor</role>
                  <xref ref-type="aff" rid="edit1"/>
                </contrib>
              </contrib-group>
              <aff id="edit1">
                <addr-line>Universidad de Salamanca, SPAIN</addr-line>
              </aff>
              <author-notes>
                <fn fn-type="COI-statement" id="coi001">
                  <p><bold>Competing Interests: </bold>The authors have declared that no competing interests exist.</p>
                </fn>
                <corresp id="cor001">* E-mail: <email>zachary.burchill@rochester.edu</email></corresp>
              </author-notes>
              <pub-date pub-type="epub">
                <day>7</day>
                <month>8</month>
                <year>2018</year>
              </pub-date>
              <pub-date pub-type="collection">
                <year>2018</year>
              </pub-date>
              <volume>13</volume>
              <issue>8</issue>
              <elocation-id>e0199358</elocation-id>
              <history>
                <date date-type="received">
                  <day>28</day>
                  <month>1</month>
                  <year>2018</year>
                </date>
                <date date-type="accepted">
                  <day>6</day>
                  <month>6</month>
                  <year>2018</year>
                </date>
              </history>
              <permissions>
                <copyright-statement>© 2018 Burchill et al</copyright-statement>
                <copyright-year>2018</copyright-year>
                <copyright-holder>Burchill et al</copyright-holder>
                <license xlink:href="http://creativecommons.org/licenses/by/4.0/">
                  <license-p>This is an open access article distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution License</ext-link>, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited.</license-p>
                </license>
              </permissions>
              <self-uri content-type="pdf" xlink:href="pone.0199358.pdf"/>
              <abstract>
                <p>Speech understanding can be thought of as inferring progressively more abstract representations from a rapidly unfolding signal. One common view of this process holds that lower-level information is discarded as soon as higher-level units have been inferred. However, there is evidence that subcategorical information about speech percepts is not immediately discarded, but is maintained past word boundaries and integrated with subsequent input. Previous evidence for such subcategorical information maintenance has come from paradigms that lack many of the demands typical to everyday language use. We ask whether information maintenance is also possible under more typical constraints, and in particular whether it can facilitate accent adaptation. In a web-based paradigm, participants listened to isolated foreign-accented words in one of three conditions: subtitles were displayed concurrently with the speech, after speech offset, or not displayed at all. The delays between speech offset and subtitle presentation were manipulated. In a subsequent test phase, participants then transcribed novel words in the same accent without the aid of subtitles. We find that subtitles facilitate accent adaptation, even when displayed with a 6 second delay. Listeners thus maintained subcategorical information for sufficiently long to allow it to benefit adaptation. We close by discussing what type of information listeners maintain—subcategorical phonetic information, or just uncertainty about speech categories.</p>
              </abstract>
              <funding-group>
                <award-group id="award001">
                  <funding-source>
                    <institution-wrap>
                      <institution-id institution-id-type="funder-id">http://dx.doi.org/10.13039/100000071</institution-id>
                      <institution>National Institute of Child Health and Human Development</institution>
                    </institution-wrap>
                  </funding-source>
                  <award-id>RHD075797A</award-id>
                  <principal-award-recipient>
                    <contrib-id authenticated="true" contrib-id-type="orcid">http://orcid.org/0000-0002-1158-7308</contrib-id>
                    <name>
                      <surname>Jaeger</surname>
                      <given-names>Tim Florian</given-names>
                    </name>
                  </principal-award-recipient>
                </award-group>
                <award-group id="award002">
                  <funding-source>
                    <institution>Center of Language Sciences, University of Rochester</institution>
                  </funding-source>
                  <principal-award-recipient>
                    <contrib-id authenticated="true" contrib-id-type="orcid">http://orcid.org/0000-0003-0103-941X</contrib-id>
                    <name>
                      <surname>Burchill</surname>
                      <given-names>Zachary</given-names>
                    </name>
                  </principal-award-recipient>
                </award-group>
                <funding-statement>This work was supported by an NIHCD R01 (RHD075797) to T. Florian Jaeger via the The Eunice Kennedy Shriver National Institute of Child Health and Human Development and partially funded through graduate support for Zachary Burchill from the Center of Language Sciences, University of Rochester. The funders had no role in study design, data collection and analysis, decision to publish, or preparation of the manuscript.</funding-statement>
              </funding-group>
              <counts>
                <fig-count count="8"/>
                <table-count count="2"/>
                <page-count count="24"/>
              </counts>
              <custom-meta-group>
                <custom-meta id="data-availability">
                  <meta-name>Data Availability</meta-name>
                  <meta-value>The one participant data file is available at the Open Science Framework repository at the following URL: <ext-link ext-link-type="uri" xlink:href="https://osf.io/pgqru/">https://osf.io/pgqru/</ext-link>.</meta-value>
                </custom-meta>
              </custom-meta-group>
            </article-meta>
            <notes>
              <title>Data Availability</title>
              <p>The one participant data file is available at the Open Science Framework repository at the following URL: <ext-link ext-link-type="uri" xlink:href="https://osf.io/pgqru/">https://osf.io/pgqru/</ext-link>.</p>
            </notes>
          </front>
          <body>
            <sec sec-type="intro" id="sec001">
              <title>Introduction</title>
              <p>Speech understanding can be thought of as inferring progressively more abstract linguistic representations, such as phonemes, words, meanings, etc., from a rapidly unfolding signal. One common view is that this abstraction process is accompanied by complete and immediate <italic>compression</italic>, whereby information about lower-level representations is discarded as soon as a higher-level unit has been inferred. The motivation underlying this view is that attention and memory resources are strongly bounded, so that the high-dimensional perceptual signal <italic>needs</italic> to be simplified immediately [<xref rid="pone.0199358.ref001" ref-type="bibr">1</xref>] (see also [<xref rid="pone.0199358.ref002" ref-type="bibr">2</xref>]; for discussion, see [<xref rid="pone.0199358.ref003" ref-type="bibr">3</xref>]). An intuitive example in line with this immediate compression view is the experience of having difficulty recalling the exact word sequences used in a past conversation, even when one recalls its content.</p>
              <p>Beyond such intuitive examples, the immediate compression view was motivated in part by the early finding that listeners rapidly categorize the speech signal into invariant phonological categories (categorical perception [<xref rid="pone.0199358.ref004" ref-type="bibr">4</xref>]), suggesting that subcategorical information was abandoned early during speech perception. Additional support came from early research on "perceptual stores". This work found that memory of detailed perceptual information often decays within fractions of a second [<xref rid="pone.0199358.ref005" ref-type="bibr">5</xref>,<xref rid="pone.0199358.ref006" ref-type="bibr">6</xref>]. Evidence like this shows that information about the original signal is lost as perceptual processing proceeds. It does, however, leave open whether <italic>all</italic> subcategorical information is lost and <italic>when</italic> such information is lost during processing.</p>
              <p>Indeed, there are at least two reasons to believe that speech perception is more than just complete and immediate compression. First, there is evidence that at least <italic>some</italic> subcategorical information is maintained long enough to become part of the representation of the word in long-term memory. For example, word recognition is facilitated for words that have previously been heard from the same talker [<xref rid="pone.0199358.ref007" ref-type="bibr">7</xref>–<xref rid="pone.0199358.ref010" ref-type="bibr">10</xref>] (see also [<xref rid="pone.0199358.ref011" ref-type="bibr">11</xref>–<xref rid="pone.0199358.ref015" ref-type="bibr">15</xref>], for review, see [<xref rid="pone.0199358.ref016" ref-type="bibr">16</xref>,<xref rid="pone.0199358.ref017" ref-type="bibr">17</xref>]). This is only possible if subcategorical information somehow is maintained for long enough to become part of long-term memory.</p>
              <p>Second, there is now a small but growing body of research suggesting that some auditory information does not decay as rapidly as previously assumed. For example, listeners can maintain certain amounts of fine-grained auditory information about pure tones in memory for up to 10 seconds [<xref rid="pone.0199358.ref018" ref-type="bibr">18</xref>]. At least under experimental conditions, even more complex auditory information such as vowel formants seem to be maintained for at least three seconds [<xref rid="pone.0199358.ref019" ref-type="bibr">19</xref>]. At the speeds typical of conversational speech, three seconds would correspond to approximately 9 to 15 words [<xref rid="pone.0199358.ref020" ref-type="bibr">20</xref>], much longer than immediate compression would suggest. Furthermore, evidence suggests that attentional resources can be directed toward auditory representations maintained in short-term memory (for review, see [<xref rid="pone.0199358.ref021" ref-type="bibr">21</xref>]). For example, Backer &amp; Alain [<xref rid="pone.0199358.ref022" ref-type="bibr">22</xref>] showed that cueing attention to auditory representations even four seconds after stimulus offset enhanced listeners' ability to attenuate change deafness. It seems that attentional resources can even be oriented toward specific features within the auditory representation, such pitch [<xref rid="pone.0199358.ref021" ref-type="bibr">21</xref>]. This suggests that attentional resources could be invested to facilitate maintenance of rich perceptual information.</p>
              <p>However, many of these earlier works investigated the limits of auditory memory under conditions hardly representative of every day speech perception. Specifically, these studies typically employed relatively simple speech stimuli presented in isolation (e.g., single vowels [<xref rid="pone.0199358.ref019" ref-type="bibr">19</xref>], syllables [<xref rid="pone.0199358.ref023" ref-type="bibr">23</xref>], for review, see [<xref rid="pone.0199358.ref024" ref-type="bibr">24</xref>]). Crowder [<xref rid="pone.0199358.ref019" ref-type="bibr">19</xref>] tasked participants with deciding if two auditory stimuli presented with varying intervening lag were identical vowels. Typical studies of this variety also involve many repetitions of the same stimulus type. As a consequence, participants knew what information from the speech signal to maintain to successfully complete the task. These studies might thus be more informative about speech perception under these very specific conditions, rather than being reflective of the limits of subcategorical information maintenance during everyday speech perception. As we argue below, similar concerns apply to the interpretation of more recent work, including research on the limits of subcategorical maintenance during sentence understanding [<xref rid="pone.0199358.ref003" ref-type="bibr">3</xref>,<xref rid="pone.0199358.ref025" ref-type="bibr">25</xref>–<xref rid="pone.0199358.ref029" ref-type="bibr">29</xref>].</p>
              <p>The goal of the present research is to contribute to the understanding of attentional and memory limitations during speech perception. Specifically, we focus on the perception of—and adaptation to—foreign accented speech. We ask how long after its occurrence in the signal does information remain available to guide accent adaptation? We begin with a summary of previous work and then outline the approach we take to expand on these studies.</p>
              <sec id="sec002">
                <title>Subcategorical information maintenance during speech perception</title>
                <p>A number of studies address the most literal interpretation of the immediate compression view, probing the extent to which subcategorical information can be maintained beyond the segment [<xref rid="pone.0199358.ref025" ref-type="bibr">25</xref>–<xref rid="pone.0199358.ref031" ref-type="bibr">31</xref>] (for a review, see [<xref rid="pone.0199358.ref032" ref-type="bibr">32</xref>]). For example, McMurray et al. [<xref rid="pone.0199358.ref031" ref-type="bibr">31</xref>] had participants view scenes of five objects and click on the object that was spoken aloud. Participants’ eye-movements were tracked, while they listened to and executed these instructions. On critical trials, the scene contained both the target word (e.g., "telephone") and an onset competitor (e.g., "Delaware") which shared the segments following the onset (/t/ or /d/) with the target. Such scenes elicit competitor effects: in addition to the target referent, participants fixate competitor referent more often than other unrelated distractor referents in the scene [<xref rid="pone.0199358.ref033" ref-type="bibr">33</xref>]. In McMurray et al. [<xref rid="pone.0199358.ref031" ref-type="bibr">31</xref>], the target and competitor always differed in whether the onset was a voiced (e.g., /d/ as in "Delaware") or voiceless plosive (e.g., /t/ as in "telephone"). Between trials, McMurray and colleagues manipulated the voice onset time (VOT) of the target word’s onset plosive, making its voicing more or less ambiguous (VOT is the primary cue to voicing in English [<xref rid="pone.0199358.ref034" ref-type="bibr">34</xref>]). Participants exhibited increasingly slower recovery times—the time it took to shift their gaze from the competitor to the target after the point of disambiguation (e.g. the /f/ vs. /w/ in "telephone" and "Delaware")—the more ambiguous the VOT was. If listeners were immediately discarding the (subcategorical) VOT information, they should not have exhibited such within-category differences in eye-movements four-to-five phonemes after the initial phone (for similar results from neuro-imaging, see also [<xref rid="pone.0199358.ref035" ref-type="bibr">35</xref>]).</p>
                <p>Another line of studies has found that some subcategorical information can be maintained for longer, even <italic>past word boundaries</italic>. For example, following original work by Connine et al. [<xref rid="pone.0199358.ref028" ref-type="bibr">28</xref>], Bicknell et al. [<xref rid="pone.0199358.ref003" ref-type="bibr">3</xref>] had participants listen to sentences with words in which a contrast was artificially manipulated along the voicing continuum (e.g., from "tent" to "dent") to create more or less ambiguous "(d/t)ent" instances. In all target sentences these manipulated words were followed by disambiguating contexts (e.g., either "There’s a (d/t)ent in the fender", biasing towards "dent", or "There’s a (d/t)ent in the forest", biasing towards "tent"). The disambiguating right-context occurred either 3 syllables or 6–8 syllables after the manipulated word. After each sentence, participants answered which word they heard (e.g. "tent" or "dent"). Participants' answers were affected by both the phonetic information (e.g., the VOT of the initial phone of the target word) and the disambiguating right-context (whether the sentence continued with "fender" or "forest"). Similar results were obtained in other experiments [<xref rid="pone.0199358.ref027" ref-type="bibr">27</xref>,<xref rid="pone.0199358.ref029" ref-type="bibr">29</xref>]. Connine et al. [<xref rid="pone.0199358.ref028" ref-type="bibr">28</xref>] reported that subcategorical information is maintained for three but not six to eight syllables. However, Bicknell et al. [<xref rid="pone.0199358.ref025" ref-type="bibr">25</xref>] pointed to a procedural problem in Connine et al.’s study. Once this problem was removed, Bicknell and colleagues found no evidence of decay in the maintenance of subcategorical information even at the longest lag tested, 6–8 syllables).</p>
                <p>In all these studies, the contrast that participants were asked to categorize remained constant across the entire experiment. Furthermore, the repeated critical segment always occurred in a predictable location in the target sentences in each study. For example, in the second experiment of Szostak &amp; Pitt [<xref rid="pone.0199358.ref029" ref-type="bibr">29</xref>], participants judged whether they heard /s/, as in "sip", or /ʃ/, as in "ship", in the target word. The /s/-/ʃ/ contrast was the only one participants needed to keep track of, and all sentences began with "The (?sʃ)ip was …". Thus participants knew <underline><italic toggle="yes">what</italic></underline> aspect of the signal to maintain (e.g. the /s/-/ʃ/ contrast) and <underline><italic toggle="yes">when</italic></underline> that aspect appeared in the signal (e.g. at the onset of the second word). It is possible, if not likely, that this made the task much easier: participants could in principle improve performance by only maintaining information about a specific small part of the signal. Thus, these studies leave open whether subcategorical information is maintained past word boundaries under circumstances that more closely resemble everyday speech processing.</p>
                <p>This is question we seek to address here. In order to approach questions about the limits of maintaining information under task demands that more closely resemble everyday speech perception, we explore a paradigm novel for the study of uncertainty/information maintenance during speech perception. This paradigm draws on another line of work, which we introduce next: research on adaptation to an unfamiliar foreign-accent. As we detail in the discussion, the paradigm we employ—and the results we obtain—also speak to another pressing question about information maintenance: the nature of the maintained information. Specifically, we will discuss how our result suggest that listeners can maintain at least some aspects of information—at the <italic>phonetic</italic> level or below—for much longer than previously assumed (rather than to just maintain degrees of uncertainty, cf. [<xref rid="pone.0199358.ref003" ref-type="bibr">3</xref>] for discussion).</p>
              </sec>
              <sec id="sec003">
                <title>Using foreign-accents to study information maintenance</title>
                <p>Understanding an unfamiliarly accented talker can be difficult initially [<xref rid="pone.0199358.ref036" ref-type="bibr">36</xref>], but listeners tend to get better with exposure to the talker [<xref rid="pone.0199358.ref037" ref-type="bibr">37</xref>,<xref rid="pone.0199358.ref038" ref-type="bibr">38</xref>]. This process, sometimes referred to as <italic>accent adaptation</italic>, can be facilitated by context: knowing what word was intended does not only facilitate recognition of that word [<xref rid="pone.0199358.ref039" ref-type="bibr">39</xref>], but can also lead to better recognition of subsequent materials [<xref rid="pone.0199358.ref040" ref-type="bibr">40</xref>] (for similar work with degraded speech, see [<xref rid="pone.0199358.ref041" ref-type="bibr">41</xref>]). For example, Mitterer &amp; McQueen [<xref rid="pone.0199358.ref040" ref-type="bibr">40</xref>] exposed native Dutch listeners to excerpts of Australian and Scottish English—either accompanied by subtitles or not. During a test phase, listeners had to transcribe other audio excerpts from the same English variety (in the absence of subtitles). Participants who had been exposed to subtitled speech performed significantly better at the transcription task, compared to listeners who had not received subtitles during exposure. This shows that labeling information provided by the context (in this case, subtitles) can facilitate adaptation to regional varieties of English (at least for L2 listeners).</p>
                <p>In Mitterer &amp; McQueen [<xref rid="pone.0199358.ref040" ref-type="bibr">40</xref>], subtitles were displayed simultaneously with the accented speech, similar to standard subtitling in movies. Here we adapt this paradigm to our goals. We expose listeners to foreign-accented speech, while subtitles are displayed either simultaneously, at various delays, or not at all. During a later test phase—where speech from the same foreign-accented talker is now presented without subtitles—we then assess how well listeners have adapted to the accented speech. By comparing performance during the test phase, we can assess the effect of subtitle timing during exposure. Research with a similar paradigm has been used to study how contextual information enhances speech comprehension in the moment: Sohoglu et al. [<xref rid="pone.0199358.ref042" ref-type="bibr">42</xref>] manipulated the timing of subtitles in relation to acoustically degraded speech to determine the delay at which the enhancement in perceived clarity of speech from the contextual information would begin to decline.</p>
                <p>In this paradigm, the subtitles thus parallel the role of right-context in the previous work discussed above [<xref rid="pone.0199358.ref003" ref-type="bibr">3</xref>,<xref rid="pone.0199358.ref025" ref-type="bibr">25</xref>–<xref rid="pone.0199358.ref029" ref-type="bibr">29</xref>]. As we detail next, this paradigm allows us to test whether information about the speech signal is maintained, and for how long. Specifically, the paradigm we explore here allows us to address the questions we highlighted above.</p>
                <p>Accented speech exhibits high-dimensional deviations from familiar speech, and optimal adaptation to an accented talker would require learning the nature of these deviations. In our paradigm listeners, who are specifically selected as being unfamiliar with the particular foreign accent, do not <italic>a priori</italic> know exactly what phonetic information is critical for adaptation. And being unfamiliar with this accent, listeners do not know <italic>where</italic> in the signal relevant phonetic information will occur, or <italic>what</italic> type of information that will be. This makes the task demands of the present paradigm somewhat more similar to everyday speech perception, while still allowing control over the relative timing of labeling information.</p>
                <p>Using this paradigm, we present two web-based experiments on accent adaptation. Experiment 1 serves two aims. First, we test whether adaptation to accented speech is facilitated when listeners are exposed to subtitled accented speech, compared to exposure to accented speech without subtitles. Like previous work [<xref rid="pone.0199358.ref040" ref-type="bibr">40</xref>,<xref rid="pone.0199358.ref042" ref-type="bibr">42</xref>,<xref rid="pone.0199358.ref043" ref-type="bibr">43</xref>] this first manipulation uses subtitles that are presented concurrently with the speech input. Second and key to the current study is the question of whether subtitles that are displayed <underline>after</underline> the speech input—thereby constituting right-context—facilitate accent adaptation. If so, this would suggest that listeners can maintain information past word boundaries and integrate this information with later context for learning (in the general discussion, we discuss alternative explanations and why we do not think that our data supports them). These results would imply that the speech signal is not immediately compressed and discarded, contrary to the extreme compression view.</p>
                <p>To anticipate our results, Experiment 1 finds that subtitles provide a small but significant facilitatory effect for accent adaptation. Critically, statistically identical facilitatory effects are also observed when the subtitles are displayed after the speech input. Encouraged by this result, we conducted an additional experiment to explore the temporal limits of such maintenance. In order to test for how long information relevant to accent adaptation can be maintained perfectly, Experiment 2 manipulates the amount of time that passes between the presentation of the speech stimulus and the presentation of the subtitles (right-context).</p>
              </sec>
            </sec>
            <sec id="sec004">
              <title>Experiment 1</title>
              <p>In an exposure-test paradigm, participants listened and responded to speech produced by a Spanish-accented talker. In the previous work that inspired us to use a subtitle-based paradigm [<xref rid="pone.0199358.ref040" ref-type="bibr">40</xref>], participants watched a 30-minute video of accented talkers in a commercial television show or a film. We adapt this paradigm for the study of information maintenance, which requires control over the timing of subtitles relative to the speech input. We thus present small chunks of speech input. In the following experiments, we present isolated words.</p>
              <p>Each participant was randomly assigned to one of three between-participant conditions that manipulated the use of subtitles during the <bold>exposure phase</bold>. Participants in the Absent condition received speech input (which was one word per trial) without any subtitles. Participants the Concurrent subtitle condition saw the subtitle for each word presented simultaneously with the speech input. Finally, participants in the Delayed condition saw the subtitle immediately <italic>after</italic> the end of the speech input. In the <bold>test phase</bold>, we use transcription accuracy as a measure of accent adaptation (following [<xref rid="pone.0199358.ref037" ref-type="bibr">37</xref>]). During test, participants in all three conditions heard novel isolated words produced by the same talker they heard during exposure (but without subtitles), and transcribed the word they heard for each of these trials.</p>
              <sec id="sec005" sec-type="materials|methods">
                <title>Methods</title>
                <sec id="sec006">
                  <title>Participants</title>
                  <p>Experiment 1 and 2 aimed for 60 successful participants per condition (see Exclusions below). To this end, 245 participants were recruited using Amazon's Mechanical Turk (<ext-link ext-link-type="uri" xlink:href="http://www.mturk.com/">www.mturk.com</ext-link>), with the human subject research for both experiments being approved by the University of Rochester's Research Subjects Review Board. Recruitment asked for monolingual talkers of English. Participants were randomly assigned to one of the three subtitle conditions.</p>
                </sec>
                <sec id="sec007">
                  <title>Procedure</title>
                  <p>The experiment was conducted over the web with Mechanical Turk. The experiment consisted of four phases: it began with a practice phase, followed by an exposure phase, then a test phase, and finally a survey.</p>
                  <p>After four practice trials, participants went through the <bold>exposure phase</bold>, which consisted of 80 experimental trials. The structure of the exposure trials is illustrated in <xref ref-type="fig" rid="pone.0199358.g001">Fig 1</xref>. Each trial started with a fixation cross that was displayed for 500 milliseconds (ms). The speech input started playing immediately afterward. In the Concurrent condition, the subtitle was displayed for 1500 ms, starting concurrently with the speech input (<xref ref-type="fig" rid="pone.0199358.g001">Fig 1</xref>, top-right). In the Delayed condition, subtitles were also displayed for 1500 ms, but immediately following the offset of the speech input (<xref ref-type="fig" rid="pone.0199358.g001">Fig 1</xref>, middle-right). The duration for which the subtitles were displayed was thus held constant across the Concurrent and Delayed conditions. In the Absent condition, no subtitles were displayed (<xref ref-type="fig" rid="pone.0199358.g001">Fig 1</xref>, bottom-right). The total duration of each trial was held constant across conditions: the next trial always began 2500 ms after the offset of the speech input. When no subtitles were displayed, the screen was blank (labeled "ITI" in <xref ref-type="fig" rid="pone.0199358.g001">Fig 1</xref>).</p>
                  <fig id="pone.0199358.g001" orientation="portrait" position="float">
                    <object-id pub-id-type="doi">10.1371/journal.pone.0199358.g001</object-id>
                    <label>Fig 1</label>
                    <caption>
                      <title>Schematic illustration of exposure phase trial structure.</title>
                      <p>The audio icon on the image represents the beginning of the audio; no such icon was visually displayed to participants.</p>
                    </caption>
                    <graphic xlink:href="pone.0199358.g001"/>
                  </fig>
                  <p>To measure whether participants were actively engaged in the exposure phase, 12 catch trials were distributed throughout exposure. These trials were identical to other trials, but a pure tone was played briefly before the beginning of these trials. Participants were instructed to press the space bar when they heard this tone (participants were forced to repeat the practice block until they correctly responded to the two additional catch trials that were in that block). We recorded the number of false positives (pressing the space bar when no tone was played) and false negatives (failure to press the space bar during catch trials) during exposure.</p>
                  <p>During the <bold>test phase</bold> in all conditions, each trial began with the accented talker saying the word. Participants then transcribed the word to the best of their ability in a text box and pressed the enter key to move on to the next trial. The test phase consisted of 40 such trials. Regardless of the exposure condition, test trials never contained subtitles.</p>
                  <p>After the test phase, participants took a short <bold>exit survey</bold> that asked about their language background and ethnicity, audio quality, and what type of audio equipment they were using. For a complete lists of survey questions, see <xref ref-type="supplementary-material" rid="pone.0199358.s003">S1 Questionnaire</xref>.</p>
                </sec>
                <sec id="sec008" sec-type="materials|methods">
                  <title>Materials</title>
                  <p>The 120 monosyllabic CVC words participants listened to during exposure and testing were produced in isolation by a Spanish-accented female talker and the recordings were taken from the Hoosier Database of Native and Nonnative Speech for Children [<xref rid="pone.0199358.ref044" ref-type="bibr">44</xref>]. These particular words were chosen as they represented the 120 least comprehensible words produced by the talker, as measured by a separate pre-test norming experiment also conducted over Amazon's Mechanical Turk. The mean amplitude of all clips was adjusted to 70 dB.</p>
                  <p>The stimuli for the four practice trials consisted of recordings of isolated monosyllabic words spoken by a Japanese-accented female talker taken from the same corpus (but included CVC and more complex words).</p>
                  <p>The 120 Spanish-accented audio clips were divided into three blocks of 40 words in such a way that the types of onsets, vowels, and codas were roughly equal across blocks. Using a Latin-square design, we rotated which block served as test and which two blocks served as exposure across participants so that each block served as test to equal numbers of participants in each condition. We also reversed the internal order of all block across participants so that each block order and block-internal order was heard an equal number of times in each condition. The block structure for the first two blocks (exposure) was opaque to participants. For the third block (test), the task changed from passive listening to transcription.</p>
                </sec>
                <sec id="sec009">
                  <title>Scoring</title>
                  <p>All transcriptions were automatically scored for accuracy. A transcription was counted as "correct" if it matched the spoken word or matched an existing homophone of the word. After a subset of automatically processed transcriptions was found to be highly accurate by manual review, all transcriptions were processed automatically.</p>
                </sec>
                <sec id="sec010">
                  <title>Exclusions</title>
                  <p>Sixty-nine participants were excluded from Experiment 1 (see <xref rid="pone.0199358.t001" ref-type="table">Table 1</xref>). Although participants were told beforehand that they were required to be monolingual speakers of American English, a number of participants reported on the post-test survey that they had not met those requirements and were therefore excluded. Since the purpose of the current study is to investigate maintenance under more naturalistic constraints, i.e., when participants do not know <italic>a priori</italic> what aspects of the signal to maintain and where in the signal these parts will occur, participants likely to be familiar with similar accents (those who reported that they had family members with Hispanic backgrounds or those who reported familiarity with equally strong foreign accents) were excluded from the analyses.</p>
                  <table-wrap id="pone.0199358.t001" orientation="portrait" position="float">
                    <object-id pub-id-type="doi">10.1371/journal.pone.0199358.t001</object-id>
                    <label>Table 1</label>
                    <caption>
                      <title>Participant exclusions in Experiment 1.</title>
                      <p>Some participants were excluded for multiple reasons.</p>
                    </caption>
                    <alternatives>
                      <graphic id="pone.0199358.t001g" xlink:href="pone.0199358.t001"/>
                      <table frame="hsides" rules="groups">
                        <colgroup span="1">
                          <col align="left" valign="middle" span="1"/>
                          <col align="left" valign="middle" span="1"/>
                          <col align="left" valign="middle" span="1"/>
                        </colgroup>
                        <thead>
                          <tr>
                            <th align="left" rowspan="1" colspan="1">Total participants:</th>
                            <th align="right" rowspan="1" colspan="1">245</th>
                            <th align="right" rowspan="1" colspan="1">100%</th>
                          </tr>
                          <tr>
                            <th align="left" style="border-bottom:thick" rowspan="1" colspan="1">Reason for exclusion</th>
                            <th align="right" style="border-bottom:thick" rowspan="1" colspan="1">n</th>
                            <th align="right" style="border-bottom:thick" rowspan="1" colspan="1">%</th>
                          </tr>
                        </thead>
                        <tbody>
                          <tr>
                            <td align="left" rowspan="1" colspan="1">Participant's family likely to include Spanish speakers</td>
                            <td align="right" rowspan="1" colspan="1">20</td>
                            <td align="right" rowspan="1" colspan="1">(8%)</td>
                          </tr>
                          <tr>
                            <td align="left" rowspan="1" colspan="1">Participant not monolingual</td>
                            <td align="right" rowspan="1" colspan="1">47</td>
                            <td align="right" rowspan="1" colspan="1">(19%)</td>
                          </tr>
                          <tr>
                            <td align="left" rowspan="1" colspan="1">Participant reported familiarity with strong foreign accents</td>
                            <td align="right" rowspan="1" colspan="1">6</td>
                            <td align="right" rowspan="1" colspan="1">(2%)</td>
                          </tr>
                          <tr>
                            <td align="left" rowspan="1" colspan="1">Used computer speakers or unknown audio quality</td>
                            <td align="right" rowspan="1" colspan="1">6</td>
                            <td align="right" rowspan="1" colspan="1">(2%)</td>
                          </tr>
                          <tr>
                            <td align="left" rowspan="1" colspan="1">Client-side computing error</td>
                            <td align="right" rowspan="1" colspan="1">1</td>
                            <td align="right" rowspan="1" colspan="1">(0%)</td>
                          </tr>
                          <tr>
                            <td align="left" rowspan="1" colspan="1">Failed &gt;4 catch trials (out of 12)</td>
                            <td align="right" rowspan="1" colspan="1">7</td>
                            <td align="right" rowspan="1" colspan="1">(3%)</td>
                          </tr>
                          <tr>
                            <td align="left" style="border-bottom:thick" rowspan="1" colspan="1">Outliers in transcription performance</td>
                            <td align="right" style="border-bottom:thick" rowspan="1" colspan="1">2</td>
                            <td align="right" style="border-bottom:thick" rowspan="1" colspan="1">(1%)</td>
                          </tr>
                          <tr>
                            <td align="left" rowspan="1" colspan="1">Participants remaining</td>
                            <td align="right" rowspan="1" colspan="1">176</td>
                            <td align="right" rowspan="1" colspan="1">(72%)</td>
                          </tr>
                        </tbody>
                      </table>
                    </alternatives>
                  </table-wrap>
                  <p>Because the current study was run over the web, audio quality and audio equipment varied from participant to participant. Previous studies have demonstrated that despite this variability, web-based experiments on speech perception are feasible [<xref rid="pone.0199358.ref045" ref-type="bibr">45</xref>–<xref rid="pone.0199358.ref047" ref-type="bibr">47</xref>]. This includes studies on accent adaptation [<xref rid="pone.0199358.ref048" ref-type="bibr">48</xref>–<xref rid="pone.0199358.ref050" ref-type="bibr">50</xref>]. To reduce the between-speaker variability, we required participants to use either in-ear or over-ear headphones when taking the experiment. Participants who did not comply with these restrictions (based on their response in the exit survey) were excluded. Participants who made more than four catch trial errors or who had experienced technical difficulties were also excluded. After these exclusions, we excluded any participants whose mean transcription accuracy during test was more than three standard deviations from the overall mean of all remaining participants. This left 62 participants in the Absent condition, 57 in the Concurrent condition, and 57 in the Delayed condition.</p>
                </sec>
                <sec id="sec011">
                  <title>Determining which items benefit from subtitles</title>
                  <p>Here we are interested in whether (and when) delayed subtitles facilitate accent adaptation. For this reason, test items that do not benefit from exposure to concurrent subtitles, compared to exposure without subtitles, are not informative for the present purpose. This issue, its reasons, and its consequences occurred to us only after all experiments had already been conducted. Here we describe how we decided to address it.</p>
                  <p>There are several reasons why a test item might not benefit from subtitling during exposure. One important reason for the present purpose is that the test item might not have benefitted from <italic>any</italic> exposure (subtitled or not). For example, an item might contain a speech error or other accent-unrelated problem that leads participants to misunderstand it. An item—recall that items were isolated words—might also contain accent features that, even if perfectly learned, do not necessarily lead to improved transcription. For example, the /ɪ/ in "sip" is not a phoneme in Spanish, and L2 English learners often pronounce it as /i/, pronouncing "sip" as "seep". Even a listener who has learned this only has a 50% chance of determining if /sip/ referred to "sip" or "seep" (provided that all other phonemes were recognized accurately). Test items for which performance is largely driven by such accent features will not benefit from subtitled exposure.</p>
                  <p>When we chose the 120 spoken word stimuli, we did so without considering these factors. This decreases the power of our experiments. Indeed, Experiment 1 found only a very small effect when all items were considered: whereas participants in the Absent condition on average had 47% accurate transcriptions, participants in the Concurrent condition on averaged had 50% accurate transcriptions.</p>
                  <p>To increase the signal-to-noise ratio of our experiments, we are thus interested in determining the test items that benefit from exposure with concurrent subtitles, so that we can ask whether—for those items—<italic>delayed</italic> subtitles also facilitated accuracy during test. We thus conducted a separate norming study using a paradigm identical to Experiment 1 to determine which of the 120 items benefitted the most from the presence of subtitles. We recruited 148 new participants; after applying exclusion criteria identical to Experiment 1, this left 59 participants in the Concurrent condition and 40 in the Absent condition. We then calculated the extent to which exposure to concurrent subtitles improved accuracy compared to exposure without subtitles. Specifically, we repeatedly analyzed the results of the norming study while incrementally removing the items for which subtitle exposure increased performance the least (the analysis we used for these data is the same as reported for Experiment 1 in the result section below). The comparison between the Concurrent vs. Absent subtitles became significant at the <italic>p</italic> &lt; 0.0025 level when items for which subtitle exposure <italic>decreased</italic> performance by 25% or more (a log-odds difference of -1 or less) were excluded (20 items total). These same twenty items were then excluded from the analyses of Experiment 1.</p>
                  <p>This procedure increased the effect of concurrent subtitles for Experiment 1, compared to the absence of subtitles, from 0.11 to 0.16 log-odds (or 4% improvement, compared to 3% improvement, as described below). We note that this approach makes our analyses anti-conservative with regard to comparisons of the Concurrent condition against any other condition, including any condition with delayed subtitles. However, our approach should not be anti-conservative with regard to the critical comparison of delayed subtitles vs. the absence of subtitles.</p>
                </sec>
              </sec>
              <sec id="sec012" sec-type="results">
                <title>Results</title>
                <p>We analyzed transcription accuracy by means of a mixed logit regression [<xref rid="pone.0199358.ref051" ref-type="bibr">51</xref>,<xref rid="pone.0199358.ref052" ref-type="bibr">52</xref>]. Each trial provided one data point. The analysis included three predictors: subtitle condition (coding described below) and two control predictors, the type of audio equipment used (in-ear headphones = 0, over-ear headphones = 1) and the frequency for which participants reported having encountered equally strong accents (never = 0, more than once = 1). The analysis also included the maximal random effect structure justified by the design: by-participant random intercepts and by-item (test word) random intercepts and slopes for subtitle condition.</p>
                <p>To establish that we could detect the subtitle benefit in our paradigm, we first Helmert-coded subtitle condition: the first contrast compared Concurrent (1) and Delayed (1) to the Absent condition (-2), while the second contrast compared Concurrent (1) to Delayed (-1; Absent = 0). There was no sign of excessive multi-collinearity (fixed effect correlations <italic>r</italic>s &lt; 0.37). Below, we also present an additional analysis that compares each subtitle condition against the Absent condition.</p>
                <p>There was a significant main effect of subtitle condition on participants' transcription accuracy (<xref ref-type="fig" rid="pone.0199358.g002">Fig 2</xref>). As predicted, the presence of subtitles facilitates accent adaptation compared to exposure without subtitles: participants in the Concurrent and Delayed subtitle condition transcribed words more accurately during test than participants in the Absent condition without subtitles with marginal significance (<inline-formula id="pone.0199358.e001"><alternatives><graphic xlink:href="pone.0199358.e001.jpg" id="pone.0199358.e001g" mimetype="image" position="anchor" orientation="portrait"/><mml:math id="M1"><mml:mover accent="true"><mml:mrow><mml:mi>β</mml:mi></mml:mrow><mml:mo>^</mml:mo></mml:mover><mml:mo>=</mml:mo><mml:mn>0.078</mml:mn></mml:math></alternatives></inline-formula>, <italic>z</italic> = 2.34, <italic>p</italic> &lt; 0.02). The timing of the subtitles did not seem to matter: transcription accuracy did not significantly differ when subtitles were presented concurrently in exposure, compared to when they were presented immediately afterwards (<inline-formula id="pone.0199358.e002"><alternatives><graphic xlink:href="pone.0199358.e002.jpg" id="pone.0199358.e002g" mimetype="image" position="anchor" orientation="portrait"/><mml:math id="M2"><mml:mover accent="true"><mml:mrow><mml:mi>β</mml:mi></mml:mrow><mml:mo>^</mml:mo></mml:mover><mml:mo>=</mml:mo><mml:mn>0.021</mml:mn></mml:math></alternatives></inline-formula>, <italic>z</italic> = 0.339, <italic>p</italic> &gt; 0.7).</p>
                <fig id="pone.0199358.g002" orientation="portrait" position="float">
                  <object-id pub-id-type="doi">10.1371/journal.pone.0199358.g002</object-id>
                  <label>Fig 2</label>
                  <caption>
                    <title>Transcription performance during test in Experiment 1.</title>
                    <p>The values plotted in this graph are adjusted to control for the effects of nuisance variables (see text for details). Dots show individual participants. Error bars show 95% confidence intervals based on non-parametric bootstrap over by-subject means.</p>
                  </caption>
                  <graphic xlink:href="pone.0199358.g002"/>
                </fig>
                <p>The two control variables affected transcription accuracy in the expected direction: accent familiarity significantly improved accuracy during test (<inline-formula id="pone.0199358.e003"><alternatives><graphic xlink:href="pone.0199358.e003.jpg" id="pone.0199358.e003g" mimetype="image" position="anchor" orientation="portrait"/><mml:math id="M3"><mml:mover accent="true"><mml:mrow><mml:mi>β</mml:mi></mml:mrow><mml:mo>^</mml:mo></mml:mover><mml:mo>=</mml:mo><mml:mn>0.305</mml:mn></mml:math></alternatives></inline-formula>, <italic>z</italic> = 2.97, <italic>p</italic> &lt; 0.003); over-the-ear headphones (which tend to be of higher quality) lead to numerically improved accuracy compared to in-ear-headphones, but the effect was not significant (<inline-formula id="pone.0199358.e004"><alternatives><graphic xlink:href="pone.0199358.e004.jpg" id="pone.0199358.e004g" mimetype="image" position="anchor" orientation="portrait"/><mml:math id="M4"><mml:mover accent="true"><mml:mrow><mml:mi>β</mml:mi></mml:mrow><mml:mo>^</mml:mo></mml:mover><mml:mo>=</mml:mo><mml:mn>0.042</mml:mn></mml:math></alternatives></inline-formula>, <italic>z</italic> = 0.895, <italic>p</italic> &gt; 0.35).</p>
                <p>In order to visualize the effects of subtitle condition in a way that discounts the effects of both of these controls, <xref ref-type="fig" rid="pone.0199358.g002">Fig 2</xref> and similar figures below plot <italic>adjusted</italic> transcription accuracy. These adjusted accuracy scores were obtained by using the values predicted by the model for each data point (including the estimated random effects), and holding constant the effect of the control variables at their average values.</p>
                <p>To determine the benefits of exposure for the two subtitle conditions independent of one another, a second analysis compared the two conditions for which subtitles were presented (Concurrent and Delayed) to the Absent subtitle condition, using simple contrast coding. I.e., the first contrast being Absent = -1, Delayed = 2, Concurrent = -1, and the second being Absent = -1, Delayed = -1, Concurrent = 2. Transcription accuracy was significantly higher for participants in the Concurrent condition compared to the Absent condition (Concurrent vs. Absent: <inline-formula id="pone.0199358.e005"><alternatives><graphic xlink:href="pone.0199358.e005.jpg" id="pone.0199358.e005g" mimetype="image" position="anchor" orientation="portrait"/><mml:math id="M5"><mml:mover accent="true"><mml:mrow><mml:mi>β</mml:mi></mml:mrow><mml:mo>^</mml:mo></mml:mover><mml:mo>=</mml:mo><mml:mn>0.253</mml:mn></mml:math></alternatives></inline-formula>, <italic>z</italic> = 2.12, <italic>p</italic> = 0.034). When subtitles were presented immediately after the offset of the word, participants only transcribed more accurately with marginal significance (Delayed vs. Absent: <inline-formula id="pone.0199358.e006"><alternatives><graphic xlink:href="pone.0199358.e006.jpg" id="pone.0199358.e006g" mimetype="image" position="anchor" orientation="portrait"/><mml:math id="M6"><mml:mover accent="true"><mml:mrow><mml:mi>β</mml:mi></mml:mrow><mml:mo>^</mml:mo></mml:mover><mml:mo>=</mml:mo><mml:mn>0.212</mml:mn></mml:math></alternatives></inline-formula>, <italic>z</italic> = 1.88, <italic>p</italic> = 0.060).</p>
              </sec>
              <sec id="sec013" sec-type="conclusions">
                <title>Discussion</title>
                <p>First, we find improved transcription accuracy for participants who hear the accented speech presented with subtitles (49%) compared to participants who did not receive subtitles (45%). This extends previous findings that subtitles facilitate the comprehension of regionally accented English for second language learners of English [<xref rid="pone.0199358.ref040" ref-type="bibr">40</xref>]. In this experiment however, we find the same effect for foreign-accented speech in listeners' <italic>native</italic> language.</p>
                <p>We also find a marginally significant effect of delayed subtitles compared to the absence of subtitles, suggesting that accent adaptation can—at least in principle—benefit from labeling context that occurs <italic>after</italic> the critical speech input. In fact, we fail to find a significant difference between delayed and concurrent subtitles. This tentatively suggests little or no decay in relevant information over the duration of the word stimulus. Recall that we excluded items that did not show clear effects of concurrent subtitles. Although our exclusion criterion was based on data from another experiment, this approach should <italic>in</italic>flate estimates of the transcription accuracy for the Concurrent condition. If anything, there is thus a bias <italic>towards</italic> a difference higher performance in the Concurrent, compared to the Delayed, condition—biasing against the result we observe here.</p>
                <p>This finding is contrary to what would be expected if listeners immediately compress and discard lower-level information. The results of Experiment 1 thus replicate previous findings that listeners can maintain information past word boundaries (e.g., [<xref rid="pone.0199358.ref025" ref-type="bibr">25</xref>–<xref rid="pone.0199358.ref027" ref-type="bibr">27</xref>,<xref rid="pone.0199358.ref029" ref-type="bibr">29</xref>]).</p>
                <p>Unlike in previous work on uncertainty maintenance [<xref rid="pone.0199358.ref025" ref-type="bibr">25</xref>–<xref rid="pone.0199358.ref029" ref-type="bibr">29</xref>], the present experiment avoided several properties that would make it easier for participants to determine which aspects of the speech signal to maintain. First, unlike the previous studies which repeated the same one target word pair (e.g. 144 times [<xref rid="pone.0199358.ref028" ref-type="bibr">28</xref>] and 80 times [<xref rid="pone.0199358.ref029" ref-type="bibr">29</xref>]) with the similar preceding contexts (e.g. 24 times each [<xref rid="pone.0199358.ref028" ref-type="bibr">28</xref>] and 80 times [<xref rid="pone.0199358.ref029" ref-type="bibr">29</xref>]), the present experiment never repeated words and did not let participants use preceding context. This makes it less likely that the present results are driven by task-specific strategies uncommon in everyday speech processing. Second, participants had to either maintain information about the <italic>entire</italic> word on each trial, or—if the limits of the systems do not allow this—participants would have had to decide which aspect of the signal to maintain, without easily knowing which aspect would later be most informative.</p>
                <p>In the General Discussion, we summarize additional analyses that confirm this interpretation: the overall benefits of delayed subtitles during exposure on transcription accuracy during the test phase originate in benefits across many different types of phonemes. That is, participants did indeed not just attend to a few phones or types of phonological contrasts, providing additional validation for the subtitle paradigm.</p>
              </sec>
            </sec>
            <sec id="sec014">
              <title>Experiment 2</title>
              <p>In Experiment 1, we presented the delayed subtitles immediately after word offset. In order to investigate the limits of maintenance and how sensory decay affects this process, Experiment 2 introduces longer delays between the percept and right context. If rapid sensory decay strongly constrains maintenance, we should expect to see the subtitle facilitation quickly disappear as the amount of time participants need to maintain information increases.</p>
              <p>We collected data for four new subtitled conditions: a Concurrent and 0ms Delayed condition (both as in Experiment 1, but with some changes described below), as well as two additional Delayed conditions in which subtitles were displayed after word offset with a lag of 1500 ms or 6000 ms, respectively. These four conditions were compared against the Absent condition from Experiment 1.</p>
              <sec id="sec015" sec-type="materials|methods">
                <title>Methods</title>
                <sec id="sec016">
                  <title>Participants</title>
                  <p>With the aim to again recruit 60 successful participants for each of the four new between-participant conditions, 300 participants were recruited using Amazon's Mechanical Turk (<ext-link ext-link-type="uri" xlink:href="http://www.mturk.com/">www.mturk.com</ext-link>). Participants were randomly assigned to one of the four subtitled conditions. Recruitment criteria were identical to Experiment 1.</p>
                </sec>
                <sec id="sec017">
                  <title>Procedure</title>
                  <p>The paradigm and stimuli were the same as in Experiment 1, with one small change to the timing of the procedure. In the three Delayed conditions and the Concurrent condition in Experiment 2, the duration of the subtitles was shortened from 1500 ms to the duration of the speech input (which ranged from 379 ms to 848 ms, mean = 586, SD = 86; see <xref ref-type="fig" rid="pone.0199358.g003">Fig 3</xref>). Our decision to shorten the subtitle duration was motivated by additional pilot experiments conducted between Experiments 1 and 2, in which we found that shortening the duration of the subtitles actually increased performance, perhaps due to increased task engagement. In Experiment 1, we found no difference between the Concurrent and Delayed condition. By aiming to increase the effect size associated with subtitles, we hoped to also increase our ability to detect differences between the Concurrent and Delayed conditions. The ITI for the four subtitle conditions was set to 1000 ms.</p>
                  <fig id="pone.0199358.g003" orientation="portrait" position="float">
                    <object-id pub-id-type="doi">10.1371/journal.pone.0199358.g003</object-id>
                    <label>Fig 3</label>
                    <caption>
                      <title>The trial structure of the conditions in Experiment 2 (lengths not to scale).</title>
                      <p>The Absent condition was identical to that of Experiment 1. The analyses presented below compare the four subtitle conditions against the data from the Absent condition of Experiment 1.</p>
                    </caption>
                    <graphic xlink:href="pone.0199358.g003"/>
                  </fig>
                </sec>
                <sec id="sec018">
                  <title>Scoring and exclusions</title>
                  <p>Following the same scoring and exclusion criteria as in Experiment 1, 108 participants in total were excluded from Experiment 2 (<xref rid="pone.0199358.t002" ref-type="table">Table 2</xref>). As with Experiment 1, a majority of these were excluded due to the fact that they were not monolingual. After exclusions there were 62 in Absent condition (taken from Experiment 1), 52 participants in the Concurrent condition, 52 in the 0 ms Delayed, 54 in the 1500 ms Delayed, and 53 in the 6000 ms Delayed condition.</p>
                  <table-wrap id="pone.0199358.t002" orientation="portrait" position="float">
                    <object-id pub-id-type="doi">10.1371/journal.pone.0199358.t002</object-id>
                    <label>Table 2</label>
                    <caption>
                      <title>Participant exclusions in Experiment 2.</title>
                      <p>Some participants were excluded for multiple reasons.</p>
                    </caption>
                    <alternatives>
                      <graphic id="pone.0199358.t002g" xlink:href="pone.0199358.t002"/>
                      <table frame="hsides" rules="groups">
                        <colgroup span="1">
                          <col align="left" valign="middle" span="1"/>
                          <col align="left" valign="middle" span="1"/>
                          <col align="left" valign="middle" span="1"/>
                        </colgroup>
                        <thead>
                          <tr>
                            <th align="left" rowspan="1" colspan="1">Total participants:</th>
                            <th align="right" rowspan="1" colspan="1">300</th>
                            <th align="right" rowspan="1" colspan="1">100%</th>
                          </tr>
                          <tr>
                            <th align="left" style="border-bottom:thick" rowspan="1" colspan="1">Reason for exclusion</th>
                            <th align="right" style="border-bottom:thick" rowspan="1" colspan="1">n</th>
                            <th align="right" style="border-bottom:thick" rowspan="1" colspan="1">%</th>
                          </tr>
                        </thead>
                        <tbody>
                          <tr>
                            <td align="left" rowspan="1" colspan="1">Participant's family likely to include Spanish speakers</td>
                            <td align="right" rowspan="1" colspan="1">17</td>
                            <td align="right" rowspan="1" colspan="1">(6%)</td>
                          </tr>
                          <tr>
                            <td align="left" rowspan="1" colspan="1">Participant not monolingual</td>
                            <td align="right" rowspan="1" colspan="1">53</td>
                            <td align="right" rowspan="1" colspan="1">(18%)</td>
                          </tr>
                          <tr>
                            <td align="left" rowspan="1" colspan="1">Participant reported familiarity with strong foreign accents</td>
                            <td align="right" rowspan="1" colspan="1">8</td>
                            <td align="right" rowspan="1" colspan="1">(3%)</td>
                          </tr>
                          <tr>
                            <td align="left" rowspan="1" colspan="1">Used computer speakers or unknown audio quality</td>
                            <td align="right" rowspan="1" colspan="1">11</td>
                            <td align="right" rowspan="1" colspan="1">(4%)</td>
                          </tr>
                          <tr>
                            <td align="left" rowspan="1" colspan="1">Client-side computing error</td>
                            <td align="right" rowspan="1" colspan="1">3</td>
                            <td align="right" rowspan="1" colspan="1">(1%)</td>
                          </tr>
                          <tr>
                            <td align="left" rowspan="1" colspan="1">Failed &gt;4 catch trials (out of 12)</td>
                            <td align="right" rowspan="1" colspan="1">11</td>
                            <td align="right" rowspan="1" colspan="1">(4%)</td>
                          </tr>
                          <tr>
                            <td align="left" style="border-bottom:thick" rowspan="1" colspan="1">Outliers in transcription performance</td>
                            <td align="right" style="border-bottom:thick" rowspan="1" colspan="1">0</td>
                            <td align="right" style="border-bottom:thick" rowspan="1" colspan="1">(0%)</td>
                          </tr>
                          <tr>
                            <td align="left" rowspan="1" colspan="1">Participants remaining</td>
                            <td align="right" rowspan="1" colspan="1">176</td>
                            <td align="right" rowspan="1" colspan="1">(70%)</td>
                          </tr>
                        </tbody>
                      </table>
                    </alternatives>
                  </table-wrap>
                </sec>
              </sec>
              <sec id="sec019" sec-type="results">
                <title>Results</title>
                <p>We follow the same analysis and data visualization approach as in Experiment 1. This includes that we limit our analysis to the same items that we analyzed in Experiment 1. <xref ref-type="fig" rid="pone.0199358.g004">Fig 4</xref> shows the adjusted transcription accuracy for all conditions. We present three analyses to fully assess the effects of the different subtitle conditions.</p>
                <fig id="pone.0199358.g004" orientation="portrait" position="float">
                  <object-id pub-id-type="doi">10.1371/journal.pone.0199358.g004</object-id>
                  <label>Fig 4</label>
                  <caption>
                    <title>Transcription performance during test in Experiment 2.</title>
                    <p>The values plotted in this graph are adjusted to control for the effects of control variables (see text for details). Dots show individual participants. Error bars show 95% confidence intervals based on non-parametric bootstrap over by-subject means. Despite the general trend of longer subtitle delays conferring reduced subtitle facilitation, even subtitles delayed 6000 ms after the offset of the word significantly improve participants' transcription accuracy compared to the Absent condition.</p>
                  </caption>
                  <graphic xlink:href="pone.0199358.g004"/>
                </fig>
                <p>We first Helmert-coded the different subtitle conditions to compare Absent to everything else (following Experiment 1). Specifically, the contrasts were: 1) Absent = -4, all subtitled conditions = 1; 2) Absent = 0, 6000 ms Delayed = -3, all subtitled conditions with delays &gt;6000 ms = 1; 3) Absent = 0, 6000 ms Delayed = 0, 1500 ms Delayed = -2, all subtitled conditions with delays &gt;1500 ms = 1; 4) 0 ms Delayed = -1, Concurrent = 1, all others = 0. This analysis failed to converge with the full random effect structure, prompting us to remove the correlations among the random effects for the by-item slopes for condition. The comparison of all subtitle conditions against the Absent condition was significant, replicating the benefit of subtitles observed in Experiment 1 (<inline-formula id="pone.0199358.e007"><alternatives><graphic xlink:href="pone.0199358.e007.jpg" id="pone.0199358.e007g" mimetype="image" position="anchor" orientation="portrait"/><mml:math id="M7"><mml:mover accent="true"><mml:mrow><mml:mi>β</mml:mi></mml:mrow><mml:mo>^</mml:mo></mml:mover><mml:mo>=</mml:mo><mml:mn>0.058</mml:mn></mml:math></alternatives></inline-formula>, <italic>z</italic> = 3.22, <italic>p</italic> = 0.0013). The remaining Helmert contrasts comparing the subtitle conditions against each other found that participants in the 1500 ms Delayed condition correctly transcribed significantly fewer words, compared to the average accuracy across the 0 ms Delayed and Concurrent conditions (<inline-formula id="pone.0199358.e008"><alternatives><graphic xlink:href="pone.0199358.e008.jpg" id="pone.0199358.e008g" mimetype="image" position="anchor" orientation="portrait"/><mml:math id="M8"><mml:mover accent="true"><mml:mrow><mml:mi>β</mml:mi></mml:mrow><mml:mo>^</mml:mo></mml:mover><mml:mo>=</mml:mo><mml:mn>0.072</mml:mn></mml:math></alternatives></inline-formula>, <italic>z</italic> = 2.88, <italic>p</italic> = 0.037). All other comparisons were non-significant (<italic>p</italic>s &gt; 0.3).</p>
                <p>The effects of the two control variables numerically replicated those found in Experiment 1. However, this time the effect of accent familiarity did not reach significance (<inline-formula id="pone.0199358.e009"><alternatives><graphic xlink:href="pone.0199358.e009.jpg" id="pone.0199358.e009g" mimetype="image" position="anchor" orientation="portrait"/><mml:math id="M9"><mml:mover accent="true"><mml:mrow><mml:mi>β</mml:mi></mml:mrow><mml:mo>^</mml:mo></mml:mover><mml:mo>=</mml:mo><mml:mn>0.061</mml:mn></mml:math></alternatives></inline-formula>, <italic>z</italic> = 0.797, <italic>p</italic> &gt; 0.4), whereas the audio equipment reached marginal significance (<inline-formula id="pone.0199358.e010"><alternatives><graphic xlink:href="pone.0199358.e010.jpg" id="pone.0199358.e010g" mimetype="image" position="anchor" orientation="portrait"/><mml:math id="M10"><mml:mover accent="true"><mml:mrow><mml:mi>β</mml:mi></mml:mrow><mml:mo>^</mml:mo></mml:mover><mml:mo>=</mml:mo><mml:mn>0.067</mml:mn></mml:math></alternatives></inline-formula>, <italic>z</italic> = 1.862, <italic>p</italic> &lt; 0.063).</p>
                <p>Next, to test <italic>which</italic> subtitle conditions facilitate accent adaptation, we compared each subtitle condition against the Absent condition using simple contrast coding, as in Experiment 1. This analysis converged with the full random effects structure. There were no signs of multi-collinearity (all fixed effect correlations <italic>r</italic>s &lt; 0.50). We found highly significant differences for both Concurrent vs. Absent (<inline-formula id="pone.0199358.e011"><alternatives><graphic xlink:href="pone.0199358.e011.jpg" id="pone.0199358.e011g" mimetype="image" position="anchor" orientation="portrait"/><mml:math id="M11"><mml:mover accent="true"><mml:mrow><mml:mi>β</mml:mi></mml:mrow><mml:mo>^</mml:mo></mml:mover><mml:mo>=</mml:mo><mml:mn>0.358</mml:mn></mml:math></alternatives></inline-formula>, <italic>z</italic> = 3.10, <italic>p</italic> &lt; 0.002) and 0ms Delayed vs. Absent (<inline-formula id="pone.0199358.e012"><alternatives><graphic xlink:href="pone.0199358.e012.jpg" id="pone.0199358.e012g" mimetype="image" position="anchor" orientation="portrait"/><mml:math id="M12"><mml:mover accent="true"><mml:mrow><mml:mi>β</mml:mi></mml:mrow><mml:mo>^</mml:mo></mml:mover><mml:mo>=</mml:mo><mml:mn>0.413</mml:mn></mml:math></alternatives></inline-formula>, <italic>z</italic> = 3.35, <italic>p</italic> &lt; 0.001), with participants in both of the subtitled conditions achieving higher accuracy than those without subtitles. Numerically, the same trend—facilitation—was also observed for the two remaining conditions with delayed subtitles. However, this effect reached marginal significance only for the 6000ms Delayed condition (<inline-formula id="pone.0199358.e013"><alternatives><graphic xlink:href="pone.0199358.e013.jpg" id="pone.0199358.e013g" mimetype="image" position="anchor" orientation="portrait"/><mml:math id="M13"><mml:mover accent="true"><mml:mrow><mml:mi>β</mml:mi></mml:mrow><mml:mo>^</mml:mo></mml:mover><mml:mo>=</mml:mo><mml:mn>0.212</mml:mn></mml:math></alternatives></inline-formula>, <italic>z</italic> = 1.89, <italic>p</italic> &lt; 0.059), and did not reach significance in the 1500 ms Delayed condition (<inline-formula id="pone.0199358.e014"><alternatives><graphic xlink:href="pone.0199358.e014.jpg" id="pone.0199358.e014g" mimetype="image" position="anchor" orientation="portrait"/><mml:math id="M14"><mml:mover accent="true"><mml:mrow><mml:mi>β</mml:mi></mml:mrow><mml:mo>^</mml:mo></mml:mover><mml:mo>=</mml:mo><mml:mn>0.171</mml:mn></mml:math></alternatives></inline-formula>, <italic>z</italic> = 1.48, <italic>p</italic> &lt; 0.14).</p>
                <p>Finally, to assess whether accent adaptation decreased with increasing delay of the subtitles, we repeated the analysis using backward difference coding (also known as "slide contrast" coding, i.e., 6000 ms Delayed vs. Absent, 1500 ms Delayed vs. 6000 ms Delayed, 0 ms Delayed vs. 1500 ms Delayed, Concurrent vs. 0 ms Delayed). This analysis converged with the full random effects structure. There were no signs of multi-collinearity (all fixed effect correlations <italic>r</italic>s &lt; 0.51). The comparison of 6000 ms Delayed vs. Absent was marginally significant (<inline-formula id="pone.0199358.e015"><alternatives><graphic xlink:href="pone.0199358.e015.jpg" id="pone.0199358.e015g" mimetype="image" position="anchor" orientation="portrait"/><mml:math id="M15"><mml:mover accent="true"><mml:mrow><mml:mi>β</mml:mi></mml:mrow><mml:mo>^</mml:mo></mml:mover><mml:mo>=</mml:mo><mml:mn>0.212</mml:mn></mml:math></alternatives></inline-formula>, <italic>z</italic> = 1.89, <italic>p</italic> &lt; 0.059). The comparison of the 0 ms Delayed vs. 1500 ms Delayed condition was significant: with participants in the 0 ms Delayed condition had higher transcription accuracy (<inline-formula id="pone.0199358.e016"><alternatives><graphic xlink:href="pone.0199358.e016.jpg" id="pone.0199358.e016g" mimetype="image" position="anchor" orientation="portrait"/><mml:math id="M16"><mml:mover accent="true"><mml:mrow><mml:mi>β</mml:mi></mml:mrow><mml:mo>^</mml:mo></mml:mover><mml:mo>=</mml:mo><mml:mn>0.242</mml:mn></mml:math></alternatives></inline-formula>, <italic>z</italic> = 2.03, <italic>p</italic> = 0.042). All other comparisons were non-significant (<italic>p</italic>s &gt; 0.6).</p>
              </sec>
              <sec id="sec020" sec-type="conclusions">
                <title>Discussion</title>
                <p>In Experiment 2, we find further evidence that even when the right-context is delayed until after the offset of the percept, subtitles still facilitate accent adaptation. This suggests that participants are able to maintain information and use it for learning even past word boundaries. There also appear to be marginal improvements in adaptation even after a delay of six seconds between the offset of the percept and the right-context, far longer than previous proposals might have suggested.</p>
                <p>We also observe a trend wherein shorter delays have a tendency to improve transcription accuracy more (see <xref ref-type="fig" rid="pone.0199358.g004">Fig 4</xref>). There is a significant decrease in transcription accuracy when subtitles (right-context) are delayed 1.5 seconds. This is the first evidence in our paradigm suggesting the limits to such maintenance. Overall, this pattern suggests that while listeners <italic>can</italic> maintain information past word boundaries (tentatively, even after six second delays), longer sensory decay reduces how much relevant information can be maintained for sufficiently long to support accent adaptation.</p>
              </sec>
            </sec>
            <sec id="sec021">
              <title>General discussion</title>
              <p>Some views conceptualize language understanding as "compression", whereby listeners move from more detailed, lower-level representations to more abstract, higher-level ones. However, optimal inferences at a higher-level (e.g., parsing) can require subcategorical information from lower levels (e.g., phonetic properties, [<xref rid="pone.0199358.ref003" ref-type="bibr">3</xref>,<xref rid="pone.0199358.ref053" ref-type="bibr">53</xref>]). Implicit knowledge about subcategorical differences between talkers can also play a crucial role in robust speech perception [<xref rid="pone.0199358.ref054" ref-type="bibr">54</xref>]. How listeners navigate the trade-offs between compressing information and the potential benefits of maintaining information is an open question.</p>
              <p>According to the <italic>immediate and complete compression</italic> hypothesis, strong limitations of the cognitive system force us to compress the auditory signal as quickly as possible down to the invariant categories, discarding lower-level, subcategorical information. This view was motivated by early findings such as categorical perception, which quickly dichotomizes input varying on an acoustic continuum into two distinct categories [<xref rid="pone.0199358.ref004" ref-type="bibr">4</xref>]. Although often implicit, this view continues to be influential in research on language processing (for review, see [<xref rid="pone.0199358.ref001" ref-type="bibr">1</xref>]).</p>
              <p>An alternative view holds that at least <italic>some</italic> subcategorical information is maintained for longer periods of time, thereby allowing this information to be integrated with subsequent context to affect categorization of earlier percepts. As summarized in the introduction, a number of findings have lent support to this idea. First, auditory information can sometimes persist for relatively long (e.g., pitch for 10 seconds [<xref rid="pone.0199358.ref018" ref-type="bibr">18</xref>], vowel information for three seconds or more [<xref rid="pone.0199358.ref019" ref-type="bibr">19</xref>]). Second, listeners implicit knowledge of words seems to include at least some subcategorical information about talker identity (e.g., [<xref rid="pone.0199358.ref009" ref-type="bibr">9</xref>,<xref rid="pone.0199358.ref010" ref-type="bibr">10</xref>,<xref rid="pone.0199358.ref015" ref-type="bibr">15</xref>]). Third, a number of studies have found that later right-context information can change interpretation of previously encountered percepts in ways that suggest that subcategorical information has been maintained about the earlier percept [<xref rid="pone.0199358.ref030" ref-type="bibr">30</xref>,<xref rid="pone.0199358.ref031" ref-type="bibr">31</xref>,<xref rid="pone.0199358.ref035" ref-type="bibr">35</xref>]. These right-context effects are observed past word boundaries. Indeed, some recent work has found right-context effects up to 6–8 syllables [<xref rid="pone.0199358.ref025" ref-type="bibr">25</xref>–<xref rid="pone.0199358.ref027" ref-type="bibr">27</xref>,<xref rid="pone.0199358.ref029" ref-type="bibr">29</xref>], far beyond what would be expected under immediate and complete compression.</p>
              <p>The present study provides further support for the latter view. We find that subtitles during exposure to speech of an unfamiliar accented talker facilitate adaptation, even when the subtitles are delayed. Next, we elaborate on three specific contributions our paradigm makes to research on information maintenance and accent adaptation. We also discuss potential caveats to our interpretation of the results. Following that, we raise considerations for future work within the subtitle paradigm or similar paradigms. We close by discussing the specificity of the information that listeners seem to maintain about the speech input.</p>
              <sec id="sec022">
                <title>Contributions of the present study</title>
                <p>The first two contributions we discuss pertain to the literature on information maintenance, and both contributions originate in the use of the subtitle paradigm. This paradigm has previously been employed in studies on second language processing [<xref rid="pone.0199358.ref040" ref-type="bibr">40</xref>], modulation of early auditory processing [<xref rid="pone.0199358.ref042" ref-type="bibr">42</xref>], and phonetic adaptation [<xref rid="pone.0199358.ref043" ref-type="bibr">43</xref>]. Here, we extended it to the study of information maintenance (see also [<xref rid="pone.0199358.ref042" ref-type="bibr">42</xref>], discussed in more detail below).</p>
                <p>The first contribution of the present work is that it allows us to avoid some of the methodological concerns that have been raised about previous work on information maintenance (for a discussion, see also [<xref rid="pone.0199358.ref027" ref-type="bibr">27</xref>]). Most of the paradigms employed in previous work allow participants to limit their attention to specific sounds (e.g., onset plosives [<xref rid="pone.0199358.ref028" ref-type="bibr">28</xref>]) and sometimes even specific locations in the signal (e.g., the onset of the second word [<xref rid="pone.0199358.ref029" ref-type="bibr">29</xref>]). These paradigms also often involve massive repetition of similar stimuli. The subtitle paradigm employed here avoids such repetitions (see also the paradigm in [<xref rid="pone.0199358.ref026" ref-type="bibr">26</xref>]). Additionally, participants cannot easily determine what subcategorical information will be relevant in adapting to an accent. The results of Experiments 1 and 2 thus suggest that listeners can maintain information even when it is unclear which aspects of the signal should be maintained, and where in the signal those aspects might appear. This interpretation of our results is supported by additional analyses of Experiments 1 and 2, which we summarize here briefly (for details, see <xref ref-type="supplementary-material" rid="pone.0199358.s001">S1 Appendix</xref>).</p>
                <p><italic>A priori</italic>, one property of our design makes it unlikely that participants attended to only a few phones: each phone occurred only a few times during exposure and test. <xref ref-type="fig" rid="pone.0199358.g005">Fig 5</xref> shows this for phones. This would make it difficult to achieve the observed benefit of subtitles on transcription accuracy by attending to only a small subset of all sound categories during exposure.</p>
                <fig id="pone.0199358.g005" orientation="portrait" position="float">
                  <object-id pub-id-type="doi">10.1371/journal.pone.0199358.g005</object-id>
                  <label>Fig 5</label>
                  <caption>
                    <title>The number of occurrences of each position-specific phone during both the exposure and test phases of Experiments 1 and 2.</title>
                    <p>The dot size indicates the number of phones for that particular set of values. Panels show the three stimuli lists, of which participants were randomly assigned to one. Select phones are labeled in IPA, with # indicating word boundaries (see <xref ref-type="fig" rid="pone.0199358.g006">Fig 6</xref> for the subtitle benefit for the same phones).</p>
                  </caption>
                  <graphic xlink:href="pone.0199358.g005"/>
                </fig>
                <p>Indeed, the benefit of subtitles during exposure is distributed across a large number of phones in our experiment. This is evidenced in <xref ref-type="fig" rid="pone.0199358.g006">Fig 6</xref>, where almost all phones show positive benefits of concurrent subtitles. This pattern is expected if participants maintain information about multiple types of phones, validating the motivation for our paradigm.</p>
                <fig id="pone.0199358.g006" orientation="portrait" position="float">
                  <object-id pub-id-type="doi">10.1371/journal.pone.0199358.g006</object-id>
                  <label>Fig 6</label>
                  <caption>
                    <title>Benefit concurrent (y-axis) and delayed (x-axis) subtitles in exposure for each position-dependent phone during test.</title>
                    <p>The benefit shown here is the difference in transcription accuracy for words containing each phone in the subtitled conditions compared to the Absent condition, in empirical log-odds. Point size indicates the number of instances of each phone during exposure and test. Select phones are labeled with # indicating word boundaries. The realization of word-final /d/, for example, is known to be strongly affected by a Spanish accent [<xref rid="pone.0199358.ref055" ref-type="bibr">55</xref>,<xref rid="pone.0199358.ref056" ref-type="bibr">56</xref>], whereas /ɛ/ is not. To convey a sense of the variability in the data, the 95% confidence intervals for three of the more outlying phones are provided.</p>
                  </caption>
                  <graphic xlink:href="pone.0199358.g006"/>
                </fig>
                <p>There is one more property of our data that further supports this interpretation. Previous work suggests that listeners adapt to multiple accented sound categories in parallel in circumstances where contextual information is present concurrently [<xref rid="pone.0199358.ref057" ref-type="bibr">57</xref>]. Our results suggest that this is likely to be equally true for when relevant contextual information occurs after the accented sound (as in our delayed subtitle conditions): we find that the phone-specific benefit of delayed subtitle condition closely resembles the benefit of concurrent subtitles (<xref ref-type="fig" rid="pone.0199358.g006">Fig 6</xref>). This suggests that listeners use the same information in both the concurrent and delayed conditions. We take these findings to argue that listeners do not adopt special strategies for delayed subtitles (for corroborating evidence that information maintenance is indeed a default behavior in spoken language understanding, see [<xref rid="pone.0199358.ref027" ref-type="bibr">27</xref>]). Rather, it seems that participants were able to maintain information about multiple sound categories in parallel—despite the high dimensionality of accented speech and as intended by our choice of paradigm—and that this information maintenance facilitated accent adaptation. The subtitle paradigm thus brings us one step closer to understanding the role of subcategorical information maintenance in everyday language processing.</p>
                <p>A second contribution of the subtitle paradigm is that it makes it easier for the researcher to control the amount of time between the percept and right-context. This makes it possible to investigate how intervening time affects information maintenance, as we began to do in Experiment 2. In that experiment we found that longer delays resulted in reduced subtitle benefit, suggesting that information maintenance is not without limits, at least not with regard to the information that is required for accent adaptation. A related finding is presented by Sohoglu et al. [<xref rid="pone.0199358.ref042" ref-type="bibr">42</xref>]. Sohoglu and colleagues used fine adjustments to the stimulus-onset asynchronies between degraded speech and subtitles to pinpoint the delay at which the "pop-out effect" of context declined. They found that the perceived clarity of the degraded speech began to decline when subtitles were delayed by ~120 milliseconds past word onset, although significant benefits of subtitles were still observed at delays as large as ~1,600 milliseconds (the longest delay tested).</p>
                <p>While the existence of limits is in line with our findings, the rapid decay of the subtitle benefit (on perceptual clarity) seen in [<xref rid="pone.0199358.ref042" ref-type="bibr">42</xref>] seems to be in conflict with the present study. One possible explanation for this seeming conflict lies in the different types of information that subjects might need to maintain for the two different tasks. Sohoglu and colleagues suggest that the effect they examine—whereby information from multiple modalities (i.e., lexical information from vision and auditory information from the degraded speech) is bound together into a single "enhanced" percept—requires <italic>sensory</italic> memory of the audio to be present when the contextual information is integrated (in line with similar results for the integration of other types of visual information, such as video of talkers' lips [<xref rid="pone.0199358.ref058" ref-type="bibr">58</xref>]).</p>
                <p>In the current study, we are not measuring the clarity with which participants perceive speech input: we measure how much right context can contribute to whatever information is necessary for <italic>accent adaptation</italic>. It is possible, if not plausible, that such adaptation operates over <italic>phonetic</italic>, rather than <italic>perceptual</italic>, representations, thus requiring maintenance of less fine-grained information (compared to the study by Sohoglu and colleagues). Previous work has found that listeners can maintain phonetic information about isolated segments (e.g., an isolated vowel) for at least 1–3 seconds, whereas memory of pre-phonetic perceptual information seems to decay more quickly (~200-300msecs, [<xref rid="pone.0199358.ref024" ref-type="bibr">24</xref>]). If this asymmetry in ability to maintain phonetic, compared to perceptual, information carries over to whole word recognition, this explains why we observe subtitle effects for delays far longer than those in [<xref rid="pone.0199358.ref042" ref-type="bibr">42</xref>].</p>
                <p>The third contribution of the present work is to research on accent adaptation. We find that subtitles facilitate native language accent adaptation with far less exposure than in used in previous work on second language accent learning (approximately 45 seconds of speech input, compared to 30 minutes in [<xref rid="pone.0199358.ref040" ref-type="bibr">40</xref>]). This suggests that a small amount of exposure material is sufficient to investigate maintenance (see also [<xref rid="pone.0199358.ref038" ref-type="bibr">38</xref>,<xref rid="pone.0199358.ref050" ref-type="bibr">50</xref>]). Next, we discuss considerations for future research on information maintenance.</p>
              </sec>
              <sec id="sec023">
                <title>Considerations for future studies within the subtitle paradigm</title>
                <p>The effects of subtitles observed in Experiments 1 and 2 are relatively small. This can become a problem for future research within this paradigm, especially research on the limits of information maintenance: as the benefit of delayed subtitles begins to decline, the predicted effects will fall between a lower bound expected for the absence of subtitles and an upper bound expected for concurrent subtitles. As the difference between these bounds was approximately 4% or less in Experiments 1 and 2, it could be difficult to reliably assess the ranking of different exposure conditions between these upper and lower bounds.</p>
                <p>This is, however, not an inherent limitation of the subtitle paradigm. Rather, the small effect sizes might result from our choice of stimuli. By carefully choosing the materials for exposure and test so as to maximize the expected benefit of concurrent subtitles, it should be possible to address this shortcoming of the present study. For example, whereas English has both /ɪ/ (as in “sip”) and /i/ (as in “seep”), Spanish has only one corresponding vowel. In Spanish-accented English, the two English vowels often are pronounced in similar, if not identical, ways [<xref rid="pone.0199358.ref059" ref-type="bibr">59</xref>]. Even perfect adaptation to Spanish-accented English then would only guarantee a 50% chance of differentiating "sip" vs. "seep." Our experiments did not avoid stimuli with such phonological properties (cf. <xref ref-type="fig" rid="pone.0199358.g005">Fig 5</xref>). This is likely to have contributed to the relatively small effect of subtitles (compared to contrasts such as voicing in Spanish-accented plosives, which is separable has a one-to-one mapping), as the effectiveness of subtitles depends on the effectiveness of adaptation with regard to the test items. To avoid this issue, future work would benefit from employing items for which larger benefits of adaptation are expected. This in turn might require the use of foreign, dialectal, or regional accented English that have one-to-one, rather than many-to-one category mappings between native and accented pronunciations.</p>
                <p>Future studies could also manipulate the amount of speech (or other auditory) material—rather than time—intervening between the percept and the subtitle. This would make it possible to compare to effect of time and the effect of intervening speech on the maintenance of subcategorical speech information. Consider, for example that some previous studies have found effects of right-context for even six to eight syllables past the word boundary [<xref rid="pone.0199358.ref025" ref-type="bibr">25</xref>–<xref rid="pone.0199358.ref027" ref-type="bibr">27</xref>] (but see [<xref rid="pone.0199358.ref028" ref-type="bibr">28</xref>]), while other studies suggest that auditory sensory memory decays relatively rapidly (for a review, see [<xref rid="pone.0199358.ref024" ref-type="bibr">24</xref>]). Understanding how these pieces of evidence relate to each other is important for developing theories of how linguistic information processing: it is possible, for example, that information maintenance is limited more by the amount or type of information that is being maintained than by the time between percept and right-context.</p>
              </sec>
              <sec id="sec024">
                <title>What information is being maintained?</title>
                <p>A big open question for research on information maintenance pertains to the specificity, or type, of the information listeners maintain. There are at least two qualitatively different hypotheses. One possibility is that listeners maintain information that is at or below the level of phonetic information, which we take to be the default assumption of the field. The second hypothesis is that listeners maintain only gradient <italic>uncertainty</italic> about phonological category labels, rather than specific phonetic information. The present experiment was not designed to deliver a decisive answer to this question. It does, however, favor one of the two hypotheses. We elaborate on the two hypotheses and their plausibility given existing evidence—also in an effort to simulate further research on this question.</p>
                <p>Consider, for example, the type of paradigm employed by [<xref rid="pone.0199358.ref003" ref-type="bibr">3</xref>,<xref rid="pone.0199358.ref028" ref-type="bibr">28</xref>]. Participants heard sentences like "There's a (d/t)ent in the fender," where (d/t) was artificially ambiguous between /d/ and /t/. Under the first hypothesis, listeners maintain subcategorical phonetic information (e.g., VOT information about the ambiguous /t/-/d/ contrast) and then upon receiving right-context (e.g., "fender"), integrate these two sources of information when reporting what they heard (see <xref ref-type="fig" rid="pone.0199358.g007">Fig 7A</xref>). But these results are also compatible with the competing hypothesis that listeners maintain only uncertainties about the intended category of the phonemes. Under this hypothesis, after encountering the ambiguous (d/t), a participant might only maintain the information that they were 80% sure it was a /d/ and 20% sure it was a /t/, and after getting the context of "fender", merely combine this uncertainty with the right-context when reporting what they heard (see <xref ref-type="fig" rid="pone.0199358.g007">Fig 7B</xref>). This evidential ambiguity is not unique to the paradigm employed by Connine and colleagues: in fact, almost all existing evidence is compatible with either view. Indeed, the theoretical distinction between phonetic and uncertainty maintenance seems to have been raised only recently [<xref rid="pone.0199358.ref025" ref-type="bibr">25</xref>,<xref rid="pone.0199358.ref027" ref-type="bibr">27</xref>,<xref rid="pone.0199358.ref026" ref-type="bibr">26</xref>].</p>
                <fig id="pone.0199358.g007" orientation="portrait" position="float">
                  <object-id pub-id-type="doi">10.1371/journal.pone.0199358.g007</object-id>
                  <label>Fig 7</label>
                  <caption>
                    <title>Illustration of two accounts of right-context effects.</title>
                    <p>(a) <italic>Phonetic maintenance</italic>: Listeners maintain some level of subcategorical phonetic information about a phone and integrate this information with later contextual information. (b) <italic>Uncertainty maintenance</italic>: Listeners only maintain the barest amount of subcategorical information to integrate with later context: their relatively certainty about the possible categories. Evidence based on offline categorization tasks employed in previous work is compatible is compatible with either account.</p>
                  </caption>
                  <graphic xlink:href="pone.0199358.g007"/>
                </fig>
                <p>There are a number of studies that speak to the question at hand (although they were not intended to address it). One set of evidence comes from studies on the acquisition of non-native phonemic contrasts during second language (L2) learning. In order to learn non-native phonemic contrasts, listeners must learn the specific ways in which novel phonetic cue combinations relate to L2 phonological categories. L2 contrast-learning studies often train participants to do this by asking them to categorize words or syllables containing the non-native contrast and then giving them feedback on their response. Although many studies present the delayed feedback simultaneously with a repetition of the original audio [<xref rid="pone.0199358.ref060" ref-type="bibr">60</xref>,<xref rid="pone.0199358.ref061" ref-type="bibr">61</xref>], some studies have provided feedback without repeating the audio input. In those studies, the feedback then has a similar right-context function as delayed subtitles in the present study.</p>
                <p>For example, in McCandliss et al. [<xref rid="pone.0199358.ref062" ref-type="bibr">62</xref>] native Japanese listeners learned the English /r/-/l/ contrast (which is not present in Japanese) by repeatedly categorizing instances of "rock" vs. "lock." In order to improve, participants must learn to use specific novel phonetic cues to differentiate the unfamiliar phonemic categories. Participants who were given the correct label after each categorization learned to accurately categorize the novel categories more quickly, compared to participants who received no feedback [<xref rid="pone.0199358.ref062" ref-type="bibr">62</xref>]. Under the standard assumption that accuracy improvements in this task reflect learning of novel phonetic cue dimensions (and their relative weighting for categorization), this provides evidence that listeners can maintain <italic>phonetic</italic> information: in order for learning of phonetic cues to benefit from the delayed feedback, it is necessary that <italic>some</italic> phonetic information is maintained until the feedback becomes available.</p>
                <p>Phonetic maintenance also provides a natural explanation for own results, following similar logic (see also <xref ref-type="fig" rid="pone.0199358.g008">Fig 8</xref>). If listeners can maintain phonetic information until the right-context of the subtitles in the Delayed conditions, they would able to pair the phonetic information with the intended phonological category labels—essentially facilitating learning by access to a teaching signal.</p>
                <fig id="pone.0199358.g008" orientation="portrait" position="float">
                  <object-id pub-id-type="doi">10.1371/journal.pone.0199358.g008</object-id>
                  <label>Fig 8</label>
                  <caption>
                    <title>Illustration of how right-contexts might facilitate accent adaptation.</title>
                    <p>(a) When a listener first hears an ambiguous /d/ in "dent," there are multiple likely inferences about the phoneme category. (b) But after hearing the right-context of "fender," /d/ becomes the most likely category for the sound. (c) If listeners learn to connect elements of the percept of the sound to the phoneme category, it would suggest that they maintained some amount of subcategorical information about the percept until they received the right-context.</p>
                  </caption>
                  <graphic xlink:href="pone.0199358.g008"/>
                </fig>
              </sec>
            </sec>
            <sec sec-type="conclusions" id="sec025">
              <title>Conclusions</title>
              <p>The idea of quick, if not immediate, compression continues to dominate many researchers' understanding of language processing. There is now a mounting body of work on right-context effects that suggests that this intuition needs to be revisited carefully [<xref rid="pone.0199358.ref003" ref-type="bibr">3</xref>,<xref rid="pone.0199358.ref025" ref-type="bibr">25</xref>–<xref rid="pone.0199358.ref027" ref-type="bibr">27</xref>,<xref rid="pone.0199358.ref029" ref-type="bibr">29</xref>,<xref rid="pone.0199358.ref035" ref-type="bibr">35</xref>] (for a review of earlier works, see [<xref rid="pone.0199358.ref032" ref-type="bibr">32</xref>]). At the same time, subcategorical maintenance must have limits. As it stands, we know relatively little about the nature of these limits and, in particular, how they pertain to everyday language understanding.</p>
              <p>Together with the evidence from studies such as [<xref rid="pone.0199358.ref062" ref-type="bibr">62</xref>], the present work extends previous arguments for <italic>why</italic> listeners might want to maintain phonetic information: doing so can facilitate learning and adaptation to different talkers and accents [<xref rid="pone.0199358.ref031" ref-type="bibr">31</xref>,<xref rid="pone.0199358.ref032" ref-type="bibr">32</xref>,<xref rid="pone.0199358.ref033" ref-type="bibr">33</xref>] (and others).</p>
            </sec>
            <sec sec-type="supplementary-material" id="sec026">
              <title>Supporting information</title>
              <supplementary-material content-type="local-data" id="pone.0199358.s001">
                <label>S1 Appendix</label>
                <caption>
                  <title>Analyzing subtitle benefits and strategies.</title>
                  <p>(DOCX)</p>
                </caption>
                <media xlink:href="pone.0199358.s001.docx">
                  <caption>
                    <p>Click here for additional data file.</p>
                  </caption>
                </media>
              </supplementary-material>
              <supplementary-material content-type="local-data" id="pone.0199358.s002">
                <label>S2 Appendix</label>
                <caption>
                  <title>Experimental controls.</title>
                  <p>(DOCX)</p>
                </caption>
                <media xlink:href="pone.0199358.s002.docx">
                  <caption>
                    <p>Click here for additional data file.</p>
                  </caption>
                </media>
              </supplementary-material>
              <supplementary-material content-type="local-data" id="pone.0199358.s003">
                <label>S1 Questionnaire</label>
                <caption>
                  <title>Post-test survey questionnaire.</title>
                  <p>(DOCX)</p>
                </caption>
                <media xlink:href="pone.0199358.s003.docx">
                  <caption>
                    <p>Click here for additional data file.</p>
                  </caption>
                </media>
              </supplementary-material>
            </sec>
          </body>
          <back>
            <ack>
              <p>We would like to thank our editor and anonymous reviewers for their feedback on our manuscript. In particular, we thank Reviewer 1, whose feedback was especially helpful.</p>
            </ack>
            <ref-list>
              <title>References</title>
              <ref id="pone.0199358.ref001">
                <label>1</label>
                <mixed-citation publication-type="journal"><name><surname>Christiansen</surname><given-names>MH</given-names></name>, <name><surname>Chater</surname><given-names>N</given-names></name>. <article-title>The Now-or-Never bottleneck: A fundamental constraint on language</article-title>. <year>2016</year>; <fpage>1</fpage>–<lpage>72</lpage>. <pub-id pub-id-type="doi">10.1017/S0140525X1500031X</pub-id>
<?supplied-pmid 25869618?><pub-id pub-id-type="pmid">25869618</pub-id></mixed-citation>
              </ref>
              <ref id="pone.0199358.ref002">
                <label>2</label>
                <mixed-citation publication-type="journal"><name><surname>Haber</surname><given-names>RN</given-names></name>. <article-title>The icon is finally dead</article-title>. <source>Behav Brain Sci</source>. Cambridge Univ Press; <year>1983</year>;<volume>6</volume>: <fpage>43</fpage>–<lpage>54</lpage>.</mixed-citation>
              </ref>
              <ref id="pone.0199358.ref003">
                <label>3</label>
                <mixed-citation publication-type="journal"><name><surname>Bicknell</surname><given-names>K</given-names></name>, <name><surname>Jaeger</surname><given-names>TF</given-names></name>, <name><surname>Tanenhaus</surname><given-names>MK</given-names></name>. <article-title>Now or … later: Perceptual data is not immediately forgotten during language processing</article-title>. <source>Behav Brain Sci</source>. <year>2016</year>;<volume>39</volume>: <fpage>23</fpage>–<lpage>24</lpage>.</mixed-citation>
              </ref>
              <ref id="pone.0199358.ref004">
                <label>4</label>
                <mixed-citation publication-type="journal"><name><surname>Liberman</surname><given-names>AM</given-names></name>, <name><surname>Harris</surname><given-names>KS</given-names></name>, <name><surname>Hoffman</surname><given-names>HS</given-names></name>, <name><surname>Griffith</surname><given-names>BC</given-names></name>. <article-title>The discrimination of speech sounds within and across phoneme boundaries</article-title>. <source>J Exp Psychol</source>. American Psychological Association; <year>1957</year>;<volume>54</volume>: <fpage>358</fpage><?supplied-pmid 13481283?><pub-id pub-id-type="pmid">13481283</pub-id></mixed-citation>
              </ref>
              <ref id="pone.0199358.ref005">
                <label>5</label>
                <mixed-citation publication-type="journal"><name><surname>Sperling</surname><given-names>G</given-names></name>. <article-title>The information available in brief visual presentations</article-title>. <source>Psychol Monogr Gen Appl</source>. American Psychological Association; <year>1960</year>;<volume>74</volume>: <fpage>1</fpage>.</mixed-citation>
              </ref>
              <ref id="pone.0199358.ref006">
                <label>6</label>
                <mixed-citation publication-type="journal"><name><surname>Cherry</surname><given-names>EC</given-names></name>. <article-title>Some experiments on the recognition of speech, with one and with two ears</article-title>. <source>J Acoust Soc Am</source>. Acoustical Society of America; <year>1953</year>;<volume>25</volume>: <fpage>975</fpage>–<lpage>979</lpage>.</mixed-citation>
              </ref>
              <ref id="pone.0199358.ref007">
                <label>7</label>
                <mixed-citation publication-type="journal"><name><surname>Bradlow</surname><given-names>AR</given-names></name>, <name><surname>Nygaard</surname><given-names>LC</given-names></name>, <name><surname>Pisoni</surname><given-names>DB</given-names></name>. <article-title>Effects of talker, rate, and amplitude variation on recognition memory for spoken words</article-title>. <source>Percept Psychophys</source>. Springer; <year>1999</year>;<volume>61</volume>: <fpage>206</fpage>–<lpage>219</lpage>. <?supplied-pmid 10089756?><pub-id pub-id-type="pmid">10089756</pub-id></mixed-citation>
              </ref>
              <ref id="pone.0199358.ref008">
                <label>8</label>
                <mixed-citation publication-type="journal"><name><surname>Craik</surname><given-names>FIM</given-names></name>, <name><surname>Kirsner</surname><given-names>K</given-names></name>. <article-title>The effect of speaker’s voice on word recognition</article-title>. <source>Q J Exp Psychol</source>. Taylor &amp; Francis; <year>1974</year>;<volume>26</volume>: <fpage>274</fpage>–<lpage>284</lpage>.</mixed-citation>
              </ref>
              <ref id="pone.0199358.ref009">
                <label>9</label>
                <mixed-citation publication-type="journal"><name><surname>Goldinger</surname><given-names>SD</given-names></name>. <article-title>Words and voices: episodic traces in spoken word identification and recognition memory</article-title>. <source>J Exp Psychol Learn Mem Cogn</source>. American Psychological Association; <year>1996</year>;<volume>22</volume>: <fpage>1166</fpage><?supplied-pmid 8926483?><pub-id pub-id-type="pmid">8926483</pub-id></mixed-citation>
              </ref>
              <ref id="pone.0199358.ref010">
                <label>10</label>
                <mixed-citation publication-type="journal"><name><surname>Goldinger</surname><given-names>SD</given-names></name>. <article-title>Echoes of echoes? An episodic theory of lexical access</article-title>. <source>Psychol Rev</source>. <year>1998</year>;<volume>105</volume>: <fpage>251</fpage>–<lpage>279</lpage>. <?supplied-pmid 9577239?><pub-id pub-id-type="pmid">9577239</pub-id></mixed-citation>
              </ref>
              <ref id="pone.0199358.ref011">
                <label>11</label>
                <mixed-citation publication-type="journal"><name><surname>Creel</surname><given-names>SC</given-names></name>, <name><surname>Aslin</surname><given-names>RN</given-names></name>, <name><surname>Tanenhaus</surname><given-names>MK</given-names></name>. <article-title>Heeding the voice of experience: The role of talker variation in lexical access</article-title>. <source>Cognition</source>. <year>2008</year>;<volume>106</volume>: <fpage>633</fpage>–<lpage>664</lpage>. <pub-id pub-id-type="doi">10.1016/j.cognition.2007.03.013</pub-id>
<?supplied-pmid 17507006?><pub-id pub-id-type="pmid">17507006</pub-id></mixed-citation>
              </ref>
              <ref id="pone.0199358.ref012">
                <label>12</label>
                <mixed-citation publication-type="journal"><name><surname>Hawkins</surname><given-names>S</given-names></name>. <article-title>Roles and representations of systematic fine phonetic detail in speech understanding</article-title>. <source>J Phon</source>. Elsevier; <year>2003</year>;<volume>31</volume>: <fpage>373</fpage>–<lpage>405</lpage>.</mixed-citation>
              </ref>
              <ref id="pone.0199358.ref013">
                <label>13</label>
                <mixed-citation publication-type="book"><name><surname>Pierrehumbert</surname><given-names>J</given-names></name>. <chapter-title>Word-specific phonetics</chapter-title> In: <name><surname>Gussenhoven</surname><given-names>C</given-names></name>, <name><surname>Warner</surname><given-names>N</given-names></name>, editors. <source>Laboratory Phonology VII</source>. <publisher-loc>Berlin</publisher-loc>: <publisher-name>Mouton de Gruyter</publisher-name>; <year>2002</year> pp. <fpage>101</fpage>–<lpage>139</lpage>.</mixed-citation>
              </ref>
              <ref id="pone.0199358.ref014">
                <label>14</label>
                <mixed-citation publication-type="book"><name><surname>Strand</surname><given-names>EA</given-names></name>, <name><surname>Johnson</surname><given-names>K</given-names></name>. <chapter-title>Gradient and Visual Speaker Normalization in the Perception of Fricatives</chapter-title> In: <name><surname>Gibson</surname><given-names>D</given-names></name>, editor. <source>Natural language processing and speech technology: Results of the 3rd konvens conference, bielfelt</source>. <publisher-loc>Berlin</publisher-loc>: <publisher-name>Mouton de Gruyter</publisher-name>; <year>1996</year> pp. <fpage>14</fpage>–<lpage>26</lpage>.</mixed-citation>
              </ref>
              <ref id="pone.0199358.ref015">
                <label>15</label>
                <mixed-citation publication-type="journal"><name><surname>Walker</surname><given-names>A</given-names></name>, <name><surname>Hay</surname><given-names>J</given-names></name>. <article-title>Congruence between “word age” and “voice age” facilitates lexical access</article-title>. <source>Laboratory Phonology</source>. <year>2011</year> pp. <fpage>219</fpage>–<lpage>237</lpage>. <pub-id pub-id-type="doi">10.1515/labphon.2011.007</pub-id></mixed-citation>
              </ref>
              <ref id="pone.0199358.ref016">
                <label>16</label>
                <mixed-citation publication-type="journal"><name><surname>Foulkes</surname><given-names>P</given-names></name>, <name><surname>Hay</surname><given-names>J</given-names></name>. <article-title>The Emergence of Sociophonetic Structure</article-title>. <year>2015</year>; <fpage>292</fpage>–<lpage>313</lpage>.</mixed-citation>
              </ref>
              <ref id="pone.0199358.ref017">
                <label>17</label>
                <mixed-citation publication-type="book"><name><surname>Weatherholtz</surname><given-names>K</given-names></name>, <name><surname>Jaeger</surname><given-names>TF</given-names></name>. <chapter-title>Speech Perception and Generalization Across Talkers and Accents</chapter-title><source>Oxford Research Encyclopedia of Linguistics</source>. <publisher-name>Oxford University Press</publisher-name>; <year>2016</year><pub-id pub-id-type="doi">10.1093/acrefore/9780199384655.013.95</pub-id></mixed-citation>
              </ref>
              <ref id="pone.0199358.ref018">
                <label>18</label>
                <mixed-citation publication-type="journal"><name><surname>Böttcher-Gandor</surname><given-names>C</given-names></name>, <name><surname>Ullsperger</surname><given-names>P</given-names></name>. <article-title>Mismatch Negativity in Event-Related Potentials to Auditory Stimuli as a Function of Varying Interstimulus Interval</article-title>. <source>Psychophysiology</source>. Wiley Online Library; <year>1992</year>;<volume>29</volume>: <fpage>546</fpage>–<lpage>550</lpage>. <?supplied-pmid 1410183?><pub-id pub-id-type="pmid">1410183</pub-id></mixed-citation>
              </ref>
              <ref id="pone.0199358.ref019">
                <label>19</label>
                <mixed-citation publication-type="journal"><name><surname>Crowder</surname><given-names>RG</given-names></name>. <article-title>Decay of auditory memory in vowel discrimination</article-title>. <source>J Exp Psychol Learn Mem Cogn</source>. <year>1982</year>;<volume>8</volume>: <fpage>153</fpage>–<lpage>162</lpage>. <pub-id pub-id-type="doi">10.1037/0278-7393.8.2.153</pub-id>
<?supplied-pmid 6210749?><pub-id pub-id-type="pmid">6210749</pub-id></mixed-citation>
              </ref>
              <ref id="pone.0199358.ref020">
                <label>20</label>
                <mixed-citation publication-type="book"><name><surname>Studdert-Kennedy</surname><given-names>M</given-names></name>. <chapter-title>Some developments in research on language behavior</chapter-title> In: <name><surname>Smelser</surname><given-names>NJ</given-names></name>, <name><surname>Gerstein</surname><given-names>DR</given-names></name>, editors. <source>Behavioral and social science: Fifty years of discovery: In commemoration of the fiftieth anniversary of the “Ogburn Report,” recent social trends in the United States</source>. <publisher-name>National Academies Press</publisher-name>; <year>1986</year> pp. <fpage>208</fpage>–<lpage>248</lpage>.</mixed-citation>
              </ref>
              <ref id="pone.0199358.ref021">
                <label>21</label>
                <mixed-citation publication-type="journal"><name><surname>Zimmermann</surname><given-names>JF</given-names></name>, <name><surname>Moscovitch</surname><given-names>M</given-names></name>, <name><surname>Alain</surname><given-names>C</given-names></name>. <article-title>Attending to auditory memory</article-title>. <source>Brain Res</source>. Elsevier; <year>2016</year>;<volume>1640</volume>: <fpage>208</fpage>–<lpage>221</lpage>. <pub-id pub-id-type="doi">10.1016/j.brainres.2015.11.032</pub-id>
<?supplied-pmid 26638836?><pub-id pub-id-type="pmid">26638836</pub-id></mixed-citation>
              </ref>
              <ref id="pone.0199358.ref022">
                <label>22</label>
                <mixed-citation publication-type="journal"><name><surname>Backer</surname><given-names>KC</given-names></name>, <name><surname>Alain</surname><given-names>C</given-names></name>. <article-title>Orienting attention to sound object representations attenuates change deafness</article-title>. <source>J Exp Psychol Hum Percept Perform</source>. American Psychological Association; <year>2012</year>;<volume>38</volume>: <fpage>1554</fpage><pub-id pub-id-type="doi">10.1037/a0027858</pub-id><?supplied-pmid 22506788?><pub-id pub-id-type="pmid">22506788</pub-id></mixed-citation>
              </ref>
              <ref id="pone.0199358.ref023">
                <label>23</label>
                <mixed-citation publication-type="journal"><name><surname>Pisoni</surname><given-names>DB</given-names></name>. <article-title>Auditory and phonetic memory codes in the discrimination of consonants and vowels</article-title>. <source>Percept Psychophys</source>. <year>1973</year>;<volume>13</volume>: <fpage>253</fpage>–<lpage>260</lpage>. <pub-id pub-id-type="doi">10.3758/BF03214136</pub-id>
<?supplied-pmid 23226880?><pub-id pub-id-type="pmid">23226880</pub-id></mixed-citation>
              </ref>
              <ref id="pone.0199358.ref024">
                <label>24</label>
                <mixed-citation publication-type="journal"><name><surname>Cowan</surname><given-names>N</given-names></name>. <article-title>On short and long auditory stores</article-title>. <source>Psychol Bull</source>. <year>1984</year>;<volume>96</volume>: <fpage>341</fpage>–<lpage>370</lpage>. <pub-id pub-id-type="doi">10.1037/0033-2909.96.2.341</pub-id>
<?supplied-pmid 6385047?><pub-id pub-id-type="pmid">6385047</pub-id></mixed-citation>
              </ref>
              <ref id="pone.0199358.ref025">
                <label>25</label>
                <mixed-citation publication-type="other">Bicknell K, Bushong W, Jaeger TF, Tanenhaus MK. Listeners can maintain and rationally update uncertainty about prior words. Under review.</mixed-citation>
              </ref>
              <ref id="pone.0199358.ref026">
                <label>26</label>
                <mixed-citation publication-type="book"><name><surname>Brown-Schmidt</surname><given-names>S</given-names></name>, <name><surname>Toscano</surname><given-names>JC</given-names></name>. <chapter-title>Gradient acoustic information induces long-lasting referential uncertainty in short discourses</chapter-title><source>Lang Cogn Neurosci</source>. Taylor \&amp; Francis; <year>2017</year>; <fpage>1</fpage>–<lpage>18</lpage>.</mixed-citation>
              </ref>
              <ref id="pone.0199358.ref027">
                <label>27</label>
                <mixed-citation publication-type="other">Bushong W, Jaeger TF. Maintenance of perceptual information in speech perception. Proceedings of the Thirty-Ninth Annual Conference of the Cognitive Science Society. 2017.</mixed-citation>
              </ref>
              <ref id="pone.0199358.ref028">
                <label>28</label>
                <mixed-citation publication-type="journal"><name><surname>Connine</surname><given-names>CM</given-names></name>, <name><surname>Blasko</surname><given-names>DG</given-names></name>, <name><surname>Hall</surname><given-names>M</given-names></name>. <article-title>Effects of subsequent sentence context in auditory word recognition: Temporal and linguistic constrainst</article-title>. <source>J Mem Lang</source>. <year>1991</year>;<volume>30</volume>: <fpage>234</fpage>–<lpage>250</lpage>. <pub-id pub-id-type="doi">10.1016/0749-596X(91)90005-5</pub-id></mixed-citation>
              </ref>
              <ref id="pone.0199358.ref029">
                <label>29</label>
                <mixed-citation publication-type="journal"><name><surname>Szostak</surname><given-names>CM</given-names></name>, <name><surname>Pitt</surname><given-names>MA</given-names></name>. <article-title>The prolonged influence of subsequent context on spoken word recognition</article-title>. <year>2013</year>; <fpage>1533</fpage>–<lpage>1546</lpage>. <pub-id pub-id-type="doi">10.3758/s13414-013-0492-3</pub-id>
<?supplied-pmid 23801323?><pub-id pub-id-type="pmid">23801323</pub-id></mixed-citation>
              </ref>
              <ref id="pone.0199358.ref030">
                <label>30</label>
                <mixed-citation publication-type="journal"><name><surname>Andruski</surname><given-names>JE</given-names></name>, <name><surname>Blumstein</surname><given-names>SE</given-names></name>, <name><surname>Burton</surname><given-names>M</given-names></name>. <article-title>The effect of subphonetic differences on lexical access</article-title>. <source>Cognition</source>. <year>1994</year>;<volume>52</volume>: <fpage>163</fpage>–<lpage>187</lpage>. <pub-id pub-id-type="doi">10.1016/0010-0277(94)90042-6</pub-id>
<?supplied-pmid 7956004?><pub-id pub-id-type="pmid">7956004</pub-id></mixed-citation>
              </ref>
              <ref id="pone.0199358.ref031">
                <label>31</label>
                <mixed-citation publication-type="journal"><name><surname>McMurray</surname><given-names>B</given-names></name>, <name><surname>Tanenhaus</surname><given-names>MK</given-names></name>, <name><surname>Aslin</surname><given-names>RN</given-names></name>. <article-title>Within-category VOT affects recovery from “lexical” garden paths: Evidence against phoneme-level inhibition</article-title>. <source>J Mem Lang</source>. <year>2009</year>;<volume>60</volume>: <fpage>65</fpage>–<lpage>91</lpage>. <pub-id pub-id-type="doi">10.1016/j.jml.2008.07.002</pub-id>
<?supplied-pmid 20046217?><pub-id pub-id-type="pmid">20046217</pub-id></mixed-citation>
              </ref>
              <ref id="pone.0199358.ref032">
                <label>32</label>
                <mixed-citation publication-type="journal"><name><surname>Dahan</surname><given-names>D</given-names></name>. <article-title>The Time Course of Interpretation in Speech Comprehension</article-title>. <source>Curr Dir Psychol Sci</source>. <year>2010</year>;<volume>19</volume>: <fpage>121</fpage>–<lpage>126</lpage>. <pub-id pub-id-type="doi">10.1177/0963721410364726</pub-id></mixed-citation>
              </ref>
              <ref id="pone.0199358.ref033">
                <label>33</label>
                <mixed-citation publication-type="journal"><name><surname>Allopenna</surname><given-names>PD</given-names></name>, <name><surname>Magnuson</surname><given-names>JS</given-names></name>, <name><surname>Tanenhaus</surname><given-names>MK</given-names></name>. <article-title>Tracking the Time Course of Spoken Word Recognition Using Eye Movements: Evidence for Continuous Mapping Models</article-title>. <source>J Mem Lang</source>. <year>1998</year>;<volume>38</volume>: <fpage>419</fpage>–<lpage>439</lpage>. <pub-id pub-id-type="doi">10.1006/jmla.1997.2558</pub-id></mixed-citation>
              </ref>
              <ref id="pone.0199358.ref034">
                <label>34</label>
                <mixed-citation publication-type="journal"><name><surname>Lisker</surname><given-names>L</given-names></name>, <name><surname>Abramson</surname><given-names>AS</given-names></name>. <article-title>Some effects of context on voice onset time in English stops</article-title>. <source>Lang Speech</source>. Sage Publications; <year>1967</year>;<volume>10</volume>: <fpage>1</fpage>–<lpage>28</lpage>. <pub-id pub-id-type="doi">10.1177/002383096701000101</pub-id>
<?supplied-pmid 6044530?><pub-id pub-id-type="pmid">6044530</pub-id></mixed-citation>
              </ref>
              <ref id="pone.0199358.ref035">
                <label>35</label>
                <mixed-citation publication-type="other">Gwilliams L, Linzen T, Poeppel D, Marantz A. In spoken word recognition the future predicts the past. Submitted.</mixed-citation>
              </ref>
              <ref id="pone.0199358.ref036">
                <label>36</label>
                <mixed-citation publication-type="journal"><name><surname>Lane</surname><given-names>H</given-names></name>. <article-title>Foreign accent and speech distortion</article-title>. <source>J Acoust Soc Am</source>. Acoustical Society of America; <year>1963</year>;<volume>35</volume>: <fpage>451</fpage>–<lpage>453</lpage>.</mixed-citation>
              </ref>
              <ref id="pone.0199358.ref037">
                <label>37</label>
                <mixed-citation publication-type="journal"><name><surname>Bradlow</surname><given-names>AR</given-names></name>, <name><surname>Bent</surname><given-names>T</given-names></name>. <article-title>Perceptual Adaptation to Non-Native Speech</article-title>. <source>Cognition</source>. <year>2008</year>;<volume>106</volume>: <fpage>707</fpage>–<lpage>729</lpage>. <pub-id pub-id-type="doi">10.1016/j.cognition.2007.04.005</pub-id>
<?supplied-pmid 17532315?><pub-id pub-id-type="pmid">17532315</pub-id></mixed-citation>
              </ref>
              <ref id="pone.0199358.ref038">
                <label>38</label>
                <mixed-citation publication-type="journal"><name><surname>Clarke</surname><given-names>CM</given-names></name>, <name><surname>Garrett</surname><given-names>MF</given-names></name>. <article-title>Rapid adaptation to foreign-accented English</article-title>. <source>J Acoust Soc Am</source>. <year>2004</year>;<volume>116</volume>: <fpage>3647</fpage>–<lpage>3658</lpage>. <pub-id pub-id-type="doi">10.1121/1.1815131</pub-id>
<?supplied-pmid 15658715?><pub-id pub-id-type="pmid">15658715</pub-id></mixed-citation>
              </ref>
              <ref id="pone.0199358.ref039">
                <label>39</label>
                <mixed-citation publication-type="journal"><name><surname>Kennedy</surname><given-names>S</given-names></name>, <name><surname>Trofimovich</surname><given-names>P</given-names></name>. <article-title>Intelligibility, Comprehensibility, and Accentedness of L2 Speech: The Role of Listener Experience and Semantic Context</article-title>. <source>Can Mod Lang Rev La Rev Can des langues vivantes</source>. <year>2008</year>;<volume>64</volume>: <fpage>459</fpage>–<lpage>489</lpage>. <pub-id pub-id-type="doi">10.3138/cmlr.64.3.459</pub-id></mixed-citation>
              </ref>
              <ref id="pone.0199358.ref040">
                <label>40</label>
                <mixed-citation publication-type="journal"><name><surname>Mitterer</surname><given-names>H</given-names></name>, <name><surname>McQueen</surname><given-names>JM</given-names></name>. <article-title>Foreign subtitles help but native-language subtitles harm foreign speech perception</article-title>. <source>PLoS One</source>. <year>2009</year>;<volume>4</volume>: <fpage>e7785</fpage><pub-id pub-id-type="doi">10.1371/journal.pone.0007785</pub-id><?supplied-pmid 19918371?><pub-id pub-id-type="pmid">19918371</pub-id></mixed-citation>
              </ref>
              <ref id="pone.0199358.ref041">
                <label>41</label>
                <mixed-citation publication-type="journal"><name><surname>Davis</surname><given-names>MH</given-names></name>, <name><surname>Johnsrude</surname><given-names>IS</given-names></name>, <name><surname>Hervais-Adelman</surname><given-names>A</given-names></name>, <name><surname>Taylor</surname><given-names>K</given-names></name>, <name><surname>McGettigan</surname><given-names>C</given-names></name>. <article-title>Lexical information drives perceptual learning of distorted speech: evidence from the comprehension of noise-vocoded sentences</article-title>. <source>J Exp Psychol Gen</source>. <year>2005</year>;<volume>134</volume>: <fpage>222</fpage>–<lpage>241</lpage>. <pub-id pub-id-type="doi">10.1037/0096-3445.134.2.222</pub-id>
<?supplied-pmid 15869347?><pub-id pub-id-type="pmid">15869347</pub-id></mixed-citation>
              </ref>
              <ref id="pone.0199358.ref042">
                <label>42</label>
                <mixed-citation publication-type="journal"><name><surname>Sohoglu</surname><given-names>E</given-names></name>, <name><surname>Peelle</surname><given-names>JE</given-names></name>, <name><surname>Carlyon</surname><given-names>RP</given-names></name>, <name><surname>Davis</surname><given-names>MH</given-names></name>. <article-title>Top-down influences of written text on perceived clarity of degraded speech</article-title>. <source>J Exp Psychol Hum Percept Perform</source>. American Psychological Association; <year>2014</year>;<volume>40</volume>: <fpage>186</fpage><pub-id pub-id-type="doi">10.1037/a0033206</pub-id><?supplied-pmid 23750966?><pub-id pub-id-type="pmid">23750966</pub-id></mixed-citation>
              </ref>
              <ref id="pone.0199358.ref043">
                <label>43</label>
                <mixed-citation publication-type="journal"><name><surname>Keetels</surname><given-names>M</given-names></name>, <name><surname>Schakel</surname><given-names>L</given-names></name>, <name><surname>Bonte</surname><given-names>M</given-names></name>, <name><surname>Vroomen</surname><given-names>J</given-names></name>. <article-title>Phonetic recalibration of speech by text</article-title>. <source>Atten Percept Psychophys</source>. <year>2016</year>;<volume>78</volume>: <fpage>938</fpage>–<lpage>45</lpage>. <pub-id pub-id-type="doi">10.3758/s13414-015-1034-y</pub-id>
<?supplied-pmid 26704562?><pub-id pub-id-type="pmid">26704562</pub-id></mixed-citation>
              </ref>
              <ref id="pone.0199358.ref044">
                <label>44</label>
                <mixed-citation publication-type="journal"><name><surname>Bent</surname><given-names>T</given-names></name>. <article-title>Native and non-native speech database for children</article-title>. <source>J Acoust Soc Am</source>. Acoustical Society of America; <year>2010</year>;<volume>127</volume>: <fpage>1905</fpage>.</mixed-citation>
              </ref>
              <ref id="pone.0199358.ref045">
                <label>45</label>
                <mixed-citation publication-type="journal"><name><surname>Munson</surname><given-names>B</given-names></name>. <article-title>Assessing the utility of judgments of children’s speech production made by untrained listeners in uncontrolled listening environments</article-title>. <source>INTERSPEECH</source>. <year>2013</year> pp. <fpage>2147</fpage>–<lpage>2151</lpage>.</mixed-citation>
              </ref>
              <ref id="pone.0199358.ref046">
                <label>46</label>
                <mixed-citation publication-type="other">Kleinschmidt DF, Raizada R, Jaeger TF. Supervised and unsupervised learning in phonetic adaptation. Proceedings of the 37th annual conference of the cognitive science society. 2015. pp. 1129–1134.</mixed-citation>
              </ref>
              <ref id="pone.0199358.ref047">
                <label>47</label>
                <mixed-citation publication-type="other">Kleinschmidt DF, Jaeger TF. A continuum of phonetic adaptation: Evaluating an incremental belief-updating model of recalibration and selective adaptation. Proceedings of the 34th annual conference of the cognitive science society. 2012. pp. 605–610.</mixed-citation>
              </ref>
              <ref id="pone.0199358.ref048">
                <label>48</label>
                <mixed-citation publication-type="journal"><name><surname>Liu</surname><given-names>L</given-names></name>, <name><surname>Jaeger</surname><given-names>TF</given-names></name>. <article-title>Inferring causes during speech perception</article-title>. <source>Cognition</source>. <year>2018</year>;<volume>174</volume>.</mixed-citation>
              </ref>
              <ref id="pone.0199358.ref049">
                <label>49</label>
                <mixed-citation publication-type="other">Liu L, Xie X, Weatherholtz K, Jaeger TF. Accent adaptation and generalization. Under revision.</mixed-citation>
              </ref>
              <ref id="pone.0199358.ref050">
                <label>50</label>
                <mixed-citation publication-type="journal"><name><surname>Xie</surname><given-names>X</given-names></name>, <name><surname>Weatherholtz</surname><given-names>K</given-names></name>, <name><surname>Bainton</surname><given-names>L</given-names></name>, <name><surname>Rowe</surname><given-names>E</given-names></name>, <name><surname>Burchill</surname><given-names>Z</given-names></name>, <name><surname>Liu</surname><given-names>L</given-names></name>, <etal>et al</etal><article-title>Rapid adaptation to foreign-accented speech and its transfer to an unfamiliar talker</article-title>. <source>J Acoust Soc Am</source>. <year>2018</year>;<volume>143</volume>.</mixed-citation>
              </ref>
              <ref id="pone.0199358.ref051">
                <label>51</label>
                <mixed-citation publication-type="journal"><name><surname>Breslow</surname><given-names>NE</given-names></name>, <name><surname>Clayton</surname><given-names>DG</given-names></name>. <article-title>Approximate inference in generalized linear mixed models</article-title>. <source>J Am Stat Assoc</source>. Taylor &amp; Francis Group; <year>1993</year>;<volume>88</volume>: <fpage>9</fpage>–<lpage>25</lpage>.</mixed-citation>
              </ref>
              <ref id="pone.0199358.ref052">
                <label>52</label>
                <mixed-citation publication-type="journal"><name><surname>Jaeger</surname><given-names>TF</given-names></name>. <article-title>Categorical data analysis: Away from ANOVAs (transformation or not) and towards logit mixed models</article-title>. <source>J Mem Lang</source>. Elsevier Inc.; <year>2008</year>;<volume>59</volume>: <fpage>434</fpage>–<lpage>446</lpage>. <pub-id pub-id-type="doi">10.1016/j.jml.2007.11.007</pub-id>
<?supplied-pmid 19884961?><pub-id pub-id-type="pmid">19884961</pub-id></mixed-citation>
              </ref>
              <ref id="pone.0199358.ref053">
                <label>53</label>
                <mixed-citation publication-type="other">Levy R. A noisy-channel model of rational human sentence comprehension under uncertain input. Proceedings of the conference on empirical methods in natural language processing. 2008. pp. 234–243.</mixed-citation>
              </ref>
              <ref id="pone.0199358.ref054">
                <label>54</label>
                <mixed-citation publication-type="journal"><name><surname>Kleinschmidt</surname><given-names>DF</given-names></name>, <name><surname>Jaeger</surname><given-names>TF</given-names></name>. <article-title>Robust speech perception: recognize the familiar, generalize to the similar, and adapt to the novel</article-title>. <source>Psychol Rev</source>. American Psychological Association; <year>2015</year>;<volume>122</volume>: <fpage>148</fpage><pub-id pub-id-type="doi">10.1037/a0038695</pub-id><?supplied-pmid 25844873?><pub-id pub-id-type="pmid">25844873</pub-id></mixed-citation>
              </ref>
              <ref id="pone.0199358.ref055">
                <label>55</label>
                <mixed-citation publication-type="journal"><name><surname>Flege</surname><given-names>JE</given-names></name>, <name><surname>Munro</surname><given-names>MJ</given-names></name>, <name><surname>Skelton</surname><given-names>L</given-names></name>. <article-title>Production of the word-final English/t/—/d/contrast by native speakers of English, Mandarin, and Spanish</article-title>. <source>J Acoust Soc Am</source>. ASA; <year>1992</year>;<volume>92</volume>: <fpage>128</fpage>–<lpage>143</lpage>. <?supplied-pmid 1512319?><pub-id pub-id-type="pmid">1512319</pub-id></mixed-citation>
              </ref>
              <ref id="pone.0199358.ref056">
                <label>56</label>
                <mixed-citation publication-type="book"><name><surname>Harris James</surname><given-names>W</given-names></name><chapter-title>. Spanish phonology</chapter-title><publisher-loc>Cambridge, Mass.</publisher-loc>: <publisher-name>MIT Press</publisher-name>; <year>1969</year>.</mixed-citation>
              </ref>
              <ref id="pone.0199358.ref057">
                <label>57</label>
                <mixed-citation publication-type="journal"><name><surname>Reinisch</surname><given-names>E</given-names></name>, <name><surname>Holt</surname><given-names>LL</given-names></name>. <article-title>Lexically Guided Phonetic Retuning of Foreign-Accented Speech and Its Generalization</article-title>. <source>J Exp Psychol Hum Percept Perform</source>. <year>2014</year>;<volume>29</volume>: <fpage>997</fpage>–<lpage>1003</lpage>. <pub-id pub-id-type="doi">10.1016/j.biotechadv.2011.08.021.Secreted</pub-id></mixed-citation>
              </ref>
              <ref id="pone.0199358.ref058">
                <label>58</label>
                <mixed-citation publication-type="other">Grant KW, Greenberg S. Speech intelligibility derived from asynchronous processing of auditory-visual information. AVSP 2001-International Conference on Auditory-Visual Speech Processing. 2001.</mixed-citation>
              </ref>
              <ref id="pone.0199358.ref059">
                <label>59</label>
                <mixed-citation publication-type="book"><name><surname>Swan</surname><given-names>M</given-names></name>, <name><surname>Smith</surname><given-names>B</given-names></name>. <chapter-title>Learner English: A teacher’s guide to interference and other problems</chapter-title>(<source>Cambridge handbooks for language teachers; Cambridge handbooks for language teachers</source>). <publisher-loc>Cambridge, UK</publisher-loc>: <publisher-name>University Press</publisher-name>; <year>2001</year>.</mixed-citation>
              </ref>
              <ref id="pone.0199358.ref060">
                <label>60</label>
                <mixed-citation publication-type="journal"><name><surname>Logan</surname><given-names>JS</given-names></name>, <name><surname>Lively</surname><given-names>SE</given-names></name>, <name><surname>Pisoni</surname><given-names>DB</given-names></name>. <article-title>Training Japanese listeners to identify English/r/and/l: A first report</article-title>. <source>J Acoust Soc Am</source>. ASA; <year>1991</year>;<volume>89</volume>: <fpage>874</fpage>–<lpage>886</lpage>. <?supplied-pmid 2016438?><pub-id pub-id-type="pmid">2016438</pub-id></mixed-citation>
              </ref>
              <ref id="pone.0199358.ref061">
                <label>61</label>
                <mixed-citation publication-type="journal"><name><surname>Francis</surname><given-names>AL</given-names></name>, <name><surname>Baldwin</surname><given-names>K</given-names></name>, <name><surname>Nusbaum</surname><given-names>HC</given-names></name>. <article-title>Effects of training on attention to acoustic cues</article-title>. <source>Attention, Perception, \&amp; Psychophys</source>. Springer; <year>2000</year>;<volume>62</volume>: <fpage>1668</fpage>–<lpage>1680</lpage>.</mixed-citation>
              </ref>
              <ref id="pone.0199358.ref062">
                <label>62</label>
                <mixed-citation publication-type="journal"><name><surname>McCandliss</surname><given-names>BD</given-names></name>, <name><surname>Fiez</surname><given-names>J a</given-names></name>, <name><surname>Protopapas</surname><given-names>A</given-names></name>, <name><surname>Conway</surname><given-names>M</given-names></name>, <name><surname>McClelland</surname><given-names>JL</given-names></name>. <article-title>Success and failure in teaching the [r]-[l] contrast to Japanese adults: tests of a Hebbian model of plasticity and stabilization in spoken language perception</article-title>. <source>Cogn Affect Behav Neurosci</source>. <year>2002</year>;<volume>2</volume>: <fpage>89</fpage>–<lpage>108</lpage>. <pub-id pub-id-type="doi">10.3758/CABN.2.2.89</pub-id>
<?supplied-pmid 12455678?><pub-id pub-id-type="pmid">12455678</pub-id></mixed-citation>
              </ref>
            </ref-list>
          </back>
        </article>
      </metadata>
    </record>
  </GetRecord>
</OAI-PMH>
